{"config":{"lang":["en","zh"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#mindcv","title":"MindCV","text":""},{"location":"#introduction","title":"Introduction","text":"<p>MindCV is an open-source toolbox for computer vision research and development based on MindSpore. It collects a series of classic and SoTA vision models, such as ResNet and SwinTransformer, along with their pre-trained weights and training strategies. SoTA methods such as auto augmentation are also provided for performance improvement. With the decoupled module design, it is easy to apply or adapt MindCV to your own CV tasks.</p>"},{"location":"#major-features","title":"Major Features","text":"<ul> <li> <p>Easy-to-Use. MindCV decomposes the vision framework into various configurable components. It is easy to customize   your data pipeline, models, and learning pipeline with MindCV:</p> <pre><code>&gt;&gt;&gt; import mindcv\n# create a dataset\n&gt;&gt;&gt; dataset = mindcv.create_dataset('cifar10', download=True)\n# create a model\n&gt;&gt;&gt; network = mindcv.create_model('resnet50', pretrained=True)\n</code></pre> </li> </ul> <p>Users can customize and launch their transfer learning or training task in one command line.</p> <pre><code>```shell\n# transfer learning in one command line\npython train.py --model=swin_tiny --pretrained --opt=adamw --lr=0.001 --data_dir=/path/to/data\n```\n</code></pre> <ul> <li> <p>State-of-The-Art. MindCV provides various CNN-based and Transformer-based vision models including SwinTransformer.   Their pretrained weights and performance reports are provided to help users select and reuse the right model:</p> </li> <li> <p>Flexibility and efficiency. MindCV is built on MindSpore which is an efficient DL framework that can be run on   different hardware platforms (GPU/CPU/Ascend). It supports both graph mode for high efficiency and pynative mode for   flexibility.</p> </li> </ul>"},{"location":"#model-zoo","title":"Model Zoo","text":"<p>The performance of the models trained with MindCV is summarized in here, where the training recipes and weights are both available.</p> <p>Model introduction and training details can be viewed in each sub-folder under configs.</p>"},{"location":"#installation","title":"Installation","text":"<p>See Installation for details.</p>"},{"location":"#getting-started","title":"Getting Started","text":""},{"location":"#hands-on-tutorial","title":"Hands-on Tutorial","text":"<p>To get started with MindCV, please see the Quick Start, which will give you a quick tour of each key component and the train/validate/predict pipelines.</p> <p>Below are a few code snippets for your taste.</p> <pre><code>&gt;&gt;&gt; import mindcv\n# List and find a pretrained vision model\n&gt;&gt;&gt; mindcv.list_models(\"swin*\", pretrained=True)\n['swin_tiny']\n# Create the model object\n&gt;&gt;&gt; network = mindcv.create_model('swin_tiny', pretrained=True)\n# Validate its accuracy\n&gt;&gt;&gt; !python validate.py --model=swin_tiny --pretrained --dataset=imagenet --val_split=validation\n{'Top_1_Accuracy': 0.80824, 'Top_5_Accuracy': 0.94802, 'loss': 1.7331367141008378}\n</code></pre> Image Classification Demo <p>Right click on the image below and save as <code>dog.jpg</code>.</p> <p><p> </p></p> <p>Classify the downloaded image with a pretrained SoTA model:</p> <pre><code>&gt;&gt;&gt; !python infer.py --model=swin_tiny --image_path='./dog.jpg'\n{'Labrador retriever': 0.5700152, 'golden retriever': 0.034551315, 'kelpie': 0.010108651, 'Chesapeake Bay retriever': 0.008229004, 'Walker hound, Walker foxhound': 0.007791956}\n</code></pre> <p>The top-1 prediction result is labrador retriever, which is the breed of this cut dog.</p>"},{"location":"#training","title":"Training","text":"<p>It is easy to train your model on a standard or customized dataset using <code>train.py</code>, where the training strategy (e.g., augmentation, LR scheduling) can be configured with external arguments or a yaml config file.</p> <ul> <li> <p>Standalone Training</p> <pre><code># standalone training\npython train.py --model=resnet50 --dataset=cifar10 --dataset_download\n</code></pre> </li> </ul> <p>Above is an example of training ResNet50 on CIFAR10 dataset on a CPU/GPU/Ascend device</p> <ul> <li>Distributed Training</li> </ul> <p>For large datasets like ImageNet, it is necessary to do training in distributed mode on multiple devices. This can be   achieved with <code>mpirun</code> and parallel features supported by MindSpore.</p> <pre><code>```shell\n# distributed training\n# assume you have 4 GPUs/NPUs\nmpirun -n 4 python train.py --distribute \\\n    --model=densenet121 --dataset=imagenet --data_dir=/path/to/imagenet\n```\n</code></pre> <p>Notes: If the script is executed by the root user, the <code>--allow-run-as-root</code> parameter must be added to <code>mpirun</code>.</p> <p>Detailed parameter definitions can be seen in <code>config.py</code> and checked by running 'python train.py --help'.</p> <p>To resume training, please set the <code>--ckpt_path</code> and <code>--ckpt_save_dir</code> arguments. The optimizer state including the   learning rate of the last stopped epoch will also be recovered.</p> <ul> <li>Config and Training Strategy</li> </ul> <p>You can configure your model and other components either by specifying external parameters or by writing a yaml config   file. Here is an example of training using a preset yaml file.</p> <pre><code>```shell\nmpirun --allow-run-as-root -n 4 python train.py -c configs/squeezenet/squeezenet_1.0_gpu.yaml\n```\n</code></pre> <p>!!! tip \"Pre-defined Training Strategies\"   We provide more than 20 training recipes that achieve SoTA results on ImageNet currently.   Please look into the <code>configs</code> folder for details.   Please feel free to adapt these training strategies to your own model for performance improvement, which can be easily   done by modifying the yaml file.</p> <ul> <li>Train on ModelArts/OpenI Platform</li> </ul> <p>To run training on the ModelArts   or OpenI cloud platform:</p> <pre><code>```text\n1. Create a new training task on the cloud platform.\n2. Add the parameter 'config' and specify the path to the yaml config file on the website UI interface.\n3. Add the parameter 'enable_modelarts' and set True on the website UI interface.\n4. Fill in other blanks on the website and launch the training task.\n```\n</code></pre> <p>Graph Mode and PyNative Mode</p> <p>By default, the training pipeline <code>train.py</code> is run in graph mode on MindSpore, which is optimized for efficiency and parallel computing with a compiled static graph. In contrast, pynative mode is optimized for flexibility and easy debugging. You may alter the parameter <code>--mode</code> to switch to pure pynative mode for debugging purpose.</p> <p>Mixed Mode</p> <p>PyNative mode with mindspore.jit is a mixed mode for comprising flexibility and efficiency in MindSpore. To apply pynative mode with mindspore.jit for training, please run <code>train_with_func.py</code>, e.g.,</p> <pre><code>python train_with_func.py --model=resnet50 --dataset=cifar10 --dataset_download  --epoch_size=10\n</code></pre> <p>Note: this is an experimental function under improvement. It is not stable on MindSpore 1.8.1 or earlier versions.</p>"},{"location":"#validation","title":"Validation","text":"<p>To evaluate the model performance, please run <code>validate.py</code></p> <pre><code># validate a trained checkpoint\npython validate.py --model=resnet50 --dataset=imagenet --data_dir=/path/to/data --ckpt_path=/path/to/model.ckpt\n</code></pre> <p>Validation while Training</p> <p>You can also track the validation accuracy during training by enabling the <code>--val_while_train</code> option.</p> <pre><code>python train.py --model=resnet50 --dataset=cifar10 \\\n    --val_while_train --val_split=test --val_interval=1\n</code></pre> <p>The training loss and validation accuracy for each epoch will be saved in <code>${ckpt_save_dir}/results.log</code>.</p> <p>More examples about training and validation can be seen in examples.</p>"},{"location":"#tutorials","title":"Tutorials","text":"<p>We provide the following jupyter notebook tutorials to help users learn to use MindCV.</p> <ul> <li>Learn about configs</li> <li>Inference with a pretrained model</li> <li>Finetune a pretrained model on custom datasets</li> <li>Customize your model</li> <li>Optimizing performance for vision transformer //coming soon</li> <li>Deployment demo</li> </ul>"},{"location":"#supported-algorithms","title":"Supported Algorithms","text":"Supported algorithms  <ul> <li>Augmentation<ul> <li>AutoAugment</li> <li>RandAugment</li> <li>Repeated Augmentation</li> <li>RandErasing (Cutout)</li> <li>CutMix</li> <li>MixUp</li> <li>RandomResizeCrop</li> <li>Color Jitter, Flip, etc</li> </ul> </li> <li>Optimizer<ul> <li>Adam</li> <li>AdamW</li> <li>Lion</li> <li>Adan (experimental)</li> <li>AdaGrad</li> <li>LAMB</li> <li>Momentum</li> <li>RMSProp</li> <li>SGD</li> <li>NAdam</li> </ul> </li> <li>LR Scheduler<ul> <li>Warmup Cosine Decay</li> <li>Step LR</li> <li>Polynomial Decay</li> <li>Exponential Decay</li> </ul> </li> <li>Regularization<ul> <li>Weight Decay</li> <li>Label Smoothing</li> <li>Stochastic Depth (depends on networks)</li> <li>Dropout (depends on networks)</li> </ul> </li> <li>Loss<ul> <li>Cross Entropy (w/ class weight and auxiliary logit support)</li> <li>Binary Cross Entropy  (w/ class weight and auxiliary logit support)</li> <li>Soft Cross Entropy Loss (automatically enabled if mixup or label smoothing is used)</li> <li>Soft Binary Cross Entropy Loss (automatically enabled if mixup or label smoothing is used)</li> </ul> </li> <li>Ensemble<ul> <li>Warmup EMA (Exponential Moving Average)</li> </ul> </li> </ul>"},{"location":"#how-to-contribute","title":"How to Contribute","text":"<p>We appreciate all kinds of contributions including issues and PRs to make MindCV better.</p> <p>Please refer to CONTRIBUTING for the contributing guideline. Please follow the Model Template and Guideline for contributing a model that fits the overall interface :)</p>"},{"location":"#license","title":"License","text":"<p>This project follows the Apache License 2.0 open-source license.</p>"},{"location":"#acknowledgement","title":"Acknowledgement","text":"<p>MindCV is an open-source project jointly developed by the MindSpore team, Xidian University, and Xi'an Jiaotong University. Sincere thanks to all participating researchers and developers for their hard work on this project. We also acknowledge the computing resources provided by OpenI.</p>"},{"location":"#citation","title":"Citation","text":"<p>If you find this project useful in your research, please consider citing:</p> <pre><code>@misc{MindSpore Computer Vision 2022,\n    title={{MindSpore Computer  Vision}:MindSpore Computer Vision Toolbox and Benchmark},\n    author={MindSpore Vision Contributors},\n    howpublished = {\\url{https://github.com/mindspore-lab/mindcv/}},\n    year={2022}\n}\n</code></pre>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#dependency","title":"Dependency","text":"<ul> <li>mindspore &gt;= 1.8.1</li> <li>numpy &gt;= 1.17.0</li> <li>pyyaml &gt;= 5.3</li> <li>tqdm</li> <li>openmpi 4.0.3 (for distributed mode)</li> </ul> <p>To install the python library dependency, just run:</p> <pre><code>pip install -r requirements.txt\n</code></pre> <p>Tip</p> <p>MindSpore can be easily installed by following the official instructions where you can select your hardware platform for the best fit. To run in distributed mode, OpenMPI is required to install.</p> <p>The following instructions assume the desired dependency is fulfilled.</p>"},{"location":"installation/#install-with-pypi","title":"Install with PyPI","text":"<p>MindCV is published as a Python package and can be installed with <code>pip</code>, ideally by using a virtual environment. Open up a terminal and install MindCV with:</p> stablenightly <pre><code>pip install mindcv\n</code></pre> <pre><code># working on it using test.pypi\n</code></pre> <p>This will automatically install compatible versions of dependencies: NumPy, PyYAML and tqdm.</p> <p>Tip</p> <p>If you don't have prior experience with Python, we recommend reading Using Python's pip to Manage Your Projects' Dependencies, which is a really good introduction to the mechanics of Python package management and helps you troubleshoot if you run into errors.</p> <p>Warning</p> <p>The above command will NOT install MindSpore. We highly recommend you install MindSpore following the official instructions.</p>"},{"location":"installation/#install-from-source-bleeding-edge-version","title":"Install from Source (Bleeding Edge Version)","text":""},{"location":"installation/#from-vcs","title":"from VCS","text":"<pre><code>pip install git+https://github.com/mindspore-lab/mindcv.git\n</code></pre>"},{"location":"installation/#from-local-src","title":"from local src","text":"<p>Tip</p> <p>As this project is in active development, if you are a developer or contributor, please prefer this installation!</p> <p>MindCV can be directly used from GitHub by cloning the repository into a local folder which might be useful if you want to use the very latest version:</p> <pre><code>git clone https://github.com/mindspore-lab/mindcv.git\n</code></pre> <p>After cloning from <code>git</code>, it is recommended that you install using \"editable\" mode, which can help resolve potential module import issues:</p> <pre><code>cd mindcv\npip install -e .\n</code></pre>"},{"location":"modelzoo/","title":"Model Zoo","text":"Model Context Top-1 (%) Top-5 (%) Params(M) Recipe Download bit_resnet50 D910x8-G 76.81 93.17 25.55 yaml weights bit_resnet50x3 D910x8-G 80.63 95.12 217.31 yaml weights bit_resnet101 D910x8-G 77.93 93.75 44.54 yaml weights cmt_small D910x8-G 83.24 96.41 26.09 yaml weights coat_lite_tiny D910x8-G 77.35 93.43 5.72 yaml weights coat_lite_mini D910x8-G 78.51 93.84 11.01 yaml weights coat_tiny D910x8-G 79.67 94.88 5.50 yaml weights coat_mini D910x8-G 81.08 95.34 10.34 yaml weights convit_tiny D910x8-G 73.66 91.72 5.71 yaml weights convit_tiny_plus D910x8-G 77.00 93.60 9.97 yaml weights convit_small D910x8-G 81.63 95.59 27.78 yaml weights convit_small_plus D910x8-G 81.80 95.42 48.98 yaml weights convit_base D910x8-G 82.10 95.52 86.54 yaml weights convit_base_plus D910x8-G 81.96 95.04 153.13 yaml weights convnext_tiny D910x64-G 81.91 95.79 28.59 yaml weights convnext_small D910x64-G 83.40 96.36 50.22 yaml weights convnext_base D910x64-G 83.32 96.24 88.59 yaml weights convnextv2_tiny D910x8-G 82.43 95.98 28.64 yaml weights crossvit_9 D910x8-G 73.56 91.79 8.55 yaml weights crossvit_15 D910x8-G 81.08 95.33 27.27 yaml weights crossvit_18 D910x8-G 81.93 95.75 43.27 yaml weights densenet121 D910x8-G 75.64 92.84 8.06 yaml weights densenet161 D910x8-G 79.09 94.66 28.90 yaml weights densenet169 D910x8-G 77.26 93.71 14.31 yaml weights densenet201 D910x8-G 78.14 94.08 20.24 yaml weights dpn92 D910x8-G 79.46 94.49 37.79 yaml weights dpn98 D910x8-G 79.94 94.57 61.74 yaml weights dpn107 D910x8-G 80.05 94.74 87.13 yaml weights dpn131 D910x8-G 80.07 94.72 79.48 yaml weights edgenext_xx_small D910x8-G 71.02 89.99 1.33 yaml weights edgenext_x_small D910x8-G 75.14 92.50 2.34 yaml weights edgenext_small D910x8-G 79.15 94.39 5.59 yaml weights edgenext_base D910x8-G 82.24 95.94 18.51 yaml weights efficientnet_b0 D910x64-G 76.89 93.16 5.33 yaml weights efficientnet_b1 D910x64-G 78.95 94.34 7.86 yaml weights ghostnet_050 D910x8-G 66.03 86.64 2.60 yaml weights ghostnet_100 D910x8-G 73.78 91.66 5.20 yaml weights ghostnet_130 D910x8-G 75.50 92.56 7.39 yaml weights googlenet D910x8-G 72.68 90.89 6.99 yaml weights halonet_50t D910X8-G 79.53 94.79 22.79 yaml weights hrnet_w32 D910x8-G 80.64 95.44 41.30 yaml weights hrnet_w48 D910x8-G 81.19 95.69 77.57 yaml weights inception_v3 D910x8-G 79.11 94.40 27.20 yaml weights inception_v4 D910x8-G 80.88 95.34 42.74 yaml weights mixnet_s D910x8-G 75.52 92.52 4.17 yaml weights mixnet_m D910x8-G 76.64 93.05 5.06 yaml weights mixnet_l D910x8-G 78.73 94.31 7.38 yaml weights mnasnet_050 D910x8-G 68.07 88.09 2.14 yaml weights mnasnet_075 D910x8-G 71.81 90.53 3.20 yaml weights mnasnet_100 D910x8-G 74.28 91.70 4.42 yaml weights mnasnet_130 D910x8-G 75.65 92.64 6.33 yaml weights mnasnet_140 D910x8-G 76.01 92.83 7.16 yaml weights mobilenet_v1_025 D910x8-G 53.87 77.66 0.47 yaml weights mobilenet_v1_050 D910x8-G 65.94 86.51 1.34 yaml weights mobilenet_v1_075 D910x8-G 70.44 89.49 2.60 yaml weights mobilenet_v1_100 D910x8-G 72.95 91.01 4.25 yaml weights mobilenet_v2_075 D910x8-G 69.98 89.32 2.66 yaml weights mobilenet_v2_100 D910x8-G 72.27 90.72 3.54 yaml weights mobilenet_v2_140 D910x8-G 75.56 92.56 6.15 yaml weights mobilenet_v3_small_100 D910x8-G 68.10 87.86 2.55 yaml weights mobilenet_v3_large_100 D910x8-G 75.23 92.31 5.51 yaml weights mobilevit_xx_small D910x8-G 68.91 88.91 1.27 yaml weights mobilevit_x_small D910x8-G 74.99 92.32 2.32 yaml weights mobilevit_small D910x8-G 78.47 94.18 5.59 yaml weights nasnet_a_4x1056 D910x8-G 73.65 91.25 5.33 yaml weights pit_ti D910x8-G 72.96 91.33 4.85 yaml weights pit_xs D910x8-G 78.41 94.06 10.61 yaml weights pit_s D910x8-G 80.56 94.80 23.46 yaml weights pit_b D910x8-G 81.87 95.04 73.76 yaml weights poolformer_s12 D910x8-G 77.33 93.34 11.92 yaml weights pvt_tiny D910x8-G 74.81 92.18 13.23 yaml weights pvt_small D910x8-G 79.66 94.71 24.49 yaml weights pvt_medium D910x8-G 81.82 95.81 44.21 yaml weights pvt_large D910x8-G 81.75 95.70 61.36 yaml weights pvt_v2_b0 D910x8-G 71.50 90.60 3.67 yaml weights pvt_v2_b1 D910x8-G 78.91 94.49 14.01 yaml weights pvt_v2_b2 D910x8-G 81.99 95.74 25.35 yaml weights pvt_v2_b3 D910x8-G 82.84 96.24 45.24 yaml weights pvt_v2_b4 D910x8-G 83.14 96.27 62.56 yaml weights regnet_x_200mf D910x8-G 68.74 88.38 2.68 yaml weights regnet_x_400mf D910x8-G 73.16 91.35 5.16 yaml weights regnet_x_600mf D910x8-G 74.34 92.00 6.20 yaml weights regnet_x_800mf D910x8-G 76.04 92.97 7.26 yaml weights regnet_y_200mf D910x8-G 70.30 89.61 3.16 yaml weights regnet_y_400mf D910x8-G 73.91 91.84 4.34 yaml weights regnet_y_600mf D910x8-G 75.69 92.50 6.06 yaml weights regnet_y_800mf D910x8-G 76.52 93.10 6.26 yaml weights regnet_y_16gf D910x8-G 82.92 96.29 83.71 yaml weights repmlp_t224 D910x8-G 76.71 93.30 38.30 yaml weights repvgg_a0 D910x8-G 72.19 90.75 9.13 yaml weights repvgg_a1 D910x8-G 74.19 91.89 14.12 yaml weights repvgg_a2 D910x8-G 76.63 93.42 28.25 yaml weights repvgg_b0 D910x8-G 74.99 92.40 15.85 yaml weights repvgg_b1 D910x8-G 78.81 94.37 57.48 yaml weights repvgg_b2 D910x64-G 79.29 94.66 89.11 yaml weights repvgg_b3 D910x64-G 80.46 95.34 123.19 yaml weights repvgg_b1g2 D910x8-G 78.03 94.09 45.85 yaml weights repvgg_b1g4 D910x8-G 77.64 94.03 40.03 yaml weights repvgg_b2g4 D910x8-G 78.8 94.36 61.84 yaml weights res2net50 D910x8-G 79.35 94.64 25.76 yaml weights res2net101 D910x8-G 79.56 94.70 45.33 yaml weights res2net50_v1b D910x8-G 80.32 95.09 25.77 yaml weights res2net101_v1b D910x8-G 81.14 95.41 45.35 yaml weights resnest50 D910x8-G 80.81 95.16 27.55 yaml weights resnest101 D910x8-G 82.90 96.12 48.41 yaml weights resnet18 D910x8-G 70.21 89.62 11.70 yaml weights resnet34 D910x8-G 74.15 91.98 21.81 yaml weights resnet50 D910x8-G 76.69 93.50 25.61 yaml weights resnet101 D910x8-G 78.24 94.09 44.65 yaml weights resnet152 D910x8-G 78.72 94.45 60.34 yaml weights resnetv2_50 D910x8-G 76.90 93.37 25.60 yaml weights resnetv2_101 D910x8-G 78.48 94.23 44.55 yaml weights resnext50_32x4d D910x8-G 78.53 94.10 25.10 yaml weights resnext101_32x4d D910x8-G 79.83 94.80 44.32 yaml weights resnext101_64x4d D910x8-G 80.30 94.82 83.66 yaml weights resnext152_64x4d D910x8-G 80.52 95.00 115.27 yaml weights rexnet_09 D910x8-G 77.06 93.41 4.13 yaml weights rexnet_10 D910x8-G 77.38 93.60 4.84 yaml weights rexnet_13 D910x8-G 79.06 94.28 7.61 yaml weights rexnet_15 D910x8-G 79.95 94.74 9.79 yaml weights rexnet_20 D910x8-G 80.64 94.99 16.45 yaml weights seresnet18 D910x8-G 71.81 90.49 11.80 yaml weights seresnet34 D910x8-G 75.38 92.50 21.98 yaml weights seresnet50 D910x8-G 78.32 94.07 28.14 yaml weights seresnext26_32x4d D910x8-G 77.17 93.42 16.83 yaml weights seresnext50_32x4d D910x8-G 78.71 94.36 27.63 yaml weights shufflenet_v1_g3_05 D910x8-G 57.05 79.73 0.73 yaml weights shufflenet_v1_g3_10 D910x8-G 67.77 87.73 1.89 yaml weights shufflenet_v2_x0_5 D910x8-G 60.53 82.11 1.37 yaml weights shufflenet_v2_x1_0 D910x8-G 69.47 88.88 2.29 yaml weights shufflenet_v2_x1_5 D910x8-G 72.79 90.93 3.53 yaml weights shufflenet_v2_x2_0 D910x8-G 75.07 92.08 7.44 yaml weights skresnet18 D910x8-G 73.09 91.20 11.97 yaml weights skresnet34 D910x8-G 76.71 93.10 22.31 yaml weights skresnext50_32x4d D910x8-G 79.08 94.60 37.31 yaml weights squeezenet1_0 D910x8-G 59.01 81.01 1.25 yaml weights squeezenet1_0 GPUx8-G 58.83 81.08 1.25 yaml weights squeezenet1_1 D910x8-G 58.44 80.84 1.24 yaml weights squeezenet1_1 GPUx8-G 59.18 81.41 1.24 yaml weights swin_tiny D910x8-G 80.82 94.80 33.38 yaml weights swinv2_tiny_window8 D910x8-G 81.42 95.43 28.78 yaml weights vgg11 D910x8-G 71.86 90.50 132.86 yaml weights vgg13 D910x8-G 72.87 91.02 133.04 yaml weights vgg16 D910x8-G 74.61 91.87 138.35 yaml weights vgg19 D910x8-G 75.21 92.56 143.66 yaml weights visformer_tiny D910x8-G 78.28 94.15 10.33 yaml weights visformer_tiny_v2 D910x8-G 78.82 94.41 9.38 yaml weights visformer_small D910x8-G 81.76 95.88 40.25 yaml weights visformer_small_v2 D910x8-G 82.17 95.90 23.52 yaml weights vit_b_32_224 D910x8-G 75.86 92.08 87.46 yaml weights vit_l_16_224 D910x8-G 76.34 92.79 303.31 yaml weights vit_l_32_224 D910x8-G 73.71 90.92 305.52 yaml weights volo_d1 D910x8-G 82.59 95.99 27 yaml weights xception D910x8-G 79.01 94.25 22.91 yaml weights xcit_tiny_12_p16_224 D910x8-G 77.67 93.79 7.00 yaml weights"},{"location":"modelzoo/#notes","title":"Notes","text":"<ul> <li>Context: Training context denoted as {device}x{pieces}-{MS mode}, where mindspore mode can be G - graph mode or F - pynative mode with ms function. For example, D910x8-G is for training on 8 pieces of Ascend 910 NPU using graph mode.</li> <li>Top-1 and Top-5: Accuracy reported on the validation set of ImageNet-1K.</li> </ul>"},{"location":"how_to_guides/feature_extraction/","title":"Multi-Scale Feature Extraction","text":"<p>In this guide, you will learn how to apply multi-scale feature extraction to the models in MindCV. In real deep learning model projects, we often exploit classic CV backbones, such as ResNet, VGG, for the purposes of better performance and fast development. Generally, using only the final output of backbones is not enough. We need outputs from intermediate layers, which act as multi-scale abstractions of the input, to help further boost the performance of our downstream tasks. To this end, we have designed a mechanism for extracting multi-scale features from backbones in MindCV. At the time of composing this guide, MindCV has supported extracting features with this mechanism from ResNet, MobileNetV3, ConvNeXt, ResNeST, EfficientNet, RepVGG, HRNet, and ReXNet. For more details of the feature extraction mechanism, please refer to <code>FeatureExtractWrapper</code>.</p> <p>This guide will help you learn how to add pieces of code to extracting multi-scale features from the rest of backbones. There are mainly two steps to achieve this:</p> <ol> <li>In <code>__init__()</code> of a model, register the intermediate layers whose outputted feature needs to be extracted in <code>self.feature_info</code>.</li> <li>Add a wrapper function for model creation.</li> <li>Pass <code>feature_only=True</code> and <code>out_indices</code> to <code>create_model()</code>.</li> </ol>"},{"location":"how_to_guides/feature_extraction/#layer-registration","title":"Layer Registration","text":"<p>There are mainly three possible scenarios when implementing code for feature extraction in MindCV, i.e., * a model with separate sequential module for each layer, * a model with one sequential module for all layers, and * a model with nonsequential modules.</p>"},{"location":"how_to_guides/feature_extraction/#scenario-1-separate-sequential-module-for-each-layer","title":"Scenario 1: Separate Sequential Module for Each Layer","text":"<p>An example of scenario 1 is shown as follows.</p> <pre><code>class DummyNet1(nn.Cell):\n    def __init__(self, **kwargs):\n        super().__init__()\n\n        # separate sequential module for each layer\n        self.layer1 = Layer()\n        self.layer2 = Layer()\n        self.layer3 = Layer()\n        self.layer4 = Layer()\n\n    def forward_features(self, x):\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        return x\n\n    def construct(self, x):\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n</code></pre> <p>To implement feature extraction for this scenario, we add a member variable <code>self.feature_info</code> into <code>__init__()</code> to register the extractable layers, e.g.,</p> <pre><code>class DummyNet1(nn.Cell):\n    def __init__(self, **kwargs):\n        super().__init__()\n        self.feature_info = []  # for layer registration\n\n        self.layer1 = Layer()\n        self.feature_info.append(dict(chs=, reduction=, name=\u201dlayer1\u201d))  # register layer\n        self.layer2 = Layer()\n        self.feature_info.append(dict(chs=, reduction=, name=\u201dlayer2\u201d))  # register layer\n        self.layer3 = Layer()\n        self.feature_info.append(dict(chs=, reduction=, name=\u201dlayer3\u201d))  # register layer\n        self.layer4 = Layer()\n        self.feature_info.append(dict(chs=, reduction=, name=\u201dlayer4\u201d))  # register layer\n</code></pre> <p>As we can see above, <code>self.feature_info</code> is a list of dictionaries, each of which contains three key-value pairs. Specifically, <code>chs</code> denotes the channel number of the produced feature, <code>reduction</code> denotes the total stride at the current layer, and <code>name</code> indicates the name of this layer stored in the model parameters which can be found using <code>get_parameters()</code>.</p> <p>For a real example of this scenario, please refer to ResNet.</p>"},{"location":"how_to_guides/feature_extraction/#scenario-2-one-sequential-module-for-all-layers","title":"Scenario 2: One Sequential Module for All Layers","text":"<p>For some models, the layers are in one sequential module. An example of scenario 2 is shown as follows.</p> <pre><code>class DummyNet2(nn.Cell):\n    def __init__(self, **kwargs):\n        super().__init__()\n        layers = []\n\n        for i in range(4):\n            layers.append(Layer())\n\n        # the layers are in one sequential module\n        self.layers = nn.SequentialCell(layers)\n\n    def forward_features(self, x):\n        x = self.layers(x)\n        return x\n\n    def construct(self, x):\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n</code></pre> <p>To implement feature extraction for this scenario, we also need to add <code>self.feature_info</code> into <code>__init__()</code> as in scenario 1, as well as create a member variable <code>self.flatten_sequential = True</code> to indicate that the sequential module in this model needs to be flattened before extracting features, e.g.,</p> <pre><code>class DummyNet2(nn.Cell):\n    def __init__(self, **kwargs):\n        super().__init__()\n        self.feature_info = []  # for layer registration\n        self.flatten_sequential = True  # indication of flattening the sequential module\n\n        layers = []\n\n        for i in range(4):\n            layers.append(Layer())\n            self.feature_info.append(dict(chs=, reduction=, name=f\u201dlayer{i}\u201d))  # register layer\n\n        self.layers = nn.SequentialCell(layers)\n</code></pre> <p>Please be reminded that the order of the module instantiations in <code>__init__()</code> is very important. The order must be kept as same as the order that these modules are called in <code>forward_features()</code> and <code>construct()</code>. Furthermore, only the modules called in <code>forward_features()</code> and <code>construct()</code> should be instantiated as member variables with the type of <code>nn.Cell</code>. Otherwise, the feature extraction mechanism will not work.</p> <p>For a real example of this scenario, please refer to MobileNetV3.</p>"},{"location":"how_to_guides/feature_extraction/#scenario-3-nonsequential-modules","title":"Scenario 3: Nonsequential Modules","text":"<p>Layers in models sometimes are nonsequential modules. An example of scenario 3 is shown as follows.</p> <pre><code>class DummyNet3(nn.Cell):\n    def __init__(self, **kwargs):\n        super().__init__()\n        layer1 = []\n\n        for i in range(3):\n                layer1.append(Layer())\n\n        # layers in self.layer1 are not sequential\n        self.layer1 = nn.CellList(layer1)\n\n        self.stage1 = Stage()\n\n        layer2 = []\n\n        for i in range(3):\n                layer2.append(Layer())\n\n        # layers in self.layer2 are not sequential\n        self.layer2 = nn.CellList(layer2)\n\n        self.stage2 = Stage()\n\n    def forward_features(self, x):\n        x_list = []\n\n        # layers are parallel instead of sequential\n        for i in range(3):\n                x_list.append(self.layer1[i](x))\n\n        x = self.stage1(x_list)\n\n        x_list = []\n\n        # layers are parallel instead of sequential\n        for i in range(3):\n                x_list.append(self.layer2[i](x))\n\n        x = self.stage2(x_list)\n\n        return x\n\n    def construct(self, x):\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n</code></pre> <p>To implement feature extraction for this scenario, we need to first create a new model feature class by inheriting the original model class. Then, we add the <code>self.feature_info</code> and a member variable <code>self.is_rewritten = True</code> to indicate that this class is rewritten for feature extraction. Finally, we reimplement <code>forward_features()</code> and <code>construct()</code> with feature extraction logic. Here is an example.</p> <pre><code>class DummyFeatureNet3(DummyNet3):\n    def __init__(self, **kwargs):\n        super(DummyFeatureNet3, self).__init__(**kwargs)\n        self.feature_info = []  # for layer registration\n        self.is_rewritten = True  # indication of rewriting for feature extraction\n\n        self.feature_info.append(dict(chs=, reduction=, name=\u201dstage1\u201d)  # register layer\n        self.feature_info.append(dict(chs=, reduction=, name=\u201dstage2\u201d)  # register layer\n\n    def forward_features(self, x):  # reimplement feature extraction logic\n        out = []\n        x_list = []\n\n        for i in range(3):\n                x_list.append(self.layer1[i](x))\n\n        x = self.stage1(x_list)\n        out.append(x)\n\n        x_list = []\n\n        for i in range(3):\n                x_list.append(self.layer2[i](x))\n\n        x = self.stage2(x_list)\n        out.append(x)\n\n        return out\n\n    def construct(self, x):\n        x = self.forward_features(x)\n        return x\n</code></pre> <p>For a real example of this scenario, please refer to HRNet.</p>"},{"location":"how_to_guides/feature_extraction/#adding-a-wrapper-function-for-model-creation","title":"Adding A Wrapper Function for Model Creation","text":"<p>After adding layer registration, we need to add one more simple wrapper function for model creation, so that the model instance can be passed to <code>build_model_with_cfg()</code> for feature extraction.</p> <p>Usually, the original creation function of a model in MindCV looks like this,</p> <pre><code>@register_model\ndef dummynet18(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; DummyNet:\n    default_cfg = default_cfgs[\"dummynet18\"]\n    model = DummyNet(..., num_classes=num_classes, in_channels=in_channels, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <p>As for the models falling into scenarios 1 &amp; 2, in the wrapper function of model creation, simply pass the arguements to <code>build_model_with_cfg()</code>, e.g.,</p> <pre><code>def _create_dummynet(pretrained=False, **kwargs):\n    return build_model_with_cfg(DummyNet, pretrained, **kwargs)\n\n@register_model\ndef dummynet18(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; DummyNet:\n    default_cfg = default_cfgs[\"dummynet18\"]\n    model_args = dict(..., num_classes=num_classes, in_channels=in_channels, **kwargs)\n    return _create_dummynet(pretrained, **dict(default_cfg=default_cfg, **model_args))\n</code></pre> <p>As for the models falling into scenario 3, most part of the wrapper function is the same as the ones for scenarios 1 &amp; 2. The difference lies in the part of deciding which model class to be instantiated. This is conditioned on <code>feature_only</code>, e.g.,</p> <pre><code>def _create_dummynet(pretrained=False, **kwargs):\n    if not kwargs.get(\"features_only\", False):\n        return build_model_with_cfg(DummyNet3, pretrained, **kwargs)\n    else:\n        return build_model_with_cfg(DummyFeatureNet3, pretrained, **kwargs)\n\n@register_model\ndef dummynet18(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; DummyNet:\n    default_cfg = default_cfgs[\"dummynet18\"]\n    model_args = dict(..., num_classes=num_classes, in_channels=in_channels, **kwargs)\n    return _create_dummynet(pretrained, **dict(default_cfg=default_cfg, **model_args))\n</code></pre> <p>For real examples, please refer to ResNet and HRNet.</p>"},{"location":"how_to_guides/feature_extraction/#passing-arguements-to-create_model","title":"Passing Arguements to <code>create_model()</code>","text":"<p>After the previous two steps are done, we can simply create the backbone that outputs the desired features by passing <code>feature_only=True</code> and <code>out_indices</code> to <code>create_model()</code>, e.g.,</p> <pre><code>from mindcv.models import create_model\n\n\nbackbone = create_model(\n    model_name=\"dummynet18\",\n    features_only=True,  # set features_only to be True\n    out_indices=[0, 1, 2],  # specify the feature_info indices of the desired layers\n)\n</code></pre> <p>In addtion, if we want to load a checkpoint into the backbone for feature extraction and this backbone falls into scenarios 2, we need to also set <code>auto_mapping=True</code>, e.g.,</p> <pre><code>from mindcv.models import create_model\n\n\nbackbone = create_model(\n    model_name=\"dummynet18\",\n    checkpoint_path=\"/path/to/dummynet18.ckpt\",\n    auto_mapping=True,  # set auto_mapping to be True when loading a checkpoint for scenarios 2 models\n    features_only=True,  # set features_only to be True\n    out_indices=[0, 1, 2],  # specify the feature_info indices of the desired layers\n)\n</code></pre> <p>Congradulations! Now you have learnt how to apply multi-scale feature extraction to the models in MindCV.</p>"},{"location":"how_to_guides/finetune_with_a_custom_dataset/","title":"Fine-tune with A Custom Dataset","text":"<p>This document introduces the process for fine-tuning a custom dataset in MindCV and the implementation of fine-tuning techniques such as reading the dataset online, setting the learning rate for specific layers, freezing part of the parameters, etc. The main code is in./example/finetune.py, you can make changes to it based on this tutorial as needed.</p> <p>Next, we will use the FGVC-Aircraft dataset as an example to show how to fine-tune the pre-trained model mobilenet v3-small. Fine-Grained Visual Classification of Aircraft is a commonly used fine-grained image Classification benchmark dataset, which contains 10,000 aircraft images from 100 different types of aircraft (a.k.a variants), that is, 100 images for each aircraft type.</p> <p>First, extract the downloaded dataset to . /data folder, the directory structure of the Aircraft dataset is:</p> <pre><code>aircraft\n\u2514\u2500\u2500 data\n    \u251c\u2500\u2500 images\n    \u2502   \u251c\u2500\u2500 image1.jpg\n    \u2502   \u251c\u2500\u2500 image2.jpg\n    \u2502   \u2514\u2500\u2500 ....\n    \u251c\u2500\u2500 images_variant_test.txt\n    \u251c\u2500\u2500 images_variant_trainval.txt\n    \u2514\u2500\u2500 ....\n</code></pre> <p>The folder \"images\" contains all the 10,000 images, and the airplane types and subset names of each image are recorded in images_variant_*.txt. When this dataset is used for fine-tuning, the training set is usually set by annotation file: images_variant_trainval.txt. Hence, the training set should contain 6667 images and the test set should contain 3333 images after the dataset has been split.</p>"},{"location":"how_to_guides/finetune_with_a_custom_dataset/#data-preprocessing","title":"Data Preprocessing","text":""},{"location":"how_to_guides/finetune_with_a_custom_dataset/#read-custom-dataset","title":"Read Custom Dataset","text":"<p>For custom datasets, you can either organize the dataset file directory locally into a tree structure similar to ImageNet, and then use the function <code>create_dataset</code> to read the dataset (offline way), or if your dataset is medium-scale or above, which is not suitable to use offline way, you can also directly read all the images into a mappable or iterable object, replacing the file splitting and the <code>create_dataset</code> steps (online way).</p>"},{"location":"how_to_guides/finetune_with_a_custom_dataset/#read-dataset-offline","title":"Read Dataset Offline","text":"<p>The function <code>create_dataset</code> uses <code>mindspore.Dataset.ImageFolderDataset</code> function to build a dataset object, all images in the same folder will be assigned a same label, which is the folder name. Therefore, the prerequisite for using this function is that the file directory of the source dataset should follow the following tree structure:</p> <pre><code>DATASET_NAME\n    \u251c\u2500\u2500 split1(e.g. train)/\n    \u2502  \u251c\u2500\u2500 class1/\n    \u2502  \u2502   \u251c\u2500\u2500 000001.jpg\n    \u2502  \u2502   \u251c\u2500\u2500 000002.jpg\n    \u2502  \u2502   \u2514\u2500\u2500 ....\n    \u2502  \u2514\u2500\u2500 class2/\n    \u2502      \u251c\u2500\u2500 000001.jpg\n    \u2502      \u251c\u2500\u2500 000002.jpg\n    \u2502      \u2514\u2500\u2500 ....\n    \u2514\u2500\u2500 split2/\n       \u251c\u2500\u2500 class1/\n       \u2502   \u251c\u2500\u2500 000001.jpg\n       \u2502   \u251c\u2500\u2500 000002.jpg\n       \u2502   \u2514\u2500\u2500 ....\n       \u2514\u2500\u2500 class2/\n           \u251c\u2500\u2500 000001.jpg\n           \u251c\u2500\u2500 000002.jpg\n           \u2514\u2500\u2500 ....\n</code></pre> <p>Next, we'll take the annotation file ./aircraft/data/images_variant_trainval.txt as an example, locally generate the file of train set ./aircraft/data/images/trainval/, which meets the request of a tree-structure directory.</p> <pre><code>import os\nimport shutil\n\n\n# only for Aircraft dataset but not a general one\ndef extract_images(images_path, subset_name, annotation_file_path, copy=True):\n    # read the annotation file to get the label of each image\n    def annotations(annotation_file_path):\n        image_label = {}\n        with open(annotation_file_path, \"r\") as f:\n            lines = f.readlines()\n            for line in lines:\n                label = \" \".join(line.split(\" \")[1:]).replace(\"\\n\", \"\").replace(\"/\", \"_\")\n                if label not in image_label.keys():\n                    image_label[label] = []\n                    image_label[label].append(line.split(\" \")[0])\n                else:\n                    image_label[label].append(line.split(\" \")[0])\n        return image_label\n\n    # make a new folder for subset\n    subset_path = images_path + subset_name\n    os.mkdir(subset_path)\n\n    # extract and copy/move images to the new folder\n    image_label = annotations(annotation_file_path)\n    for label in image_label.keys():\n        label_folder = subset_path + \"/\" + label\n        os.mkdir(label_folder)\n        for image in image_label[label]:\n            image_name = image + \".jpg\"\n            if copy:\n                shutil.copy(images_path + image_name, label_folder + image_name)\n            else:\n                shutil.move(images_path + image_name, label_folder)\n\n\n# take train set of aircraft dataset as an example\nimages_path = \"./aircraft/data/images/\"\nsubset_name = \"trainval\"\nannotation_file_path = \"./aircraft/data/images_variant_trainval.txt\"\nextract_images(images_path, subset_name, annotation_file_path)\n</code></pre> <p>The splitting method of the test set is the same as that of the training set. The file structure of the whole Aircraft dataset should be:</p> <pre><code>aircraft\n\u2514\u2500\u2500 data\n    \u2514\u2500\u2500 images\n        \u251c\u2500\u2500 trainval\n        \u2502   \u251c\u2500\u2500 707-320\n        \u2502   \u2502   \u251c\u2500\u2500 0056978.jpg\n        \u2502   \u2502   \u2514\u2500\u2500 ....\n        \u2502   \u251c\u2500\u2500 727-200\n        \u2502   \u2502   \u251c\u2500\u2500 0048341.jpg\n        \u2502   \u2502   \u2514\u2500\u2500 ....\n        \u2502   \u2514\u2500\u2500 ....\n        \u2514\u2500\u2500 test\n            \u251c\u2500\u2500 707-320\n            \u2502   \u251c\u2500\u2500 0062765.jpg\n            \u2502   \u2514\u2500\u2500 ....\n            \u251c\u2500\u2500 727-200\n            \u2502   \u251c\u2500\u2500 0061581.jpg\n            \u2502   \u2514\u2500\u2500 ....\n            \u2514\u2500\u2500 ....\n</code></pre> <p>./example/finetune.py integrates the whole training pipeline, from pre-processing to the establishment and training of the model: <code>create_dataset</code> -&gt; <code>create_transforms</code> -&gt; <code>create_loader</code> -&gt; <code>create_model</code> -&gt;..., thus, the dataset with the adequate file directory structure can be sent directly to the fine-tuning script to start the subsequent processes including loading dataset and model training by running <code>python ./example/finetune.py --data_dir=./aircraft/data/images/</code> command. For custom datasets, please note that the dataset parameter in the configuration file must be set to an empty string <code>\"\"</code>  in advance.</p>"},{"location":"how_to_guides/finetune_with_a_custom_dataset/#read-dataset-online","title":"Read Dataset Online","text":"<p>Offline data reading takes up extra local disk space to store the newly generated data files. Therefore, when the local storage space is insufficient or the data cannot be backed up to the local environment, the local data files cannot be read using <code>create_dataset</code> directly, you can write a function to read the dataset in an online way.</p> <p>Here's how we generate a random-accessible dataset object that stores the images of the training set and realize a map from indices to data samples:</p> <ul> <li> <p>First, we define a class <code>ImageClsDataset</code> to read the raw data and transform them into a random-accessible dataset:</p> <ul> <li>In the initialization function <code>__init__()</code>, the annotation file path such as ./aircraft/data/images_variant_trainval.txt is taken as input, and used to generate a dictionary <code>self.annotation</code> that stores the one-to-one correspondence between images and tags;</li> <li>Since <code>create_loader</code> will perform a map operation on this iterated object, which does not support string format labels, it is also necessary to generate <code>self.label2id</code> to convert the string format label in <code>self.annotation</code> to integer type;</li> <li>Based on the information stored in <code>self.annotation</code>, we next read each image in the training set as a one-dimensional array  from the folder ./aircraft/data/images/ (the image data must be read as an one-dimensional array due to map operation restrictions in <code>create_loader</code>). The image information and label are stored in <code>self._data</code> and <code>self._label</code> respectively.</li> <li>Next, the mappable object is constructed using the <code>__getitem__</code> function.</li> <li>After writing the ImageClsDataset class, we can pass it the path of the annotation file to instantiate it, and load it as a dataset that can be read by the model through <code>mindspore.dataset.GeneratorDataset</code>. Note that the parameter <code>column_names</code> must be set to be [\"image\", \"label\"] for subsequent reading by other functions. What we've got now is supposed to be the same as what's generated by <code>create_dataset</code>.</li> </ul> </li> </ul> <pre><code>import numpy as np\n\nfrom mindspore.dataset import GeneratorDataset\n\n\nclass ImageClsDataset:\n    def __init__(self, annotation_dir, images_dir):\n        # Read annotations\n        self.annotation = {}\n        with open(annotation_dir, \"r\") as f:\n            lines = f.readlines()\n            for line in lines:\n                image_label = line.replace(\"\\n\", \"\").replace(\"/\", \"_\").split(\" \")\n                image = image_label[0] + \".jpg\"\n                label = \" \".join(image_label[1:])\n                self.annotation[image] = label\n\n        # Transfer string-type label to int-type label\n        self.label2id = {}\n        labels = sorted(list(set(self.annotation.values())))\n        for i in labels:\n            self.label2id[i] = labels.index(i)\n\n        for image, label in self.annotation.items():\n            self.annotation[image] = self.label2id[label]\n\n        # Read image-labels as mappable object\n        label2images = {key: [] for key in self.label2id.values()}\n        for image, label in self.annotation.items():\n            read_image = np.fromfile(images_dir + image, dtype=np.uint8)\n            label2images[label].append(read_image)\n\n        self._data = sum(list(label2images.values()), [])\n        self._label = sum([[i] * len(label2images[i]) for i in label2images.keys()], [])\n\n    # make class ImageClsDataset a mappable object\n    def __getitem__(self, index):\n        return self._data[index], self._label[index]\n\n    def __len__(self):\n        return len(self._data)\n\n\n# take aircraft dataset as an example\nannotation_dir = \"./aircraft/data/images_variant_trainval.txt\"\nimages_dir = \"./aircraft/data/images/\"\ndataset = ImageClsDataset(annotation_dir, images_dir)\ndataset_train = GeneratorDataset(source=dataset, column_names=[\"image\", \"label\"], shuffle=True)\n</code></pre> <p>Compared with the offline way, the online way skipped the step of splitting the data file locally and reading the local file with the <code>create_dataset</code> function. So in the subsequent training, simply replace the part of finetune.py that uses <code>create_dataset</code> with the above code, then you can start training by running finetune.py directly as what you do after reading the dataset offline.</p>"},{"location":"how_to_guides/finetune_with_a_custom_dataset/#augmentation-and-batching","title":"Augmentation and Batching","text":"<p>MindCV uses the <code>create_loader</code> function to perform data augmentation and batching for the dataset read in the previous chapter. Augmentation strategies are defined in advance by the <code>create_transforms</code> function. Batching is set by the parameter <code>batch_size</code> in the <code>create_loader</code> function. All hyperparameters mentioned above can be passed through the model configuration file. Hyper-parameters' specific usage see the API documentation.</p> <p>For small-size custom datasets, it is suggested that data augmentation can be used to the training set to enhance the generalization of the model and prevent overfitting. For the dataset of fine-grained image classification tasks, such as the Aircraft dataset in this tutorial, the classification effect may be not that ideal due to the large variance within the data class, the image size can be set larger by adjusting the hyper-parameter <code>image_resize</code> (such as 448, 512, 600, etc.).</p>"},{"location":"how_to_guides/finetune_with_a_custom_dataset/#fine-tuning","title":"Fine-tuning","text":"<p>Referring to Stanford University CS231n, fine-tuning all the parameters, freezing feature network, and setting learning rates for specific layers are commonly used fine-tuning skills. The first one uses pre-trained weights to initialize the parameters of the target model, and then updates all parameters based on the new dataset, so it's usually time-consuming but will get a high precision. Freezing feature networks are divided into freezing all feature networks(linear probe) and freezing partial feature networks. The former uses the pre-trained model as a feature extractor and only updates the parameters of the full connection layer, which takes a short time but has low accuracy; The latter generally freezes the parameters of shallow layers, which only learn the basic features of images, and only updates the parameters of the deep network and the full connection layer. Setting learning rate for specific layers is similar but more elaborate, it specifies the learning rates used by certain layers during training.</p> <p>For hyper-parameters used in fine-tuning training, you can refer to the configuration file used when pre-training on the ImageNet-1k dataset in ./configs. Note that for fine-tuning, the hyper-parameter <code>pretrained</code> should be set to be <code>True</code> to load the pre-training weight,  <code>num_classes</code>\u00a0should be set to be the number of labels of the custom dataset (e.g. 100 for the Aircraft dataset here), moreover, don't forget to reduce batch_size and epoch_size based on the size of the custom dataset. In addition, since the pre-trained weight already contains a lot of information for identifying images, in order not to destroy this information too much, it is also necessary to reduce the learning rate <code>lr</code> , and it is also recommended to start training and adjust from at most one-tenth of the pre-trained learning rate or 0.0001. These parameters can be modified in the configuration file or added in the shell command as shown below. The training results can be viewed in the file ./ckpt/results.txt.</p> <pre><code>python .examples/finetune/finetune.py --config=./configs/mobilenetv3/mobilnet_v3_small_ascend.yaml --data_dir=./aircraft/data --pretrained=True\n</code></pre> <p>When fine-tuning mobilenet v3-small based on Aircraft dataset, this tutorial mainly made the following changes to the hyper-parameters:</p> Hyper-parameter Pretrain Fine-tune dataset \"imagenet\" \"\" batch_size 75 8 image_resize 224 600 auto_augment - \"randaug-m7-mstd0.5\" num_classes 1000 100 pretrained False True epoch_size 470 50 lr 0.77 0.002"},{"location":"how_to_guides/finetune_with_a_custom_dataset/#fine-tuning-all-the-parameters","title":"Fine-tuning All the Parameters","text":"<p>Since the progress of this type of fine-tuning is the same as training from scratch, simply start the training by running finetune.py and adjust the parameters as training from scratch.</p>"},{"location":"how_to_guides/finetune_with_a_custom_dataset/#freeze-feature-network","title":"Freeze Feature Network","text":""},{"location":"how_to_guides/finetune_with_a_custom_dataset/#linear-probe","title":"Linear Probe","text":"<p>We prevent parameters from updating by setting <code>requires_grad=False</code> for all parameters except those in the full connection layer. In finetune.py, add the following code after <code>create_model</code> :</p> <pre><code>from mindcv.models.registry import _model_pretrained_cfgs\n\n# ...create_model()\n\n# number of parameters to be updated\nnum_params = 2\n\n# read names of parameters in FC layer\nclassifier_names = [_model_pretrained_cfgs[args.model][\"classifier\"] + \".weight\",\n                    _model_pretrained_cfgs[args.model][\"classifier\"] + \".bias\"]\n\n# prevent parameters in network(except the classifier) from updating\nfor param in network.trainable_params():\n    if param.name not in classifier_names:\n        param.requires_grad = False\n</code></pre>"},{"location":"how_to_guides/finetune_with_a_custom_dataset/#freeze-part-of-the-feature-network","title":"Freeze Part of the Feature Network","text":"<p>To balance the speed and precision of fine-tuning, we can also fix some target network parameters and train the parameters in the deep layer only. It is necessary to extract the parameter names in those layers to be frozen and slightly modify the code in the last chapter. By printing the result of <code>create_model</code> -- <code>network</code>, we can see that in MindCV, each layer of the network of mobilenet v3-small is named with <code>features.*</code>. Suppose that we freeze only the first 7 layers of the network, add the following code after <code>create_model</code>:</p> <pre><code># ...create_model()\n\n# read names of network layers\nfreeze_layer=[\"features.\"+str(i) for i in range(7)]\n\n# prevent parameters in the first 7 layers of the network from updating\nfor param in network.trainable_params():\n    for layer in freeze_layer:\n        if layer in param.name:\n            param.requires_grad = False\n</code></pre>"},{"location":"how_to_guides/finetune_with_a_custom_dataset/#set-learning-rate-for-specific-layers","title":"Set Learning Rate for Specific Layers","text":"<p>To further improve the training accuracy of a fine-tuned model, we can set different learning rates for different layers in the network. This is because the shallow part of the network generally recognizes common contours or features, so even if parameters in this part will be updated, the learning rate should be set relatively small; The deep part generally recognizes the detailed personal characteristics of an object, so the learning rate can be set relatively large; Compared with the feature network that needs to retain the pre-training information as much as possible, the classifier needs to be trained from the beginning, hence the learning rate can be appropriately increased. Since this operation is elaborate, we need to enter finetune.py to specify the parameter names of specific layers and the corresponding learning rates.</p> <p>MindCV uses <code>create_optimizer</code> to generate the optimizer and passes the learning rate to the optimizer. To set the tiered learning rate, simply change the <code>params</code> parameter of <code>create_optimizer</code> function in finetune.py from <code>network.trainable_params()</code> to a list containing the names of the specific parameters and the corresponding learning rate, which you can refer to the API documentation of optimizers. The specific structure of the network and the parameter names in each layer can be viewed by printing the result of <code>create_model</code> -- <code>network</code>.</p> <p>Tips: You can also use the same operation to set different weight_decay for parameters.</p>"},{"location":"how_to_guides/finetune_with_a_custom_dataset/#set-learning-rate-for-classifier","title":"Set Learning Rate for Classifier","text":"<p>Taking mobilenet v3-small as an example, the model classifier name starts with \"classifier\", so if we only increase the learning rate of the classifier, we need to specify it at each step of training. <code>lr_scheduler</code> is a learning rate list generated by <code>create_scheduler</code>, which contains the learning rate at each step of training. Suppose we adjust the learning rate of the classifier to 1.2 times that on the feature network. The changes to the finetune.py code are as follows:</p> <pre><code># ...\n\n\n# Note: a)the params-lr dict must contain all the parameters. b)Also, you're recommended to set a dict with a key \"order_params\" to make sure the parameters will be updated in the right order.\nparams_lr_group = [{\"params\": list(filter(lambda x: 'classifier' in x.name, network.trainable_params())),\n                    \"lr\": [i*1.2 for i in lr_scheduler]},\n                   {\"params\": list(filter(lambda x: 'classifier' not in x.name, network.trainable_params())),\n                    \"lr\": lr_scheduler},\n                   {\"order_params\": network.trainable_params()}]\n\noptimizer = create_optimizer(params_lr_group,\n                             opt=args.opt,\n                             lr=lr_scheduler,\n                             ...)\n</code></pre>"},{"location":"how_to_guides/finetune_with_a_custom_dataset/#set-learning-rate-for-any-layers-in-feature-network","title":"Set Learning Rate for Any Layers in Feature Network","text":"<p>Similar to adjusting the learning rate of a classifier alone, setting the learning rate of layers in feature network requires a list specifying the learning rate for each layer. Assuming that we only increase the learning rate of the last three layers of the feature network (with prefix features.13, features.14, features.15), the code for creating the optimizer in finetune.py will be changed as follows:</p> <pre><code># ...\n\n\n# Note: a)the params-lr dict must contain all the parameters. b)Also, you're recommended to set a dict with a key \"order_params\" to make sure the parameters will be updated in the right order.\nparams_lr_group = [{\"params\": list(filter(lambda x: 'features.13' in x.name, network.trainable_params())),\n                    \"lr\": [i * 1.05 for i in lr_scheduler]},\n                   {\"params\": list(filter(lambda x: 'features.14' in x.name, network.trainable_params())),\n                    \"lr\": [i * 1.1 for i in lr_scheduler]},\n                   {\"params\": list(filter(lambda x: 'features.15' in x.name, network.trainable_params())),\n                    \"lr\": [i * 1.15 for i in lr_scheduler]},\n                   {\"params\": list(filter(\n                       lambda x: \".\".join(x.name.split(\".\")[:2]) not in [\"features.13\", \"features.14\", \"features.15\"],\n                       network.trainable_params())),\n                    \"lr\": lr_scheduler},\n                   {\"order_params\": network.trainable_params()}]\n\noptimizer = create_optimizer(params_lr_group,\n                             opt=args.opt,\n                             lr=lr_scheduler,\n                             ...)\n</code></pre>"},{"location":"how_to_guides/finetune_with_a_custom_dataset/#evaluation","title":"Evaluation","text":"<p>After training, use the model weights stored in <code>*_best-ckpt</code> format in the./ckpt folder to evaluate the performance of the network on the test set. Just run validate.py and pass the file path of the model configuration file as well as the model weight to it:</p> <pre><code>python validate.py --config=./configs/mobilenetv3/mobilnet_v3_small_ascend.yaml --data_dir=./aircraft/data --ckpt_path=./ckpt/mobilenet_v3_small_100_best.ckpt\n</code></pre> <p>The following table summarizes the Top-1 accuracy of the fine-tuned mobilenet v3-small on the Aircraft dataset with the same training configuration but different fine-tuning skills:</p> Network Freeze All the Feature Work Freeze Shallow Part of Feature Network Full Fine-tuning Full Fine-tuning with Increasing Learning Rate of Classifier Full Fine-tuning with Increasing Learning Rate of Deep Layers mobilenet v3-small 48.66% 76.83% 88.35% 88.89% 88.68%"},{"location":"how_to_guides/finetune_with_a_custom_dataset/#prediction","title":"Prediction","text":"<p>Refer to this section of the MindCV fine-tuning tutorial: visual model reasoning results, or add the following code in validate.py to generate a text file ./ckpt/pred.txt that stores the true and predicted labels of the test set:</p> <pre><code># ... after model.eval()\n\n# predited label\npred = np.argmax(model.predict(images).asnumpy(), axis=1)\n\n# real label\nimages, labels = next(loader_eval.create_tuple_iterator())\n\n# write pred.txt\nprediction = np.array([pred, labels]).transpose()\nnp.savetxt(\"./ckpt/pred.txt\", prediction, fmt=\"%s\", header=\"pred \\t real\")\n</code></pre>"},{"location":"how_to_guides/finetune_with_a_custom_dataset/#appendix","title":"Appendix","text":"<p>The following table shows the Top-1 accuracy (%) of full-model fine-tuning on the Aircraft dataset on several CNNs. For the classification accuracy that can be achieved on this dataset, see Aircraft Leaderboard and Paper With Code.</p> Network Full Fine-tuning Accuracy with Mindcv Accuracy in Papers mobilenet v3-small 88.35% - mobilenet v3-large 92.22% 83.8% convnext-tiny 93.69% 84.23% resnest50 86.82% -"},{"location":"how_to_guides/write_a_new_model/","title":"Write A New Model","text":"<p>This document provides a reference template for writing the model definition file <code>model.py</code> in the MindSpore, aiming to provide a unified code style.</p> <p>Next, let's take <code>MLP-Mixer</code> as an example.</p>"},{"location":"how_to_guides/write_a_new_model/#file-header","title":"File Header","text":"<p>A brief description of the document. Include the model name and paper title. As follows:</p> <pre><code>\"\"\"\nMindSpore implementation of `${MODEL_NAME}`.\nRefer to ${PAPER_NAME}.\n\"\"\"\n</code></pre>"},{"location":"how_to_guides/write_a_new_model/#module-import","title":"Module Import","text":"<p>There are three types of module imports. Respectively</p> <ul> <li>Python native or third-party libraries. For example, <code>import math</code> and <code>import numpy as np</code>. It should be placed in the first echelon.</li> <li>MindSpore related modules. For example, <code>import mindspore.nn as nn</code> and <code>import mindspore.ops as ops</code>. It should be placed in the second echelon.</li> <li>The module in the MindCV package. For example, <code>from .layers.classifier import ClassifierHead</code>. It should be placed in the third echelon and use relative import.</li> </ul> <p>Examples are as follows:</p> <pre><code>import math\nfrom collections import OrderedDict\n\nimport mindspore.nn as nn\nimport mindspore.ops as ops\nimport mindspore.common.initializer as init\n\nfrom .utils import load_pretrained\nfrom .layers.classifier import ClassifierHead\n</code></pre> <p>Only import necessary modules or packages to avoid importing useless packages.</p>"},{"location":"how_to_guides/write_a_new_model/#__all__","title":"<code>__all__</code>","text":"<p>Python has no native visibility control, its visibility is maintained by a set of \"conventions\" that everyone should consciously abide by <code>__all__</code> is a convention for exposing interfaces to modules and provides a \"white list\" to expose the interface. If <code>__all__</code> is defined, other files use <code>from xxx import *</code> to import this file, only the members listed in <code>__all__</code> will be imported, and other members can be excluded.</p> <p>We agree that the exposed interfaces in the model include the main model class and functions that return models of different specifications, such as:</p> <pre><code>__all__ = [\n    \"MLPMixer\",\n    \"mlp_mixer_s_p32\",\n    \"mlp_mixer_s_p16\",\n    ...\n]\n</code></pre> <p>Where <code>MLPMixer</code> is the main model class, and <code>mlp_mixer_s_p32</code> and <code>mlp_mixer_s_p16</code> are functions that return models of different specifications. Generally speaking, a submodel, that is, a <code>Layer</code> or a <code>Block</code>, should not be shared by other files. If this is the case, you should consider extracting the submodel under <code>${MINDCLS}/models/layers</code> as a common module, such as <code>SEBlock</code>.</p>"},{"location":"how_to_guides/write_a_new_model/#submodel","title":"Submodel","text":"<p>We all know that a depth model is a network composed of multiple layers. Some of these layers can form sub-models of the same topology, which we generally call <code>Layer</code> or <code>Block</code>, such as <code>ResidualBlock</code>. This kind of abstraction is conducive to our understanding of the whole model structure and is also conducive to code writing.</p> <p>We should briefly describe the function of the sub-model through class annotations. In <code>MindSpore</code>, the model class inherits from <code>nn.Cell</code>. Generally speaking, we need to overload the following two functions:</p> <ul> <li>In the <code>__init__</code> function, we should define the neural network layer that needs to be used in the model (the parameters in <code>__init__</code> should be declared with parameter types, that is, type hint).</li> <li>In the <code>construct</code> function, we define the model forward logic.</li> </ul> <p>Examples are as follows:</p> <pre><code>class MixerBlock(nn.Cell):\n    \"\"\"Mixer Layer with token-mixing MLP and channel-mixing MLP\"\"\"\n\n    def __init__(self,\n                 n_patches: int,\n                 n_channels: int,\n                 token_dim: int,\n                 channel_dim: int,\n                 dropout: float = 0.\n                 ) -&gt; None:\n        super().__init__()\n        self.token_mix = nn.SequentialCell(\n            nn.LayerNorm((n_channels,)),\n            TransPose((0, 2, 1)),\n            FeedForward(n_patches, token_dim, dropout),\n            TransPose((0, 2, 1))\n        )\n        self.channel_mix = nn.SequentialCell(\n            nn.LayerNorm((n_channels,)),\n            FeedForward(n_channels, channel_dim, dropout),\n        )\n\n    def construct(self, x):\n        x = x + self.token_mix(x)\n        x = x + self.channel_mix(x)\n        return x\n</code></pre> <p>In the process of compiling the <code>nn.Cell</code> class, there are two noteworthy aspects</p> <ul> <li> <p>CellList &amp; SequentialCell</p> </li> <li> <p>CellList is just a container that contains a list of neural network layers(Cell). The Cells contained by it can be properly registered and will be visible by all Cell methods. We must overwrite the forward calculation, that is, the construct function.</p> </li> <li> <p>SequentialCell is a container that holds a sequential list of layers(Cell). The Cells may have a name(OrderedDict) or not(List). We don't need to implement forward computation, which is done according to the order of the sequential list.</p> </li> <li> <p>construct</p> </li> <li> <p>Assert is not supported. [RuntimeError: ParseStatement] Unsupported statement 'Assert'.</p> </li> <li> <p>Usage of single operator. When calling an operator (such as concat, reshape, mean), use the functional interface mindspore.ops.functional (such as output=ops.concat((x1, x2)) to avoid instantiating the original operator ops.Primary (such as self.Concat()) in init before calling it in construct (output=self.concat((x1, x2)).</p> </li> </ul>"},{"location":"how_to_guides/write_a_new_model/#master-model","title":"Master Model","text":"<p>The main model is the network model definition proposed in the paper, which is composed of multiple sub-models. It is the top-level network suitable for classification, detection, and other tasks. It is basically similar to the submodel in code writing, but there are several differences.</p> <ul> <li>Class annotations. We should give the title and link of the paper here. In addition, since this class is exposed to the outside world, we'd better also add a description of the class initialization parameters. See code below.</li> <li><code>forward_features</code> function. The operational definition of the characteristic network of the model in the function.</li> <li><code>forward_head</code> function. The operation of the classifier of the model is defined in the function.</li> <li><code>construct</code> function. In function call feature network and classifier operation.</li> <li><code>_initialize_weights</code> function. We agree that the random initialization of model parameters is completed by this member function. See code below.</li> </ul> <p>Examples are as follows:</p> <pre><code>class MLPMixer(nn.Cell):\n    r\"\"\"MLP-Mixer model class, based on\n    `\"MLP-Mixer: An all-MLP Architecture for Vision\" &lt;https://arxiv.org/abs/2105.01601&gt;`_\n\n    Args:\n        depth (int) : number of MixerBlocks.\n        patch_size (Union[int, tuple]) : size of a single image patch.\n        n_patches (int) : number of patches.\n        n_channels (int) : channels(dimension) of a single embedded patch.\n        token_dim (int) : hidden dim of token-mixing MLP.\n        channel_dim (int) : hidden dim of channel-mixing MLP.\n        in_channels(int): number the channels of the input. Default: 3.\n        n_classes (int) : number of classification classes. Default: 1000.\n    \"\"\"\n\n    def __init__(self,\n                 depth: int,\n                 patch_size: Union[int, tuple],\n                 n_patches: int,\n                 n_channels: int,\n                 token_dim: int,\n                 channel_dim: int,\n                 in_channels: int = 3,\n                 n_classes: int = 1000,\n                 ) -&gt; None:\n        super().__init__()\n        self.n_patches = n_patches\n        self.n_channels = n_channels\n        # patch with shape of (3, patch_size, patch_size) is embedded to n_channels dim feature.\n        self.to_patch_embedding = nn.SequentialCell(\n            nn.Conv2d(in_chans, n_channels, patch_size, patch_size, pad_mode=\"pad\", padding=0),\n            TransPose(permutation=(0, 2, 1), embedding=True),\n        )\n        self.mixer_blocks = nn.SequentialCell()\n        for _ in range(depth):\n            self.mixer_blocks.append(MixerBlock(n_patches, n_channels, token_dim, channel_dim))\n        self.layer_norm = nn.LayerNorm((n_channels,))\n        self.mlp_head = nn.Dense(n_channels, n_classes)\n        self._initialize_weights()\n\n    def forward_features(self, x: Tensor) -&gt; Tensor:\n        x = self.to_patch_embedding(x)\n        x = self.mixer_blocks(x)\n        x = self.layer_norm(x)\n        return ops.mean(x, 1)\n\n    def forward_head(self, x: Tensor)-&gt; Tensor:\n        return self.mlp_head(x)\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        x = self.forward_features(x)\n        return self.forward_head(x)\n\n    def _initialize_weights(self) -&gt; None:\n        for name, m in self.cells_and_names():\n            if isinstance(m, nn.Conv2d):\n                m.weight.set_data(init.initializer(init.Normal(0.01, 0), m.weight.shape))\n                if m.bias is not None:\n                    m.bias.set_data(init.initializer(init.Constant(0), m.bias.shape))\n            elif isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm1d):\n                m.gamma.set_data(init.initializer(init.Constant(1), m.gamma.shape))\n                if m.beta is not None:\n                    m.beta.set_data(init.initializer(init.Constant(0.0001), m.beta.shape))\n            elif isinstance(m, nn.Dense):\n                m.weight.set_data(init.initializer(init.Normal(0.01, 0), m.weight.shape))\n                if m.bias is not None:\n                    m.bias.set_data(init.initializer(init.Constant(0), m.bias.shape))\n</code></pre>"},{"location":"how_to_guides/write_a_new_model/#specification-function","title":"Specification Function","text":"<p>The model proposed in the paper may have different specifications, such as the size of the <code>channel</code>, the size of the <code>depth</code>, and so on. The specific configuration of these variants should be reflected through the specification function. The specification interface parameters: pretrained, num_classes, in_channels should be named uniformly. At the same time, the pretrain loading operation should be performed in the specification function. Each specification function corresponds to a specification variant that determines the configuration. The configuration transfers the definition of the main model class through the input parameter and returns the instantiated main model class. In addition, you need to register this specification of the model in the package by adding the decorator <code>@register_model</code>.</p> <p>Examples are as follows:</p> <pre><code>@register_model\ndef mlp_mixer_s_p16(pretrained: bool = False, num_classes: int = 1000, in_channels: int = 3, **kwargs):\n    nl, pr, ls, hs, ds, dc = 8, 16, 196, 512, 256, 2048\n    _check_resolution_and_length_of_patch(pr, ls)\n    model = MLPMixer(depth=nl, patch_size=pr, n_patches=ls, n_channels=hs, token_dim=ds,\n                    channel_dim=dc, in_chans=in_chans, n_classes=num_classes, **kwargs)\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n    return model\n\n@register_model\ndef mlp_mixer_b_p32(pretrained: bool = False, num_classes: int = 1000, in_channels: int = 3, **kwargs):\n    nl, pr, ls, hs, ds, dc = 12, 32, 49, 768, 384, 3072\n    _check_resolution_and_length_of_patch(pr, ls)\n    model = MLPMixer(depth=nl, patch_size=pr, n_patches=ls, n_channels=hs, token_dim=ds,\n                    channel_dim=dc, in_chans=in_chans, n_classes=num_classes, **kwargs)\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n    return model\n</code></pre>"},{"location":"how_to_guides/write_a_new_model/#verify-main-optional","title":"Verify Main (Optional)","text":"<p>The initial writing phase should ensure that the model is operational. The following code blocks can be used for basic verification:</p> <pre><code>if __name__ == '__main__':\n    import numpy as np\n    import mindspore\n    from mindspore import Tensor\n\n    model = mlp_mixer_s_p16()\n    print(model)\n    dummy_input = Tensor(np.random.rand(8, 3, 224, 224), dtype=mindspore.float32)\n    y = model(dummy_input)\n    print(y.shape)\n</code></pre>"},{"location":"how_to_guides/write_a_new_model/#reference-example","title":"Reference Example","text":"<ul> <li>densenet.py</li> <li>shufflenetv1.py</li> <li>shufflenetv2.py</li> <li>mixnet.py</li> <li>mlp_mixer.py</li> </ul>"},{"location":"notes/changelog/","title":"Change Log","text":"<p>Coming soon.</p>"},{"location":"notes/code_of_conduct/","title":"Code of Conduct","text":"<p>Coming soon.</p>"},{"location":"notes/contributing/","title":"Contributing","text":""},{"location":"notes/contributing/#mindcv-contributing-guidelines","title":"MindCV Contributing Guidelines","text":"<p>Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given.</p>"},{"location":"notes/contributing/#contributor-license-agreement","title":"Contributor License Agreement","text":"<p>It's required to sign CLA before your first code submission to MindCV community.</p> <p>For individual contributor, please refer to ICLA online document for the detailed information.</p>"},{"location":"notes/contributing/#types-of-contributions","title":"Types of Contributions","text":""},{"location":"notes/contributing/#report-bugs","title":"Report Bugs","text":"<p>Report bugs at https://github.com/mindspore-lab/mindcv/issues.</p> <p>If you are reporting a bug, please include:</p> <ul> <li>Your operating system name and version.</li> <li>Any details about your local setup that might be helpful in troubleshooting.</li> <li>Detailed steps to reproduce the bug.</li> </ul>"},{"location":"notes/contributing/#fix-bugs","title":"Fix Bugs","text":"<p>Look through the GitHub issues for bugs. Anything tagged with \"bug\" and \"help wanted\" is open to whoever wants to implement it.</p>"},{"location":"notes/contributing/#implement-features","title":"Implement Features","text":"<p>Look through the GitHub issues for features. Anything tagged with \"enhancement\" and \"help wanted\" is open to whoever wants to implement it.</p>"},{"location":"notes/contributing/#write-documentation","title":"Write Documentation","text":"<p>MindCV could always use more documentation, whether as part of the official MindCV docs, in docstrings, or even on the web in blog posts, articles, and such.</p>"},{"location":"notes/contributing/#submit-feedback","title":"Submit Feedback","text":"<p>The best way to send feedback is to file an issue at https://github.com/mindspore-lab/mindcv/issues.</p> <p>If you are proposing a feature:</p> <ul> <li>Explain in detail how it would work.</li> <li>Keep the scope as narrow as possible, to make it easier to implement.</li> <li>Remember that this is a volunteer-driven project, and that contributions are welcome :)</li> </ul>"},{"location":"notes/contributing/#getting-started","title":"Getting Started","text":"<p>Ready to contribute? Here's how to set up <code>mindcv</code> for local development.</p> <ol> <li>Fork the <code>mindcv</code> repo on GitHub.</li> <li>Clone your fork locally:</li> </ol> <pre><code>git clone git@github.com:your_name_here/mindcv.git\n</code></pre> <p>After that, you should add official repository as the upstream repository:</p> <pre><code>git remote add upstream git@github.com:mindspore-lab/mindcv\n</code></pre> <ol> <li>Install your local copy into a conda environment. Assuming you have conda installed, this is how you set up your fork for local development:</li> </ol> <pre><code>conda create -n mindcv python=3.8\nconda activate mindcv\ncd mindcv\npip install -e .\n</code></pre> <ol> <li>Create a branch for local development:</li> </ol> <pre><code>git checkout -b name-of-your-bugfix-or-feature\n</code></pre> <p>Now you can make your changes locally.</p> <ol> <li>When you're done making changes, check that your changes pass the linters and the tests:</li> </ol> <pre><code>pre-commit run --show-diff-on-failure --color=always --all-files\npytest\n</code></pre> <p>If all static linting are passed, you will get output like:</p> <p></p> <p>otherwise, you need to fix the warnings according to the output:</p> <p></p> <p>To get pre-commit and pytest, just pip install them into your conda environment.</p> <ol> <li>Commit your changes and push your branch to GitHub:</li> </ol> <pre><code>git add .\ngit commit -m \"Your detailed description of your changes.\"\ngit push origin name-of-your-bugfix-or-feature\n</code></pre> <ol> <li>Submit a pull request through the GitHub website.</li> </ol>"},{"location":"notes/contributing/#pull-request-guidelines","title":"Pull Request Guidelines","text":"<p>Before you submit a pull request, check that it meets these guidelines:</p> <ol> <li>The pull request should include tests.</li> <li>If the pull request adds functionality, the docs should be updated. Put    your new functionality into a function with a docstring, and add the    feature to the list in README.md.</li> <li>The pull request should work for Python 3.7, 3.8 and 3.9, and for PyPy. Check    https://github.com/mindspore-lab/mindcv/actions    and make sure that the tests pass for all supported Python versions.</li> </ol>"},{"location":"notes/contributing/#tips","title":"Tips","text":"<p>You can install the git hook scripts instead of linting with <code>pre-commit run -a</code> manually.</p> <p>run flowing command to set up the git hook scripts</p> <pre><code>pre-commit install\n</code></pre> <p>now <code>pre-commit</code> will run automatically on <code>git commit</code>!</p>"},{"location":"notes/contributing/#releasing","title":"Releasing","text":"<p>A reminder for the maintainers on how to deploy. Make sure all your changes are committed (including an entry in HISTORY.md). Then run:</p> <pre><code>bump2version patch # possible: major / minor / patch\ngit push\ngit push --tags\n</code></pre> <p>GitHub Action will then deploy to PyPI if tests pass.</p>"},{"location":"notes/faq/","title":"FAQ","text":"<p>Coming soon.</p>"},{"location":"reference/data/","title":"Data","text":""},{"location":"reference/data/#auto-augmentation","title":"Auto Augmentation","text":""},{"location":"reference/data/#mindcv.data.auto_augment.auto_augment_transform","title":"<code>mindcv.data.auto_augment.auto_augment_transform(configs, hparams)</code>","text":"<p>Create a AutoAugment transform Args:     configs: A string that defines the automatic augmentation configuration.         It is composed of multiple parts separated by dashes (\"-\"). The first part defines         the AutoAugment policy ('autoaug', 'autoaugr' or '3a':         'autoaug' for the original AutoAugment policy with PosterizeOriginal,         'autoaugr' for the AutoAugment policy with PosterizeIncreasing operation,          '3a' for the AutoAugment only with 3 augmentations.)         There is no order requirement for the remaining config parts.</p> <pre><code>    - mstd: Float standard deviation of applied magnitude noise.\n\n    Example: 'autoaug-mstd0.5' will be automatically augment using the autoaug strategy\n    and magnitude_std 0.5.\nhparams: Other hparams of the automatic augmentation scheme.\n</code></pre> Source code in <code>mindcv\\data\\auto_augment.py</code> <pre><code>def auto_augment_transform(configs, hparams):\n    \"\"\"\n    Create a AutoAugment transform\n    Args:\n        configs: A string that defines the automatic augmentation configuration.\n            It is composed of multiple parts separated by dashes (\"-\"). The first part defines\n            the AutoAugment policy ('autoaug', 'autoaugr' or '3a':\n            'autoaug' for the original AutoAugment policy with PosterizeOriginal,\n            'autoaugr' for the AutoAugment policy with PosterizeIncreasing operation,\n             '3a' for the AutoAugment only with 3 augmentations.)\n            There is no order requirement for the remaining config parts.\n\n            - mstd: Float standard deviation of applied magnitude noise.\n\n            Example: 'autoaug-mstd0.5' will be automatically augment using the autoaug strategy\n            and magnitude_std 0.5.\n        hparams: Other hparams of the automatic augmentation scheme.\n    \"\"\"\n    config = configs.split(\"-\")\n    policy_name = config[0]\n    config = config[1:]\n    hparams.setdefault(\"magnitude_std\", 0.5)  # default magnitude_std is set to 0.5\n    for c in config:\n        cs = re.split(r\"(\\d.*)\", c)\n        if len(cs) &lt; 2:\n            continue\n        key, val = cs[:2]\n        if key == \"mstd\":\n            # noise param injected via hparams for now\n            hparams.setdefault(\"magnitude_std\", float(val))\n        else:\n            assert False, \"Unknown AutoAugment config section\"\n    aa_policy = auto_augment_policy(policy_name, hparams=hparams)\n    return AutoAugment(aa_policy)\n</code></pre>"},{"location":"reference/data/#mindcv.data.auto_augment.rand_augment_transform","title":"<code>mindcv.data.auto_augment.rand_augment_transform(configs, hparams)</code>","text":"<p>Create a RandAugment transform Args:     configs: A string that defines the random augmentation configuration.         It is composed of multiple parts separated by dashes (\"-\").         The first part defines the AutoAugment policy ('randaug' policy).         There is no order requirement for the remaining config parts.</p> <pre><code>    - m: Integer magnitude of rand augment. Default: 10\n    - n: Integer num layer (number of transform operations selected for each image). Default: 2\n    - w: Integer probability weight index (the index that affects a group of weights selected by operations).\n    - mstd: Floating standard deviation of applied magnitude noise,\n        or uniform sampling at infinity (or greater than 100).\n    - mmax: Set the upper range limit for magnitude to a value\n        other than the default value of _LEVEL_DENOM (10).\n    - inc: Integer (bool), using the severity increase with magnitude (default: 0).\n\n    Example: 'randaug-w0-n3-mstd0.5' will be random augment\n        using the weights 0, num_layers 3, magnitude_std 0.5.\nhparams: Other hparams (kwargs) for the RandAugmentation scheme.\n</code></pre> Source code in <code>mindcv\\data\\auto_augment.py</code> <pre><code>def rand_augment_transform(configs, hparams):\n    \"\"\"\n    Create a RandAugment transform\n    Args:\n        configs: A string that defines the random augmentation configuration.\n            It is composed of multiple parts separated by dashes (\"-\").\n            The first part defines the AutoAugment policy ('randaug' policy).\n            There is no order requirement for the remaining config parts.\n\n            - m: Integer magnitude of rand augment. Default: 10\n            - n: Integer num layer (number of transform operations selected for each image). Default: 2\n            - w: Integer probability weight index (the index that affects a group of weights selected by operations).\n            - mstd: Floating standard deviation of applied magnitude noise,\n                or uniform sampling at infinity (or greater than 100).\n            - mmax: Set the upper range limit for magnitude to a value\n                other than the default value of _LEVEL_DENOM (10).\n            - inc: Integer (bool), using the severity increase with magnitude (default: 0).\n\n            Example: 'randaug-w0-n3-mstd0.5' will be random augment\n                using the weights 0, num_layers 3, magnitude_std 0.5.\n        hparams: Other hparams (kwargs) for the RandAugmentation scheme.\n    \"\"\"\n    magnitude = _LEVEL_DENOM  # default to _LEVEL_DENOM for magnitude (currently 10)\n    num_layers = 2  # default to 2 ops per image\n    hparams.setdefault(\"magnitude_std\", 0.5)  # default magnitude_std is set to 0.5\n    weight_idx = None  # default to no probability weights for op choice\n    transforms = _RAND_TRANSFORMS\n    config = configs.split(\"-\")\n    assert config[0] == \"randaug\"\n    config = config[1:]\n    for c in config:\n        cs = re.split(r\"(\\d.*)\", c)\n        if len(cs) &lt; 2:\n            continue\n        key, val = cs[:2]\n        if key == \"mstd\":\n            # noise param / randomization of magnitude values\n            mstd = float(val)\n            if mstd &gt; 100:\n                # use uniform sampling in 0 to magnitude if mstd is &gt; 100\n                mstd = float(\"inf\")\n            hparams.setdefault(\"magnitude_std\", mstd)\n        elif key == \"mmax\":\n            # clip magnitude between [0, mmax] instead of default [0, _LEVEL_DENOM]\n            hparams.setdefault(\"magnitude_max\", int(val))\n        elif key == \"inc\":\n            if bool(val):\n                transforms = _RAND_INCREASING_TRANSFORMS\n        elif key == \"m\":\n            magnitude = int(val)\n        elif key == \"n\":\n            num_layers = int(val)\n        elif key == \"w\":\n            weight_idx = int(val)\n        else:\n            assert False, \"Unknown RandAugment config section\"\n    ra_ops = rand_augment_ops(magnitude=magnitude, hparams=hparams, transforms=transforms)\n    choice_weights = None if weight_idx is None else _select_rand_weights(weight_idx)\n    return RandAugment(ra_ops, num_layers, choice_weights=choice_weights)\n</code></pre>"},{"location":"reference/data/#mindcv.data.auto_augment.trivial_augment_wide_transform","title":"<code>mindcv.data.auto_augment.trivial_augment_wide_transform(configs, hparams)</code>","text":"<p>Create a TrivialAugmentWide transform Args:     configs: A string that defines the TrivialAugmentWide configuration.         It is composed of multiple parts separated by dashes (\"-\").         The first part defines the AutoAugment name, it should be 'trivialaugwide'.         the second part(not necessary) the maximum value of magnitude.</p> <pre><code>    - m: final magnitude of a operation will uniform sampling from [0, m] . Default: 31\n\n    Example: 'trivialaugwide-m20' will be TrivialAugmentWide\n    with mgnitude uniform sampling from [0, 20],\nhparams: Other hparams (kwargs) for the TrivialAugment scheme.\n</code></pre> <p>Returns:     A Mindspore compatible Transform</p> Source code in <code>mindcv\\data\\auto_augment.py</code> <pre><code>def trivial_augment_wide_transform(configs, hparams):\n    \"\"\"\n    Create a TrivialAugmentWide transform\n    Args:\n        configs: A string that defines the TrivialAugmentWide configuration.\n            It is composed of multiple parts separated by dashes (\"-\").\n            The first part defines the AutoAugment name, it should be 'trivialaugwide'.\n            the second part(not necessary) the maximum value of magnitude.\n\n            - m: final magnitude of a operation will uniform sampling from [0, m] . Default: 31\n\n            Example: 'trivialaugwide-m20' will be TrivialAugmentWide\n            with mgnitude uniform sampling from [0, 20],\n        hparams: Other hparams (kwargs) for the TrivialAugment scheme.\n    Returns:\n        A Mindspore compatible Transform\n    \"\"\"\n    magnitude = 31\n    transforms = _TRIVIALAUGMENT_WIDE_TRANSFORMS\n    config = configs.split(\"-\")\n    assert config[0] == \"trivialaugwide\"\n    config = config[1:]\n    for c in config:\n        cs = re.split(r\"(\\d.*)\", c)\n        if len(cs) &lt; 2:\n            continue\n        key, val = cs[:2]\n        if key == \"m\":\n            magnitude = int(val)\n        else:\n            assert False, \"Unknown TrivialAugmentWide config section\"\n    if not hparams:\n        hparams = dict()\n    hparams[\"magnitude_max\"] = magnitude\n    hparams[\"magnitude_std\"] = float(\"inf\")  # default to uniform sampling\n    hparams[\"trivialaugwide\"] = True\n    ta_ops = trivial_augment_wide_ops(magnitude=magnitude, hparams=hparams, transforms=transforms)\n    return TrivialAugmentWide(ta_ops)\n</code></pre>"},{"location":"reference/data/#mindcv.data.auto_augment.augment_and_mix_transform","title":"<code>mindcv.data.auto_augment.augment_and_mix_transform(configs, hparams=None)</code>","text":"<p>Create AugMix PyTorch transform</p> PARAMETER DESCRIPTION <code>configs</code> <p>String defining configuration of AugMix augmentation. Consists of multiple sections separated by dashes ('-'). The first section defines the specific name of augment, it should be 'augmix'. The remaining sections, not order sepecific determine     'm' - integer magnitude (severity) of augmentation mix (default: 3)     'w' - integer width of augmentation chain (default: 3)     'd' - integer depth of augmentation chain (-1 is random [1, 3], default: -1)     'a' - integer or float, the args of beta deviation of beta for generate the weight, default 1.. Ex 'augmix-m5-w4-d2' results in AugMix with severity 5, chain width 4, chain depth 2</p> <p> TYPE: <code>str</code> </p> <code>hparams</code> <p>Other hparams (kwargs) for the Augmentation transforms</p> <p> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <p>A Mindspore compatible Transform</p> Source code in <code>mindcv\\data\\auto_augment.py</code> <pre><code>def augment_and_mix_transform(configs, hparams=None):\n    \"\"\"Create AugMix PyTorch transform\n\n    Args:\n        configs (str): String defining configuration of AugMix augmentation. Consists of multiple sections separated\n            by dashes ('-'). The first section defines the specific name of augment, it should be 'augmix'.\n            The remaining sections, not order sepecific determine\n                'm' - integer magnitude (severity) of augmentation mix (default: 3)\n                'w' - integer width of augmentation chain (default: 3)\n                'd' - integer depth of augmentation chain (-1 is random [1, 3], default: -1)\n                'a' - integer or float, the args of beta deviation of beta for generate the weight, default 1..\n            Ex 'augmix-m5-w4-d2' results in AugMix with severity 5, chain width 4, chain depth 2\n\n        hparams: Other hparams (kwargs) for the Augmentation transforms\n\n    Returns:\n         A Mindspore compatible Transform\n    \"\"\"\n    magnitude = 3\n    width = 3\n    depth = -1\n    alpha = 1.0\n    config = configs.split(\"-\")\n    assert config[0] == \"augmix\"\n    config = config[1:]\n    for c in config:\n        cs = re.split(r\"(\\d.*)\", c)\n        if len(cs) &lt; 2:\n            continue\n        key, val = cs[:2]\n        if key == \"m\":\n            magnitude = int(val)\n        elif key == \"w\":\n            width = int(val)\n        elif key == \"d\":\n            depth = int(val)\n        elif key == \"a\":\n            alpha = float(val)\n        else:\n            assert False, \"Unknown AugMix config section\"\n    if not hparams:\n        hparams = dict()\n    hparams[\"magnitude_std\"] = float(\"inf\")  # default to uniform sampling (if not set via mstd arg)\n    ops = augmix_ops(magnitude=magnitude, hparams=hparams)\n    return AugMixAugment(ops, alpha=alpha, width=width, depth=depth)\n</code></pre>"},{"location":"reference/data/#dataset-factory","title":"Dataset Factory","text":""},{"location":"reference/data/#mindcv.data.dataset_factory.create_dataset","title":"<code>mindcv.data.dataset_factory.create_dataset(name='', root=None, split='train', shuffle=True, num_samples=None, num_shards=None, shard_id=None, num_parallel_workers=None, download=False, num_aug_repeats=0, **kwargs)</code>","text":"<p>Creates dataset by name.</p> PARAMETER DESCRIPTION <code>name</code> <p>dataset name like MNIST, CIFAR10, ImageNeT, ''. '' means a customized dataset. Default: ''.</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> <code>root</code> <p>dataset root dir. Default: None.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>split</code> <p>data split: '' or split name string (train/val/test), if it is '', no split is used. Otherwise, it is a subfolder of root dir, e.g., train, val, test. Default: 'train'.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'train'</code> </p> <code>shuffle</code> <p>whether to shuffle the dataset. Default: True.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>num_samples</code> <p>Number of elements to sample (default=None, which means sample all elements).</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>num_shards</code> <p>Number of shards that the dataset will be divided into (default=None). When this argument is specified, <code>num_samples</code> reflects the maximum sample number of per shard.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>shard_id</code> <p>The shard ID within <code>num_shards</code> (default=None). This argument can only be specified when <code>num_shards</code> is also specified.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>num_parallel_workers</code> <p>Number of workers to read the data (default=None, set in the config).</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>download</code> <p>whether to download the dataset. Default: False</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>num_aug_repeats</code> <p>Number of dataset repetition for repeated augmentation. If 0 or 1, repeated augmentation is disabled. Otherwise, repeated augmentation is enabled and the common choice is 3. (Default: 0)</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> Note <p>For custom datasets and imagenet, the dataset dir should follow the structure like: .dataset_name/ \u251c\u2500\u2500 split1/ \u2502  \u251c\u2500\u2500 class1/ \u2502  \u2502   \u251c\u2500\u2500 000001.jpg \u2502  \u2502   \u251c\u2500\u2500 000002.jpg \u2502  \u2502   \u2514\u2500\u2500 .... \u2502  \u2514\u2500\u2500 class2/ \u2502      \u251c\u2500\u2500 000001.jpg \u2502      \u251c\u2500\u2500 000002.jpg \u2502      \u2514\u2500\u2500 .... \u2514\u2500\u2500 split2/    \u251c\u2500\u2500 class1/    \u2502   \u251c\u2500\u2500 000001.jpg    \u2502   \u251c\u2500\u2500 000002.jpg    \u2502   \u2514\u2500\u2500 ....    \u2514\u2500\u2500 class2/        \u251c\u2500\u2500 000001.jpg        \u251c\u2500\u2500 000002.jpg        \u2514\u2500\u2500 ....</p> RETURNS DESCRIPTION <p>Dataset object</p> Source code in <code>mindcv\\data\\dataset_factory.py</code> <pre><code>def create_dataset(\n    name: str = \"\",\n    root: Optional[str] = None,\n    split: str = \"train\",\n    shuffle: bool = True,\n    num_samples: Optional[int] = None,\n    num_shards: Optional[int] = None,\n    shard_id: Optional[int] = None,\n    num_parallel_workers: Optional[int] = None,\n    download: bool = False,\n    num_aug_repeats: int = 0,\n    **kwargs,\n):\n    r\"\"\"Creates dataset by name.\n\n    Args:\n        name: dataset name like MNIST, CIFAR10, ImageNeT, ''. '' means a customized dataset. Default: ''.\n        root: dataset root dir. Default: None.\n        split: data split: '' or split name string (train/val/test), if it is '', no split is used.\n            Otherwise, it is a subfolder of root dir, e.g., train, val, test. Default: 'train'.\n        shuffle: whether to shuffle the dataset. Default: True.\n        num_samples: Number of elements to sample (default=None, which means sample all elements).\n        num_shards: Number of shards that the dataset will be divided into (default=None).\n            When this argument is specified, `num_samples` reflects the maximum sample number of per shard.\n        shard_id: The shard ID within `num_shards` (default=None).\n            This argument can only be specified when `num_shards` is also specified.\n        num_parallel_workers: Number of workers to read the data (default=None, set in the config).\n        download: whether to download the dataset. Default: False\n        num_aug_repeats: Number of dataset repetition for repeated augmentation.\n            If 0 or 1, repeated augmentation is disabled.\n            Otherwise, repeated augmentation is enabled and the common choice is 3. (Default: 0)\n\n    Note:\n        For custom datasets and imagenet, the dataset dir should follow the structure like:\n        .dataset_name/\n        \u251c\u2500\u2500 split1/\n        \u2502  \u251c\u2500\u2500 class1/\n        \u2502  \u2502   \u251c\u2500\u2500 000001.jpg\n        \u2502  \u2502   \u251c\u2500\u2500 000002.jpg\n        \u2502  \u2502   \u2514\u2500\u2500 ....\n        \u2502  \u2514\u2500\u2500 class2/\n        \u2502      \u251c\u2500\u2500 000001.jpg\n        \u2502      \u251c\u2500\u2500 000002.jpg\n        \u2502      \u2514\u2500\u2500 ....\n        \u2514\u2500\u2500 split2/\n           \u251c\u2500\u2500 class1/\n           \u2502   \u251c\u2500\u2500 000001.jpg\n           \u2502   \u251c\u2500\u2500 000002.jpg\n           \u2502   \u2514\u2500\u2500 ....\n           \u2514\u2500\u2500 class2/\n               \u251c\u2500\u2500 000001.jpg\n               \u251c\u2500\u2500 000002.jpg\n               \u2514\u2500\u2500 ....\n\n    Returns:\n        Dataset object\n    \"\"\"\n    name = name.lower()\n    if root is None:\n        root = os.path.join(get_dataset_download_root(), name)\n\n    assert (num_samples is None) or (num_aug_repeats == 0), \"num_samples and num_aug_repeats can NOT be set together.\"\n\n    # subset sampling\n    if num_samples is not None and num_samples &gt; 0:\n        # TODO: rewrite ordered distributed sampler (subset sampling in distributed mode is not tested)\n        if num_shards is not None and num_shards &gt; 1:  # distributed\n            _logger.info(f\"number of shards: {num_shards}, number of samples: {num_samples}\")\n            sampler = DistributedSampler(num_shards, shard_id, shuffle=shuffle, num_samples=num_samples)\n        else:  # standalone\n            if shuffle:\n                sampler = ds.RandomSampler(replacement=False, num_samples=num_samples)\n            else:\n                sampler = ds.SequentialSampler(num_samples=num_samples)\n        mindspore_kwargs = dict(\n            shuffle=None,\n            sampler=sampler,\n            num_parallel_workers=num_parallel_workers,\n            **kwargs,\n        )\n    else:\n        sampler = None\n        mindspore_kwargs = dict(\n            shuffle=shuffle,\n            sampler=sampler,\n            num_shards=num_shards,\n            shard_id=shard_id,\n            num_parallel_workers=num_parallel_workers,\n            **kwargs,\n        )\n\n    # sampler for repeated augmentation\n    if num_aug_repeats &gt; 0:\n        dataset_size = get_dataset_size(name, root, split)\n        _logger.info(\n            f\"Repeated augmentation is enabled, num_aug_repeats: {num_aug_repeats}, \"\n            f\"original dataset size: {dataset_size}.\"\n        )\n        # since drop_remainder is usually True, we don't need to do rounding in sampling\n        sampler = RepeatAugSampler(\n            dataset_size,\n            num_shards=num_shards,\n            rank_id=shard_id,\n            num_repeats=num_aug_repeats,\n            selected_round=0,\n            shuffle=shuffle,\n        )\n        mindspore_kwargs = dict(shuffle=None, sampler=sampler, num_shards=None, shard_id=None, **kwargs)\n\n    # create dataset\n    if name in _MINDSPORE_BASIC_DATASET:\n        dataset_class = _MINDSPORE_BASIC_DATASET[name][0]\n        dataset_download = _MINDSPORE_BASIC_DATASET[name][1]\n        dataset_new_path = None\n        if download:\n            if shard_id is not None:\n                root = os.path.join(root, f\"dataset_{str(shard_id)}\")\n            dataset_download = dataset_download(root)\n            dataset_download.download()\n            dataset_new_path = dataset_download.path\n\n        dataset = dataset_class(\n            dataset_dir=dataset_new_path if dataset_new_path else root,\n            usage=split,\n            **mindspore_kwargs,\n        )\n        # address ms dataset num_classes empty issue\n        if name == \"mnist\":\n            dataset.num_classes = lambda: 10\n        elif name == \"cifar10\":\n            dataset.num_classes = lambda: 10\n        elif name == \"cifar100\":\n            dataset.num_classes = lambda: 100\n\n    else:\n        if name == \"imagenet\" and download:\n            raise ValueError(\n                \"Imagenet dataset download is not supported. \"\n                \"Please download imagenet from https://www.image-net.org/download.php, \"\n                \"and parse the path of dateset directory via args.data_dir.\"\n            )\n\n        if os.path.isdir(root):\n            root = os.path.join(root, split)\n        dataset = ImageFolderDataset(dataset_dir=root, **mindspore_kwargs)\n        \"\"\" Another implementation which a bit slower than ImageFolderDataset\n            imagenet_dataset = ImageNetDataset(dataset_dir=root)\n            sampler = RepeatAugSampler(len(imagenet_dataset), num_shards=num_shards, rank_id=shard_id,\n                                       num_repeats=repeated_aug, selected_round=1, shuffle=shuffle)\n            dataset = ds.GeneratorDataset(imagenet_dataset, column_names=imagenet_dataset.column_names, sampler=sampler)\n        \"\"\"\n    return dataset\n</code></pre>"},{"location":"reference/data/#sampler","title":"Sampler","text":""},{"location":"reference/data/#mindcv.data.distributed_sampler.RepeatAugSampler","title":"<code>mindcv.data.distributed_sampler.RepeatAugSampler</code>","text":"<p>Sampler that restricts data loading to a subset of the dataset for distributed, with repeated augmentation. It ensures that different each augmented version of a sample will be visible to a different process.</p> <p>This sampler was adapted from https://github.com/facebookresearch/deit/blob/0c4b8f60/samplers.py</p> PARAMETER DESCRIPTION <code>dataset_size</code> <p>dataset size.</p> <p> </p> <code>num_shards</code> <p>num devices.</p> <p> DEFAULT: <code>None</code> </p> <code>rank_id</code> <p>device id.</p> <p> DEFAULT: <code>None</code> </p> <code>shuffle(bool)</code> <p>True for using shuffle, False for not using.</p> <p> </p> <code>num_repeats(int)</code> <p>num of repeated instances in repeated augmentation, Default:3.</p> <p> </p> <code>selected_round(int)</code> <p>round the total num of samples by this factor, Defailt:256.</p> <p> </p> Source code in <code>mindcv\\data\\distributed_sampler.py</code> <pre><code>class RepeatAugSampler:\n    \"\"\"Sampler that restricts data loading to a subset of the dataset for distributed,\n    with repeated augmentation.\n    It ensures that different each augmented version of a sample will be visible to a\n    different process.\n\n    This sampler was adapted from https://github.com/facebookresearch/deit/blob/0c4b8f60/samplers.py\n\n    Args:\n        dataset_size: dataset size.\n        num_shards: num devices.\n        rank_id: device id.\n        shuffle(bool): True for using shuffle, False for not using.\n        num_repeats(int): num of repeated instances in repeated augmentation, Default:3.\n        selected_round(int): round the total num of samples by this factor, Defailt:256.\n    \"\"\"\n\n    def __init__(\n        self,\n        dataset_size,\n        num_shards=None,\n        rank_id=None,\n        shuffle=True,\n        num_repeats=3,\n        selected_round=256,\n    ):\n        if num_shards is None:\n            _logger.warning(\"num_shards is set to 1 in RepeatAugSampler since it is not passed in\")\n            num_shards = 1\n        if rank_id is None:\n            rank_id = 0\n\n        # assert isinstance(num_repeats, int), f'num_repeats should be Type integer, but got {type(num_repeats)}'\n\n        self.dataset_size = dataset_size\n        self.num_shards = num_shards\n        self.rank_id = rank_id\n        self.shuffle = shuffle\n        self.num_repeats = int(num_repeats)\n        self.epoch = 0\n        self.num_samples = int(math.ceil(self.dataset_size * num_repeats / self.num_shards))\n        self.total_size = self.num_samples * self.num_shards\n        # Determine the number of samples to select per epoch for each rank.\n        if selected_round:\n            self.num_selected_samples = int(\n                math.floor(self.dataset_size // selected_round * selected_round / num_shards)\n            )\n        else:\n            self.num_selected_samples = int(math.ceil(self.dataset_size / num_shards))\n\n    def __iter__(self):\n        # deterministically shuffle based on epoch\n        # print('__iter__  generating new shuffled indices: ', self.epoch)\n        if self.shuffle:\n            indices = np.random.RandomState(seed=self.epoch).permutation(self.dataset_size)\n            indices = indices.tolist()\n            self.epoch += 1\n            # print(indices[:30])\n        else:\n            indices = list(range(self.dataset_size))\n        # produce repeats e.g. [0, 0, 0, 1, 1, 1, 2, 2, 2....]\n        indices = [ele for ele in indices for i in range(self.num_repeats)]\n\n        # add extra samples to make it evenly divisible\n        padding_size = self.total_size - len(indices)\n        if padding_size &gt; 0:\n            indices += indices[:padding_size]\n        assert len(indices) == self.total_size\n\n        # subsample per rank\n        indices = indices[self.rank_id : self.total_size : self.num_shards]\n        assert len(indices) == self.num_samples\n\n        # return up to num selected samples\n        return iter(indices[: self.num_selected_samples])\n\n    def __len__(self):\n        return self.num_selected_samples\n\n    def set_epoch(self, epoch):\n        self.epoch = epoch\n</code></pre>"},{"location":"reference/data/#dataloader","title":"DataLoader","text":""},{"location":"reference/data/#mindcv.data.loader.create_loader","title":"<code>mindcv.data.loader.create_loader(dataset, batch_size, drop_remainder=False, is_training=False, mixup=0.0, cutmix=0.0, cutmix_prob=0.0, num_classes=1000, transform=None, target_transform=None, num_parallel_workers=None, python_multiprocessing=False, separate=False)</code>","text":"<p>Creates dataloader.</p> <p>Applies operations such as transform and batch to the <code>ms.dataset.Dataset</code> object created by the <code>create_dataset</code> function to get the dataloader.</p> PARAMETER DESCRIPTION <code>dataset</code> <p>dataset object created by <code>create_dataset</code>.</p> <p> TYPE: <code>Dataset</code> </p> <code>batch_size</code> <p>The number of rows each batch is created with. An int or callable object which takes exactly 1 parameter, BatchInfo.</p> <p> TYPE: <code>int or function</code> </p> <code>drop_remainder</code> <p>Determines whether to drop the last block whose data row number is less than batch size (default=False). If True, and if there are less than batch_size rows available to make the last batch, then those rows will be dropped and not propagated to the child node.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>is_training</code> <p>whether it is in train mode. Default: False.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>mixup</code> <p>mixup alpha, mixup will be enabled if &gt; 0. (default=0.0).</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>cutmix</code> <p>cutmix alpha, cutmix will be enabled if &gt; 0. (default=0.0). This operation is experimental.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>cutmix_prob</code> <p>prob of doing cutmix for an image (default=0.0)</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>num_classes</code> <p>the number of classes. Default: 1000.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1000</code> </p> <code>transform</code> <p>the list of transformations that wil be applied on the image, which is obtained by <code>create_transform</code>. If None, the default imagenet transformation for evaluation will be applied. Default: None.</p> <p> TYPE: <code>list or None</code> DEFAULT: <code>None</code> </p> <code>target_transform</code> <p>the list of transformations that will be applied on the label. If None, the label will be converted to the type of ms.int32. Default: None.</p> <p> TYPE: <code>list or None</code> DEFAULT: <code>None</code> </p> <code>num_parallel_workers</code> <p>Number of workers(threads) to process the dataset in parallel (default=None).</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>python_multiprocessing</code> <p>Parallelize Python operations with multiple worker processes. This option could be beneficial if the Python operation is computational heavy (default=False).</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>separate(bool,</code> <p>separate the image clean and the image been transformed. If separate==True, that means the dataset returned has 3 parts: * the first part called image \"clean\", which means the image without auto_augment (e.g., auto-aug) * the second and third parts called image transformed, hence, with the auto_augment transform. Refer to \".transforms_factory.create_transforms\" for more information.</p> <p> TYPE: <code>optional</code> </p> Note <ol> <li>cutmix is now experimental (which means performance gain is not guarantee)     and can not be used together with mixup due to the label int type conflict.</li> <li><code>is_training</code>, <code>mixup</code>, <code>num_classes</code> is used for MixUp, which is a kind of transform operation.   However, we are not able to merge it into <code>transform</code>, due to the limitations of the <code>mindspore.dataset</code> API.</li> </ol> RETURNS DESCRIPTION <p>BatchDataset, dataset batched.</p> Source code in <code>mindcv\\data\\loader.py</code> <pre><code>def create_loader(\n    dataset,\n    batch_size,\n    drop_remainder=False,\n    is_training=False,\n    mixup=0.0,\n    cutmix=0.0,\n    cutmix_prob=0.0,\n    num_classes=1000,\n    transform=None,\n    target_transform=None,\n    num_parallel_workers=None,\n    python_multiprocessing=False,\n    separate=False,\n):\n    r\"\"\"Creates dataloader.\n\n    Applies operations such as transform and batch to the `ms.dataset.Dataset` object\n    created by the `create_dataset` function to get the dataloader.\n\n    Args:\n        dataset (ms.dataset.Dataset): dataset object created by `create_dataset`.\n        batch_size (int or function): The number of rows each batch is created with. An\n            int or callable object which takes exactly 1 parameter, BatchInfo.\n        drop_remainder (bool, optional): Determines whether to drop the last block\n            whose data row number is less than batch size (default=False). If True, and if there are less\n            than batch_size rows available to make the last batch, then those rows will\n            be dropped and not propagated to the child node.\n        is_training (bool): whether it is in train mode. Default: False.\n        mixup (float): mixup alpha, mixup will be enabled if &gt; 0. (default=0.0).\n        cutmix (float): cutmix alpha, cutmix will be enabled if &gt; 0. (default=0.0). This operation is experimental.\n        cutmix_prob (float): prob of doing cutmix for an image (default=0.0)\n        num_classes (int): the number of classes. Default: 1000.\n        transform (list or None): the list of transformations that wil be applied on the image,\n            which is obtained by `create_transform`. If None, the default imagenet transformation\n            for evaluation will be applied. Default: None.\n        target_transform (list or None): the list of transformations that will be applied on the label.\n            If None, the label will be converted to the type of ms.int32. Default: None.\n        num_parallel_workers (int, optional): Number of workers(threads) to process the dataset in parallel\n            (default=None).\n        python_multiprocessing (bool, optional): Parallelize Python operations with multiple worker processes. This\n            option could be beneficial if the Python operation is computational heavy (default=False).\n        separate(bool, optional): separate the image clean and the image been transformed.\n            If separate==True, that means the dataset returned has 3 parts:\n            * the first part called image \"clean\", which means the image without auto_augment (e.g., auto-aug)\n            * the second and third parts called image transformed, hence, with the auto_augment transform.\n            Refer to \".transforms_factory.create_transforms\" for more information.\n\n    Note:\n        1. cutmix is now experimental (which means performance gain is not guarantee)\n            and can not be used together with mixup due to the label int type conflict.\n        2. `is_training`, `mixup`, `num_classes` is used for MixUp, which is a kind of transform operation.\n          However, we are not able to merge it into `transform`, due to the limitations of the `mindspore.dataset` API.\n\n\n    Returns:\n        BatchDataset, dataset batched.\n    \"\"\"\n\n    if target_transform is None:\n        target_transform = transforms.TypeCast(ms.int32)\n    target_input_columns = \"label\" if \"label\" in dataset.get_col_names() else \"fine_label\"\n    dataset = dataset.map(\n        operations=target_transform,\n        input_columns=target_input_columns,\n        num_parallel_workers=num_parallel_workers,\n        python_multiprocessing=python_multiprocessing,\n    )\n\n    if transform is None:\n        warnings.warn(\n            \"Using None as the default value of transform will set it back to \"\n            \"traditional image transform, which is not recommended. \"\n            \"You should explicitly call `create_transforms` and pass it to `create_loader`.\"\n        )\n        transform = create_transforms(\"imagenet\", is_training=False)\n\n    # only apply augment splits to train dataset\n    if separate and is_training:\n        assert isinstance(transform, tuple) and len(transform) == 3\n\n        # Note: mindspore-2.0 delete the parameter column_order\n        sig = inspect.signature(dataset.map)\n        pass_column_order = False if \"kwargs\" in sig.parameters else True\n\n        # map all the transform\n        dataset = map_transform_splits(\n            dataset, transform, num_parallel_workers, python_multiprocessing, pass_column_order\n        )\n        # after batch, datasets has 4 columns\n        dataset = dataset.batch(batch_size=batch_size, drop_remainder=drop_remainder)\n        # concat the 3 columns of image\n        dataset = dataset.map(\n            operations=concat_per_batch_map,\n            input_columns=[\"image_clean\", \"image_aug1\", \"image_aug2\", \"label\"],\n            output_columns=[\"image\", \"label\"],\n            column_order=[\"image\", \"label\"] if pass_column_order else None,\n            num_parallel_workers=num_parallel_workers,\n            python_multiprocessing=python_multiprocessing,\n        )\n\n    else:\n        dataset = dataset.map(\n            operations=transform,\n            input_columns=\"image\",\n            num_parallel_workers=num_parallel_workers,\n            python_multiprocessing=python_multiprocessing,\n        )\n\n        dataset = dataset.batch(batch_size=batch_size, drop_remainder=drop_remainder)\n\n    if is_training:\n        if (mixup + cutmix &gt; 0.0) and batch_size &gt; 1:\n            # TODO: use mindspore vision cutmix and mixup after the confliction fixed in later release\n            # set label_smoothing 0 here since label smoothing is computed in loss module\n            mixup_fn = Mixup(\n                mixup_alpha=mixup,\n                cutmix_alpha=cutmix,\n                cutmix_minmax=None,\n                prob=cutmix_prob,\n                switch_prob=0.5,\n                label_smoothing=0.0,\n                num_classes=num_classes,\n            )\n            # images in a batch are mixed. labels are converted soft onehot labels.\n            dataset = dataset.map(\n                operations=mixup_fn,\n                input_columns=[\"image\", target_input_columns],\n                num_parallel_workers=num_parallel_workers,\n            )\n\n    return dataset\n</code></pre>"},{"location":"reference/data/#mixup","title":"MixUp","text":""},{"location":"reference/data/#mindcv.data.mixup.Mixup","title":"<code>mindcv.data.mixup.Mixup</code>","text":"<p>Mixup/Cutmix that applies different params to each element or whole batch</p> PARAMETER DESCRIPTION <code>mixup_alpha</code> <p>mixup alpha value, mixup is active if &gt; 0.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>cutmix_alpha</code> <p>cutmix alpha value, cutmix is active if &gt; 0.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>cutmix_minmax</code> <p>cutmix min/max image ratio, cutmix is active and uses this vs alpha if not None.</p> <p> TYPE: <code>List[float]</code> DEFAULT: <code>None</code> </p> <code>prob</code> <p>probability of applying mixup or cutmix per batch or element</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>switch_prob</code> <p>probability of switching to cutmix instead of mixup when both are active</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.5</code> </p> <code>mode</code> <p>how to apply mixup/cutmix params (per 'batch', 'pair' (pair of elements), 'elem' (element)</p> <p> TYPE: <code>str</code> DEFAULT: <code>'batch'</code> </p> <code>correct_lam</code> <p>apply lambda correction when cutmix bbox clipped by image borders</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>label_smoothing</code> <p>apply label smoothing to the mixed target tensor</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.1</code> </p> <code>num_classes</code> <p>number of classes for target</p> <p> TYPE: <code>int</code> DEFAULT: <code>1000</code> </p> Source code in <code>mindcv\\data\\mixup.py</code> <pre><code>class Mixup:\n    \"\"\"Mixup/Cutmix that applies different params to each element or whole batch\n\n    Args:\n        mixup_alpha (float): mixup alpha value, mixup is active if &gt; 0.\n        cutmix_alpha (float): cutmix alpha value, cutmix is active if &gt; 0.\n        cutmix_minmax (List[float]): cutmix min/max image ratio, cutmix is active and uses this vs alpha if not None.\n        prob (float): probability of applying mixup or cutmix per batch or element\n        switch_prob (float): probability of switching to cutmix instead of mixup when both are active\n        mode (str): how to apply mixup/cutmix params (per 'batch', 'pair' (pair of elements), 'elem' (element)\n        correct_lam (bool): apply lambda correction when cutmix bbox clipped by image borders\n        label_smoothing (float): apply label smoothing to the mixed target tensor\n        num_classes (int): number of classes for target\n    \"\"\"\n\n    def __init__(\n        self,\n        mixup_alpha=1.0,\n        cutmix_alpha=0.0,\n        cutmix_minmax=None,\n        prob=1.0,\n        switch_prob=0.5,\n        mode=\"batch\",\n        correct_lam=True,\n        label_smoothing=0.1,\n        num_classes=1000,\n    ):\n        self.mixup_alpha = mixup_alpha\n        self.cutmix_alpha = cutmix_alpha\n        self.cutmix_minmax = cutmix_minmax\n        if self.cutmix_minmax is not None:\n            assert len(self.cutmix_minmax) == 2\n            # force cutmix alpha == 1.0 when minmax active to keep logic simple &amp; safe\n            self.cutmix_alpha = 1.0\n        self.mix_prob = prob\n        self.switch_prob = switch_prob\n        self.label_smoothing = label_smoothing\n        self.num_classes = num_classes\n        self.mode = mode\n        self.correct_lam = correct_lam  # correct lambda based on clipped area for cutmix\n        self.mixup_enabled = True  # set false to disable mixing (intended tp be set by train loop)\n\n    def _params_per_elem(self, batch_size):\n        \"\"\"_params_per_elem\"\"\"\n        lam = np.ones(batch_size, dtype=np.float32)\n        use_cutmix = np.zeros(batch_size, dtype=np.bool_)\n        if self.mixup_enabled:\n            if self.mixup_alpha &gt; 0.0 and self.cutmix_alpha &gt; 0.0:\n                use_cutmix = np.random.rand(batch_size) &lt; self.switch_prob\n                lam_mix = np.where(\n                    use_cutmix,\n                    np.random.beta(self.cutmix_alpha, self.cutmix_alpha, size=batch_size),\n                    np.random.beta(self.mixup_alpha, self.mixup_alpha, size=batch_size),\n                )\n            elif self.mixup_alpha &gt; 0.0:\n                lam_mix = np.random.beta(self.mixup_alpha, self.mixup_alpha, size=batch_size)\n            elif self.cutmix_alpha &gt; 0.0:\n                use_cutmix = np.ones(batch_size, dtype=np.bool_)\n                lam_mix = np.random.beta(self.cutmix_alpha, self.cutmix_alpha, size=batch_size)\n            else:\n                assert False, \"One of mixup_alpha &gt; 0., cutmix_alpha &gt; 0., cutmix_minmax not None should be true.\"\n            lam = np.where(np.random.rand(batch_size) &lt; self.mix_prob, lam_mix.astype(np.float32), lam)\n        return lam, use_cutmix\n\n    def _params_per_batch(self):\n        \"\"\"_params_per_batch\"\"\"\n        lam = 1.0\n        use_cutmix = False\n        if self.mixup_enabled and np.random.rand() &lt; self.mix_prob:\n            if self.mixup_alpha &gt; 0.0 and self.cutmix_alpha &gt; 0.0:\n                use_cutmix = np.random.rand() &lt; self.switch_prob\n                lam_mix = (\n                    np.random.beta(self.cutmix_alpha, self.cutmix_alpha)\n                    if use_cutmix\n                    else np.random.beta(self.mixup_alpha, self.mixup_alpha)\n                )\n            elif self.mixup_alpha &gt; 0.0:\n                lam_mix = np.random.beta(self.mixup_alpha, self.mixup_alpha)\n            elif self.cutmix_alpha &gt; 0.0:\n                use_cutmix = True\n                lam_mix = np.random.beta(self.cutmix_alpha, self.cutmix_alpha)\n            else:\n                assert False, \"One of mixup_alpha &gt; 0., cutmix_alpha &gt; 0., cutmix_minmax not None should be true.\"\n            lam = float(lam_mix)\n        return lam, use_cutmix\n\n    def _mix_elem(self, x):\n        \"\"\"_mix_elem\"\"\"\n        batch_size = len(x)\n        lam_batch, use_cutmix = self._params_per_elem(batch_size)\n        x_orig = x.clone()  # need to keep an unmodified original for mixing source\n        for i in range(batch_size):\n            j = batch_size - i - 1\n            lam = lam_batch[i]\n            if lam != 1.0:\n                if use_cutmix[i]:\n                    (yl, yh, xl, xh), lam = cutmix_bbox_and_lam(\n                        x[i].shape, lam, ratio_minmax=self.cutmix_minmax, correct_lam=self.correct_lam\n                    )\n                    x[i][:, yl:yh, xl:xh] = x_orig[j][:, yl:yh, xl:xh]\n                    lam_batch[i] = lam\n                else:\n                    x[i] = x[i] * lam + x_orig[j] * (1 - lam)\n        return P.ExpandDims()(Tensor(lam_batch, dtype=mstype.float32), 1)\n\n    def _mix_pair(self, x):\n        \"\"\"_mix_pair\"\"\"\n        batch_size = len(x)\n        lam_batch, use_cutmix = self._params_per_elem(batch_size // 2)\n        x_orig = x.clone()  # need to keep an unmodified original for mixing source\n        for i in range(batch_size // 2):\n            j = batch_size - i - 1\n            lam = lam_batch[i]\n            if lam != 1.0:\n                if use_cutmix[i]:\n                    (yl, yh, xl, xh), lam = cutmix_bbox_and_lam(\n                        x[i].shape, lam, ratio_minmax=self.cutmix_minmax, correct_lam=self.correct_lam\n                    )\n                    x[i][:, yl:yh, xl:xh] = x_orig[j][:, yl:yh, xl:xh]\n                    x[j][:, yl:yh, xl:xh] = x_orig[i][:, yl:yh, xl:xh]\n                    lam_batch[i] = lam\n                else:\n                    x[i] = x[i] * lam + x_orig[j] * (1 - lam)\n                    x[j] = x[j] * lam + x_orig[i] * (1 - lam)\n        lam_batch = np.concatenate((lam_batch, lam_batch[::-1]))\n        return P.ExpandDims()(Tensor(lam_batch, dtype=mstype.float32), 1)\n\n    def _mix_batch(self, x):\n        \"\"\"_mix_batch\"\"\"\n        lam, use_cutmix = self._params_per_batch()\n        if lam == 1.0:\n            return 1.0\n        if use_cutmix:\n            (yl, yh, xl, xh), lam = cutmix_bbox_and_lam(\n                x.shape, lam, ratio_minmax=self.cutmix_minmax, correct_lam=self.correct_lam\n            )\n            x[:, :, yl:yh, xl:xh] = np.flip(x, axis=0)[:, :, yl:yh, xl:xh]\n        else:\n            x_flipped = np.flip(x, axis=0) * (1.0 - lam)\n            x *= lam\n            x += x_flipped\n        return lam\n\n    def __call__(self, x, target):\n        \"\"\"Mixup apply\"\"\"\n        # the same to image, label\n        assert len(x) % 2 == 0, \"Batch size should be even when using this\"\n        if self.mode == \"elem\":\n            lam = self._mix_elem(x)\n        elif self.mode == \"pair\":\n            lam = self._mix_pair(x)\n        else:\n            lam = self._mix_batch(x)\n        target = mixup_target(target, self.num_classes, lam, self.label_smoothing)\n        return x.astype(np.float32), target.astype(np.float32)\n</code></pre>"},{"location":"reference/data/#transform-factory","title":"Transform Factory","text":""},{"location":"reference/data/#mindcv.data.transforms_factory.create_transforms","title":"<code>mindcv.data.transforms_factory.create_transforms(dataset_name='', image_resize=224, is_training=False, auto_augment=None, separate=False, **kwargs)</code>","text":"<p>Creates a list of transform operation on image data.</p> PARAMETER DESCRIPTION <code>dataset_name</code> <p>if '', customized dataset. Currently, apply the same transform pipeline as ImageNet. if standard dataset name is given including imagenet, cifar10, mnist, preset transforms will be returned. Default: ''.</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> <code>image_resize</code> <p>the image size after resize for adapting to network. Default: 224.</p> <p> TYPE: <code>int</code> DEFAULT: <code>224</code> </p> <code>is_training</code> <p>if True, augmentation will be applied if support. Default: False.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>separate</code> <p>separate the image clean and the image been transformed. If separate==True, the transformers are returned as a tuple of 3 separate transforms for use in a mixing dataset that  passes: * all data through the primary transform, called \"clean\" data * a portion of the data through the secondary transform (e.g., auto-aug) * normalized and converts the branches above with the third, transform</p> <p> DEFAULT: <code>False</code> </p> <code>**kwargs</code> <p>additional args parsed to <code>transforms_imagenet_train</code> and <code>transforms_imagenet_eval</code></p> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <p>A list of transformation operations</p> Source code in <code>mindcv\\data\\transforms_factory.py</code> <pre><code>def create_transforms(\n    dataset_name=\"\",\n    image_resize=224,\n    is_training=False,\n    auto_augment=None,\n    separate=False,\n    **kwargs,\n):\n    r\"\"\"Creates a list of transform operation on image data.\n\n    Args:\n        dataset_name (str): if '', customized dataset. Currently, apply the same transform pipeline as ImageNet.\n            if standard dataset name is given including imagenet, cifar10, mnist, preset transforms will be returned.\n            Default: ''.\n        image_resize (int): the image size after resize for adapting to network. Default: 224.\n        is_training (bool): if True, augmentation will be applied if support. Default: False.\n        auto_augment(str)\uff1aaugmentation strategies, such as \"augmix\", \"autoaug\" etc.\n        separate: separate the image clean and the image been transformed. If separate==True, the transformers are\n            returned as a tuple of 3 separate transforms for use in a mixing dataset that  passes:\n            * all data through the primary transform, called \"clean\" data\n            * a portion of the data through the secondary transform (e.g., auto-aug)\n            * normalized and converts the branches above with the third, transform\n        **kwargs: additional args parsed to `transforms_imagenet_train` and `transforms_imagenet_eval`\n\n    Returns:\n        A list of transformation operations\n    \"\"\"\n\n    dataset_name = dataset_name.lower()\n\n    if dataset_name in (\"imagenet\", \"\"):\n        trans_args = dict(image_resize=image_resize, **kwargs)\n        if is_training:\n            return transforms_imagenet_train(auto_augment=auto_augment, separate=separate, **trans_args)\n\n        return transforms_imagenet_eval(**trans_args)\n    elif dataset_name in (\"cifar10\", \"cifar100\"):\n        trans_list = transforms_cifar(resize=image_resize, is_training=is_training)\n        return trans_list\n    elif dataset_name == \"mnist\":\n        trans_list = transforms_mnist(resize=image_resize)\n        return trans_list\n    else:\n        raise NotImplementedError(\n            f\"Only supports creating transforms for ['imagenet'] datasets, but got {dataset_name}.\"\n        )\n</code></pre>"},{"location":"reference/loss/","title":"Loss","text":""},{"location":"reference/loss/#loss-factory","title":"Loss Factory","text":""},{"location":"reference/loss/#mindcv.loss.loss_factory.create_loss","title":"<code>mindcv.loss.loss_factory.create_loss(name='CE', weight=None, reduction='mean', label_smoothing=0.0, aux_factor=0.0)</code>","text":"<p>Creates loss function</p> PARAMETER DESCRIPTION <code>name</code> <p>loss name : 'CE' for cross_entropy. 'BCE': binary cross entropy. Default: 'CE'.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'CE'</code> </p> <code>weight</code> <p>Class weight. A rescaling weight given to the loss of each batch element. If given, has to be a Tensor of size 'nbatch'. Data type must be float16 or float32.</p> <p> TYPE: <code>Tensor</code> DEFAULT: <code>None</code> </p> <code>reduction</code> <p>Apply specific reduction method to the output: 'mean' or 'sum'. By default, the sum of the output will be divided by the number of elements in the output. 'sum': the output will be summed. Default:'mean'.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'mean'</code> </p> <code>label_smoothing</code> <p>Label smoothing factor, a regularization tool used to prevent the model from overfitting when calculating Loss. The value range is [0.0, 1.0]. Default: 0.0.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>aux_factor</code> <p>Auxiliary loss factor. Set aux_factor &gt; 0.0 if the model has auxiliary logit outputs (i.e., deep supervision), like inception_v3. Default: 0.0.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> Inputs <ul> <li>logits (Tensor or Tuple of Tensor): Input logits. Shape [N, C], where N means the number of samples,     C means number of classes. Tuple of two input logits are supported in order (main_logits, aux_logits)     for auxiliary loss used in networks like inception_v3. Data type must be float16 or float32.</li> <li>labels (Tensor): Ground truth labels. Shape: [N] or [N, C].     (1) If in shape [N], sparse labels representing the class indices. Must be int type.     (2) shape [N, C], dense labels representing the ground truth class probability values,     or the one-hot labels. Must be float type. If the loss type is BCE, the shape of labels must be [N, C].</li> </ul> RETURNS DESCRIPTION <p>Loss function to compute the loss between the input logits and labels.</p> Source code in <code>mindcv\\loss\\loss_factory.py</code> <pre><code>def create_loss(\n    name: str = \"CE\",\n    weight: Optional[Tensor] = None,\n    reduction: str = \"mean\",\n    label_smoothing: float = 0.0,\n    aux_factor: float = 0.0,\n):\n    r\"\"\"Creates loss function\n\n    Args:\n        name (str):  loss name : 'CE' for cross_entropy. 'BCE': binary cross entropy. Default: 'CE'.\n        weight (Tensor): Class weight. A rescaling weight given to the loss of each batch element.\n            If given, has to be a Tensor of size 'nbatch'. Data type must be float16 or float32.\n        reduction: Apply specific reduction method to the output: 'mean' or 'sum'.\n            By default, the sum of the output will be divided by the number of elements in the output.\n            'sum': the output will be summed. Default:'mean'.\n        label_smoothing: Label smoothing factor, a regularization tool used to prevent the model\n            from overfitting when calculating Loss. The value range is [0.0, 1.0]. Default: 0.0.\n        aux_factor (float): Auxiliary loss factor. Set aux_factor &gt; 0.0 if the model has auxiliary logit outputs\n            (i.e., deep supervision), like inception_v3. Default: 0.0.\n\n    Inputs:\n        - logits (Tensor or Tuple of Tensor): Input logits. Shape [N, C], where N means the number of samples,\n            C means number of classes. Tuple of two input logits are supported in order (main_logits, aux_logits)\n            for auxiliary loss used in networks like inception_v3. Data type must be float16 or float32.\n        - labels (Tensor): Ground truth labels. Shape: [N] or [N, C].\n            (1) If in shape [N], sparse labels representing the class indices. Must be int type.\n            (2) shape [N, C], dense labels representing the ground truth class probability values,\n            or the one-hot labels. Must be float type. If the loss type is BCE, the shape of labels must be [N, C].\n\n    Returns:\n       Loss function to compute the loss between the input logits and labels.\n    \"\"\"\n    name = name.lower()\n\n    if name == \"ce\":\n        loss = CrossEntropySmooth(smoothing=label_smoothing, aux_factor=aux_factor, reduction=reduction, weight=weight)\n    elif name == \"bce\":\n        loss = BinaryCrossEntropySmooth(\n            smoothing=label_smoothing, aux_factor=aux_factor, reduction=reduction, weight=weight, pos_weight=None\n        )\n    elif name == \"asl_single_label\":\n        loss = AsymmetricLossSingleLabel(smoothing=label_smoothing)\n    elif name == \"asl_multi_label\":\n        loss = AsymmetricLossMultilabel()\n    elif name == \"jsd\":\n        loss = JSDCrossEntropy(smoothing=label_smoothing, aux_factor=aux_factor, reduction=reduction, weight=weight)\n    else:\n        raise NotImplementedError\n\n    return loss\n</code></pre>"},{"location":"reference/loss/#cross-entropy","title":"Cross Entropy","text":""},{"location":"reference/loss/#mindcv.loss.cross_entropy_smooth.CrossEntropySmooth","title":"<code>mindcv.loss.cross_entropy_smooth.CrossEntropySmooth</code>","text":"<p>               Bases: <code>LossBase</code></p> <p>Cross entropy loss with label smoothing. Apply softmax activation function to input <code>logits</code>, and uses the given logits to compute cross entropy between the logits and the label.</p> PARAMETER DESCRIPTION <code>smoothing</code> <p>Label smoothing factor, a regularization tool used to prevent the model from overfitting when calculating Loss. The value range is [0.0, 1.0]. Default: 0.0.</p> <p> DEFAULT: <code>0.0</code> </p> <code>aux_factor</code> <p>Auxiliary loss factor. Set aux_factor &gt; 0.0 if the model has auxiliary logit outputs (i.e., deep supervision), like inception_v3.  Default: 0.0.</p> <p> DEFAULT: <code>0.0</code> </p> <code>reduction</code> <p>Apply specific reduction method to the output: 'mean' or 'sum'. Default: 'mean'.</p> <p> DEFAULT: <code>'mean'</code> </p> <code>weight</code> <p>Class weight. Shape [C]. A rescaling weight applied to the loss of each batch element. Data type must be float16 or float32.</p> <p> TYPE: <code>Tensor</code> DEFAULT: <code>None</code> </p> Inputs <p>logits (Tensor or Tuple of Tensor): Input logits. Shape [N, C], where N is # samples, C is # classes.     Tuple composed of multiple logits are supported in order (main_logits, aux_logits)     for auxiliary loss used in networks like inception_v3. labels (Tensor): Ground truth label. Shape: [N] or [N, C].     (1) Shape (N), sparse labels representing the class indices. Must be int type.     (2) Shape [N, C], dense labels representing the ground truth class probability values,     or the one-hot labels. Must be float type.</p> Source code in <code>mindcv\\loss\\cross_entropy_smooth.py</code> <pre><code>class CrossEntropySmooth(nn.LossBase):\n    \"\"\"\n    Cross entropy loss with label smoothing.\n    Apply softmax activation function to input `logits`, and uses the given logits to compute cross entropy\n    between the logits and the label.\n\n    Args:\n        smoothing: Label smoothing factor, a regularization tool used to prevent the model\n            from overfitting when calculating Loss. The value range is [0.0, 1.0]. Default: 0.0.\n        aux_factor: Auxiliary loss factor. Set aux_factor &gt; 0.0 if the model has auxiliary logit outputs\n            (i.e., deep supervision), like inception_v3.  Default: 0.0.\n        reduction: Apply specific reduction method to the output: 'mean' or 'sum'. Default: 'mean'.\n        weight (Tensor): Class weight. Shape [C]. A rescaling weight applied to the loss of each batch element.\n            Data type must be float16 or float32.\n\n    Inputs:\n        logits (Tensor or Tuple of Tensor): Input logits. Shape [N, C], where N is # samples, C is # classes.\n            Tuple composed of multiple logits are supported in order (main_logits, aux_logits)\n            for auxiliary loss used in networks like inception_v3.\n        labels (Tensor): Ground truth label. Shape: [N] or [N, C].\n            (1) Shape (N), sparse labels representing the class indices. Must be int type.\n            (2) Shape [N, C], dense labels representing the ground truth class probability values,\n            or the one-hot labels. Must be float type.\n    \"\"\"\n\n    def __init__(self, smoothing=0.0, aux_factor=0.0, reduction=\"mean\", weight=None):\n        super().__init__()\n        self.smoothing = smoothing\n        self.aux_factor = aux_factor\n        self.reduction = reduction\n        self.weight = weight\n\n    def construct(self, logits, labels):\n        loss_aux = 0\n\n        if isinstance(logits, tuple):\n            main_logits = logits[0]\n            for aux in logits[1:]:\n                if self.aux_factor &gt; 0:\n                    loss_aux += F.cross_entropy(\n                        aux, labels, weight=self.weight, reduction=self.reduction, label_smoothing=self.smoothing\n                    )\n        else:\n            main_logits = logits\n\n        loss_logits = F.cross_entropy(\n            main_logits, labels, weight=self.weight, reduction=self.reduction, label_smoothing=self.smoothing\n        )\n        loss = loss_logits + self.aux_factor * loss_aux\n        return loss\n</code></pre>"},{"location":"reference/loss/#binary-cross-entropy","title":"Binary Cross Entropy","text":""},{"location":"reference/loss/#mindcv.loss.binary_cross_entropy_smooth.BinaryCrossEntropySmooth","title":"<code>mindcv.loss.binary_cross_entropy_smooth.BinaryCrossEntropySmooth</code>","text":"<p>               Bases: <code>LossBase</code></p> <p>Binary cross entropy loss with label smoothing. Apply sigmoid activation function to input <code>logits</code>, and uses the given logits to compute binary cross entropy between the logits and the label.</p> PARAMETER DESCRIPTION <code>smoothing</code> <p>Label smoothing factor, a regularization tool used to prevent the model from overfitting when calculating Loss. The value range is [0.0, 1.0]. Default: 0.0.</p> <p> DEFAULT: <code>0.0</code> </p> <code>aux_factor</code> <p>Auxiliary loss factor. Set aux_factor &gt; 0.0 if the model has auxiliary logit outputs (i.e., deep supervision), like inception_v3.  Default: 0.0.</p> <p> DEFAULT: <code>0.0</code> </p> <code>reduction</code> <p>Apply specific reduction method to the output: 'mean' or 'sum'. Default: 'mean'.</p> <p> DEFAULT: <code>'mean'</code> </p> <code>weight</code> <p>Class weight. A rescaling weight applied to the loss of each batch element. Shape [C]. It can be broadcast to a tensor with shape of <code>logits</code>. Data type must be float16 or float32.</p> <p> TYPE: <code>Tensor</code> DEFAULT: <code>None</code> </p> <code>pos_weight</code> <p>Positive weight for each class. A weight of positive examples. Shape [C]. Must be a vector with length equal to the number of classes. It can be broadcast to a tensor with shape of <code>logits</code>. Data type must be float16 or float32.</p> <p> TYPE: <code>Tensor</code> DEFAULT: <code>None</code> </p> Inputs <p>logits (Tensor or Tuple of Tensor): (1) Input logits. Shape [N, C], where N is # samples, C is # classes.     Or (2) Tuple of two input logits (main_logits and aux_logits) for auxiliary loss. labels (Tensor): Ground truth label, (1) shape [N, C], has the same shape as <code>logits</code> or (2) shape [N].     can be a class probability matrix or one-hot labels. Data type must be float16 or float32.</p> Source code in <code>mindcv\\loss\\binary_cross_entropy_smooth.py</code> <pre><code>class BinaryCrossEntropySmooth(nn.LossBase):\n    \"\"\"\n    Binary cross entropy loss with label smoothing.\n    Apply sigmoid activation function to input `logits`, and uses the given logits to compute binary cross entropy\n    between the logits and the label.\n\n    Args:\n        smoothing: Label smoothing factor, a regularization tool used to prevent the model\n            from overfitting when calculating Loss. The value range is [0.0, 1.0]. Default: 0.0.\n        aux_factor: Auxiliary loss factor. Set aux_factor &gt; 0.0 if the model has auxiliary logit outputs\n            (i.e., deep supervision), like inception_v3.  Default: 0.0.\n        reduction: Apply specific reduction method to the output: 'mean' or 'sum'. Default: 'mean'.\n        weight (Tensor): Class weight. A rescaling weight applied to the loss of each batch element. Shape [C].\n            It can be broadcast to a tensor with shape of `logits`. Data type must be float16 or float32.\n        pos_weight (Tensor): Positive weight for each class. A weight of positive examples. Shape [C].\n            Must be a vector with length equal to the number of classes.\n            It can be broadcast to a tensor with shape of `logits`. Data type must be float16 or float32.\n\n    Inputs:\n        logits (Tensor or Tuple of Tensor): (1) Input logits. Shape [N, C], where N is # samples, C is # classes.\n            Or (2) Tuple of two input logits (main_logits and aux_logits) for auxiliary loss.\n        labels (Tensor): Ground truth label, (1) shape [N, C], has the same shape as `logits` or (2) shape [N].\n            can be a class probability matrix or one-hot labels. Data type must be float16 or float32.\n    \"\"\"\n\n    def __init__(self, smoothing=0.0, aux_factor=0.0, reduction=\"mean\", weight=None, pos_weight=None):\n        super().__init__()\n        self.smoothing = smoothing\n        self.aux_factor = aux_factor\n        self.reduction = reduction\n        self.weight = weight\n        self.pos_weight = pos_weight\n        self.ones = P.OnesLike()\n        self.one_hot = P.OneHot()\n\n    def construct(self, logits, labels):\n        loss_aux = 0\n        aux_logits = None\n\n        if isinstance(logits, tuple):\n            main_logits = logits[0]\n        else:\n            main_logits = logits\n\n        if main_logits.size != labels.size:\n            # We must explicitly convert the label to one-hot,\n            # for binary_cross_entropy_with_logits restricting input and label have the same shape.\n            class_dim = 0 if main_logits.ndim == 1 else 1\n            n_classes = main_logits.shape[class_dim]\n            labels = self.one_hot(labels, n_classes, Tensor(1.0), Tensor(0.0))\n\n        ones_input = self.ones(main_logits)\n        if self.weight is not None:\n            weight = self.weight\n        else:\n            weight = ones_input\n        if self.pos_weight is not None:\n            pos_weight = self.pos_weight\n        else:\n            pos_weight = ones_input\n\n        if self.smoothing &gt; 0.0:\n            class_dim = 0 if main_logits.ndim == 1 else -1\n            n_classes = main_logits.shape[class_dim]\n            labels = labels * (1 - self.smoothing) + self.smoothing / n_classes\n\n        if self.aux_factor &gt; 0 and aux_logits is not None:\n            for aux_logits in logits[1:]:\n                loss_aux += F.binary_cross_entropy_with_logits(\n                    aux_logits, labels, weight=weight, pos_weight=pos_weight, reduction=self.reduction\n                )\n        # else:\n        #    warnings.warn(\"There are logit tuple input, but the auxiliary loss factor is 0.\")\n\n        loss_logits = F.binary_cross_entropy_with_logits(\n            main_logits, labels, weight=weight, pos_weight=pos_weight, reduction=self.reduction\n        )\n\n        loss = loss_logits + self.aux_factor * loss_aux\n\n        return loss\n</code></pre>"},{"location":"reference/models.layers/","title":"Common Layers in Model","text":""},{"location":"reference/models.layers/#activation","title":"Activation","text":""},{"location":"reference/models.layers/#mindcv.models.layers.activation.Swish","title":"<code>mindcv.models.layers.activation.Swish</code>","text":"<p>               Bases: <code>Cell</code></p> <p>Swish activation function: x * sigmoid(x).</p> Return <p>Tensor</p> Example <p>x = Tensor(((20, 16), (50, 50)), mindspore.float32) Swish()(x)</p> Source code in <code>mindcv\\models\\layers\\activation.py</code> <pre><code>class Swish(nn.Cell):\n    \"\"\"\n    Swish activation function: x * sigmoid(x).\n\n    Args:\n        None\n\n    Return:\n        Tensor\n\n    Example:\n        &gt;&gt;&gt; x = Tensor(((20, 16), (50, 50)), mindspore.float32)\n        &gt;&gt;&gt; Swish()(x)\n    \"\"\"\n\n    def __init__(self):\n        super().__init__()\n        self.result = None\n        self.sigmoid = nn.Sigmoid()\n\n    def construct(self, x):\n        result = x * self.sigmoid(x)\n        return result\n</code></pre>"},{"location":"reference/models.layers/#droppath","title":"DropPath","text":""},{"location":"reference/models.layers/#mindcv.models.layers.drop_path.DropPath","title":"<code>mindcv.models.layers.drop_path.DropPath</code>","text":"<p>               Bases: <code>Cell</code></p> <p>DropPath (Stochastic Depth) regularization layers</p> Source code in <code>mindcv\\models\\layers\\drop_path.py</code> <pre><code>class DropPath(nn.Cell):\n    \"\"\"DropPath (Stochastic Depth) regularization layers\"\"\"\n\n    def __init__(\n        self,\n        drop_prob: float = 0.0,\n        scale_by_keep: bool = True,\n    ) -&gt; None:\n        super().__init__()\n        self.keep_prob = 1.0 - drop_prob\n        self.scale_by_keep = scale_by_keep\n        self.dropout = Dropout(p=drop_prob)\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        if self.keep_prob == 1.0 or not self.training:\n            return x\n        shape = (x.shape[0],) + (1,) * (x.ndim - 1)\n        random_tensor = self.dropout(ones(shape))\n        if not self.scale_by_keep:\n            random_tensor = ops.mul(random_tensor, self.keep_prob)\n        return x * random_tensor\n</code></pre>"},{"location":"reference/models.layers/#identity","title":"Identity","text":""},{"location":"reference/models.layers/#mindcv.models.layers.identity.Identity","title":"<code>mindcv.models.layers.identity.Identity</code>","text":"<p>               Bases: <code>Cell</code></p> <p>Identity</p> Source code in <code>mindcv\\models\\layers\\identity.py</code> <pre><code>class Identity(nn.Cell):\n    \"\"\"Identity\"\"\"\n\n    def construct(self, x):\n        return x\n</code></pre>"},{"location":"reference/models.layers/#mlp","title":"MLP","text":""},{"location":"reference/models.layers/#mindcv.models.layers.mlp.Mlp","title":"<code>mindcv.models.layers.mlp.Mlp</code>","text":"<p>               Bases: <code>Cell</code></p> Source code in <code>mindcv\\models\\layers\\mlp.py</code> <pre><code>class Mlp(nn.Cell):\n    def __init__(\n        self,\n        in_features: int,\n        hidden_features: Optional[int] = None,\n        out_features: Optional[int] = None,\n        act_layer: Optional[nn.Cell] = nn.GELU,\n        drop: float = 0.0,\n    ) -&gt; None:\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Dense(in_channels=in_features, out_channels=hidden_features, has_bias=True)\n        self.act = act_layer()\n        self.fc2 = nn.Dense(in_channels=hidden_features, out_channels=out_features, has_bias=True)\n        self.drop = Dropout(p=drop)\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.drop(x)\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x\n</code></pre>"},{"location":"reference/models.layers/#patch-embedding","title":"Patch Embedding","text":""},{"location":"reference/models.layers/#mindcv.models.layers.patch_embed.PatchEmbed","title":"<code>mindcv.models.layers.patch_embed.PatchEmbed</code>","text":"<p>               Bases: <code>Cell</code></p> <p>Image to Patch Embedding</p> PARAMETER DESCRIPTION <code>image_size</code> <p>Image size.  Default: 224.</p> <p> TYPE: <code>int</code> DEFAULT: <code>224</code> </p> <code>patch_size</code> <p>Patch token size. Default: 4.</p> <p> TYPE: <code>int</code> DEFAULT: <code>4</code> </p> <code>in_chans</code> <p>Number of input image channels. Default: 3.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>embed_dim</code> <p>Number of linear projection output channels. Default: 96.</p> <p> TYPE: <code>int</code> DEFAULT: <code>96</code> </p> <code>norm_layer</code> <p>Normalization layer. Default: None</p> <p> TYPE: <code>Cell</code> DEFAULT: <code>None</code> </p> Source code in <code>mindcv\\models\\layers\\patch_embed.py</code> <pre><code>class PatchEmbed(nn.Cell):\n    \"\"\"Image to Patch Embedding\n\n    Args:\n        image_size (int): Image size.  Default: 224.\n        patch_size (int): Patch token size. Default: 4.\n        in_chans (int): Number of input image channels. Default: 3.\n        embed_dim (int): Number of linear projection output channels. Default: 96.\n        norm_layer (nn.Cell, optional): Normalization layer. Default: None\n    \"\"\"\n    output_fmt: Format\n\n    def __init__(\n        self,\n        image_size: Optional[int] = 224,\n        patch_size: int = 4,\n        in_chans: int = 3,\n        embed_dim: int = 96,\n        norm_layer: Optional[nn.Cell] = None,\n        flatten: bool = True,\n        output_fmt: Optional[str] = None,\n        bias: bool = True,\n        strict_img_size: bool = True,\n        dynamic_img_pad: bool = False,\n    ) -&gt; None:\n        super().__init__()\n        self.patch_size = to_2tuple(patch_size)\n        if image_size is not None:\n            self.image_size = to_2tuple(image_size)\n            self.patches_resolution = tuple([s // p for s, p in zip(self.image_size, self.patch_size)])\n            self.num_patches = self.patches_resolution[0] * self.patches_resolution[1]\n        else:\n            self.image_size = None\n            self.patches_resolution = None\n            self.num_patches = None\n\n        if output_fmt is not None:\n            self.flatten = False\n            self.output_fmt = Format(output_fmt)\n        else:\n            self.flatten = flatten\n            self.output_fmt = Format.NCHW\n\n        self.strict_img_size = strict_img_size\n        self.dynamic_img_pad = dynamic_img_pad\n        self.embed_dim = embed_dim\n\n        self.proj = nn.Conv2d(in_channels=in_chans, out_channels=embed_dim, kernel_size=patch_size, stride=patch_size,\n                              pad_mode='pad', has_bias=bias, weight_init=\"TruncatedNormal\")\n\n        if norm_layer is not None:\n            if isinstance(embed_dim, int):\n                embed_dim = (embed_dim,)\n            self.norm = norm_layer(embed_dim, epsilon=1e-5)\n        else:\n            self.norm = None\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        \"\"\"docstring\"\"\"\n        B, C, H, W = x.shape\n        if self.image_size is not None:\n            if self.strict_img_size:\n                if (H, W) != (self.image_size[0], self.image_size[1]):\n                    raise ValueError(f\"Input height and width ({H},{W}) doesn't match model ({self.image_size[0]},\"\n                                     f\"{self.image_size[1]}).\")\n            elif not self.dynamic_img_pad:\n                if H % self.patch_size[0] != 0:\n                    raise ValueError(f\"Input height ({H}) should be divisible by patch size ({self.patch_size[0]}).\")\n                if W % self.patch_size[1] != 0:\n                    raise ValueError(f\"Input width ({W}) should be divisible by patch size ({self.patch_size[1]}).\")\n        if self.dynamic_img_pad:\n            pad_h = (self.patch_size[0] - H % self.patch_size[0]) % self.patch_size[0]\n            pad_w = (self.patch_size[1] - W % self.patch_size[1]) % self.patch_size[1]\n            x = ops.pad(x, (0, pad_w, 0, pad_h))\n\n        # FIXME look at relaxing size constraints\n        x = self.proj(x)\n        if self.flatten:\n            x = ops.Reshape()(x, (B, self.embed_dim, -1))  # B Ph*Pw C\n            x = ops.Transpose()(x, (0, 2, 1))\n        elif self.output_fmt != \"NCHW\":\n            x = nchw_to(x, self.output_fmt)\n        if self.norm is not None:\n            x = self.norm(x)\n        return x\n</code></pre>"},{"location":"reference/models.layers/#mindcv.models.layers.patch_embed.PatchEmbed.construct","title":"<code>mindcv.models.layers.patch_embed.PatchEmbed.construct(x)</code>","text":"<p>docstring</p> Source code in <code>mindcv\\models\\layers\\patch_embed.py</code> <pre><code>def construct(self, x: Tensor) -&gt; Tensor:\n    \"\"\"docstring\"\"\"\n    B, C, H, W = x.shape\n    if self.image_size is not None:\n        if self.strict_img_size:\n            if (H, W) != (self.image_size[0], self.image_size[1]):\n                raise ValueError(f\"Input height and width ({H},{W}) doesn't match model ({self.image_size[0]},\"\n                                 f\"{self.image_size[1]}).\")\n        elif not self.dynamic_img_pad:\n            if H % self.patch_size[0] != 0:\n                raise ValueError(f\"Input height ({H}) should be divisible by patch size ({self.patch_size[0]}).\")\n            if W % self.patch_size[1] != 0:\n                raise ValueError(f\"Input width ({W}) should be divisible by patch size ({self.patch_size[1]}).\")\n    if self.dynamic_img_pad:\n        pad_h = (self.patch_size[0] - H % self.patch_size[0]) % self.patch_size[0]\n        pad_w = (self.patch_size[1] - W % self.patch_size[1]) % self.patch_size[1]\n        x = ops.pad(x, (0, pad_w, 0, pad_h))\n\n    # FIXME look at relaxing size constraints\n    x = self.proj(x)\n    if self.flatten:\n        x = ops.Reshape()(x, (B, self.embed_dim, -1))  # B Ph*Pw C\n        x = ops.Transpose()(x, (0, 2, 1))\n    elif self.output_fmt != \"NCHW\":\n        x = nchw_to(x, self.output_fmt)\n    if self.norm is not None:\n        x = self.norm(x)\n    return x\n</code></pre>"},{"location":"reference/models.layers/#pooling","title":"Pooling","text":""},{"location":"reference/models.layers/#mindcv.models.layers.pooling.GlobalAvgPooling","title":"<code>mindcv.models.layers.pooling.GlobalAvgPooling</code>","text":"<p>               Bases: <code>Cell</code></p> <p>GlobalAvgPooling, same as torch.nn.AdaptiveAvgPool2d when output shape is 1</p> Source code in <code>mindcv\\models\\layers\\pooling.py</code> <pre><code>class GlobalAvgPooling(nn.Cell):\n    \"\"\"\n    GlobalAvgPooling, same as torch.nn.AdaptiveAvgPool2d when output shape is 1\n    \"\"\"\n\n    def __init__(self, keep_dims: bool = False) -&gt; None:\n        super().__init__()\n        self.keep_dims = keep_dims\n\n    def construct(self, x):\n        x = ops.mean(x, axis=(2, 3), keep_dims=self.keep_dims)\n        return x\n</code></pre>"},{"location":"reference/models.layers/#selective-kernel","title":"Selective Kernel","text":""},{"location":"reference/models.layers/#mindcv.models.layers.selective_kernel.SelectiveKernelAttn","title":"<code>mindcv.models.layers.selective_kernel.SelectiveKernelAttn</code>","text":"<p>               Bases: <code>Cell</code></p> <p>Selective Kernel Attention Module Selective Kernel attention mechanism factored out into its own module.</p> Source code in <code>mindcv\\models\\layers\\selective_kernel.py</code> <pre><code>class SelectiveKernelAttn(nn.Cell):\n    \"\"\"Selective Kernel Attention Module\n    Selective Kernel attention mechanism factored out into its own module.\n    \"\"\"\n\n    def __init__(\n        self,\n        channels: int,\n        num_paths: int = 2,\n        attn_channels: int = 32,\n        activation: Optional[nn.Cell] = nn.ReLU,\n        norm: Optional[nn.Cell] = nn.BatchNorm2d,\n    ):\n        super().__init__()\n        self.num_paths = num_paths\n        self.mean = GlobalAvgPooling(keep_dims=True)\n        self.fc_reduce = nn.Conv2d(channels, attn_channels, kernel_size=1, has_bias=False)\n        self.bn = norm(attn_channels)\n        self.act = activation()\n        self.fc_select = nn.Conv2d(attn_channels, channels * num_paths, kernel_size=1)\n        self.softmax = nn.Softmax(axis=1)\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        x = self.mean((x.sum(1)))\n        x = self.fc_reduce(x)\n        x = self.bn(x)\n        x = self.act(x)\n        x = self.fc_select(x)\n        b, c, h, w = x.shape\n        x = x.reshape((b, self.num_paths, c // self.num_paths, h, w))\n        x = self.softmax(x)\n        return x\n</code></pre>"},{"location":"reference/models.layers/#mindcv.models.layers.selective_kernel.SelectiveKernel","title":"<code>mindcv.models.layers.selective_kernel.SelectiveKernel</code>","text":"<p>               Bases: <code>Cell</code></p> <p>Selective Kernel Convolution Module As described in Selective Kernel Networks (https://arxiv.org/abs/1903.06586) with some modifications. Largest change is the input split, which divides the input channels across each convolution path, this can be viewed as a grouping of sorts, but the output channel counts expand to the module level value. This keeps the parameter count from ballooning when the convolutions themselves don't have groups, but still provides a noteworthy increase in performance over similar param count models without this attention layer. -Ross W Args:     in_channels (int):  module input (feature) channel count     out_channels (int):  module output (feature) channel count     kernel_size (int, list): kernel size for each convolution branch     stride (int): stride for convolutions     dilation (int): dilation for module as a whole, impacts dilation of each branch     groups (int): number of groups for each branch     rd_ratio (int, float): reduction factor for attention features     rd_channels(int): reduction channels can be specified directly by arg (if rd_channels is set)     rd_divisor(int): divisor can be specified to keep channels     keep_3x3 (bool): keep all branch convolution kernels as 3x3, changing larger kernels for dilations     split_input (bool): split input channels evenly across each convolution branch, keeps param count lower,         can be viewed as grouping by path, output expands to module out_channels count     activation (nn.Module): activation layer to use     norm (nn.Module): batchnorm/norm layer to use</p> Source code in <code>mindcv\\models\\layers\\selective_kernel.py</code> <pre><code>class SelectiveKernel(nn.Cell):\n    \"\"\"Selective Kernel Convolution Module\n    As described in Selective Kernel Networks (https://arxiv.org/abs/1903.06586) with some modifications.\n    Largest change is the input split, which divides the input channels across each convolution path, this can\n    be viewed as a grouping of sorts, but the output channel counts expand to the module level value. This keeps\n    the parameter count from ballooning when the convolutions themselves don't have groups, but still provides\n    a noteworthy increase in performance over similar param count models without this attention layer. -Ross W\n    Args:\n        in_channels (int):  module input (feature) channel count\n        out_channels (int):  module output (feature) channel count\n        kernel_size (int, list): kernel size for each convolution branch\n        stride (int): stride for convolutions\n        dilation (int): dilation for module as a whole, impacts dilation of each branch\n        groups (int): number of groups for each branch\n        rd_ratio (int, float): reduction factor for attention features\n        rd_channels(int): reduction channels can be specified directly by arg (if rd_channels is set)\n        rd_divisor(int): divisor can be specified to keep channels\n        keep_3x3 (bool): keep all branch convolution kernels as 3x3, changing larger kernels for dilations\n        split_input (bool): split input channels evenly across each convolution branch, keeps param count lower,\n            can be viewed as grouping by path, output expands to module out_channels count\n        activation (nn.Module): activation layer to use\n        norm (nn.Module): batchnorm/norm layer to use\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: Optional[int] = None,\n        kernel_size: Optional[Union[int, List]] = None,\n        stride: int = 1,\n        dilation: int = 1,\n        groups: int = 1,\n        rd_ratio: float = 1.0 / 16,\n        rd_channels: Optional[int] = None,\n        rd_divisor: int = 8,\n        keep_3x3: bool = True,\n        split_input: bool = True,\n        activation: Optional[nn.Cell] = nn.ReLU,\n        norm: Optional[nn.Cell] = nn.BatchNorm2d,\n    ):\n        super().__init__()\n        out_channels = out_channels or in_channels\n        kernel_size = kernel_size or [3, 5]  # default to one 3x3 and one 5x5 branch. 5x5 -&gt; 3x3 + dilation\n        _kernel_valid(kernel_size)\n        if not isinstance(kernel_size, list):\n            kernel_size = [kernel_size] * 2\n        if keep_3x3:\n            dilation = [dilation * (k - 1) // 2 for k in kernel_size]\n            kernel_size = [3] * len(kernel_size)\n        else:\n            dilation = [dilation] * len(kernel_size)\n        self.num_paths = len(kernel_size)\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.split_input = split_input\n        if self.split_input:\n            assert in_channels % self.num_paths == 0\n            in_channels = in_channels // self.num_paths\n        groups = min(out_channels, groups)\n        self.split = Split(split_size_or_sections=self.in_channels // self.num_paths, output_num=self.num_paths, axis=1)\n\n        self.paths = nn.CellList([\n            Conv2dNormActivation(in_channels, out_channels, kernel_size=k, stride=stride, groups=groups,\n                                 dilation=d, activation=activation, norm=norm)\n            for k, d in zip(kernel_size, dilation)\n        ])\n\n        attn_channels = rd_channels or make_divisible(out_channels * rd_ratio, divisor=rd_divisor)\n        self.attn = SelectiveKernelAttn(out_channels, self.num_paths, attn_channels)\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        x_paths = []\n        if self.split_input:\n            x_split = self.split(x)\n            for i, op in enumerate(self.paths):\n                x_paths.append(op(x_split[i]))\n        else:\n            for op in self.paths:\n                x_paths.append(op(x))\n\n        x = ops.stack(x_paths, axis=1)\n        x_attn = self.attn(x)\n        x = x * x_attn\n        x = x.sum(1)\n        return x\n</code></pre>"},{"location":"reference/models.layers/#squeeze-and-excite","title":"Squeeze and Excite","text":""},{"location":"reference/models.layers/#mindcv.models.layers.squeeze_excite.SqueezeExcite","title":"<code>mindcv.models.layers.squeeze_excite.SqueezeExcite</code>","text":"<p>               Bases: <code>Cell</code></p> <p>SqueezeExcite Module as defined in original SE-Nets with a few additions. Additions include:     * divisor can be specified to keep channels % div == 0 (default: 8)     * reduction channels can be specified directly by arg (if rd_channels is set)     * reduction channels can be specified by float rd_ratio (default: 1/16)     * customizable activation, normalization, and gate layer</p> Source code in <code>mindcv\\models\\layers\\squeeze_excite.py</code> <pre><code>class SqueezeExcite(nn.Cell):\n    \"\"\"SqueezeExcite Module as defined in original SE-Nets with a few additions.\n    Additions include:\n        * divisor can be specified to keep channels % div == 0 (default: 8)\n        * reduction channels can be specified directly by arg (if rd_channels is set)\n        * reduction channels can be specified by float rd_ratio (default: 1/16)\n        * customizable activation, normalization, and gate layer\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels: int,\n        rd_ratio: float = 1.0 / 16,\n        rd_channels: Optional[int] = None,\n        rd_divisor: int = 8,\n        norm: Optional[nn.Cell] = None,\n        act_layer: nn.Cell = nn.ReLU,\n        gate_layer: nn.Cell = nn.Sigmoid,\n    ) -&gt; None:\n        super().__init__()\n        self.norm = norm\n        self.act = act_layer()\n        self.gate = gate_layer()\n        if not rd_channels:\n            rd_channels = make_divisible(in_channels * rd_ratio, rd_divisor)\n\n        self.conv_reduce = nn.Conv2d(\n            in_channels=in_channels,\n            out_channels=rd_channels,\n            kernel_size=1,\n            has_bias=True,\n        )\n        if self.norm:\n            self.bn = nn.BatchNorm2d(rd_channels)\n        self.conv_expand = nn.Conv2d(\n            in_channels=rd_channels,\n            out_channels=in_channels,\n            kernel_size=1,\n            has_bias=True,\n        )\n        self.pool = GlobalAvgPooling(keep_dims=True)\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        x_se = self.pool(x)\n        x_se = self.conv_reduce(x_se)\n        if self.norm:\n            x_se = self.bn(x_se)\n        x_se = self.act(x_se)\n        x_se = self.conv_expand(x_se)\n        x_se = self.gate(x_se)\n        x = x * x_se\n        return x\n</code></pre>"},{"location":"reference/models.layers/#mindcv.models.layers.squeeze_excite.SqueezeExciteV2","title":"<code>mindcv.models.layers.squeeze_excite.SqueezeExciteV2</code>","text":"<p>               Bases: <code>Cell</code></p> <p>SqueezeExcite Module as defined in original SE-Nets with a few additions. V1 uses 1x1conv to replace fc layers, and V2 uses nn.Dense to implement directly.</p> Source code in <code>mindcv\\models\\layers\\squeeze_excite.py</code> <pre><code>class SqueezeExciteV2(nn.Cell):\n    \"\"\"SqueezeExcite Module as defined in original SE-Nets with a few additions.\n    V1 uses 1x1conv to replace fc layers, and V2 uses nn.Dense to implement directly.\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels: int,\n        rd_ratio: float = 1.0 / 16,\n        rd_channels: Optional[int] = None,\n        rd_divisor: int = 8,\n        norm: Optional[nn.Cell] = None,\n        act_layer: nn.Cell = nn.ReLU,\n        gate_layer: nn.Cell = nn.Sigmoid,\n    ) -&gt; None:\n        super().__init__()\n        self.norm = norm\n        self.act = act_layer()\n        self.gate = gate_layer()\n        if not rd_channels:\n            rd_channels = make_divisible(in_channels * rd_ratio, rd_divisor)\n\n        self.conv_reduce = nn.Dense(\n            in_channels=in_channels,\n            out_channels=rd_channels,\n            has_bias=True,\n        )\n        if self.norm:\n            self.bn = nn.BatchNorm2d(rd_channels)\n        self.conv_expand = nn.Dense(\n            in_channels=rd_channels,\n            out_channels=in_channels,\n            has_bias=True,\n        )\n        self.pool = GlobalAvgPooling(keep_dims=False)\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        x_se = self.pool(x)\n        x_se = self.conv_reduce(x_se)\n        if self.norm:\n            x_se = self.bn(x_se)\n        x_se = self.act(x_se)\n        x_se = self.conv_expand(x_se)\n        x_se = self.gate(x_se)\n        x_se = ops.expand_dims(x_se, -1)\n        x_se = ops.expand_dims(x_se, -1)\n        x = x * x_se\n        return x\n</code></pre>"},{"location":"reference/models/","title":"Models","text":""},{"location":"reference/models/#create-model","title":"Create Model","text":""},{"location":"reference/models/#mindcv.models.model_factory.create_model","title":"<code>mindcv.models.model_factory.create_model(model_name, num_classes=1000, pretrained=False, in_channels=3, checkpoint_path='', ema=False, auto_mapping=False, **kwargs)</code>","text":"<p>Creates model by name.</p> PARAMETER DESCRIPTION <code>model_name</code> <p>The name of model.</p> <p> TYPE: <code>str</code> </p> <code>num_classes</code> <p>The number of classes. Default: 1000.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1000</code> </p> <code>pretrained</code> <p>Whether to load the pretrained model. Default: False.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>in_channels</code> <p>The input channels. Default: 3.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>checkpoint_path</code> <p>The path of checkpoint files. Default: \"\".</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> <code>ema</code> <p>Whether use ema method. Default: False.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>auto_mapping</code> <p>Whether to automatically map the names of checkpoint weights to the names of model weights when there are differences in names. Default: False.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>**kwargs</code> <p>additional args, e.g., \"features_only\", \"out_indices\".</p> <p> DEFAULT: <code>{}</code> </p> Source code in <code>mindcv\\models\\model_factory.py</code> <pre><code>def create_model(\n    model_name: str,\n    num_classes: int = 1000,\n    pretrained: bool = False,\n    in_channels: int = 3,\n    checkpoint_path: str = \"\",\n    ema: bool = False,\n    auto_mapping: bool = False,\n    **kwargs,\n):\n    r\"\"\"Creates model by name.\n\n    Args:\n        model_name (str):  The name of model.\n        num_classes (int): The number of classes. Default: 1000.\n        pretrained (bool): Whether to load the pretrained model. Default: False.\n        in_channels (int): The input channels. Default: 3.\n        checkpoint_path (str): The path of checkpoint files. Default: \"\".\n        ema (bool): Whether use ema method. Default: False.\n        auto_mapping (bool): Whether to automatically map the names of checkpoint weights\n            to the names of model weights when there are differences in names. Default: False.\n        **kwargs: additional args, e.g., \"features_only\", \"out_indices\".\n    \"\"\"\n\n    if checkpoint_path != \"\" and pretrained:\n        raise ValueError(\"checkpoint_path is mutually exclusive with pretrained\")\n\n    model_args = dict(num_classes=num_classes, pretrained=pretrained, in_channels=in_channels)\n    kwargs = {k: v for k, v in kwargs.items() if v is not None}\n\n    if not is_model(model_name):\n        raise RuntimeError(f\"Unknown model {model_name}\")\n\n    create_fn = model_entrypoint(model_name)\n    model = create_fn(**model_args, **kwargs)\n\n    if checkpoint_path:\n        load_model_checkpoint(model, checkpoint_path, ema, auto_mapping)\n\n    return model\n</code></pre>"},{"location":"reference/models/#bit","title":"bit","text":""},{"location":"reference/models/#mindcv.models.bit","title":"<code>mindcv.models.bit</code>","text":"<p>MindSpore implementation of <code>BiT_ResNet</code>. Refer to Big Transfer (BiT): General Visual Representation Learning.</p>"},{"location":"reference/models/#mindcv.models.bit.BiT_ResNet","title":"<code>mindcv.models.bit.BiT_ResNet</code>","text":"<p>               Bases: <code>Cell</code></p> <p>BiT_ResNet model class, based on <code>\"Big Transfer (BiT): General Visual Representation Learning\" &lt;https://arxiv.org/abs/1912.11370&gt;</code>_ Args:     block(Union[Bottleneck]): block of BiT_ResNetv2.     layers(tuple(int)): number of layers of each stage.     wf(int): width of each layer. Default: 1.     num_classes(int): number of classification classes. Default: 1000.     in_channels(int): number the channels of the input. Default: 3.     groups(int): number of groups for group conv in blocks. Default: 1.     base_width(int): base width of pre group hidden channel in blocks. Default: 64.     norm(nn.Cell): normalization layer in blocks. Default: None.</p> Source code in <code>mindcv\\models\\bit.py</code> <pre><code>class BiT_ResNet(nn.Cell):\n    r\"\"\"BiT_ResNet model class, based on\n    `\"Big Transfer (BiT): General Visual Representation Learning\" &lt;https://arxiv.org/abs/1912.11370&gt;`_\n    Args:\n        block(Union[Bottleneck]): block of BiT_ResNetv2.\n        layers(tuple(int)): number of layers of each stage.\n        wf(int): width of each layer. Default: 1.\n        num_classes(int): number of classification classes. Default: 1000.\n        in_channels(int): number the channels of the input. Default: 3.\n        groups(int): number of groups for group conv in blocks. Default: 1.\n        base_width(int): base width of pre group hidden channel in blocks. Default: 64.\n        norm(nn.Cell): normalization layer in blocks. Default: None.\n    \"\"\"\n\n    def __init__(\n        self,\n        block: Type[Union[Bottleneck]],\n        layers: List[int],\n        wf: int = 1,\n        num_classes: int = 1000,\n        in_channels: int = 3,\n        groups: int = 1,\n        base_width: int = 64,\n        norm: Optional[nn.Cell] = None,\n    ) -&gt; None:\n        super().__init__()\n\n        if norm is None:\n            norm = nn.GroupNorm\n\n        self.norm: nn.Cell = norm  # add type hints to make pylint happy\n        self.input_channels = 64 * wf\n        self.groups = groups\n        self.base_with = base_width\n\n        self.conv1 = StdConv2d(in_channels, self.input_channels, kernel_size=7,\n                               stride=2, pad_mode=\"pad\", padding=3)\n        self.pad = nn.ConstantPad2d(1, 0)\n        self.max_pool = nn.MaxPool2d(kernel_size=3, stride=2, pad_mode=\"valid\")\n\n        self.layer1 = self._make_layer(block, 64 * wf, layers[0])\n        self.layer2 = self._make_layer(block, 128 * wf, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256 * wf, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, 512 * wf, layers[3], stride=2)\n\n        self.gn = norm(32, 2048 * wf)\n        self.relu = nn.ReLU()\n        self.pool = GlobalAvgPooling(keep_dims=True)\n        self.classifier = nn.Conv2d(512 * block.expansion * wf, num_classes, kernel_size=1, has_bias=True)\n\n    def _make_layer(\n        self,\n        block: Type[Union[Bottleneck]],\n        channels: int,\n        block_nums: int,\n        stride: int = 1,\n    ) -&gt; nn.SequentialCell:\n        \"\"\"build model depending on cfgs\"\"\"\n        down_sample = None\n\n        if stride != 1 or self.input_channels != channels * block.expansion:\n            down_sample = nn.SequentialCell([\n                StdConv2d(self.input_channels, channels * block.expansion, kernel_size=1, stride=stride),\n            ])\n\n        layers = []\n        layers.append(\n            block(\n                self.input_channels,\n                channels,\n                stride=stride,\n                down_sample=down_sample,\n                groups=self.groups,\n                base_width=self.base_with,\n                norm=self.norm,\n            )\n        )\n        self.input_channels = channels * block.expansion\n\n        for _ in range(1, block_nums):\n            layers.append(\n                block(\n                    self.input_channels,\n                    channels,\n                    groups=self.groups,\n                    base_width=self.base_with,\n                    norm=self.norm,\n                )\n            )\n\n        return nn.SequentialCell(layers)\n\n    def root(self, x: Tensor) -&gt; Tensor:\n        x = self.conv1(x)\n        x = self.pad(x)\n        x = self.max_pool(x)\n        return x\n\n    def forward_features(self, x: Tensor) -&gt; Tensor:\n        \"\"\"Network forward feature extraction.\"\"\"\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        return x\n\n    def forward_head(self, x: Tensor) -&gt; Tensor:\n        x = self.gn(x)\n        x = self.relu(x)\n        x = self.pool(x)\n        x = self.classifier(x)\n        return x\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        x = self.root(x)\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        assert x.shape[-2:] == (1, 1)  # We should have no spatial shape left.\n        return x[..., 0, 0]\n</code></pre>"},{"location":"reference/models/#mindcv.models.bit.BiT_ResNet.forward_features","title":"<code>mindcv.models.bit.BiT_ResNet.forward_features(x)</code>","text":"<p>Network forward feature extraction.</p> Source code in <code>mindcv\\models\\bit.py</code> <pre><code>def forward_features(self, x: Tensor) -&gt; Tensor:\n    \"\"\"Network forward feature extraction.\"\"\"\n    x = self.layer1(x)\n    x = self.layer2(x)\n    x = self.layer3(x)\n    x = self.layer4(x)\n    return x\n</code></pre>"},{"location":"reference/models/#mindcv.models.bit.Bottleneck","title":"<code>mindcv.models.bit.Bottleneck</code>","text":"<p>               Bases: <code>Cell</code></p> <p>define the basic block of BiT Args:       in_channels(int): The channel number of the input tensor of the Conv2d layer.       channels(int): The channel number of the output tensor of the middle Conv2d layer.       stride(int): The movement stride of the 2D convolution kernel. Default: 1.       groups(int): Number of groups for group conv in blocks. Default: 1.       base_width(int): Base width of pre group hidden channel in blocks. Default: 64.       norm(nn.Cell): Normalization layer in blocks. Default: None.       down_sample(nn.Cell): Down sample in blocks. Default: None.</p> Source code in <code>mindcv\\models\\bit.py</code> <pre><code>class Bottleneck(nn.Cell):\n    \"\"\"define the basic block of BiT\n    Args:\n          in_channels(int): The channel number of the input tensor of the Conv2d layer.\n          channels(int): The channel number of the output tensor of the middle Conv2d layer.\n          stride(int): The movement stride of the 2D convolution kernel. Default: 1.\n          groups(int): Number of groups for group conv in blocks. Default: 1.\n          base_width(int): Base width of pre group hidden channel in blocks. Default: 64.\n          norm(nn.Cell): Normalization layer in blocks. Default: None.\n          down_sample(nn.Cell): Down sample in blocks. Default: None.\n    \"\"\"\n\n    expansion: int = 4\n\n    def __init__(\n        self,\n        in_channels: int,\n        channels: int,\n        stride: int = 1,\n        groups: int = 1,\n        base_width: int = 64,\n        norm: Optional[nn.Cell] = None,\n        down_sample: Optional[nn.Cell] = None,\n    ) -&gt; None:\n        super().__init__()\n        if norm is None:\n            norm = nn.GroupNorm\n\n        width = int(channels * (base_width / 64.0)) * groups\n        self.gn1 = norm(32, in_channels)\n        self.conv1 = StdConv2d(in_channels, width, kernel_size=1, stride=1)\n        self.gn2 = norm(32, width)\n        self.conv2 = StdConv2d(width, width, kernel_size=3, stride=stride,\n                               padding=1, pad_mode=\"pad\", group=groups)\n        self.gn3 = norm(32, width)\n        self.conv3 = StdConv2d(width, channels * self.expansion,\n                               kernel_size=1, stride=1)\n\n        self.relu = nn.ReLU()\n        self.down_sample = down_sample\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        identity = x\n        out = self.gn1(x)\n        out = self.relu(out)\n\n        residual = out\n\n        out = self.conv1(out)\n\n        out = self.gn2(out)\n        out = self.relu(out)\n        out = self.conv2(out)\n\n        out = self.gn3(out)\n        out = self.relu(out)\n        out = self.conv3(out)\n\n        if self.down_sample is not None:\n            identity = self.down_sample(residual)\n\n        out += identity\n        # out = self.relu(out)\n\n        return out\n</code></pre>"},{"location":"reference/models/#mindcv.models.bit.StdConv2d","title":"<code>mindcv.models.bit.StdConv2d</code>","text":"<p>               Bases: <code>Conv2d</code></p> <p>Conv2d with Weight Standardization Args:     in_channels(int): The channel number of the input tensor of the Conv2d layer.     out_channels(int): The channel number of the output tensor of the Conv2d layer.     kernel_size(int): Specifies the height and width of the 2D convolution kernel.     stride(int): The movement stride of the 2D convolution kernel. Default: 1.     pad_mode(str): Specifies padding mode. The optional values are \"same\", \"valid\", \"pad\". Default: \"same\".     padding(int): The number of padding on the height and width directions of the input. Default: 0.     group(int): Splits filter into groups. Default: 1.</p> Source code in <code>mindcv\\models\\bit.py</code> <pre><code>class StdConv2d(nn.Conv2d):\n    r\"\"\"Conv2d with Weight Standardization\n    Args:\n        in_channels(int): The channel number of the input tensor of the Conv2d layer.\n        out_channels(int): The channel number of the output tensor of the Conv2d layer.\n        kernel_size(int): Specifies the height and width of the 2D convolution kernel.\n        stride(int): The movement stride of the 2D convolution kernel. Default: 1.\n        pad_mode(str): Specifies padding mode. The optional values are \"same\", \"valid\", \"pad\". Default: \"same\".\n        padding(int): The number of padding on the height and width directions of the input. Default: 0.\n        group(int): Splits filter into groups. Default: 1.\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        kernel_size,\n        stride=1,\n        pad_mode=\"same\",\n        padding=0,\n        group=1,\n    ) -&gt; None:\n        super(StdConv2d, self).__init__(\n            in_channels,\n            out_channels,\n            kernel_size,\n            stride,\n            pad_mode,\n            padding,\n            group,\n        )\n        self.mean_op = ops.ReduceMean(keep_dims=True)\n\n    def construct(self, x):\n        w = self.weight\n        m = self.mean_op(w, [1, 2, 3])\n        v = w.var((1, 2, 3), keepdims=True)\n        w = (w - m) / mindspore.ops.sqrt(v + 1e-10)\n        output = self.conv2d(x, w)\n        return output\n</code></pre>"},{"location":"reference/models/#mindcv.models.bit.BiT_resnet101","title":"<code>mindcv.models.bit.BiT_resnet101(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get 101 layers ResNet model. Refer to the base class <code>models.BiT_Resnet</code> for more details.</p> Source code in <code>mindcv\\models\\bit.py</code> <pre><code>@register_model\ndef BiT_resnet101(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs):\n    \"\"\"Get 101 layers ResNet model.\n    Refer to the base class `models.BiT_Resnet` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"BiT_resnet101\"]\n    model = BiT_ResNet(Bottleneck, [3, 4, 23, 3], num_classes=num_classes, in_channels=in_channels, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre>"},{"location":"reference/models/#mindcv.models.bit.BiT_resnet50","title":"<code>mindcv.models.bit.BiT_resnet50(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get 50 layers ResNet model. Refer to the base class <code>models.BiT_Resnet</code> for more details.</p> Source code in <code>mindcv\\models\\bit.py</code> <pre><code>@register_model\ndef BiT_resnet50(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs):\n    \"\"\"Get 50 layers ResNet model.\n    Refer to the base class `models.BiT_Resnet` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"BiT_resnet50\"]\n    model = BiT_ResNet(Bottleneck, [3, 4, 6, 3], num_classes=num_classes, in_channels=in_channels, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre>"},{"location":"reference/models/#mindcv.models.bit.BiT_resnet50x3","title":"<code>mindcv.models.bit.BiT_resnet50x3(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get 50 layers ResNet model. Refer to the base class <code>models.BiT_Resnet</code> for more details.</p> Source code in <code>mindcv\\models\\bit.py</code> <pre><code>@register_model\ndef BiT_resnet50x3(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs):\n    \"\"\"Get 50 layers ResNet model.\n     Refer to the base class `models.BiT_Resnet` for more details.\n     \"\"\"\n    default_cfg = default_cfgs[\"BiT_resnet50x3\"]\n    model = BiT_ResNet(Bottleneck, [3, 4, 6, 3], wf=3, num_classes=num_classes, in_channels=in_channels, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre>"},{"location":"reference/models/#cait","title":"cait","text":""},{"location":"reference/models/#mindcv.models.cait","title":"<code>mindcv.models.cait</code>","text":"<p>MindSpore implementation of <code>CaiT</code>. Refer to Going deeper with Image Transformers.</p>"},{"location":"reference/models/#mindcv.models.cait.AttentionTalkingHead","title":"<code>mindcv.models.cait.AttentionTalkingHead</code>","text":"<p>               Bases: <code>Cell</code></p> <p>Talking head is a trick for multi-head attention, which has two more linear map before and after the softmax compared to normal attention.</p> Source code in <code>mindcv\\models\\cait.py</code> <pre><code>class AttentionTalkingHead(nn.Cell):\n    \"\"\"\n    Talking head is a trick for multi-head attention,\n    which has two more linear map before and after\n    the softmax compared to normal attention.\n    \"\"\"\n    def __init__(self,\n                 dim: int,\n                 num_heads: int = 8,\n                 qkv_bias: bool = False,\n                 qk_scale: float = None,\n                 attn_drop_rate: float = 0.,\n                 proj_drop_rate: float = 0.) -&gt; None:\n        super(AttentionTalkingHead, self).__init__()\n        assert dim % num_heads == 0, \"dim should be divisible by num_heads.\"\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        self.scale = qk_scale or head_dim ** -0.5\n\n        self.qkv = nn.Dense(dim, dim * 3, has_bias=qkv_bias)\n        self.attn_drop = Dropout(p=attn_drop_rate)\n\n        self.proj = nn.Dense(dim, dim, has_bias=False)\n\n        self.proj_l = nn.Dense(num_heads, num_heads, has_bias=False)\n        self.proj_w = nn.Dense(num_heads, num_heads, has_bias=False)\n\n        self.proj_drop = Dropout(p=proj_drop_rate)\n\n        self.softmax = nn.Softmax(axis=-1)\n\n        self.attn_matmul_v = ops.BatchMatMul()\n        self.q_matmul_k = ops.BatchMatMul(transpose_b=True)\n\n    def construct(self, x) -&gt; Tensor:\n        B, N, C = x.shape\n        qkv = ops.reshape(self.qkv(x), (B, N, 3, self.num_heads, C // self.num_heads))\n        qkv = ops.transpose(qkv, (2, 0, 3, 1, 4))\n        q, k, v = ops.unstack(qkv, axis=0)\n        q = ops.mul(q, self.scale)\n\n        attn = self.q_matmul_k(q, k)\n\n        attn = ops.transpose(attn, (0, 2, 3, 1))\n        attn = self.proj_l(attn)\n        attn = ops.transpose(attn, (0, 3, 1, 2))\n        attn = self.softmax(attn)\n        attn = ops.transpose(attn, (0, 2, 3, 1))\n        attn = self.proj_w(attn)\n        attn = ops.transpose(attn, (0, 3, 1, 2))\n\n        attn = self.attn_drop(attn)\n\n        x = self.attn_matmul_v(attn, v)\n        x = ops.transpose(x, (0, 2, 1, 3))\n        x = ops.reshape(x, (B, N, C))\n        x = self.proj(x)\n        x = self.proj_drop(x)\n\n        return x\n</code></pre>"},{"location":"reference/models/#cmt","title":"cmt","text":""},{"location":"reference/models/#mindcv.models.cmt","title":"<code>mindcv.models.cmt</code>","text":""},{"location":"reference/models/#mindcv.models.cmt.PatchEmbed","title":"<code>mindcv.models.cmt.PatchEmbed</code>","text":"<p>               Bases: <code>Cell</code></p> <p>Image to Patch Embedding</p> Source code in <code>mindcv\\models\\cmt.py</code> <pre><code>class PatchEmbed(nn.Cell):\n    \"\"\" Image to Patch Embedding\n    \"\"\"\n\n    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n        super().__init__()\n        img_size = to_2tuple(img_size)\n        patch_size = to_2tuple(patch_size)\n        num_patches = (img_size[1] // patch_size[1]) * \\\n                      (img_size[0] // patch_size[0])\n\n        assert img_size[0] % patch_size[0] == 0 and img_size[1] % patch_size[1] == 0, \\\n            f\"img_size {img_size} should be divided by patch_size {patch_size}.\"\n\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.num_patches = num_patches\n\n        self.proj = nn.Conv2d(\n            in_chans, embed_dim, kernel_size=patch_size, stride=patch_size, has_bias=True)\n        self.norm = nn.LayerNorm([embed_dim])\n\n    def construct(self, x):\n        _, _, H, W = x.shape\n\n        assert H == self.img_size[0] and W == self.img_size[1], \\\n            f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n        x = self.proj(x)\n        _B, _C, _H, _W = x.shape\n        x = ops.transpose(x.reshape(_B, _C, _H * _W), (0, 2, 1))\n        x = self.norm(x)\n\n        H, W = H // self.patch_size[0], W // self.patch_size[1]\n        return x, (H, W)\n</code></pre>"},{"location":"reference/models/#mindcv.models.cmt.cmt_base","title":"<code>mindcv.models.cmt.cmt_base(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>CMT-Base</p> Source code in <code>mindcv\\models\\cmt.py</code> <pre><code>@register_model\ndef cmt_base(pretrained=False, num_classes: int = 1000, in_channels: int = 3, **kwargs):\n    \"\"\"\n    CMT-Base\n    \"\"\"\n    default_cfg = default_cfgs[\"cmt_base\"]\n\n    model = CMT(img_size=256, num_classes=num_classes, in_channels=in_channels, qkv_bias=True,\n                embed_dims=[76, 152, 304, 608], stem_channel=38, num_heads=[1, 2, 4, 8], depths=[4, 4, 20, 4],\n                mlp_ratios=[4, 4, 4, 4], qk_ratio=1, sr_ratios=[8, 4, 2, 1], **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre>"},{"location":"reference/models/#mindcv.models.cmt.cmt_small","title":"<code>mindcv.models.cmt.cmt_small(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>CMT-Small</p> Source code in <code>mindcv\\models\\cmt.py</code> <pre><code>@register_model\ndef cmt_small(pretrained=False, num_classes: int = 1000, in_channels: int = 3, **kwargs):\n    \"\"\"\n    CMT-Small\n    \"\"\"\n    default_cfg = default_cfgs[\"cmt_small\"]\n\n    model = CMT(img_size=224, num_classes=num_classes, in_channels=in_channels, qkv_bias=True,\n                embed_dims=[64, 128, 256, 512], stem_channel=32, num_heads=[1, 2, 4, 8], depths=[3, 3, 16, 3],\n                mlp_ratios=[4, 4, 4, 4], qk_ratio=1, sr_ratios=[8, 4, 2, 1], **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre>"},{"location":"reference/models/#mindcv.models.cmt.cmt_tiny","title":"<code>mindcv.models.cmt.cmt_tiny(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>CMT-tiny</p> Source code in <code>mindcv\\models\\cmt.py</code> <pre><code>@register_model\ndef cmt_tiny(pretrained=False, num_classes: int = 1000, in_channels: int = 3, **kwargs):\n    \"\"\"\n    CMT-tiny\n    \"\"\"\n    default_cfg = default_cfgs[\"cmt_tiny\"]\n\n    model = CMT(img_size=160, num_classes=num_classes, in_channels=in_channels, qkv_bias=True,\n                embed_dims=[46, 92, 184, 368], stem_channel=16, num_heads=[1, 2, 4, 8], depths=[2, 2, 10, 2],\n                mlp_ratios=[3.6, 3.6, 3.6, 3.6], qk_ratio=1, sr_ratios=[8, 4, 2, 1], **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre>"},{"location":"reference/models/#mindcv.models.cmt.cmt_xsmall","title":"<code>mindcv.models.cmt.cmt_xsmall(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>CMT-XSmall</p> Source code in <code>mindcv\\models\\cmt.py</code> <pre><code>@register_model\ndef cmt_xsmall(pretrained=False, num_classes: int = 1000, in_channels: int = 3, **kwargs):\n    \"\"\"\n    CMT-XSmall\n    \"\"\"\n    default_cfg = default_cfgs[\"cmt_xsmall\"]\n\n    model = CMT(img_size=192, num_classes=num_classes, in_channels=in_channels, qkv_bias=True,\n                embed_dims=[52, 104, 208, 416], stem_channel=16, num_heads=[1, 2, 4, 8], depths=[3, 3, 12, 3],\n                mlp_ratios=[3.8, 3.8, 3.8, 3.8], qk_ratio=1, sr_ratios=[8, 4, 2, 1], **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre>"},{"location":"reference/models/#coat","title":"coat","text":""},{"location":"reference/models/#mindcv.models.coat","title":"<code>mindcv.models.coat</code>","text":"<p>CoaT architecture. Modified from timm/models/vision_transformer.py</p>"},{"location":"reference/models/#mindcv.models.coat.CoaT","title":"<code>mindcv.models.coat.CoaT</code>","text":"<p>               Bases: <code>Cell</code></p> <p>CoaT class.</p> Source code in <code>mindcv\\models\\coat.py</code> <pre><code>class CoaT(nn.Cell):\n    \"\"\" CoaT class. \"\"\"\n\n    def __init__(\n        self,\n        image_size=224,\n        patch_size=16,\n        in_chans=3,\n        num_classes=1000,\n        embed_dims=[0, 0, 0, 0],\n        serial_depths=[0, 0, 0, 0],\n        parallel_depth=0,\n        num_heads=0,\n        mlp_ratios=[0, 0, 0, 0],\n        qkv_bias=True,\n        drop_rate=0.,\n        attn_drop_rate=0.,\n        drop_path_rate=0.,\n        return_interm_layers=False,\n        out_features=None,\n        crpe_window={3: 2, 5: 3, 7: 3},\n        **kwargs\n    ) -&gt; None:\n        super().__init__()\n        self.return_interm_layers = return_interm_layers\n        self.out_features = out_features\n        self.num_classes = num_classes\n\n        self.patch_embed1 = PatchEmbed(image_size=image_size, patch_size=patch_size,\n                                       in_chans=in_chans, embed_dim=embed_dims[0])\n        self.patch_embed2 = PatchEmbed(image_size=image_size // (2**2), patch_size=2,\n                                       in_chans=embed_dims[0], embed_dim=embed_dims[1])\n        self.patch_embed3 = PatchEmbed(image_size=image_size // (2**3), patch_size=2,\n                                       in_chans=embed_dims[1], embed_dim=embed_dims[2])\n        self.patch_embed4 = PatchEmbed(image_size=image_size // (2**4), patch_size=2,\n                                       in_chans=embed_dims[2], embed_dim=embed_dims[3])\n\n        self.cls_token1 = mindspore.Parameter(ops.Zeros()((1, 1, embed_dims[0]), mindspore.float32))\n        self.cls_token2 = mindspore.Parameter(ops.Zeros()((1, 1, embed_dims[1]), mindspore.float32))\n        self.cls_token3 = mindspore.Parameter(ops.Zeros()((1, 1, embed_dims[2]), mindspore.float32))\n        self.cls_token4 = mindspore.Parameter(ops.Zeros()((1, 1, embed_dims[3]), mindspore.float32))\n\n        self.cpe1 = ConvPosEnc(dim=embed_dims[0], k=3)\n        self.cpe2 = ConvPosEnc(dim=embed_dims[1], k=3)\n        self.cpe3 = ConvPosEnc(dim=embed_dims[2], k=3)\n        self.cpe4 = ConvPosEnc(dim=embed_dims[3], k=3)\n\n        self.crpe1 = ConvRelPosEnc(Ch=embed_dims[0] // num_heads, h=num_heads, window=crpe_window)\n        self.crpe2 = ConvRelPosEnc(Ch=embed_dims[1] // num_heads, h=num_heads, window=crpe_window)\n        self.crpe3 = ConvRelPosEnc(Ch=embed_dims[2] // num_heads, h=num_heads, window=crpe_window)\n        self.crpe4 = ConvRelPosEnc(Ch=embed_dims[3] // num_heads, h=num_heads, window=crpe_window)\n\n        dpr = drop_path_rate\n\n        self.serial_blocks1 = nn.CellList([\n            SerialBlock(\n                dim=embed_dims[0], num_heads=num_heads, mlp_ratio=mlp_ratios[0], qkv_bias=qkv_bias,\n                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr,\n                shared_cpe=self.cpe1, shared_crpe=self.crpe1\n            )\n            for _ in range(serial_depths[0])]\n        )\n\n        self.serial_blocks2 = nn.CellList([\n            SerialBlock(\n                dim=embed_dims[1], num_heads=num_heads, mlp_ratio=mlp_ratios[1], qkv_bias=qkv_bias,\n                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr,\n                shared_cpe=self.cpe2, shared_crpe=self.crpe2\n            )\n            for _ in range(serial_depths[1])]\n        )\n\n        self.serial_blocks3 = nn.CellList([\n            SerialBlock(\n                dim=embed_dims[2], num_heads=num_heads, mlp_ratio=mlp_ratios[2], qkv_bias=qkv_bias,\n                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr,\n                shared_cpe=self.cpe3, shared_crpe=self.crpe3\n            )\n            for _ in range(serial_depths[2])]\n        )\n\n        self.serial_blocks4 = nn.CellList([\n            SerialBlock(\n                dim=embed_dims[3], num_heads=num_heads, mlp_ratio=mlp_ratios[3], qkv_bias=qkv_bias,\n                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr,\n                shared_cpe=self.cpe4, shared_crpe=self.crpe4\n            )\n            for _ in range(serial_depths[3])]\n        )\n\n        self.parallel_depth = parallel_depth\n        if self.parallel_depth &gt; 0:\n            self.parallel_blocks = nn.CellList([\n                ParallelBlock(dims=embed_dims,\n                              num_heads=num_heads,\n                              mlp_ratios=mlp_ratios,\n                              qkv_bias=qkv_bias,\n                              drop=drop_rate,\n                              attn_drop=attn_drop_rate,\n                              drop_path=dpr,\n                              shared_cpes=[self.cpe1, self.cpe2, self.cpe3, self.cpe4],\n                              shared_crpes=[self.crpe1, self.crpe2, self.crpe3, self.crpe4]\n                              )\n                for _ in range(parallel_depth)]\n            )\n        else:\n            self.parallel_blocks = None\n\n        if not self.return_interm_layers:\n            if self.parallel_blocks is not None:\n                self.norm2 = nn.LayerNorm((embed_dims[1],), epsilon=1e-6)\n                self.norm3 = nn.LayerNorm((embed_dims[2],), epsilon=1e-6)\n            else:\n                self.norm2 = None\n                self.norm3 = None\n\n            self.norm4 = nn.LayerNorm((embed_dims[3],), epsilon=1e-6)\n\n            if self.parallel_depth &gt; 0:\n                self.aggregate = nn.Conv1d(in_channels=3,\n                                           out_channels=1,\n                                           kernel_size=1,\n                                           has_bias=True)\n                self.head = nn.Dense(embed_dims[3], num_classes) if num_classes &gt; 0 else Identity()\n            else:\n                self.aggregate = None\n                self.head = nn.Dense(embed_dims[3], num_classes) if num_classes &gt; 0 else Identity()\n\n        self.cls_token1.set_data(init.initializer(init.TruncatedNormal(sigma=.02), self.cls_token1.data.shape))\n        self.cls_token2.set_data(init.initializer(init.TruncatedNormal(sigma=.02), self.cls_token2.data.shape))\n        self.cls_token3.set_data(init.initializer(init.TruncatedNormal(sigma=.02), self.cls_token3.data.shape))\n        self.cls_token4.set_data(init.initializer(init.TruncatedNormal(sigma=.02), self.cls_token4.data.shape))\n        self._initialize_weights()\n\n    def _initialize_weights(self) -&gt; None:\n        for _, cell in self.cells_and_names():\n            if isinstance(cell, nn.Dense):\n                cell.weight.set_data(init.initializer(init.TruncatedNormal(sigma=.02), cell.weight.data.shape))\n                if cell.bias is not None:\n                    cell.bias.set_data(init.initializer(init.Constant(0), cell.bias.shape))\n            elif isinstance(cell, nn.LayerNorm):\n                cell.gamma.set_data(init.initializer(init.Constant(1.0), cell.gamma.shape))\n                cell.beta.set_data(init.initializer(init.Constant(0), cell.beta.shape))\n\n    def insert_cls(self, x, cls_token) -&gt; Tensor:\n        t0 = x.shape[0]\n        t1 = cls_token.shape[1]\n        t2 = cls_token.shape[2]\n        y = Tensor(np.ones((t0, t1, t2)))\n        cls_tokens = cls_token.expand_as(y)\n\n        x = ops.concat((cls_tokens, x), axis=1)\n        return x\n\n    def remove_cls(self, x: Tensor) -&gt; Tensor:\n        return x[:, 1:, :]\n\n    def forward_features(self, x0: Tensor) -&gt; Union[dict, Tensor]:\n        B = x0.shape[0]\n\n        x1 = self.patch_embed1(x0)\n        H1, W1 = self.patch_embed1.patches_resolution\n        x1 = self.insert_cls(x1, self.cls_token1)\n        for blk in self.serial_blocks1:\n            x1 = blk(x1, size=(H1, W1))\n        x1_nocls = self.remove_cls(x1)\n        x1_nocls = ops.reshape(x1_nocls, (B, H1, W1, -1))\n        x1_nocls = ops.transpose(x1_nocls, (0, 3, 1, 2))\n\n        x2 = self.patch_embed2(x1_nocls)\n        H2, W2 = self.patch_embed2.patches_resolution\n        x2 = self.insert_cls(x2, self.cls_token2)\n        for blk in self.serial_blocks2:\n            x2 = blk(x2, size=(H2, W2))\n        x2_nocls = self.remove_cls(x2)\n        x2_nocls = ops.reshape(x2_nocls, (B, H2, W2, -1))\n        x2_nocls = ops.transpose(x2_nocls, (0, 3, 1, 2))\n\n        x3 = self.patch_embed3(x2_nocls)\n        H3, W3 = self.patch_embed3.patches_resolution\n        x3 = self.insert_cls(x3, self.cls_token3)\n        for blk in self.serial_blocks3:\n            x3 = blk(x3, size=(H3, W3))\n        x3_nocls = self.remove_cls(x3)\n        x3_nocls = ops.reshape(x3_nocls, (B, H3, W3, -1))\n        x3_nocls = ops.transpose(x3_nocls, (0, 3, 1, 2))\n\n        x4 = self.patch_embed4(x3_nocls)\n        H4, W4 = self.patch_embed4.patches_resolution\n        x4 = self.insert_cls(x4, self.cls_token4)\n        for blk in self.serial_blocks4:\n            x4 = blk(x4, size=(H4, W4))\n        x4_nocls = self.remove_cls(x4)\n        x4_nocls = ops.reshape(x4_nocls, (B, H4, W4, -1))\n        x4_nocls = ops.transpose(x4_nocls, (0, 3, 1, 2))\n\n        if self.parallel_depth == 0:\n            if self.return_interm_layers:\n                feat_out = {}\n                if 'x1_nocls' in self.out_features:\n                    feat_out['x1_nocls'] = x1_nocls\n                if 'x2_nocls' in self.out_features:\n                    feat_out['x2_nocls'] = x2_nocls\n                if 'x3_nocls' in self.out_features:\n                    feat_out['x3_nocls'] = x3_nocls\n                if 'x4_nocls' in self.out_features:\n                    feat_out['x4_nocls'] = x4_nocls\n                return feat_out\n            else:\n                x4 = self.norm4(x4)\n                x4_cls = x4[:, 0]\n                return x4_cls\n\n        for blk in self.parallel_blocks:\n            x1, x2, x3, x4 = blk(x1, x2, x3, x4, sizes=[(H1, W1), (H2, W2), (H3, W3), (H4, W4)])\n\n        if self.return_interm_layers:\n            feat_out = {}\n            if 'x1_nocls' in self.out_features:\n                x1_nocls = x1[:, 1:, :].reshape((B, H1, W1, -1)).transpose((0, 3, 1, 2))\n                feat_out['x1_nocls'] = x1_nocls\n            if 'x2_nocls' in self.out_features:\n                x2_nocls = x2[:, 1:, :].reshape((B, H2, W2, -1)).transpose((0, 3, 1, 2))\n                feat_out['x2_nocls'] = x2_nocls\n            if 'x3_nocls' in self.out_features:\n                x3_nocls = x3[:, 1:, :].reshape((B, H3, W3, -1)).transpose((0, 3, 1, 2))\n                feat_out['x3_nocls'] = x3_nocls\n            if 'x4_nocls' in self.out_features:\n                x4_nocls = x4[:, 1:, :].reshape((B, H4, W4, -1)).transpose((0, 3, 1, 2))\n                feat_out['x4_nocls'] = x4_nocls\n            return feat_out\n        else:\n            x2 = self.norm2(x2)\n            x3 = self.norm3(x3)\n            x4 = self.norm4(x4)\n            x2_cls = x2[:, :1]\n            x3_cls = x3[:, :1]\n            x4_cls = x4[:, :1]\n            merged_cls = ops.concat((x2_cls, x3_cls, x4_cls), axis=1)\n            merged_cls = self.aggregate(merged_cls).squeeze(axis=1)\n            return merged_cls\n\n    def construct(self, x: Tensor) -&gt; Union[dict, Tensor]:\n        if self.return_interm_layers:\n            return self.forward_features(x)\n        else:\n            x = self.forward_features(x)\n            x = self.head(x)\n            return x\n</code></pre>"},{"location":"reference/models/#mindcv.models.coat.ConvPosEnc","title":"<code>mindcv.models.coat.ConvPosEnc</code>","text":"<p>               Bases: <code>Cell</code></p> <p>Convolutional Position Encoding. Note: This module is similar to the conditional position encoding in CPVT.</p> Source code in <code>mindcv\\models\\coat.py</code> <pre><code>class ConvPosEnc(nn.Cell):\n    \"\"\" Convolutional Position Encoding.\n        Note: This module is similar to the conditional position encoding in CPVT.\n    \"\"\"\n\n    def __init__(\n        self,\n        dim,\n        k=3\n    ) -&gt; None:\n        super(ConvPosEnc, self).__init__()\n        self.proj = nn.Conv2d(in_channels=dim,\n                              out_channels=dim,\n                              kernel_size=k,\n                              stride=1,\n                              padding=k // 2,\n                              group=dim,\n                              pad_mode='pad',\n                              has_bias=True)\n\n    def construct(self, x, size) -&gt; Tensor:\n        B, N, C = x.shape\n        H, W = size\n\n        cls_token, img_tokens = x[:, :1], x[:, 1:]\n\n        feat = ops.transpose(img_tokens, (0, 2, 1))\n        feat = ops.reshape(feat, (B, C, H, W))\n        x = ops.add(self.proj(feat), feat)\n\n        x = ops.reshape(x, (B, C, H * W))\n        x = ops.transpose(x, (0, 2, 1))\n\n        x = ops.concat((cls_token, x), axis=1)\n        return x\n</code></pre>"},{"location":"reference/models/#mindcv.models.coat.FactorAtt_ConvRelPosEnc","title":"<code>mindcv.models.coat.FactorAtt_ConvRelPosEnc</code>","text":"<p>               Bases: <code>Cell</code></p> <p>Factorized attention with convolutional relative position encoding class.</p> Source code in <code>mindcv\\models\\coat.py</code> <pre><code>class FactorAtt_ConvRelPosEnc(nn.Cell):\n    \"\"\"Factorized attention with convolutional relative position encoding class.\"\"\"\n\n    def __init__(\n        self,\n        dim,\n        num_heads=8,\n        qkv_bias=False,\n        attn_drop=0.,\n        proj_drop=0.,\n        shared_crpe=None\n    ) -&gt; None:\n        super().__init__()\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        self.scale = head_dim ** -0.5\n\n        self.q = nn.Dense(in_channels=dim, out_channels=dim, has_bias=qkv_bias)\n        self.k = nn.Dense(in_channels=dim, out_channels=dim, has_bias=qkv_bias)\n        self.v = nn.Dense(in_channels=dim, out_channels=dim, has_bias=qkv_bias)\n        self.attn_drop = Dropout(p=attn_drop)\n        self.proj = nn.Dense(dim, dim)\n        self.proj_drop = Dropout(p=proj_drop)\n        self.softmax = nn.Softmax(axis=-1)\n        self.batch_matmul = ops.BatchMatMul()\n\n        self.crpe = shared_crpe\n\n    def construct(self, x, size) -&gt; Tensor:\n        B, N, C = x.shape\n        q = ops.reshape(self.q(x), (B, N, self.num_heads, C // self.num_heads))\n        q = ops.transpose(q, (0, 2, 1, 3))\n        k = ops.reshape(self.k(x), (B, N, self.num_heads, C // self.num_heads))\n        k = ops.transpose(k, (0, 2, 3, 1))\n        v = ops.reshape(self.v(x), (B, N, self.num_heads, C // self.num_heads))\n        v = ops.transpose(v, (0, 2, 1, 3))\n\n        k_softmax = self.softmax(k)\n        factor_att = self.batch_matmul(q, k_softmax)\n        factor_att = self.batch_matmul(factor_att, v)\n\n        crpe = self.crpe(q, v, size=size)\n\n        x = ops.mul(self.scale, factor_att)\n        x = ops.add(x, crpe)\n        x = ops.transpose(x, (0, 2, 1, 3))\n        x = ops.reshape(x, (B, N, C))\n\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n</code></pre>"},{"location":"reference/models/#mindcv.models.coat.Mlp","title":"<code>mindcv.models.coat.Mlp</code>","text":"<p>               Bases: <code>Cell</code></p> <p>MLP Cell</p> Source code in <code>mindcv\\models\\coat.py</code> <pre><code>class Mlp(nn.Cell):\n    \"\"\"MLP Cell\"\"\"\n\n    def __init__(\n        self,\n        in_features,\n        hidden_features=None,\n        out_features=None,\n        drop=0.0\n    ) -&gt; None:\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Dense(in_channels=in_features, out_channels=hidden_features, has_bias=True)\n        self.act = nn.GELU(approximate=False)\n        self.fc2 = nn.Dense(in_channels=hidden_features, out_channels=out_features, has_bias=True)\n        self.drop = Dropout(p=drop)\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.drop(x)\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x\n</code></pre>"},{"location":"reference/models/#mindcv.models.coat.ParallelBlock","title":"<code>mindcv.models.coat.ParallelBlock</code>","text":"<p>               Bases: <code>Cell</code></p> <p>Parallel block class.</p> Source code in <code>mindcv\\models\\coat.py</code> <pre><code>class ParallelBlock(nn.Cell):\n    \"\"\" Parallel block class. \"\"\"\n\n    def __init__(\n        self,\n        dims,\n        num_heads,\n        mlp_ratios=[],\n        qkv_bias=False,\n        drop=0.,\n        attn_drop=0.,\n        drop_path=0.,\n        shared_cpes=None,\n        shared_crpes=None\n    ) -&gt; None:\n        super().__init__()\n\n        self.cpes = shared_cpes\n\n        self.norm12 = nn.LayerNorm((dims[1],), epsilon=1e-6)\n        self.norm13 = nn.LayerNorm((dims[2],), epsilon=1e-6)\n        self.norm14 = nn.LayerNorm((dims[3],), epsilon=1e-6)\n        self.factoratt_crpe2 = FactorAtt_ConvRelPosEnc(dims[1],\n                                                       num_heads=num_heads,\n                                                       qkv_bias=qkv_bias,\n                                                       attn_drop=attn_drop,\n                                                       proj_drop=drop,\n                                                       shared_crpe=shared_crpes[1]\n                                                       )\n        self.factoratt_crpe3 = FactorAtt_ConvRelPosEnc(dims[2],\n                                                       num_heads=num_heads,\n                                                       qkv_bias=qkv_bias,\n                                                       attn_drop=attn_drop,\n                                                       proj_drop=drop,\n                                                       shared_crpe=shared_crpes[2]\n                                                       )\n        self.factoratt_crpe4 = FactorAtt_ConvRelPosEnc(dims[3],\n                                                       num_heads=num_heads,\n                                                       qkv_bias=qkv_bias,\n                                                       attn_drop=attn_drop,\n                                                       proj_drop=drop,\n                                                       shared_crpe=shared_crpes[3]\n                                                       )\n        self.drop_path = DropPath(drop_path) if drop_path &gt; 0. else Identity()\n        self.interpolate_fn = Interpolate(mode=\"bilinear\", align_corners=True)\n\n        self.norm22 = nn.LayerNorm((dims[1],), epsilon=1e-6)\n        self.norm23 = nn.LayerNorm((dims[2],), epsilon=1e-6)\n        self.norm24 = nn.LayerNorm((dims[3],), epsilon=1e-6)\n\n        mlp_hidden_dim = int(dims[1] * mlp_ratios[1])\n        self.mlp2 = self.mlp3 = self.mlp4 = Mlp(in_features=dims[1], hidden_features=mlp_hidden_dim, drop=drop)\n\n    def upsample(self, x, output_size, size) -&gt; Tensor:\n        \"\"\" Feature map up-sampling. \"\"\"\n        return self.interpolate(x, output_size=output_size, size=size)\n\n    def downsample(self, x, output_size, size) -&gt; Tensor:\n        \"\"\" Feature map down-sampling. \"\"\"\n        return self.interpolate(x, output_size=output_size, size=size)\n\n    def interpolate(self, x, output_size, size) -&gt; Tensor:\n        \"\"\" Feature map interpolation. \"\"\"\n        B, N, C = x.shape\n        H, W = size\n\n        cls_token = x[:, :1, :]\n        img_tokens = x[:, 1:, :]\n\n        img_tokens = ops.transpose(img_tokens, (0, 2, 1))\n        img_tokens = ops.reshape(img_tokens, (B, C, H, W))\n        img_tokens = self.interpolate_fn(img_tokens, size=output_size)\n        img_tokens = ops.reshape(img_tokens, (B, C, -1))\n        img_tokens = ops.transpose(img_tokens, (0, 2, 1))\n\n        out = ops.concat((cls_token, img_tokens), axis=1)\n        return out\n\n    def construct(self, x1, x2, x3, x4, sizes) -&gt; tuple:\n        _, (H2, W2), (H3, W3), (H4, W4) = sizes\n\n        # Conv-Attention.\n        x2 = self.cpes[1](x2, size=(H2, W2))  # Note: x1 is ignored.\n        x3 = self.cpes[2](x3, size=(H3, W3))\n        x4 = self.cpes[3](x4, size=(H4, W4))\n\n        cur2 = self.norm12(x2)\n        cur3 = self.norm13(x3)\n        cur4 = self.norm14(x4)\n        cur2 = self.factoratt_crpe2(cur2, size=(H2, W2))\n        cur3 = self.factoratt_crpe3(cur3, size=(H3, W3))\n        cur4 = self.factoratt_crpe4(cur4, size=(H4, W4))\n        upsample3_2 = self.upsample(cur3, output_size=(H2, W2), size=(H3, W3))\n        upsample4_3 = self.upsample(cur4, output_size=(H3, W3), size=(H4, W4))\n        upsample4_2 = self.upsample(cur4, output_size=(H2, W2), size=(H4, W4))\n        downsample2_3 = self.downsample(cur2, output_size=(H3, W3), size=(H2, W2))\n        downsample3_4 = self.downsample(cur3, output_size=(H4, W4), size=(H3, W3))\n        downsample2_4 = self.downsample(cur2, output_size=(H4, W4), size=(H2, W2))\n        cur2 = cur2 + upsample3_2 + upsample4_2\n        cur3 = cur3 + upsample4_3 + downsample2_3\n        cur4 = cur4 + downsample3_4 + downsample2_4\n        x2 = x2 + self.drop_path(cur2)\n        x3 = x3 + self.drop_path(cur3)\n        x4 = x4 + self.drop_path(cur4)\n\n        cur2 = self.norm22(x2)\n        cur3 = self.norm23(x3)\n        cur4 = self.norm24(x4)\n        cur2 = self.mlp2(cur2)\n        cur3 = self.mlp3(cur3)\n        cur4 = self.mlp4(cur4)\n        x2 = x2 + self.drop_path(cur2)\n        x3 = x3 + self.drop_path(cur3)\n        x4 = x4 + self.drop_path(cur4)\n\n        return x1, x2, x3, x4\n</code></pre>"},{"location":"reference/models/#mindcv.models.coat.ParallelBlock.downsample","title":"<code>mindcv.models.coat.ParallelBlock.downsample(x, output_size, size)</code>","text":"<p>Feature map down-sampling.</p> Source code in <code>mindcv\\models\\coat.py</code> <pre><code>def downsample(self, x, output_size, size) -&gt; Tensor:\n    \"\"\" Feature map down-sampling. \"\"\"\n    return self.interpolate(x, output_size=output_size, size=size)\n</code></pre>"},{"location":"reference/models/#mindcv.models.coat.ParallelBlock.interpolate","title":"<code>mindcv.models.coat.ParallelBlock.interpolate(x, output_size, size)</code>","text":"<p>Feature map interpolation.</p> Source code in <code>mindcv\\models\\coat.py</code> <pre><code>def interpolate(self, x, output_size, size) -&gt; Tensor:\n    \"\"\" Feature map interpolation. \"\"\"\n    B, N, C = x.shape\n    H, W = size\n\n    cls_token = x[:, :1, :]\n    img_tokens = x[:, 1:, :]\n\n    img_tokens = ops.transpose(img_tokens, (0, 2, 1))\n    img_tokens = ops.reshape(img_tokens, (B, C, H, W))\n    img_tokens = self.interpolate_fn(img_tokens, size=output_size)\n    img_tokens = ops.reshape(img_tokens, (B, C, -1))\n    img_tokens = ops.transpose(img_tokens, (0, 2, 1))\n\n    out = ops.concat((cls_token, img_tokens), axis=1)\n    return out\n</code></pre>"},{"location":"reference/models/#mindcv.models.coat.ParallelBlock.upsample","title":"<code>mindcv.models.coat.ParallelBlock.upsample(x, output_size, size)</code>","text":"<p>Feature map up-sampling.</p> Source code in <code>mindcv\\models\\coat.py</code> <pre><code>def upsample(self, x, output_size, size) -&gt; Tensor:\n    \"\"\" Feature map up-sampling. \"\"\"\n    return self.interpolate(x, output_size=output_size, size=size)\n</code></pre>"},{"location":"reference/models/#mindcv.models.coat.PatchEmbed","title":"<code>mindcv.models.coat.PatchEmbed</code>","text":"<p>               Bases: <code>Cell</code></p> <p>Image to Patch Embedding</p> Source code in <code>mindcv\\models\\coat.py</code> <pre><code>class PatchEmbed(nn.Cell):\n    \"\"\" Image to Patch Embedding \"\"\"\n\n    def __init__(\n        self,\n        image_size=224,\n        patch_size=4,\n        in_chans=3,\n        embed_dim=96\n    ) -&gt; None:\n        super().__init__()\n        image_size = (image_size, image_size)\n        patch_size = (patch_size, patch_size)\n        patches_resolution = [image_size[0] // patch_size[0], image_size[1] // patch_size[1]]\n\n        self.image_size = image_size\n        self.patch_size = patch_size\n        self.patches_resolution = patches_resolution\n        self.num_patches = patches_resolution[0] * patches_resolution[1]\n\n        self.in_chans = in_chans\n        self.embed_dim = embed_dim\n\n        self.proj = nn.Conv2d(in_channels=in_chans,\n                              out_channels=embed_dim,\n                              kernel_size=patch_size,\n                              stride=patch_size,\n                              pad_mode='valid',\n                              has_bias=True)\n\n        self.norm = nn.LayerNorm((embed_dim,), epsilon=1e-5)\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        B = x.shape[0]\n\n        x = ops.reshape(self.proj(x), (B, self.embed_dim, -1))\n        x = ops.transpose(x, (0, 2, 1))\n        x = self.norm(x)\n\n        return x\n</code></pre>"},{"location":"reference/models/#mindcv.models.coat.SerialBlock","title":"<code>mindcv.models.coat.SerialBlock</code>","text":"<p>               Bases: <code>Cell</code></p> <p>Serial block class.     Note: In this implementation, each serial block only contains a conv-attention and a FFN (MLP) module.</p> Source code in <code>mindcv\\models\\coat.py</code> <pre><code>class SerialBlock(nn.Cell):\n    \"\"\"\n    Serial block class.\n        Note: In this implementation, each serial block only contains a conv-attention and a FFN (MLP) module.\n    \"\"\"\n\n    def __init__(\n        self,\n        dim,\n        num_heads,\n        mlp_ratio=4.,\n        qkv_bias=False,\n        drop=0.,\n        attn_drop=0.,\n        drop_path=0.,\n        shared_cpe=None,\n        shared_crpe=None\n    ) -&gt; None:\n        super().__init__()\n\n        self.cpe = shared_cpe\n\n        self.norm1 = nn.LayerNorm((dim,), epsilon=1e-6)\n        self.factoratt_crpe = FactorAtt_ConvRelPosEnc(dim,\n                                                      num_heads=num_heads,\n                                                      qkv_bias=qkv_bias,\n                                                      attn_drop=attn_drop,\n                                                      proj_drop=drop,\n                                                      shared_crpe=shared_crpe\n                                                      )\n        self.drop_path = DropPath(drop_path) if drop_path &gt; 0. else Identity()\n\n        self.norm2 = nn.LayerNorm((dim,), epsilon=1e-6)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, drop=drop)\n\n    def construct(self, x, size) -&gt; Tensor:\n        x = x + self.drop_path(self.factoratt_crpe(self.norm1(self.cpe(x, size)), size))\n        x = x + self.drop_path(self.mlp(self.norm2(x)))\n        return x\n</code></pre>"},{"location":"reference/models/#convit","title":"convit","text":""},{"location":"reference/models/#mindcv.models.convit","title":"<code>mindcv.models.convit</code>","text":"<p>MindSpore implementation of <code>ConViT</code>. Refer to ConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases</p>"},{"location":"reference/models/#mindcv.models.convit.Block","title":"<code>mindcv.models.convit.Block</code>","text":"<p>               Bases: <code>Cell</code></p> <p>Basic module of ConViT</p> Source code in <code>mindcv\\models\\convit.py</code> <pre><code>class Block(nn.Cell):\n    \"\"\"Basic module of ConViT\"\"\"\n\n    def __init__(\n        self,\n        dim: int,\n        num_heads: int,\n        mlp_ratio: float,\n        qkv_bias: bool = False,\n        drop: float = 0.0,\n        attn_drop: float = 0.0,\n        drop_path: float = 0.0,\n        use_gpsa: bool = True,\n        **kwargs\n    ) -&gt; None:\n        super().__init__()\n\n        self.norm1 = nn.LayerNorm((dim,))\n        if use_gpsa:\n            self.attn = GPSA(dim, num_heads=num_heads, qkv_bias=qkv_bias,\n                             attn_drop=attn_drop, proj_drop=drop, **kwargs)\n        else:\n            self.attn = MHSA(dim, num_heads=num_heads, qkv_bias=qkv_bias,\n                             attn_drop=attn_drop, proj_drop=drop, **kwargs)\n        self.drop_path = DropPath(drop_path) if drop_path &gt; 0.0 else Identity()\n        self.norm2 = nn.LayerNorm((dim,))\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=nn.GELU, drop=drop)\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        x = x + self.drop_path(self.attn(self.norm1(x)))\n        x = x + self.drop_path(self.mlp(self.norm2(x)))\n        return x\n</code></pre>"},{"location":"reference/models/#mindcv.models.convit.ConViT","title":"<code>mindcv.models.convit.ConViT</code>","text":"<p>               Bases: <code>Cell</code></p> <p>ConViT model class, based on '\"Improving Vision Transformers with Soft Convolutional Inductive Biases\" https://arxiv.org/pdf/2103.10697.pdf'</p> PARAMETER DESCRIPTION <code>in_channels</code> <p>number the channels of the input. Default: 3.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>num_classes</code> <p>number of classification classes. Default: 1000.</p> <p> TYPE: <code>int) </code> DEFAULT: <code>1000</code> </p> <code>image_size</code> <p>images input size. Default: 224.</p> <p> TYPE: <code>int) </code> DEFAULT: <code>224</code> </p> <code>patch_size</code> <p>image patch size. Default: 16.</p> <p> TYPE: <code>int) </code> DEFAULT: <code>16</code> </p> <code>embed_dim</code> <p>embedding dimension in all head. Default: 48.</p> <p> TYPE: <code>int) </code> DEFAULT: <code>48</code> </p> <code>num_heads</code> <p>number of heads. Default: 12.</p> <p> TYPE: <code>int) </code> DEFAULT: <code>12</code> </p> <code>drop_rate</code> <p>dropout rate. Default: 0.</p> <p> TYPE: <code>float) </code> DEFAULT: <code>0.0</code> </p> <code>drop_path_rate</code> <p>drop path rate. Default: 0.1.</p> <p> TYPE: <code>float) </code> DEFAULT: <code>0.1</code> </p> <code>depth</code> <p>model block depth. Default: 12.</p> <p> TYPE: <code>int) </code> DEFAULT: <code>12</code> </p> <code>mlp_ratio</code> <p>ratio of hidden features in Mlp. Default: 4.</p> <p> TYPE: <code>float) </code> DEFAULT: <code>4.0</code> </p> <code>qkv_bias</code> <p>have bias in qkv layers or not. Default: False.</p> <p> TYPE: <code>bool) </code> DEFAULT: <code>False</code> </p> <code>attn_drop_rate</code> <p>attention layers dropout rate. Default: 0.</p> <p> TYPE: <code>float) </code> DEFAULT: <code>0.0</code> </p> <code>locality_strength</code> <p>determines how focused each head is around its attention center. Default: 1.</p> <p> TYPE: <code>float) </code> DEFAULT: <code>1.0</code> </p> <code>local_up_to_layer</code> <p>number of GPSA layers. Default: 10.</p> <p> TYPE: <code>int) </code> DEFAULT: <code>10</code> </p> <code>use_pos_embed</code> <p>whether use the embeded position.  Default: True.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>locality_strength\uff08float\uff09</code> <p>the strength of locality. Default: 1.</p> <p> </p> Source code in <code>mindcv\\models\\convit.py</code> <pre><code>class ConViT(nn.Cell):\n    r\"\"\"ConViT model class, based on\n    '\"Improving Vision Transformers with Soft Convolutional Inductive Biases\"\n    &lt;https://arxiv.org/pdf/2103.10697.pdf&gt;'\n\n    Args:\n        in_channels (int): number the channels of the input. Default: 3.\n        num_classes (int) : number of classification classes. Default: 1000.\n        image_size (int) : images input size. Default: 224.\n        patch_size (int) : image patch size. Default: 16.\n        embed_dim (int) : embedding dimension in all head. Default: 48.\n        num_heads (int) : number of heads. Default: 12.\n        drop_rate (float) : dropout rate. Default: 0.\n        drop_path_rate (float) : drop path rate. Default: 0.1.\n        depth (int) : model block depth. Default: 12.\n        mlp_ratio (float) : ratio of hidden features in Mlp. Default: 4.\n        qkv_bias (bool) : have bias in qkv layers or not. Default: False.\n        attn_drop_rate (float) : attention layers dropout rate. Default: 0.\n        locality_strength (float) : determines how focused each head is around its attention center. Default: 1.\n        local_up_to_layer (int) : number of GPSA layers. Default: 10.\n        use_pos_embed (bool): whether use the embeded position.  Default: True.\n        locality_strength\uff08float\uff09: the strength of locality. Default: 1.\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels: int = 3,\n        num_classes: int = 1000,\n        image_size: int = 224,\n        patch_size: int = 16,\n        embed_dim: int = 48,\n        num_heads: int = 12,\n        drop_rate: float = 0.0,\n        drop_path_rate: float = 0.1,\n        depth: int = 12,\n        mlp_ratio: float = 4.0,\n        qkv_bias: bool = False,\n        attn_drop_rate: float = 0.0,\n        local_up_to_layer: int = 10,\n        use_pos_embed: bool = True,\n        locality_strength: float = 1.0,\n    ) -&gt; None:\n        super().__init__()\n\n        self.local_up_to_layer = local_up_to_layer\n        self.use_pos_embed = use_pos_embed\n        self.num_heads = num_heads\n        self.locality_strength = locality_strength\n        self.embed_dim = embed_dim\n\n        self.patch_embed = PatchEmbed(\n            image_size=image_size, patch_size=patch_size, in_chans=in_channels, embed_dim=embed_dim)\n        self.num_patches = self.patch_embed.num_patches\n\n        self.cls_token = Parameter(ops.Zeros()((1, 1, embed_dim), ms.float32))\n        self.pos_drop = Dropout(p=drop_rate)\n\n        if self.use_pos_embed:\n            self.pos_embed = Parameter(ops.Zeros()((1, self.num_patches, embed_dim), ms.float32))\n            self.pos_embed.set_data(init.initializer(init.TruncatedNormal(sigma=0.02), self.pos_embed.data.shape))\n\n        dpr = [x.item() for x in np.linspace(0, drop_path_rate, depth)]\n        self.blocks = nn.CellList([\n            Block(\n                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias,\n                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i],\n                use_gpsa=True)\n            if i &lt; local_up_to_layer else\n            Block(\n                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias,\n                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i],\n                use_gpsa=False)\n            for i in range(depth)])\n        self.norm = nn.LayerNorm((embed_dim,))\n\n        self.classifier = nn.Dense(in_channels=embed_dim, out_channels=num_classes) if num_classes &gt; 0 else Identity()\n        self.cls_token.set_data(init.initializer(init.TruncatedNormal(sigma=0.02), self.cls_token.data.shape))\n        self._initialize_weights()\n\n    def _initialize_weights(self) -&gt; None:\n        for _, cell in self.cells_and_names():\n            if isinstance(cell, nn.Dense):\n                cell.weight.set_data(init.initializer(init.TruncatedNormal(sigma=0.02), cell.weight.data.shape))\n                if cell.bias is not None:\n                    cell.bias.set_data(init.initializer(init.Constant(0), cell.bias.shape))\n            elif isinstance(cell, nn.LayerNorm):\n                cell.gamma.set_data(init.initializer(init.Constant(1), cell.gamma.shape))\n                cell.beta.set_data(init.initializer(init.Constant(0), cell.beta.shape))\n        # local init\n        for i in range(self.local_up_to_layer):\n            self.blocks[i].attn.v.weight.set_data(ops.eye(self.embed_dim, self.embed_dim, ms.float32), slice_shape=True)\n            locality_distance = 1\n            kernel_size = int(self.num_heads**0.5)\n            center = (kernel_size - 1) / 2 if kernel_size % 2 == 0 else kernel_size // 2\n            pos_weight_data = self.blocks[i].attn.pos_proj.weight.data\n            for h1 in range(kernel_size):\n                for h2 in range(kernel_size):\n                    position = h1 + kernel_size * h2\n                    pos_weight_data[position, 2] = -1\n                    pos_weight_data[position, 1] = 2 * (h1 - center) * locality_distance\n                    pos_weight_data[position, 0] = 2 * (h2 - center) * locality_distance\n            pos_weight_data = pos_weight_data * self.locality_strength\n            self.blocks[i].attn.pos_proj.weight.set_data(pos_weight_data)\n\n    def forward_features(self, x: Tensor) -&gt; Tensor:\n        x = self.patch_embed(x)\n        if self.use_pos_embed:\n            x = x + self.pos_embed\n        x = self.pos_drop(x)\n        cls_tokens = ops.tile(self.cls_token, (x.shape[0], 1, 1))\n        for u, blk in enumerate(self.blocks):\n            if u == self.local_up_to_layer:\n                x = ops.Cast()(x, cls_tokens.dtype)\n                x = ops.concat((cls_tokens, x), 1)\n            x = blk(x)\n        x = self.norm(x)\n        return x[:, 0]\n\n    def forward_head(self, x: Tensor) -&gt; Tensor:\n        x = self.classifier(x)\n        return x\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n</code></pre>"},{"location":"reference/models/#mindcv.models.convit.convit_base","title":"<code>mindcv.models.convit.convit_base(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get ConViT base model Refer to the base class \"models.ConViT\" for more details.</p> Source code in <code>mindcv\\models\\convit.py</code> <pre><code>@register_model\ndef convit_base(pretrained: bool = False, num_classes: int = 1000, in_channels: int = 3, **kwargs) -&gt; ConViT:\n    \"\"\"Get ConViT base model\n    Refer to the base class \"models.ConViT\" for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"convit_base\"]\n    model = ConViT(in_channels=in_channels, num_classes=num_classes,\n                   num_heads=16, embed_dim=768, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre>"},{"location":"reference/models/#mindcv.models.convit.convit_base_plus","title":"<code>mindcv.models.convit.convit_base_plus(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get ConViT base+ model Refer to the base class \"models.ConViT\" for more details.</p> Source code in <code>mindcv\\models\\convit.py</code> <pre><code>@register_model\ndef convit_base_plus(pretrained: bool = False, num_classes: int = 1000, in_channels: int = 3, **kwargs) -&gt; ConViT:\n    \"\"\"Get ConViT base+ model\n    Refer to the base class \"models.ConViT\" for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"convit_base_plus\"]\n    model = ConViT(in_channels=in_channels, num_classes=num_classes,\n                   num_heads=16, embed_dim=1024, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre>"},{"location":"reference/models/#mindcv.models.convit.convit_small","title":"<code>mindcv.models.convit.convit_small(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get ConViT small model Refer to the base class \"models.ConViT\" for more details.</p> Source code in <code>mindcv\\models\\convit.py</code> <pre><code>@register_model\ndef convit_small(pretrained: bool = False, num_classes: int = 1000, in_channels: int = 3, **kwargs) -&gt; ConViT:\n    \"\"\"Get ConViT small model\n    Refer to the base class \"models.ConViT\" for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"convit_small\"]\n    model = ConViT(in_channels=in_channels, num_classes=num_classes,\n                   num_heads=9, embed_dim=432, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre>"},{"location":"reference/models/#mindcv.models.convit.convit_small_plus","title":"<code>mindcv.models.convit.convit_small_plus(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get ConViT small+ model Refer to the base class \"models.ConViT\" for more details.</p> Source code in <code>mindcv\\models\\convit.py</code> <pre><code>@register_model\ndef convit_small_plus(pretrained: bool = False, num_classes: int = 1000, in_channels: int = 3, **kwargs) -&gt; ConViT:\n    \"\"\"Get ConViT small+ model\n    Refer to the base class \"models.ConViT\" for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"convit_small_plus\"]\n    model = ConViT(in_channels=in_channels, num_classes=num_classes,\n                   num_heads=9, embed_dim=576, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre>"},{"location":"reference/models/#mindcv.models.convit.convit_tiny","title":"<code>mindcv.models.convit.convit_tiny(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get ConViT tiny model Refer to the base class \"models.ConViT\" for more details.</p> Source code in <code>mindcv\\models\\convit.py</code> <pre><code>@register_model\ndef convit_tiny(pretrained: bool = False, num_classes: int = 1000, in_channels: int = 3, **kwargs) -&gt; ConViT:\n    \"\"\"Get ConViT tiny model\n    Refer to the base class \"models.ConViT\" for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"convit_tiny\"]\n    model = ConViT(in_channels=in_channels, num_classes=num_classes,\n                   num_heads=4, embed_dim=192, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre>"},{"location":"reference/models/#mindcv.models.convit.convit_tiny_plus","title":"<code>mindcv.models.convit.convit_tiny_plus(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get ConViT tiny+ model Refer to the base class \"models.ConViT\" for more details.</p> Source code in <code>mindcv\\models\\convit.py</code> <pre><code>@register_model\ndef convit_tiny_plus(pretrained: bool = False, num_classes: int = 1000, in_channels: int = 3, **kwargs) -&gt; ConViT:\n    \"\"\"Get ConViT tiny+ model\n    Refer to the base class \"models.ConViT\" for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"convit_tiny_plus\"]\n    model = ConViT(in_channels=in_channels, num_classes=num_classes,\n                   num_heads=4, embed_dim=256, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre>"},{"location":"reference/models/#convnext","title":"convnext","text":""},{"location":"reference/models/#mindcv.models.convnext","title":"<code>mindcv.models.convnext</code>","text":"<p>MindSpore implementation of <code>ConvNeXt</code> and <code>ConvNeXt V2</code>. Refer to: A ConvNet for the 2020s           ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders</p>"},{"location":"reference/models/#mindcv.models.convnext.Block","title":"<code>mindcv.models.convnext.Block</code>","text":"<p>               Bases: <code>Cell</code></p> <p>ConvNeXt Block There are two equivalent implementations:   (1) DwConv -&gt; LayerNorm (channels_first) -&gt; 1x1 Conv -&gt; GELU -&gt; 1x1 Conv; all in (N, C, H, W)   (2) DwConv -&gt; Permute to (N, H, W, C); LayerNorm (channels_last) -&gt; Linear -&gt; GELU -&gt; Linear; Permute back Unlike the official impl, this one allows choice of 1 or 2, 1x1 conv can be faster with appropriate choice of LayerNorm impl, however as model size increases the tradeoffs appear to change and nn.Linear is a better choice. This was observed with PyTorch 1.10 on 3090 GPU, it could change over time &amp; w/ different HW. Args:     dim: Number of input channels.     drop_path: Stochastic depth rate. Default: 0.0.     layer_scale_init_value: Init value for Layer Scale. Default: 1e-6.</p> Source code in <code>mindcv\\models\\convnext.py</code> <pre><code>class Block(nn.Cell):\n    \"\"\"ConvNeXt Block\n    There are two equivalent implementations:\n      (1) DwConv -&gt; LayerNorm (channels_first) -&gt; 1x1 Conv -&gt; GELU -&gt; 1x1 Conv; all in (N, C, H, W)\n      (2) DwConv -&gt; Permute to (N, H, W, C); LayerNorm (channels_last) -&gt; Linear -&gt; GELU -&gt; Linear; Permute back\n    Unlike the official impl, this one allows choice of 1 or 2, 1x1 conv can be faster with appropriate\n    choice of LayerNorm impl, however as model size increases the tradeoffs appear to change and nn.Linear\n    is a better choice. This was observed with PyTorch 1.10 on 3090 GPU, it could change over time &amp; w/ different HW.\n    Args:\n        dim: Number of input channels.\n        drop_path: Stochastic depth rate. Default: 0.0.\n        layer_scale_init_value: Init value for Layer Scale. Default: 1e-6.\n    \"\"\"\n\n    def __init__(\n        self,\n        dim: int,\n        drop_path: float = 0.0,\n        layer_scale_init_value: float = 1e-6,\n        use_grn: bool = False,\n    ) -&gt; None:\n        super().__init__()\n        self.dwconv = nn.Conv2d(dim, dim, kernel_size=7, group=dim, has_bias=True)  # depthwise conv\n        self.norm = ConvNextLayerNorm((dim,), epsilon=1e-6)\n        self.pwconv1 = nn.Dense(dim, 4 * dim)  # pointwise/1x1 convs, implemented with Dense layers\n        self.act = nn.GELU()\n        self.use_grn = use_grn\n        if use_grn:\n            self.grn = GRN(4 * dim)\n        self.pwconv2 = nn.Dense(4 * dim, dim)\n        self.gamma_ = Parameter(Tensor(layer_scale_init_value * np.ones((dim)), dtype=mstype.float32),\n                                requires_grad=True) if layer_scale_init_value &gt; 0 else None\n        self.drop_path = DropPath(drop_path) if drop_path &gt; 0.0 else Identity()\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        downsample = x\n        x = self.dwconv(x)\n        x = ops.transpose(x, (0, 2, 3, 1))\n        x = self.norm(x)\n        x = self.pwconv1(x)\n        x = self.act(x)\n        if self.use_grn:\n            x = self.grn(x)\n        x = self.pwconv2(x)\n        if self.gamma_ is not None:\n            x = self.gamma_ * x\n        x = ops.transpose(x, (0, 3, 1, 2))\n        x = downsample + self.drop_path(x)\n        return x\n</code></pre>"},{"location":"reference/models/#mindcv.models.convnext.ConvNeXt","title":"<code>mindcv.models.convnext.ConvNeXt</code>","text":"<p>               Bases: <code>Cell</code></p> <p>ConvNeXt and ConvNeXt V2 model class, based on <code>\"A ConvNet for the 2020s\" &lt;https://arxiv.org/abs/2201.03545&gt;</code>_ and <code>\"ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders\" &lt;https://arxiv.org/abs/2301.00808&gt;</code>_</p> PARAMETER DESCRIPTION <code>in_channels</code> <p>dim of the input channel.</p> <p> TYPE: <code>int</code> </p> <code>num_classes</code> <p>dim of the classes predicted.</p> <p> TYPE: <code>int</code> </p> <code>depths</code> <p>the depths of each layer.</p> <p> TYPE: <code>List[int]</code> </p> <code>dims</code> <p>the middle dim of each layer.</p> <p> TYPE: <code>List[int]</code> </p> <code>drop_path_rate</code> <p>the rate of droppath. Default: 0.0.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>layer_scale_init_value</code> <p>the parameter of init for the classifier. Default: 1e-6.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1e-06</code> </p> <code>head_init_scale</code> <p>the parameter of init for the head. Default: 1.0.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>use_grn</code> <p>If True, use Global Response Normalization in each block. Default: False.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Source code in <code>mindcv\\models\\convnext.py</code> <pre><code>class ConvNeXt(nn.Cell):\n    r\"\"\"ConvNeXt and ConvNeXt V2 model class, based on\n    `\"A ConvNet for the 2020s\" &lt;https://arxiv.org/abs/2201.03545&gt;`_ and\n    `\"ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders\" &lt;https://arxiv.org/abs/2301.00808&gt;`_\n\n    Args:\n        in_channels: dim of the input channel.\n        num_classes: dim of the classes predicted.\n        depths: the depths of each layer.\n        dims: the middle dim of each layer.\n        drop_path_rate: the rate of droppath. Default: 0.0.\n        layer_scale_init_value: the parameter of init for the classifier. Default: 1e-6.\n        head_init_scale: the parameter of init for the head. Default: 1.0.\n        use_grn: If True, use Global Response Normalization in each block. Default: False.\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels: int,\n        num_classes: int,\n        depths: List[int],\n        dims: List[int],\n        drop_path_rate: float = 0.0,\n        layer_scale_init_value: float = 1e-6,\n        head_init_scale: float = 1.0,\n        use_grn: bool = False,\n    ):\n        super().__init__()\n\n        downsample_layers = []  # stem and 3 intermediate down_sampling conv layers\n        stem = nn.SequentialCell(\n            nn.Conv2d(in_channels, dims[0], kernel_size=4, stride=4, has_bias=True),\n            ConvNextLayerNorm((dims[0],), epsilon=1e-6, norm_axis=1),\n        )\n        downsample_layers.append(stem)\n        for i in range(3):\n            downsample_layer = nn.SequentialCell(\n                ConvNextLayerNorm((dims[i],), epsilon=1e-6, norm_axis=1),\n                nn.Conv2d(dims[i], dims[i + 1], kernel_size=2, stride=2, has_bias=True),\n            )\n            downsample_layers.append(downsample_layer)\n\n        total_reduction = 4\n        self.feature_info = []\n        self.flatten_sequential = True\n\n        stages = []  # 4 feature resolution stages, each consisting of multiple residual blocks\n        dp_rates = list(np.linspace(0, drop_path_rate, sum(depths)))\n        cur = 0\n        for i in range(4):\n            blocks = []\n            for j in range(depths[i]):\n                blocks.append(Block(dim=dims[i], drop_path=dp_rates[cur + j],\n                                    layer_scale_init_value=layer_scale_init_value, use_grn=use_grn))\n            stage = nn.SequentialCell(blocks)\n            stages.append(stage)\n            cur += depths[i]\n\n            if i &gt; 0:\n                total_reduction *= 2\n            self.feature_info.append(dict(chs=dims[i], reduction=total_reduction, name=f'feature.{i * 2 + 1}'))\n\n        self.feature = nn.SequentialCell([\n            downsample_layers[0],\n            stages[0],\n            downsample_layers[1],\n            stages[1],\n            downsample_layers[2],\n            stages[2],\n            downsample_layers[3],\n            stages[3]\n        ])\n        self.norm = ConvNextLayerNorm((dims[-1],), epsilon=1e-6)  # final norm layer\n        self.classifier = nn.Dense(dims[-1], num_classes)  # classifier\n        self.head_init_scale = head_init_scale\n        self._initialize_weights()\n\n    def _initialize_weights(self) -&gt; None:\n        \"\"\"Initialize weights for cells.\"\"\"\n        for _, cell in self.cells_and_names():\n            if isinstance(cell, (nn.Dense, nn.Conv2d)):\n                cell.weight.set_data(\n                    init.initializer(init.TruncatedNormal(sigma=0.02), cell.weight.shape, cell.weight.dtype)\n                )\n                if isinstance(cell, nn.Dense) and cell.bias is not None:\n                    cell.bias.set_data(init.initializer(init.Zero(), cell.bias.shape, cell.bias.dtype))\n        self.classifier.weight.set_data(self.classifier.weight * self.head_init_scale)\n        self.classifier.bias.set_data(self.classifier.bias * self.head_init_scale)\n\n    def forward_head(self, x: Tensor) -&gt; Tensor:\n        x = self.classifier(x)\n        return x\n\n    def forward_features(self, x: Tensor) -&gt; Tensor:\n        x = self.feature(x)\n        return self.norm(x.mean([-2, -1]))  # global average pooling, (N, C, H, W) -&gt; (N, C)\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n</code></pre>"},{"location":"reference/models/#mindcv.models.convnext.ConvNextLayerNorm","title":"<code>mindcv.models.convnext.ConvNextLayerNorm</code>","text":"<p>               Bases: <code>LayerNorm</code></p> <p>LayerNorm for channels_first tensors with 2d spatial dimensions (ie N, C, H, W).</p> Source code in <code>mindcv\\models\\convnext.py</code> <pre><code>class ConvNextLayerNorm(nn.LayerNorm):\n    \"\"\"\n    LayerNorm for channels_first tensors with 2d spatial dimensions (ie N, C, H, W).\n    \"\"\"\n\n    def __init__(\n        self,\n        normalized_shape: Tuple[int],\n        epsilon: float,\n        norm_axis: int = -1,\n    ) -&gt; None:\n        super().__init__(normalized_shape=normalized_shape, epsilon=epsilon)\n        assert norm_axis in (-1, 1), \"ConvNextLayerNorm's norm_axis must be 1 or -1.\"\n        self.norm_axis = norm_axis\n\n    def construct(self, input_x: Tensor) -&gt; Tensor:\n        if self.norm_axis == -1:\n            y, _, _ = self.layer_norm(input_x, self.gamma, self.beta)\n        else:\n            input_x = ops.transpose(input_x, (0, 2, 3, 1))\n            y, _, _ = self.layer_norm(input_x, self.gamma, self.beta)\n            y = ops.transpose(y, (0, 3, 1, 2))\n        return y\n</code></pre>"},{"location":"reference/models/#mindcv.models.convnext.GRN","title":"<code>mindcv.models.convnext.GRN</code>","text":"<p>               Bases: <code>Cell</code></p> <p>GRN (Global Response Normalization) layer</p> Source code in <code>mindcv\\models\\convnext.py</code> <pre><code>class GRN(nn.Cell):\n    \"\"\" GRN (Global Response Normalization) layer \"\"\"\n\n    def __init__(self, dim: int):\n        super().__init__()\n        self.gamma = Parameter(Tensor(np.zeros([1, 1, 1, dim]), mstype.float32))\n        self.beta = Parameter(Tensor(np.zeros([1, 1, 1, dim]), mstype.float32))\n        self.norm = ops.LpNorm(axis=[1, 2], p=2, keep_dims=True)\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        gx = self.norm(x)\n        nx = gx / (ops.mean(gx, axis=-1, keep_dims=True) + 1e-6)\n        return self.gamma * (x * nx) + self.beta + x\n</code></pre>"},{"location":"reference/models/#mindcv.models.convnext.convnext_base","title":"<code>mindcv.models.convnext.convnext_base(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get ConvNeXt base model. Refer to the base class 'models.ConvNeXt' for more details.</p> Source code in <code>mindcv\\models\\convnext.py</code> <pre><code>@register_model\ndef convnext_base(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; ConvNeXt:\n    \"\"\"Get ConvNeXt base model.\n    Refer to the base class 'models.ConvNeXt' for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"convnext_base\"]\n    model_args = dict(\n        in_channels=in_channels, num_classes=num_classes, depths=[3, 3, 27, 3], dims=[128, 256, 512, 1024], **kwargs\n    )\n    return _create_convnext(pretrained, **dict(default_cfg=default_cfg, **model_args))\n</code></pre>"},{"location":"reference/models/#mindcv.models.convnext.convnext_large","title":"<code>mindcv.models.convnext.convnext_large(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get ConvNeXt large model. Refer to the base class 'models.ConvNeXt' for more details.</p> Source code in <code>mindcv\\models\\convnext.py</code> <pre><code>@register_model\ndef convnext_large(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; ConvNeXt:\n    \"\"\"Get ConvNeXt large model.\n    Refer to the base class 'models.ConvNeXt' for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"convnext_large\"]\n    model_args = dict(\n        in_channels=in_channels, num_classes=num_classes, depths=[3, 3, 27, 3], dims=[192, 384, 768, 1536], **kwargs\n    )\n    return _create_convnext(pretrained, **dict(default_cfg=default_cfg, **model_args))\n</code></pre>"},{"location":"reference/models/#mindcv.models.convnext.convnext_small","title":"<code>mindcv.models.convnext.convnext_small(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get ConvNeXt small model. Refer to the base class 'models.ConvNeXt' for more details.</p> Source code in <code>mindcv\\models\\convnext.py</code> <pre><code>@register_model\ndef convnext_small(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; ConvNeXt:\n    \"\"\"Get ConvNeXt small model.\n    Refer to the base class 'models.ConvNeXt' for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"convnext_small\"]\n    model_args = dict(\n        in_channels=in_channels, num_classes=num_classes, depths=[3, 3, 27, 3], dims=[96, 192, 384, 768], **kwargs\n    )\n    return _create_convnext(pretrained, **dict(default_cfg=default_cfg, **model_args))\n</code></pre>"},{"location":"reference/models/#mindcv.models.convnext.convnext_tiny","title":"<code>mindcv.models.convnext.convnext_tiny(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get ConvNeXt tiny model. Refer to the base class 'models.ConvNeXt' for more details.</p> Source code in <code>mindcv\\models\\convnext.py</code> <pre><code>@register_model\ndef convnext_tiny(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; ConvNeXt:\n    \"\"\"Get ConvNeXt tiny model.\n    Refer to the base class 'models.ConvNeXt' for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"convnext_tiny\"]\n    model_args = dict(\n        in_channels=in_channels, num_classes=num_classes, depths=[3, 3, 9, 3], dims=[96, 192, 384, 768], **kwargs\n    )\n    return _create_convnext(pretrained, **dict(default_cfg=default_cfg, **model_args))\n</code></pre>"},{"location":"reference/models/#mindcv.models.convnext.convnext_xlarge","title":"<code>mindcv.models.convnext.convnext_xlarge(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get ConvNeXt xlarge model. Refer to the base class 'models.ConvNeXt' for more details.</p> Source code in <code>mindcv\\models\\convnext.py</code> <pre><code>@register_model\ndef convnext_xlarge(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; ConvNeXt:\n    \"\"\"Get ConvNeXt xlarge model.\n    Refer to the base class 'models.ConvNeXt' for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"convnext_xlarge\"]\n    model_args = dict(\n        in_channels=in_channels, num_classes=num_classes, depths=[3, 3, 27, 3], dims=[256, 512, 1024, 2048], **kwargs\n    )\n    return _create_convnext(pretrained, **dict(default_cfg=default_cfg, **model_args))\n</code></pre>"},{"location":"reference/models/#mindcv.models.convnext.convnextv2_atto","title":"<code>mindcv.models.convnext.convnextv2_atto(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get ConvNeXt_v2 atto model. Refer to the base class 'models.ConvNeXt' for more details.</p> Source code in <code>mindcv\\models\\convnext.py</code> <pre><code>@register_model\ndef convnextv2_atto(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; ConvNeXt:\n    \"\"\"Get ConvNeXt_v2 atto model.\n    Refer to the base class 'models.ConvNeXt' for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"convnextv2_atto\"]\n    model_args = dict(in_channels=in_channels, num_classes=num_classes, depths=[2, 2, 6, 2],\n                      dims=[40, 80, 160, 320], use_grn=True, layer_scale_init_value=0.0, **kwargs)\n    return _create_convnext(pretrained, **dict(default_cfg=default_cfg, **model_args))\n</code></pre>"},{"location":"reference/models/#mindcv.models.convnext.convnextv2_base","title":"<code>mindcv.models.convnext.convnextv2_base(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get ConvNeXt_v2 base model. Refer to the base class 'models.ConvNeXt' for more details.</p> Source code in <code>mindcv\\models\\convnext.py</code> <pre><code>@register_model\ndef convnextv2_base(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; ConvNeXt:\n    \"\"\"Get ConvNeXt_v2 base model.\n    Refer to the base class 'models.ConvNeXt' for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"convnextv2_base\"]\n    model_args = dict(in_channels=in_channels, num_classes=num_classes, depths=[3, 3, 27, 3],\n                      dims=[128, 256, 512, 1024], use_grn=True, layer_scale_init_value=0.0, **kwargs)\n    return _create_convnext(pretrained, **dict(default_cfg=default_cfg, **model_args))\n</code></pre>"},{"location":"reference/models/#mindcv.models.convnext.convnextv2_femto","title":"<code>mindcv.models.convnext.convnextv2_femto(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get ConvNeXt_v2 femto model. Refer to the base class 'models.ConvNeXt' for more details.</p> Source code in <code>mindcv\\models\\convnext.py</code> <pre><code>@register_model\ndef convnextv2_femto(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; ConvNeXt:\n    \"\"\"Get ConvNeXt_v2 femto model.\n    Refer to the base class 'models.ConvNeXt' for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"convnextv2_femto\"]\n    model_args = dict(in_channels=in_channels, num_classes=num_classes, depths=[2, 2, 6, 2],\n                      dims=[48, 96, 192, 384], use_grn=True, layer_scale_init_value=0.0, **kwargs)\n    return _create_convnext(pretrained, **dict(default_cfg=default_cfg, **model_args))\n</code></pre>"},{"location":"reference/models/#mindcv.models.convnext.convnextv2_huge","title":"<code>mindcv.models.convnext.convnextv2_huge(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get ConvNeXt_v2 huge model. Refer to the base class 'models.ConvNeXt' for more details.</p> Source code in <code>mindcv\\models\\convnext.py</code> <pre><code>@register_model\ndef convnextv2_huge(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; ConvNeXt:\n    \"\"\"Get ConvNeXt_v2 huge model.\n    Refer to the base class 'models.ConvNeXt' for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"convnextv2_huge\"]\n    model_args = dict(in_channels=in_channels, num_classes=num_classes, depths=[3, 3, 27, 3],\n                      dims=[352, 704, 1408, 2816], use_grn=True, layer_scale_init_value=0.0, **kwargs)\n    return _create_convnext(pretrained, **dict(default_cfg=default_cfg, **model_args))\n</code></pre>"},{"location":"reference/models/#mindcv.models.convnext.convnextv2_large","title":"<code>mindcv.models.convnext.convnextv2_large(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get ConvNeXt_v2 large model. Refer to the base class 'models.ConvNeXt' for more details.</p> Source code in <code>mindcv\\models\\convnext.py</code> <pre><code>@register_model\ndef convnextv2_large(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; ConvNeXt:\n    \"\"\"Get ConvNeXt_v2 large model.\n    Refer to the base class 'models.ConvNeXt' for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"convnextv2_large\"]\n    model_args = dict(in_channels=in_channels, num_classes=num_classes, depths=[3, 3, 27, 3],\n                      dims=[192, 384, 768, 1536], use_grn=True, layer_scale_init_value=0.0, **kwargs)\n    return _create_convnext(pretrained, **dict(default_cfg=default_cfg, **model_args))\n</code></pre>"},{"location":"reference/models/#mindcv.models.convnext.convnextv2_nano","title":"<code>mindcv.models.convnext.convnextv2_nano(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get ConvNeXt_v2 nano model. Refer to the base class 'models.ConvNeXt' for more details.</p> Source code in <code>mindcv\\models\\convnext.py</code> <pre><code>@register_model\ndef convnextv2_nano(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; ConvNeXt:\n    \"\"\"Get ConvNeXt_v2 nano model.\n    Refer to the base class 'models.ConvNeXt' for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"convnextv2_nano\"]\n    model_args = dict(in_channels=in_channels, num_classes=num_classes, depths=[2, 2, 8, 2],\n                      dims=[80, 160, 320, 640], use_grn=True, layer_scale_init_value=0.0, **kwargs)\n    return _create_convnext(pretrained, **dict(default_cfg=default_cfg, **model_args))\n</code></pre>"},{"location":"reference/models/#mindcv.models.convnext.convnextv2_pico","title":"<code>mindcv.models.convnext.convnextv2_pico(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get ConvNeXt_v2 pico model. Refer to the base class 'models.ConvNeXt' for more details.</p> Source code in <code>mindcv\\models\\convnext.py</code> <pre><code>@register_model\ndef convnextv2_pico(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; ConvNeXt:\n    \"\"\"Get ConvNeXt_v2 pico model.\n    Refer to the base class 'models.ConvNeXt' for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"convnextv2_pico\"]\n    model_args = dict(in_channels=in_channels, num_classes=num_classes, depths=[2, 2, 6, 2],\n                      dims=[64, 128, 256, 512], use_grn=True, layer_scale_init_value=0.0, **kwargs)\n    return _create_convnext(pretrained, **dict(default_cfg=default_cfg, **model_args))\n</code></pre>"},{"location":"reference/models/#mindcv.models.convnext.convnextv2_tiny","title":"<code>mindcv.models.convnext.convnextv2_tiny(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get ConvNeXt_v2 tiny model. Refer to the base class 'models.ConvNeXt' for more details.</p> Source code in <code>mindcv\\models\\convnext.py</code> <pre><code>@register_model\ndef convnextv2_tiny(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; ConvNeXt:\n    \"\"\"Get ConvNeXt_v2 tiny model.\n    Refer to the base class 'models.ConvNeXt' for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"convnextv2_tiny\"]\n    model_args = dict(in_channels=in_channels, num_classes=num_classes, depths=[3, 3, 9, 3],\n                      dims=[96, 192, 384, 768], use_grn=True, layer_scale_init_value=0.0, **kwargs)\n    return _create_convnext(pretrained, **dict(default_cfg=default_cfg, **model_args))\n</code></pre>"},{"location":"reference/models/#crossvit","title":"crossvit","text":""},{"location":"reference/models/#mindcv.models.crossvit","title":"<code>mindcv.models.crossvit</code>","text":"<p>MindSpore implementation of <code>crossvit</code>. Refer to crossvit: Cross-Attention Multi-Scale Vision Transformer for Image Classification</p>"},{"location":"reference/models/#mindcv.models.crossvit.PatchEmbed","title":"<code>mindcv.models.crossvit.PatchEmbed</code>","text":"<p>               Bases: <code>Cell</code></p> <p>Image to Patch Embedding</p> Source code in <code>mindcv\\models\\crossvit.py</code> <pre><code>class PatchEmbed(nn.Cell):\n    \"\"\" Image to Patch Embedding\n    \"\"\"\n\n    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768, multi_conv=True):\n        super().__init__()\n        img_size = to_2tuple(img_size)\n        patch_size = to_2tuple(patch_size)\n        num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0])\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.num_patches = num_patches\n        if multi_conv:\n            if patch_size[0] == 12:\n                self.proj = nn.SequentialCell(\n                    nn.Conv2d(in_chans, embed_dim // 4, pad_mode='pad', kernel_size=7, stride=4, padding=3),\n                    nn.ReLU(),\n                    nn.Conv2d(embed_dim // 4, embed_dim // 2, pad_mode='pad', kernel_size=3, stride=3, padding=0),\n                    nn.ReLU(),\n                    nn.Conv2d(embed_dim // 2, embed_dim, pad_mode='pad', kernel_size=3, stride=1, padding=1),\n                )\n            elif patch_size[0] == 16:\n                self.proj = nn.SequentialCell(\n                    nn.Conv2d(in_chans, embed_dim // 4, pad_mode='pad', kernel_size=7, stride=4, padding=3),\n                    nn.ReLU(),\n                    nn.Conv2d(embed_dim // 4, embed_dim // 2, pad_mode='pad', kernel_size=3, stride=2, padding=1),\n                    nn.ReLU(),\n                    nn.Conv2d(embed_dim // 2, embed_dim, pad_mode='pad', kernel_size=3, stride=2, padding=1),\n                )\n        else:\n            self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size, pad_mode='valid',\n                                  has_bias=True)\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        B, C, H, W = x.shape\n        # FIXME look at relaxing size constraints\n\n        # assert H == self.img_size[0] and W == self.img_size[1], \\\n        # f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n        x = self.proj(x)\n        B, C, H, W = x.shape\n        x = x.reshape(B, C, H * W)\n        x = ops.transpose(x, (0, 2, 1))\n        return x\n</code></pre>"},{"location":"reference/models/#mindcv.models.crossvit.VisionTransformer","title":"<code>mindcv.models.crossvit.VisionTransformer</code>","text":"<p>               Bases: <code>Cell</code></p> <p>Vision Transformer with support for patch or hybrid CNN input stage</p> Source code in <code>mindcv\\models\\crossvit.py</code> <pre><code>class VisionTransformer(nn.Cell):\n    \"\"\" Vision Transformer with support for patch or hybrid CNN input stage\n    \"\"\"\n\n    def __init__(self, img_size=(224, 224), patch_size=(8, 16), in_channels=3, num_classes=1000, embed_dim=(192, 384),\n                 depth=([1, 3, 1], [1, 3, 1], [1, 3, 1]),\n                 num_heads=(6, 12), mlp_ratio=(2., 2., 4.), qkv_bias=False, qk_scale=None, drop_rate=0.,\n                 attn_drop_rate=0.,\n                 drop_path_rate=0., hybrid_backbone=None, norm_layer=nn.LayerNorm, multi_conv=False):\n        super().__init__()\n\n        self.num_classes = num_classes\n        if not isinstance(img_size, list):\n            img_size = to_2tuple(img_size)\n        self.img_size = img_size\n\n        num_patches = _compute_num_patches(img_size, patch_size)\n        self.num_branches = len(patch_size)\n        self.interpolate = Interpolate(mode=\"bilinear\", align_corners=True)\n\n        patch_embed = []\n        if hybrid_backbone is None:\n            b = []\n            for i in range(self.num_branches):\n                c = ms.Parameter(Tensor(np.zeros([1, 1 + num_patches[i], embed_dim[i]], np.float32)),\n                                 name='pos_embed.' + str(i))\n                b.append(c)\n            b = tuple(b)\n            self.pos_embed = ms.ParameterTuple(b)\n            for im_s, p, d in zip(img_size, patch_size, embed_dim):\n                patch_embed.append(\n                    PatchEmbed(img_size=im_s, patch_size=p, in_chans=in_channels, embed_dim=d, multi_conv=multi_conv))\n            self.patch_embed = nn.CellList(patch_embed)\n\n        d = []\n        for i in range(self.num_branches):\n            c = ms.Parameter(Tensor(np.zeros([1, 1, embed_dim[i]], np.float32)), name='cls_token.' + str(i))\n            d.append(c)\n        d = tuple(d)\n        self.cls_token = ms.ParameterTuple(d)\n        self.pos_drop = Dropout(p=drop_rate)\n\n        total_depth = sum([sum(x[-2:]) for x in depth])\n        dpr = np.linspace(0, drop_path_rate, total_depth)  # stochastic depth decay rule\n        dpr_ptr = 0\n        self.blocks = nn.CellList()\n        for idx, block_cfg in enumerate(depth):\n            curr_depth = max(block_cfg[:-1]) + block_cfg[-1]\n            dpr_ = dpr[dpr_ptr:dpr_ptr + curr_depth]\n            blk = MultiScaleBlock(embed_dim, num_patches, block_cfg, num_heads=num_heads, mlp_ratio=mlp_ratio,\n                                  qkv_bias=qkv_bias, qk_scale=qk_scale, drop=drop_rate, attn_drop=attn_drop_rate,\n                                  drop_path=dpr_,\n                                  norm_layer=norm_layer)\n            dpr_ptr += curr_depth\n            self.blocks.append(blk)\n\n        self.norm = nn.CellList([norm_layer((embed_dim[i],), epsilon=1e-6) for i in range(self.num_branches)])\n        self.head = nn.CellList([nn.Dense(embed_dim[i], num_classes) if num_classes &gt; 0 else Identity() for i in\n                                 range(self.num_branches)])\n\n        for i in range(self.num_branches):\n            if self.pos_embed[i].requires_grad:\n                tensor1 = init.initializer(TruncatedNormal(sigma=.02), self.pos_embed[i].data.shape, ms.float32)\n                self.pos_embed[i].set_data(tensor1)\n            tensor2 = init.initializer(TruncatedNormal(sigma=.02), self.cls_token[i].data.shape, ms.float32)\n            self.cls_token[i].set_data(tensor2)\n\n        self._initialize_weights()\n\n    def _initialize_weights(self) -&gt; None:\n        for _, cell in self.cells_and_names():\n            if isinstance(cell, nn.Dense):\n                cell.weight.set_data(init.initializer(init.TruncatedNormal(sigma=.02), cell.weight.data.shape))\n                if cell.bias is not None:\n                    cell.bias.set_data(init.initializer(init.Constant(0), cell.bias.shape))\n            elif isinstance(cell, nn.LayerNorm):\n                cell.gamma.set_data(init.initializer(init.Constant(1), cell.gamma.shape))\n                cell.beta.set_data(init.initializer(init.Constant(0), cell.beta.shape))\n\n    def no_weight_decay(self):\n        out = {'cls_token'}\n        if self.pos_embed[0].requires_grad:\n            out.add('pos_embed')\n        return out\n\n    def get_classifier(self):\n        return self.head\n\n    def reset_classifier(self, num_classes, global_pool=''):\n        self.num_classes = num_classes\n        self.head = nn.Dense(self.embed_dim, num_classes) if num_classes &gt; 0 else Identity()\n\n    def forward_features(self, x: Tensor) -&gt; Tensor:\n        B, C, H, W = x.shape\n        xs = []\n        # print(x)\n        for i in range(self.num_branches):\n            x_ = self.interpolate(x, size=(self.img_size[i], self.img_size[i])) if H != self.img_size[i] else x\n            tmp = self.patch_embed[i](x_)\n            z = self.cls_token[i].shape\n            y = Tensor(np.ones((B, z[1], z[2])), dtype=mstype.float32)\n            cls_tokens = self.cls_token[i]\n            cls_tokens = cls_tokens.expand_as(y)  # stole cls_tokens impl from Phil Wang, thanks\n            con = ops.Concat(1)\n            cls_tokens = cls_tokens.astype(\"float32\")\n            tmp = tmp.astype(\"float32\")\n            tmp = con((cls_tokens, tmp))\n            tmp = tmp + self.pos_embed[i]\n            tmp = self.pos_drop(tmp)\n            xs.append(tmp)\n\n        for blk in self.blocks:\n            xs = blk(xs)\n\n        # NOTE: was before branch token section, move to here to assure all branch token are before layer norm\n        k = 0\n        xs2 = []\n        for x in xs:\n            xs2.append(self.norm[k](x))\n            k = k + 1\n        xs = xs2\n        out = []\n        for x in xs:\n            out.append(x[:, 0])\n        return out\n\n    def forward_head(self, x: Tensor) -&gt; Tensor:\n        ce_logits = []\n        zz = 0\n        for c in x:\n            ce_logits.append(self.head[zz](c))\n            zz = zz + 1\n        z = ops.stack([ce_logits[0], ce_logits[1]])\n        op = ops.ReduceMean(keep_dims=False)\n        ce_logits = op(z, 0)\n        return ce_logits\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n</code></pre>"},{"location":"reference/models/#densenet","title":"densenet","text":""},{"location":"reference/models/#mindcv.models.densenet","title":"<code>mindcv.models.densenet</code>","text":"<p>MindSpore implementation of <code>DenseNet</code>. Refer to: Densely Connected Convolutional Networks</p>"},{"location":"reference/models/#mindcv.models.densenet.DenseNet","title":"<code>mindcv.models.densenet.DenseNet</code>","text":"<p>               Bases: <code>Cell</code></p> <p>Densenet-BC model class, based on <code>\"Densely Connected Convolutional Networks\" &lt;https://arxiv.org/pdf/1608.06993.pdf&gt;</code>_</p> PARAMETER DESCRIPTION <code>growth_rate</code> <p>how many filters to add each layer (<code>k</code> in paper). Default: 32.</p> <p> TYPE: <code>int</code> DEFAULT: <code>32</code> </p> <code>block_config</code> <p>how many layers in each pooling block. Default: (6, 12, 24, 16).</p> <p> TYPE: <code>Tuple[int, int, int, int]</code> DEFAULT: <code>(6, 12, 24, 16)</code> </p> <code>num_init_features</code> <p>number of filters in the first Conv2d. Default: 64.</p> <p> TYPE: <code>int</code> DEFAULT: <code>64</code> </p> <code>bn_size</code> <p>multiplicative factor for number of bottleneck layers (i.e. bn_size * k features in the bottleneck layer). Default: 4.</p> <p> TYPE: <code>int</code> DEFAULT: <code>4</code> </p> <code>drop_rate</code> <p>dropout rate after each dense layer. Default: 0.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>in_channels</code> <p>number of input channels. Default: 3.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>num_classes</code> <p>number of classification classes. Default: 1000.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1000</code> </p> Source code in <code>mindcv\\models\\densenet.py</code> <pre><code>class DenseNet(nn.Cell):\n    r\"\"\"Densenet-BC model class, based on\n    `\"Densely Connected Convolutional Networks\" &lt;https://arxiv.org/pdf/1608.06993.pdf&gt;`_\n\n    Args:\n        growth_rate: how many filters to add each layer (`k` in paper). Default: 32.\n        block_config: how many layers in each pooling block. Default: (6, 12, 24, 16).\n        num_init_features: number of filters in the first Conv2d. Default: 64.\n        bn_size (int): multiplicative factor for number of bottleneck layers\n          (i.e. bn_size * k features in the bottleneck layer). Default: 4.\n        drop_rate: dropout rate after each dense layer. Default: 0.\n        in_channels: number of input channels. Default: 3.\n        num_classes: number of classification classes. Default: 1000.\n    \"\"\"\n\n    def __init__(\n        self,\n        growth_rate: int = 32,\n        block_config: Tuple[int, int, int, int] = (6, 12, 24, 16),\n        num_init_features: int = 64,\n        bn_size: int = 4,\n        drop_rate: float = 0.0,\n        in_channels: int = 3,\n        num_classes: int = 1000,\n    ) -&gt; None:\n        super().__init__()\n        layers = OrderedDict()\n        # first Conv2d\n        num_features = num_init_features\n        layers[\"conv0\"] = nn.Conv2d(in_channels, num_features, kernel_size=7, stride=2, pad_mode=\"pad\", padding=3)\n        layers[\"norm0\"] = nn.BatchNorm2d(num_features)\n        layers[\"relu0\"] = nn.ReLU()\n        layers[\"pool0\"] = nn.SequentialCell([\n            nn.Pad(paddings=((0, 0), (0, 0), (1, 1), (1, 1)), mode=\"CONSTANT\"),\n            nn.MaxPool2d(kernel_size=3, stride=2),\n        ])\n\n        # DenseBlock\n        for i, num_layers in enumerate(block_config):\n            block = _DenseBlock(\n                num_layers=num_layers,\n                num_input_features=num_features,\n                bn_size=bn_size,\n                growth_rate=growth_rate,\n                drop_rate=drop_rate,\n            )\n            layers[f\"denseblock{i + 1}\"] = block\n            num_features += num_layers * growth_rate\n            if i != len(block_config) - 1:\n                transition = _Transition(num_features, num_features // 2)\n                layers[f\"transition{i + 1}\"] = transition\n                num_features = num_features // 2\n\n        # final bn+ReLU\n        layers[\"norm5\"] = nn.BatchNorm2d(num_features)\n        layers[\"relu5\"] = nn.ReLU()\n\n        self.num_features = num_features\n        self.features = nn.SequentialCell(layers)\n        self.pool = GlobalAvgPooling()\n        self.classifier = nn.Dense(self.num_features, num_classes)\n        self._initialize_weights()\n\n    def _initialize_weights(self) -&gt; None:\n        \"\"\"Initialize weights for cells.\"\"\"\n        for _, cell in self.cells_and_names():\n            if isinstance(cell, nn.Conv2d):\n                cell.weight.set_data(\n                    init.initializer(init.HeNormal(math.sqrt(5), mode=\"fan_out\", nonlinearity=\"relu\"),\n                                     cell.weight.shape, cell.weight.dtype))\n                if cell.bias is not None:\n                    cell.bias.set_data(\n                        init.initializer(init.HeUniform(math.sqrt(5), mode=\"fan_in\", nonlinearity=\"leaky_relu\"),\n                                         cell.bias.shape, cell.bias.dtype))\n            elif isinstance(cell, nn.BatchNorm2d):\n                cell.gamma.set_data(init.initializer(\"ones\", cell.gamma.shape, cell.gamma.dtype))\n                cell.beta.set_data(init.initializer(\"zeros\", cell.beta.shape, cell.beta.dtype))\n            elif isinstance(cell, nn.Dense):\n                cell.weight.set_data(\n                    init.initializer(init.HeUniform(math.sqrt(5), mode=\"fan_in\", nonlinearity=\"leaky_relu\"),\n                                     cell.weight.shape, cell.weight.dtype))\n                if cell.bias is not None:\n                    cell.bias.set_data(init.initializer(\"zeros\", cell.bias.shape, cell.bias.dtype))\n\n    def forward_features(self, x: Tensor) -&gt; Tensor:\n        x = self.features(x)\n        return x\n\n    def forward_head(self, x: Tensor) -&gt; Tensor:\n        x = self.pool(x)\n        x = self.classifier(x)\n        return x\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n</code></pre>"},{"location":"reference/models/#mindcv.models.densenet.densenet121","title":"<code>mindcv.models.densenet.densenet121(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get 121 layers DenseNet model. Refer to the base class <code>models.DenseNet</code> for more details.</p> Source code in <code>mindcv\\models\\densenet.py</code> <pre><code>@register_model\ndef densenet121(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; DenseNet:\n    \"\"\"Get 121 layers DenseNet model.\n     Refer to the base class `models.DenseNet` for more details.\"\"\"\n    default_cfg = default_cfgs[\"densenet121\"]\n    model = DenseNet(growth_rate=32, block_config=(6, 12, 24, 16), num_init_features=64, in_channels=in_channels,\n                     num_classes=num_classes, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre>"},{"location":"reference/models/#mindcv.models.densenet.densenet161","title":"<code>mindcv.models.densenet.densenet161(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get 161 layers DenseNet model. Refer to the base class <code>models.DenseNet</code> for more details.</p> Source code in <code>mindcv\\models\\densenet.py</code> <pre><code>@register_model\ndef densenet161(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; DenseNet:\n    \"\"\"Get 161 layers DenseNet model.\n     Refer to the base class `models.DenseNet` for more details.\"\"\"\n    default_cfg = default_cfgs[\"densenet161\"]\n    model = DenseNet(growth_rate=48, block_config=(6, 12, 36, 24), num_init_features=96, in_channels=in_channels,\n                     num_classes=num_classes, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre>"},{"location":"reference/models/#mindcv.models.densenet.densenet169","title":"<code>mindcv.models.densenet.densenet169(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get 169 layers DenseNet model. Refer to the base class <code>models.DenseNet</code> for more details.</p> Source code in <code>mindcv\\models\\densenet.py</code> <pre><code>@register_model\ndef densenet169(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; DenseNet:\n    \"\"\"Get 169 layers DenseNet model.\n     Refer to the base class `models.DenseNet` for more details.\"\"\"\n    default_cfg = default_cfgs[\"densenet169\"]\n    model = DenseNet(growth_rate=32, block_config=(6, 12, 32, 32), num_init_features=64, in_channels=in_channels,\n                     num_classes=num_classes, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre>"},{"location":"reference/models/#mindcv.models.densenet.densenet201","title":"<code>mindcv.models.densenet.densenet201(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get 201 layers DenseNet model. Refer to the base class <code>models.DenseNet</code> for more details.</p> Source code in <code>mindcv\\models\\densenet.py</code> <pre><code>@register_model\ndef densenet201(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; DenseNet:\n    \"\"\"Get 201 layers DenseNet model.\n     Refer to the base class `models.DenseNet` for more details.\"\"\"\n    default_cfg = default_cfgs[\"densenet201\"]\n    model = DenseNet(growth_rate=32, block_config=(6, 12, 48, 32), num_init_features=64, in_channels=in_channels,\n                     num_classes=num_classes, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre>"},{"location":"reference/models/#dpn","title":"dpn","text":""},{"location":"reference/models/#mindcv.models.dpn","title":"<code>mindcv.models.dpn</code>","text":"<p>MindSpore implementation of <code>DPN</code>. Refer to: Dual Path Networks</p>"},{"location":"reference/models/#mindcv.models.dpn.BottleBlock","title":"<code>mindcv.models.dpn.BottleBlock</code>","text":"<p>               Bases: <code>Cell</code></p> <p>A block for the Dual Path Architecture</p> Source code in <code>mindcv\\models\\dpn.py</code> <pre><code>class BottleBlock(nn.Cell):\n    \"\"\"A block for the Dual Path Architecture\"\"\"\n\n    def __init__(\n        self,\n        in_channel: int,\n        num_1x1_a: int,\n        num_3x3_b: int,\n        num_1x1_c: int,\n        inc: int,\n        g: int,\n        key_stride: int,\n    ):\n        super().__init__()\n        self.bn1 = nn.BatchNorm2d(in_channel, eps=1e-3, momentum=0.9)\n        self.conv1 = nn.Conv2d(in_channel, num_1x1_a, 1, stride=1)\n        self.bn2 = nn.BatchNorm2d(num_1x1_a, eps=1e-3, momentum=0.9)\n        self.conv2 = nn.Conv2d(num_1x1_a, num_3x3_b, 3, key_stride, pad_mode=\"pad\", padding=1, group=g)\n        self.bn3 = nn.BatchNorm2d(num_3x3_b, eps=1e-3, momentum=0.9)\n        self.conv3_r = nn.Conv2d(num_3x3_b, num_1x1_c, 1, stride=1)\n        self.conv3_d = nn.Conv2d(num_3x3_b, inc, 1, stride=1)\n\n        self.relu = nn.ReLU()\n\n    def construct(self, x: Tensor):\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.conv1(x)\n        x = self.bn2(x)\n        x = self.relu(x)\n        x = self.conv2(x)\n        x = self.bn3(x)\n        x = self.relu(x)\n        return (self.conv3_r(x), self.conv3_d(x))\n</code></pre>"},{"location":"reference/models/#mindcv.models.dpn.DPN","title":"<code>mindcv.models.dpn.DPN</code>","text":"<p>               Bases: <code>Cell</code></p> <p>DPN model class, based on <code>\"Dual Path Networks\" &lt;https://arxiv.org/pdf/1707.01629.pdf&gt;</code>_</p> PARAMETER DESCRIPTION <code>num_init_channel</code> <p>int type, the output channel of first blocks. Default: 64.</p> <p> TYPE: <code>int</code> DEFAULT: <code>64</code> </p> <code>k_r</code> <p>int type, the first channel of each stage. Default: 96.</p> <p> TYPE: <code>int</code> DEFAULT: <code>96</code> </p> <code>g</code> <p>int type,number of group in the conv2d. Default: 32.</p> <p> TYPE: <code>int</code> DEFAULT: <code>32</code> </p> <code>k_sec</code> <p>multiplicative factor for number of bottleneck layers. Default: 4.</p> <p> TYPE: <code>Tuple[int]</code> DEFAULT: <code>(3, 4, 20, 3)</code> </p> <code>inc_sec</code> <p>the first output channel in each stage. Default: (16, 32, 24, 128).</p> <p> TYPE: <code>Tuple[int]</code> DEFAULT: <code>(16, 32, 24, 128)</code> </p> <code>in_channels</code> <p>int type, number of input channels. Default: 3.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>num_classes</code> <p>int type, number of classification classes. Default: 1000.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1000</code> </p> Source code in <code>mindcv\\models\\dpn.py</code> <pre><code>class DPN(nn.Cell):\n    r\"\"\"DPN model class, based on\n    `\"Dual Path Networks\" &lt;https://arxiv.org/pdf/1707.01629.pdf&gt;`_\n\n    Args:\n        num_init_channel: int type, the output channel of first blocks. Default: 64.\n        k_r: int type, the first channel of each stage. Default: 96.\n        g: int type,number of group in the conv2d. Default: 32.\n        k_sec Tuple[int]: multiplicative factor for number of bottleneck layers. Default: 4.\n        inc_sec Tuple[int]: the first output channel in each stage. Default: (16, 32, 24, 128).\n        in_channels: int type, number of input channels. Default: 3.\n        num_classes: int type, number of classification classes. Default: 1000.\n    \"\"\"\n\n    def __init__(\n        self,\n        num_init_channel: int = 64,\n        k_r: int = 96,\n        g: int = 32,\n        k_sec: Tuple[int, int, int, int] = (3, 4, 20, 3),\n        inc_sec: Tuple[int, int, int, int] = (16, 32, 24, 128),\n        in_channels: int = 3,\n        num_classes: int = 1000,\n    ):\n        super().__init__()\n        blocks = OrderedDict()\n\n        # conv1\n        blocks[\"conv1\"] = nn.SequentialCell(OrderedDict([\n            (\"conv\", nn.Conv2d(in_channels, num_init_channel, kernel_size=7, stride=2, pad_mode=\"pad\", padding=3)),\n            (\"norm\", nn.BatchNorm2d(num_init_channel, eps=1e-3, momentum=0.9)),\n            (\"relu\", nn.ReLU()),\n            (\"maxpool\", nn.MaxPool2d(kernel_size=3, stride=2, pad_mode=\"same\")),\n        ]))\n\n        # conv2\n        bw = 256\n        inc = inc_sec[0]\n        r = int((k_r * bw) / 256)\n        blocks[\"conv2_1\"] = DualPathBlock(num_init_channel, r, r, bw, inc, g, \"proj\", False)\n        in_channel = bw + 3 * inc\n        for i in range(2, k_sec[0] + 1):\n            blocks[f\"conv2_{i}\"] = DualPathBlock(in_channel, r, r, bw, inc, g, \"normal\")\n            in_channel += inc\n\n        # conv3\n        bw = 512\n        inc = inc_sec[1]\n        r = int((k_r * bw) / 256)\n        blocks[\"conv3_1\"] = DualPathBlock(in_channel, r, r, bw, inc, g, \"down\")\n        in_channel = bw + 3 * inc\n        for i in range(2, k_sec[1] + 1):\n            blocks[f\"conv3_{i}\"] = DualPathBlock(in_channel, r, r, bw, inc, g, \"normal\")\n            in_channel += inc\n\n        # conv4\n        bw = 1024\n        inc = inc_sec[2]\n        r = int((k_r * bw) / 256)\n        blocks[\"conv4_1\"] = DualPathBlock(in_channel, r, r, bw, inc, g, \"down\")\n        in_channel = bw + 3 * inc\n        for i in range(2, k_sec[2] + 1):\n            blocks[f\"conv4_{i}\"] = DualPathBlock(in_channel, r, r, bw, inc, g, \"normal\")\n            in_channel += inc\n\n        # conv5\n        bw = 2048\n        inc = inc_sec[3]\n        r = int((k_r * bw) / 256)\n        blocks[\"conv5_1\"] = DualPathBlock(in_channel, r, r, bw, inc, g, \"down\")\n        in_channel = bw + 3 * inc\n        for i in range(2, k_sec[3] + 1):\n            blocks[f\"conv5_{i}\"] = DualPathBlock(in_channel, r, r, bw, inc, g, \"normal\")\n            in_channel += inc\n\n        self.features = nn.SequentialCell(blocks)\n        self.conv5_x = nn.SequentialCell(OrderedDict([\n            (\"norm\", nn.BatchNorm2d(in_channel, eps=1e-3, momentum=0.9)),\n            (\"relu\", nn.ReLU()),\n        ]))\n        self.avgpool = GlobalAvgPooling()\n        self.classifier = nn.Dense(in_channel, num_classes)\n        self._initialize_weights()\n\n    def _initialize_weights(self) -&gt; None:\n        \"\"\"Initialize weights for cells.\"\"\"\n        for _, cell in self.cells_and_names():\n            if isinstance(cell, nn.Conv2d):\n                cell.weight.set_data(\n                    init.initializer(init.HeNormal(math.sqrt(5), mode=\"fan_out\", nonlinearity=\"relu\"),\n                                     cell.weight.shape, cell.weight.dtype))\n                if cell.bias is not None:\n                    cell.bias.set_data(\n                        init.initializer(init.HeUniform(math.sqrt(5), mode=\"fan_in\", nonlinearity=\"leaky_relu\"),\n                                         cell.bias.shape, cell.bias.dtype))\n            elif isinstance(cell, nn.BatchNorm2d):\n                cell.gamma.set_data(init.initializer(\"ones\", cell.gamma.shape, cell.gamma.dtype))\n                cell.beta.set_data(init.initializer(\"zeros\", cell.beta.shape, cell.beta.dtype))\n            elif isinstance(cell, nn.Dense):\n                cell.weight.set_data(\n                    init.initializer(init.HeUniform(math.sqrt(5), mode=\"fan_in\", nonlinearity=\"leaky_relu\"),\n                                     cell.weight.shape, cell.weight.dtype))\n                if cell.bias is not None:\n                    cell.bias.set_data(init.initializer(\"zeros\", cell.bias.shape, cell.bias.dtype))\n\n    def forward_feature(self, x: Tensor) -&gt; Tensor:\n        x = self.features(x)\n        x = ops.concat(x, axis=1)\n        x = self.conv5_x(x)\n        return x\n\n    def forward_head(self, x: Tensor) -&gt; Tensor:\n        x = self.avgpool(x)\n        x = self.classifier(x)\n        return x\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        x = self.forward_feature(x)\n        x = self.forward_head(x)\n        return x\n</code></pre>"},{"location":"reference/models/#mindcv.models.dpn.DualPathBlock","title":"<code>mindcv.models.dpn.DualPathBlock</code>","text":"<p>               Bases: <code>Cell</code></p> <p>A block for Dual Path Networks to combine proj, residual and densely network</p> Source code in <code>mindcv\\models\\dpn.py</code> <pre><code>class DualPathBlock(nn.Cell):\n    \"\"\"A block for Dual Path Networks to combine proj, residual and densely network\"\"\"\n\n    def __init__(\n        self,\n        in_channel: int,\n        num_1x1_a: int,\n        num_3x3_b: int,\n        num_1x1_c: int,\n        inc: int,\n        g: int,\n        _type: str = \"normal\",\n        cat_input: bool = True,\n    ):\n        super().__init__()\n        self.num_1x1_c = num_1x1_c\n\n        if _type == \"proj\":\n            key_stride = 1\n            self.has_proj = True\n        if _type == \"down\":\n            key_stride = 2\n            self.has_proj = True\n        if _type == \"normal\":\n            key_stride = 1\n            self.has_proj = False\n\n        self.cat_input = cat_input\n\n        if self.has_proj:\n            self.c1x1_w_bn = nn.BatchNorm2d(in_channel, eps=1e-3, momentum=0.9)\n            self.c1x1_w_relu = nn.ReLU()\n            self.c1x1_w_r = nn.Conv2d(in_channel, num_1x1_c, kernel_size=1, stride=key_stride,\n                                      pad_mode=\"pad\", padding=0)\n            self.c1x1_w_d = nn.Conv2d(in_channel, 2 * inc, kernel_size=1, stride=key_stride,\n                                      pad_mode=\"pad\", padding=0)\n\n        self.layers = BottleBlock(in_channel, num_1x1_a, num_3x3_b, num_1x1_c, inc, g, key_stride)\n\n    def construct(self, x: Tensor):\n        if self.cat_input:\n            data_in = ops.concat(x, axis=1)\n        else:\n            data_in = x\n\n        if self.has_proj:\n            data_o = self.c1x1_w_bn(data_in)\n            data_o = self.c1x1_w_relu(data_o)\n            data_o1 = self.c1x1_w_r(data_o)\n            data_o2 = self.c1x1_w_d(data_o)\n        else:\n            data_o1 = x[0]\n            data_o2 = x[1]\n\n        out = self.layers(data_in)\n        summ = ops.add(data_o1, out[0])\n        dense = ops.concat((data_o2, out[1]), axis=1)\n        return (summ, dense)\n</code></pre>"},{"location":"reference/models/#mindcv.models.dpn.dpn107","title":"<code>mindcv.models.dpn.dpn107(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get 107 layers DPN model. Refer to the base class <code>models.DPN</code> for more details.</p> Source code in <code>mindcv\\models\\dpn.py</code> <pre><code>@register_model\ndef dpn107(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; DPN:\n    \"\"\"Get 107 layers DPN model.\n     Refer to the base class `models.DPN` for more details.\"\"\"\n    default_cfg = default_cfgs[\"dpn107\"]\n    model = DPN(num_init_channel=128, k_r=200, g=50, k_sec=(4, 8, 20, 3), inc_sec=(20, 64, 64, 128),\n                num_classes=num_classes, in_channels=in_channels, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre>"},{"location":"reference/models/#mindcv.models.dpn.dpn131","title":"<code>mindcv.models.dpn.dpn131(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get 131 layers DPN model. Refer to the base class <code>models.DPN</code> for more details.</p> Source code in <code>mindcv\\models\\dpn.py</code> <pre><code>@register_model\ndef dpn131(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; DPN:\n    \"\"\"Get 131 layers DPN model.\n     Refer to the base class `models.DPN` for more details.\"\"\"\n    default_cfg = default_cfgs[\"dpn131\"]\n    model = DPN(num_init_channel=128, k_r=160, g=40, k_sec=(4, 8, 28, 3), inc_sec=(16, 32, 32, 128),\n                num_classes=num_classes, in_channels=in_channels, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre>"},{"location":"reference/models/#mindcv.models.dpn.dpn92","title":"<code>mindcv.models.dpn.dpn92(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get 92 layers DPN model. Refer to the base class <code>models.DPN</code> for more details.</p> Source code in <code>mindcv\\models\\dpn.py</code> <pre><code>@register_model\ndef dpn92(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; DPN:\n    \"\"\"Get 92 layers DPN model.\n     Refer to the base class `models.DPN` for more details.\"\"\"\n    default_cfg = default_cfgs[\"dpn92\"]\n    model = DPN(num_init_channel=64, k_r=96, g=32, k_sec=(3, 4, 20, 3), inc_sec=(16, 32, 24, 128),\n                num_classes=num_classes, in_channels=in_channels, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre>"},{"location":"reference/models/#mindcv.models.dpn.dpn98","title":"<code>mindcv.models.dpn.dpn98(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get 98 layers DPN model. Refer to the base class <code>models.DPN</code> for more details.</p> Source code in <code>mindcv\\models\\dpn.py</code> <pre><code>@register_model\ndef dpn98(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; DPN:\n    \"\"\"Get 98 layers DPN model.\n     Refer to the base class `models.DPN` for more details.\"\"\"\n    default_cfg = default_cfgs[\"dpn98\"]\n    model = DPN(num_init_channel=96, k_r=160, g=40, k_sec=(3, 6, 20, 3), inc_sec=(16, 32, 32, 128),\n                num_classes=num_classes, in_channels=in_channels, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre>"},{"location":"reference/models/#edgenext","title":"edgenext","text":""},{"location":"reference/models/#mindcv.models.edgenext","title":"<code>mindcv.models.edgenext</code>","text":"<p>MindSpore implementation of <code>edgenext</code>. Refer to EdgeNeXt: Efficiently Amalgamated CNN-Transformer Architecture for Mobile Vision Applications.</p>"},{"location":"reference/models/#mindcv.models.edgenext.EdgeNeXt","title":"<code>mindcv.models.edgenext.EdgeNeXt</code>","text":"<p>               Bases: <code>Cell</code></p> <p>EdgeNeXt model class, based on <code>\"Efficiently Amalgamated CNN-Transformer Architecture for Mobile Vision\" &lt;https://arxiv.org/abs/2206.10589&gt;</code>_</p> PARAMETER DESCRIPTION <code>in_channels</code> <p>number of input channels. Default: 3</p> <p> </p> <code>num_classes</code> <p>number of classification classes. Default: 1000</p> <p> DEFAULT: <code>1000</code> </p> <code>depths</code> <p>the depths of each layer. Default: [0, 0, 0, 3]</p> <p> DEFAULT: <code>[3, 3, 9, 3]</code> </p> <code>dims</code> <p>the middle dim of each layer. Default: [24, 48, 88, 168]</p> <p> DEFAULT: <code>[24, 48, 88, 168]</code> </p> <code>global_block</code> <p>number of global block. Default: [0, 0, 0, 3]</p> <p> DEFAULT: <code>[0, 0, 0, 3]</code> </p> <code>global_block_type</code> <p>type of global block. Default: ['None', 'None', 'None', 'SDTA']</p> <p> DEFAULT: <code>['None', 'None', 'None', 'SDTA']</code> </p> <code>drop_path_rate</code> <p>Stochastic Depth. Default: 0.</p> <p> DEFAULT: <code>0.0</code> </p> <code>layer_scale_init_value</code> <p>value of layer scale initialization. Default: 1e-6</p> <p> DEFAULT: <code>1e-06</code> </p> <code>head_init_scale</code> <p>scale of head initialization. Default: 1.</p> <p> DEFAULT: <code>1.0</code> </p> <code>expan_ratio</code> <p>ratio of expansion. Default: 4</p> <p> DEFAULT: <code>4</code> </p> <code>kernel_sizes</code> <p>kernel sizes of different stages. Default: [7, 7, 7, 7]</p> <p> DEFAULT: <code>[7, 7, 7, 7]</code> </p> <code>heads</code> <p>number of attention heads. Default: [8, 8, 8, 8]</p> <p> DEFAULT: <code>[8, 8, 8, 8]</code> </p> <code>use_pos_embd_xca</code> <p>use position embedding in xca or not. Default: [False, False, False, False]</p> <p> DEFAULT: <code>[False, False, False, False]</code> </p> <code>use_pos_embd_global</code> <p>use position embedding globally or not. Default: False</p> <p> DEFAULT: <code>False</code> </p> <code>d2_scales</code> <p>scales of splitting channels</p> <p> DEFAULT: <code>[2, 3, 4, 5]</code> </p> Source code in <code>mindcv\\models\\edgenext.py</code> <pre><code>class EdgeNeXt(nn.Cell):\n    r\"\"\"EdgeNeXt model class, based on\n    `\"Efficiently Amalgamated CNN-Transformer Architecture for Mobile Vision\" &lt;https://arxiv.org/abs/2206.10589&gt;`_\n\n    Args:\n        in_channels: number of input channels. Default: 3\n        num_classes: number of classification classes. Default: 1000\n        depths: the depths of each layer. Default: [0, 0, 0, 3]\n        dims: the middle dim of each layer. Default: [24, 48, 88, 168]\n        global_block: number of global block. Default: [0, 0, 0, 3]\n        global_block_type: type of global block. Default: ['None', 'None', 'None', 'SDTA']\n        drop_path_rate: Stochastic Depth. Default: 0.\n        layer_scale_init_value: value of layer scale initialization. Default: 1e-6\n        head_init_scale: scale of head initialization. Default: 1.\n        expan_ratio: ratio of expansion. Default: 4\n        kernel_sizes: kernel sizes of different stages. Default: [7, 7, 7, 7]\n        heads: number of attention heads. Default: [8, 8, 8, 8]\n        use_pos_embd_xca: use position embedding in xca or not. Default: [False, False, False, False]\n        use_pos_embd_global: use position embedding globally or not. Default: False\n        d2_scales: scales of splitting channels\n    \"\"\"\n    def __init__(self, in_chans=3, num_classes=1000,\n                 depths=[3, 3, 9, 3], dims=[24, 48, 88, 168],\n                 global_block=[0, 0, 0, 3], global_block_type=[\"None\", \"None\", \"None\", \"SDTA\"],\n                 drop_path_rate=0., layer_scale_init_value=1e-6, head_init_scale=1., expan_ratio=4,\n                 kernel_sizes=[7, 7, 7, 7], heads=[8, 8, 8, 8], use_pos_embd_xca=[False, False, False, False],\n                 use_pos_embd_global=False, d2_scales=[2, 3, 4, 5], **kwargs):\n        super().__init__()\n        for g in global_block_type:\n            assert g in [\"None\", \"SDTA\"]\n        if use_pos_embd_global:\n            self.pos_embd = PositionalEncodingFourier(dim=dims[0])\n        else:\n            self.pos_embd = None\n        self.downsample_layers = nn.CellList()  # stem and 3 intermediate downsampling conv layers\n        stem = nn.SequentialCell(\n            nn.Conv2d(in_chans, dims[0], kernel_size=4, stride=4, has_bias=True),\n            LayerNorm((dims[0],), epsilon=1e-6, norm_axis=1),\n        )\n        self.downsample_layers.append(stem)\n        for i in range(3):\n            downsample_layer = nn.SequentialCell(\n                LayerNorm((dims[i],), epsilon=1e-6, norm_axis=1),\n                nn.Conv2d(dims[i], dims[i + 1], kernel_size=2, stride=2, has_bias=True),\n            )\n            self.downsample_layers.append(downsample_layer)\n\n        self.stages = nn.CellList()  # 4 feature resolution stages, each consisting of multiple residual blocks\n        dp_rates = list(np.linspace(0, drop_path_rate, sum(depths)))\n        cur = 0\n        for i in range(4):\n            stage_blocks = []\n            for j in range(depths[i]):\n                if j &gt; depths[i] - global_block[i] - 1:\n                    if global_block_type[i] == \"SDTA\":\n                        stage_blocks.append(SDTAEncoder(dim=dims[i], drop_path=dp_rates[cur + j],\n                                                        expan_ratio=expan_ratio, scales=d2_scales[i],\n                                                        use_pos_emb=use_pos_embd_xca[i], num_heads=heads[i]))\n                    else:\n                        raise NotImplementedError\n                else:\n                    stage_blocks.append(ConvEncoder(dim=dims[i], drop_path=dp_rates[cur + j],\n                                                    layer_scale_init_value=layer_scale_init_value,\n                                                    expan_ratio=expan_ratio, kernel_size=kernel_sizes[i]))\n\n            self.stages.append(nn.SequentialCell(*stage_blocks))\n            cur += depths[i]\n        self.norm = nn.LayerNorm((dims[-1],), epsilon=1e-6)  # Final norm layer\n        self.head = nn.Dense(dims[-1], num_classes)\n\n        # self.head_dropout = Dropout(kwargs[\"classifier_dropout\"])\n        self.head_dropout = Dropout(p=0.0)\n        self.head_init_scale = head_init_scale\n        self._initialize_weights()\n\n    def _initialize_weights(self) -&gt; None:\n        \"\"\"Initialize weights for cells.\"\"\"\n        for _, cell in self.cells_and_names():\n            if isinstance(cell, (nn.Dense, nn.Conv2d)):\n                cell.weight.set_data(\n                    init.initializer(init.TruncatedNormal(sigma=0.02), cell.weight.shape, cell.weight.dtype)\n                )\n                if isinstance(cell, nn.Dense) and cell.bias is not None:\n                    cell.bias.set_data(init.initializer(init.Zero(), cell.bias.shape, cell.bias.dtype))\n            elif isinstance(cell, (nn.LayerNorm)):\n                cell.gamma.set_data(init.initializer(init.One(), cell.gamma.shape, cell.gamma.dtype))\n                cell.beta.set_data(init.initializer(init.Zero(), cell.beta.shape, cell.beta.dtype))\n        self.head.weight.set_data(self.head.weight * self.head_init_scale)\n        self.head.bias.set_data(self.head.bias * self.head_init_scale)\n\n    def forward_features(self, x):\n        x = self.downsample_layers[0](x)\n        x = self.stages[0](x)\n        if self.pos_embd is not None:\n            B, C, H, W = x.shape\n            x = x + self.pos_embd(B, H, W)\n        for i in range(1, 4):\n            x = self.downsample_layers[i](x)\n            x = self.stages[i](x)\n        return self.norm(x.mean([-2, -1]))  # Global average pooling, (N, C, H, W) -&gt; (N, C)\n\n    def construct(self, x):\n        x = self.forward_features(x)\n        x = self.head(self.head_dropout(x))\n        return x\n</code></pre>"},{"location":"reference/models/#mindcv.models.edgenext.LayerNorm","title":"<code>mindcv.models.edgenext.LayerNorm</code>","text":"<p>               Bases: <code>LayerNorm</code></p> <p>LayerNorm for channels_first tensors with 2d spatial dimensions (ie N, C, H, W).</p> Source code in <code>mindcv\\models\\edgenext.py</code> <pre><code>class LayerNorm(nn.LayerNorm):\n    r\"\"\"LayerNorm for channels_first tensors with 2d spatial dimensions (ie N, C, H, W).\"\"\"\n\n    def __init__(\n        self,\n        normalized_shape: Tuple[int],\n        epsilon: float,\n        norm_axis: int = -1,\n    ) -&gt; None:\n        super().__init__(normalized_shape=normalized_shape, epsilon=epsilon)\n        assert norm_axis in (-1, 1), \"ConvNextLayerNorm's norm_axis must be 1 or -1.\"\n        self.norm_axis = norm_axis\n\n    def construct(self, input_x: Tensor) -&gt; Tensor:\n        if self.norm_axis == -1:\n            y, _, _ = self.layer_norm(input_x, self.gamma, self.beta)\n        else:\n            input_x = ops.transpose(input_x, (0, 2, 3, 1))\n            y, _, _ = self.layer_norm(input_x, self.gamma, self.beta)\n            y = ops.transpose(y, (0, 3, 1, 2))\n        return y\n</code></pre>"},{"location":"reference/models/#mindcv.models.edgenext.edgenext_base","title":"<code>mindcv.models.edgenext.edgenext_base(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get edgenext_base model. Refer to the base class <code>models.EdgeNeXt</code> for more details.</p> Source code in <code>mindcv\\models\\edgenext.py</code> <pre><code>@register_model\ndef edgenext_base(pretrained: bool = False, num_classes: int = 1000, in_channels: int = 3, **kwargs) -&gt; EdgeNeXt:\n    \"\"\"Get edgenext_base model.\n    Refer to the base class `models.EdgeNeXt` for more details.\"\"\"\n    default_cfg = default_cfgs[\"edgenext_base\"]\n    model = EdgeNeXt(\n        depths=[3, 3, 9, 3],\n        dims=[80, 160, 288, 584],\n        expan_ratio=4,\n        num_classes=num_classes,\n        global_block=[0, 1, 1, 1],\n        global_block_type=[\"None\", \"SDTA\", \"SDTA\", \"SDTA\"],\n        use_pos_embd_xca=[False, True, False, False],\n        kernel_sizes=[3, 5, 7, 9],\n        d2_scales=[2, 2, 3, 4],\n        **kwargs\n    )\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n    return model\n</code></pre>"},{"location":"reference/models/#mindcv.models.edgenext.edgenext_small","title":"<code>mindcv.models.edgenext.edgenext_small(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get edgenext_small model. Refer to the base class <code>models.EdgeNeXt</code> for more details.</p> Source code in <code>mindcv\\models\\edgenext.py</code> <pre><code>@register_model\ndef edgenext_small(pretrained: bool = False, num_classes: int = 1000, in_channels: int = 3, **kwargs) -&gt; EdgeNeXt:\n    \"\"\"Get edgenext_small model.\n    Refer to the base class `models.EdgeNeXt` for more details.\"\"\"\n    default_cfg = default_cfgs[\"edgenext_small\"]\n    model = EdgeNeXt(\n        depths=[3, 3, 9, 3],\n        dims=[48, 96, 160, 304],\n        expan_ratio=4,\n        num_classes=num_classes,\n        global_block=[0, 1, 1, 1],\n        global_block_type=[\"None\", \"SDTA\", \"SDTA\", \"SDTA\"],\n        use_pos_embd_xca=[False, True, False, False],\n        kernel_sizes=[3, 5, 7, 9],\n        d2_scales=[2, 2, 3, 4],\n        **kwargs\n    )\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n    return model\n</code></pre>"},{"location":"reference/models/#mindcv.models.edgenext.edgenext_x_small","title":"<code>mindcv.models.edgenext.edgenext_x_small(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get edgenext_x_small model. Refer to the base class <code>models.EdgeNeXt</code> for more details.</p> Source code in <code>mindcv\\models\\edgenext.py</code> <pre><code>@register_model\ndef edgenext_x_small(pretrained: bool = False, num_classes: int = 1000, in_channels: int = 3, **kwargs) -&gt; EdgeNeXt:\n    \"\"\"Get edgenext_x_small model.\n    Refer to the base class `models.EdgeNeXt` for more details.\"\"\"\n    default_cfg = default_cfgs[\"edgenext_x_small\"]\n    model = EdgeNeXt(\n        depths=[3, 3, 9, 3],\n        dims=[32, 64, 100, 192],\n        expan_ratio=4,\n        num_classes=num_classes,\n        global_block=[0, 1, 1, 1],\n        global_block_type=[\"None\", \"SDTA\", \"SDTA\", \"SDTA\"],\n        use_pos_embd_xca=[False, True, False, False],\n        kernel_sizes=[3, 5, 7, 9],\n        heads=[4, 4, 4, 4],\n        d2_scales=[2, 2, 3, 4],\n        **kwargs\n    )\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n    return model\n</code></pre>"},{"location":"reference/models/#mindcv.models.edgenext.edgenext_xx_small","title":"<code>mindcv.models.edgenext.edgenext_xx_small(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get edgenext_xx_small model. Refer to the base class <code>models.EdgeNeXt</code> for more details.</p> Source code in <code>mindcv\\models\\edgenext.py</code> <pre><code>@register_model\ndef edgenext_xx_small(pretrained: bool = False, num_classes: int = 1000, in_channels: int = 3, **kwargs) -&gt; EdgeNeXt:\n    \"\"\"Get edgenext_xx_small model.\n        Refer to the base class `models.EdgeNeXt` for more details.\"\"\"\n    default_cfg = default_cfgs[\"edgenext_xx_small\"]\n    model = EdgeNeXt(\n        depths=[2, 2, 6, 2],\n        dims=[24, 48, 88, 168],\n        expan_ratio=4,\n        num_classes=num_classes,\n        global_block=[0, 1, 1, 1],\n        global_block_type=['None', 'SDTA', 'SDTA', 'SDTA'],\n        use_pos_embd_xca=[False, True, False, False],\n        kernel_sizes=[3, 5, 7, 9],\n        heads=[4, 4, 4, 4],\n        d2_scales=[2, 2, 3, 4],\n        **kwargs\n    )\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre>"},{"location":"reference/models/#efficientnet","title":"efficientnet","text":""},{"location":"reference/models/#mindcv.models.efficientnet","title":"<code>mindcv.models.efficientnet</code>","text":"<p>EfficientNet Architecture.</p>"},{"location":"reference/models/#mindcv.models.efficientnet.EfficientNet","title":"<code>mindcv.models.efficientnet.EfficientNet</code>","text":"<p>               Bases: <code>Cell</code></p> <p>EfficientNet architecture. <code>EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks &lt;https://arxiv.org/abs/1905.11946&gt;</code>_.</p> PARAMETER DESCRIPTION <code>arch</code> <p>The name of the model.</p> <p> TYPE: <code>str</code> </p> <code>dropout_rate</code> <p>The dropout rate of efficientnet.</p> <p> TYPE: <code>float</code> </p> <code>width_mult</code> <p>The ratio of the channel. Default: 1.0.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>depth_mult</code> <p>The ratio of num_layers. Default: 1.0.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>in_channels</code> <p>The input channels. Default: 3.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>num_classes</code> <p>The number of class. Default: 1000.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1000</code> </p> <code>inverted_residual_setting</code> <p>The settings of block. Default: None.</p> <p> TYPE: <code>Sequence[Union[MBConvConfig, FusedMBConvConfig]]</code> DEFAULT: <code>None</code> </p> <code>drop_path_prob</code> <p>The drop path rate of MBConv. Default: 0.2.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.2</code> </p> <code>norm_layer</code> <p>The normalization layer. Default: None.</p> <p> TYPE: <code>Cell</code> DEFAULT: <code>None</code> </p> Inputs <ul> <li>x (Tensor) - Tensor of shape :math:<code>(N, C_{in}, H_{in}, W_{in})</code>.</li> </ul> Outputs <p>Tensor of shape :math:<code>(N, 1000)</code>.</p> Source code in <code>mindcv\\models\\efficientnet.py</code> <pre><code>class EfficientNet(nn.Cell):\n    \"\"\"\n    EfficientNet architecture.\n    `EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks &lt;https://arxiv.org/abs/1905.11946&gt;`_.\n\n    Args:\n        arch (str): The name of the model.\n        dropout_rate (float): The dropout rate of efficientnet.\n        width_mult (float): The ratio of the channel. Default: 1.0.\n        depth_mult (float): The ratio of num_layers. Default: 1.0.\n        in_channels (int): The input channels. Default: 3.\n        num_classes (int): The number of class. Default: 1000.\n        inverted_residual_setting (Sequence[Union[MBConvConfig, FusedMBConvConfig]], optional): The settings of block.\n            Default: None.\n        drop_path_prob (float): The drop path rate of MBConv. Default: 0.2.\n        norm_layer (nn.Cell, optional): The normalization layer. Default: None.\n\n    Inputs:\n        - **x** (Tensor) - Tensor of shape :math:`(N, C_{in}, H_{in}, W_{in})`.\n\n    Outputs:\n        Tensor of shape :math:`(N, 1000)`.\n    \"\"\"\n\n    def __init__(\n        self,\n        arch: str,\n        dropout_rate: float,\n        width_mult: float = 1.0,\n        depth_mult: float = 1.0,\n        in_channels: int = 3,\n        num_classes: int = 1000,\n        inverted_residual_setting: Optional[Sequence[Union[MBConvConfig, FusedMBConvConfig]]] = None,\n        drop_path_prob: float = 0.2,\n        norm_layer: Optional[nn.Cell] = None,\n    ) -&gt; None:\n        super().__init__()\n        self.last_channel = None\n\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n            if width_mult &gt;= 1.6:\n                norm_layer = partial(nn.BatchNorm2d, eps=0.001, momentum=0.99)\n\n        layers: List[nn.Cell] = []\n\n        if not inverted_residual_setting:\n            if arch.startswith(\"efficientnet_b\"):\n                bneck_conf = partial(MBConvConfig, width_cnf=width_mult, depth_cnf=depth_mult)\n                inverted_residual_setting = [\n                    bneck_conf(1, 3, 1, 32, 16, 1),\n                    bneck_conf(6, 3, 2, 16, 24, 2),\n                    bneck_conf(6, 5, 2, 24, 40, 2),\n                    bneck_conf(6, 3, 2, 40, 80, 3),\n                    bneck_conf(6, 5, 1, 80, 112, 3),\n                    bneck_conf(6, 5, 2, 112, 192, 4),\n                    bneck_conf(6, 3, 1, 192, 320, 1),\n                ]\n            elif arch.startswith(\"efficientnet_v2_s\"):\n                inverted_residual_setting = [\n                    FusedMBConvConfig(1, 3, 1, 24, 24, 2),\n                    FusedMBConvConfig(4, 3, 2, 24, 48, 4),\n                    FusedMBConvConfig(4, 3, 2, 48, 64, 4),\n                    MBConvConfig(4, 3, 2, 64, 128, 6),\n                    MBConvConfig(6, 3, 1, 128, 160, 9),\n                    MBConvConfig(6, 3, 2, 160, 256, 15),\n                ]\n                self.last_channel = 1280\n            elif arch.startswith(\"efficientnet_v2_m\"):\n                inverted_residual_setting = [\n                    FusedMBConvConfig(1, 3, 1, 24, 24, 3),\n                    FusedMBConvConfig(4, 3, 2, 24, 48, 5),\n                    FusedMBConvConfig(4, 3, 2, 48, 80, 5),\n                    MBConvConfig(4, 3, 2, 80, 160, 7),\n                    MBConvConfig(6, 3, 1, 160, 176, 14),\n                    MBConvConfig(6, 3, 2, 176, 304, 18),\n                    MBConvConfig(6, 3, 1, 304, 512, 5),\n                ]\n                self.last_channel = 1280\n            elif arch.startswith(\"efficientnet_v2_l\"):\n                inverted_residual_setting = [\n                    FusedMBConvConfig(1, 3, 1, 32, 32, 4),\n                    FusedMBConvConfig(4, 3, 2, 32, 64, 7),\n                    FusedMBConvConfig(4, 3, 2, 64, 96, 7),\n                    MBConvConfig(4, 3, 2, 96, 192, 10),\n                    MBConvConfig(6, 3, 1, 192, 224, 19),\n                    MBConvConfig(6, 3, 2, 224, 384, 25),\n                    MBConvConfig(6, 3, 1, 384, 640, 7),\n                ]\n                self.last_channel = 1280\n            elif arch.startswith(\"efficientnet_v2_xl\"):\n                inverted_residual_setting = [\n                    FusedMBConvConfig(1, 3, 1, 32, 32, 4),\n                    FusedMBConvConfig(4, 3, 2, 32, 64, 8),\n                    FusedMBConvConfig(4, 3, 2, 64, 96, 8),\n                    MBConvConfig(4, 3, 2, 96, 192, 16),\n                    MBConvConfig(6, 3, 1, 192, 256, 24),\n                    MBConvConfig(6, 3, 2, 256, 512, 32),\n                    MBConvConfig(6, 3, 1, 512, 640, 8),\n                ]\n                self.last_channel = 1280\n\n        # building first layer\n        firstconv_output_channels = inverted_residual_setting[0].input_channels\n        layers.extend([\n            nn.Conv2d(in_channels, firstconv_output_channels, kernel_size=3, stride=2),\n            norm_layer(firstconv_output_channels),\n            Swish(),\n        ])\n\n        total_reduction = 2\n        self.feature_info = [dict(chs=firstconv_output_channels, reduction=total_reduction,\n                                  name=f'features.{len(layers) - 1}')]\n\n        # building MBConv blocks\n        total_stage_blocks = sum(cnf.num_layers for cnf in inverted_residual_setting)\n        stage_block_id = 0\n\n        # cnf is the settings of block\n        for cnf in inverted_residual_setting:\n            stage: List[nn.Cell] = []\n\n            # cnf.num_layers is the num of the same block\n            for _ in range(cnf.num_layers):\n                # copy to avoid modifications. shallow copy is enough\n                block_cnf = copy.copy(cnf)\n\n                block = MBConv\n\n                if \"FusedMBConvConfig\" in str(type(block_cnf)):\n                    block = FusedMBConv\n\n                # overwrite info if not the first conv in the stage\n                if stage:\n                    block_cnf.input_channels = block_cnf.out_channels\n                    block_cnf.stride = 1\n\n                # adjust dropout rate of blocks based on the depth of the stage block\n                sd_prob = drop_path_prob * float(stage_block_id) / total_stage_blocks\n\n                total_reduction *= block_cnf.stride\n\n                stage.append(block(block_cnf, sd_prob, norm_layer))\n                stage_block_id += 1\n\n            layers.append(nn.SequentialCell(stage))\n\n            self.feature_info.append(dict(chs=cnf.out_channels, reduction=total_reduction,\n                                          name=f'features.{len(layers) - 1}'))\n\n        # building last several layers\n        lastconv_input_channels = inverted_residual_setting[-1].out_channels\n        lastconv_output_channels = self.last_channel if self.last_channel is not None else 4 * lastconv_input_channels\n        layers.extend([\n            nn.Conv2d(lastconv_input_channels, lastconv_output_channels, kernel_size=1),\n            norm_layer(lastconv_output_channels),\n            Swish(),\n        ])\n\n        self.feature_info.append(dict(chs=lastconv_output_channels, reduction=total_reduction,\n                                      name=f'features.{len(layers) - 1}'))\n        self.flatten_sequential = True\n\n        self.features = nn.SequentialCell(layers)\n        self.avgpool = GlobalAvgPooling()\n        self.dropout = Dropout(p=dropout_rate)\n        self.mlp_head = nn.Dense(lastconv_output_channels, num_classes)\n        self._initialize_weights()\n\n    def forward_features(self, x: Tensor) -&gt; Tensor:\n        x = self.features(x)\n\n        x = self.avgpool(x)\n\n        if self.training:\n            x = self.dropout(x)\n        return x\n\n    def forward_head(self, x: Tensor) -&gt; Tensor:\n        return self.mlp_head(x)\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        \"\"\"construct\"\"\"\n        x = self.forward_features(x)\n        return self.forward_head(x)\n\n    def _initialize_weights(self) -&gt; None:\n        \"\"\"Initialize weights for cells.\"\"\"\n        for _, cell in self.cells_and_names():\n            if isinstance(cell, nn.Dense):\n                init_range = 1.0 / np.sqrt(cell.weight.shape[0])\n                cell.weight.set_data(weight_init.initializer(Uniform(init_range), cell.weight.shape, cell.weight.dtype))\n                if cell.bias is not None:\n                    cell.bias.set_data(weight_init.initializer(weight_init.Zero(), cell.bias.shape, cell.bias.dtype))\n            if isinstance(cell, nn.Conv2d):\n                out_channel, _, kernel_size_h, kernel_size_w = cell.weight.shape\n                stddev = np.sqrt(2 / int(out_channel * kernel_size_h * kernel_size_w))\n                cell.weight.set_data(\n                    weight_init.initializer(Normal(sigma=stddev), cell.weight.shape, cell.weight.dtype)\n                )\n                if cell.bias is not None:\n                    cell.bias.set_data(weight_init.initializer(weight_init.Zero(), cell.bias.shape, cell.bias.dtype))\n</code></pre>"},{"location":"reference/models/#mindcv.models.efficientnet.EfficientNet.construct","title":"<code>mindcv.models.efficientnet.EfficientNet.construct(x)</code>","text":"<p>construct</p> Source code in <code>mindcv\\models\\efficientnet.py</code> <pre><code>def construct(self, x: Tensor) -&gt; Tensor:\n    \"\"\"construct\"\"\"\n    x = self.forward_features(x)\n    return self.forward_head(x)\n</code></pre>"},{"location":"reference/models/#mindcv.models.efficientnet.FusedMBConv","title":"<code>mindcv.models.efficientnet.FusedMBConv</code>","text":"<p>               Bases: <code>Cell</code></p> <p>FusedMBConv</p> Source code in <code>mindcv\\models\\efficientnet.py</code> <pre><code>class FusedMBConv(nn.Cell):\n    \"\"\"FusedMBConv\"\"\"\n\n    def __init__(\n        self,\n        cnf: FusedMBConvConfig,\n        drop_path_prob: float,\n        norm: Optional[nn.Cell] = None,\n    ) -&gt; None:\n        super().__init__()\n\n        if not 1 &lt;= cnf.stride &lt;= 2:\n            raise ValueError(\"illegal stride value\")\n\n        self.shortcut = cnf.stride == 1 and cnf.input_channels == cnf.out_channels\n\n        layers: List[nn.Cell] = []\n\n        expanded_channels = cnf.adjust_channels(cnf.input_channels, cnf.expand_ratio)\n        if expanded_channels != cnf.input_channels:\n            # fused expand\n            layers.extend([\n                nn.Conv2d(cnf.input_channels, expanded_channels, kernel_size=cnf.kernel_size,\n                          stride=cnf.stride),\n                norm(expanded_channels),\n                Swish(),\n            ])\n\n            # project\n            layers.extend([\n                nn.Conv2d(expanded_channels, cnf.out_channels, kernel_size=1),\n                norm(cnf.out_channels),\n            ])\n        else:\n            layers.extend([\n                nn.Conv2d(cnf.input_channels, cnf.out_channels, kernel_size=cnf.kernel_size,\n                          stride=cnf.stride),\n                norm(cnf.out_channels),\n                Swish(),\n            ])\n\n        self.block = nn.SequentialCell(layers)\n        self.dropout = DropPath(drop_path_prob)\n        self.out_channels = cnf.out_channels\n\n    def construct(self, x) -&gt; Tensor:\n        result = self.block(x)\n        if self.shortcut:\n            result = self.dropout(result)\n            result += x\n        return result\n</code></pre>"},{"location":"reference/models/#mindcv.models.efficientnet.FusedMBConvConfig","title":"<code>mindcv.models.efficientnet.FusedMBConvConfig</code>","text":"<p>               Bases: <code>MBConvConfig</code></p> <p>FusedMBConvConfig</p> Source code in <code>mindcv\\models\\efficientnet.py</code> <pre><code>class FusedMBConvConfig(MBConvConfig):\n    \"\"\"FusedMBConvConfig\"\"\"\n\n    # Stores information listed at Table 4 of the EfficientNetV2 paper\n    def __init__(\n        self,\n        expand_ratio: float,\n        kernel_size: int,\n        stride: int,\n        in_chs: int,\n        out_chs: int,\n        num_layers: int,\n    ) -&gt; None:\n        super().__init__(expand_ratio, kernel_size, stride, in_chs, out_chs, num_layers)\n</code></pre>"},{"location":"reference/models/#mindcv.models.efficientnet.MBConv","title":"<code>mindcv.models.efficientnet.MBConv</code>","text":"<p>               Bases: <code>Cell</code></p> <p>MBConv Module.</p> PARAMETER DESCRIPTION <code>cnf</code> <p>The class which contains the parameters(in_channels, out_channels, nums_layers) and the functions which help calculate the parameters after multipling the expand_ratio.</p> <p> TYPE: <code>MBConvConfig</code> </p> <code>drop_path_prob</code> <p>The drop path rate in MBConv. Default: 0.2.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.2</code> </p> <code>norm</code> <p>The BatchNorm Method. Default: None.</p> <p> TYPE: <code>Cell</code> DEFAULT: <code>None</code> </p> <code>se_layer</code> <p>The squeeze-excite Module. Default: SqueezeExcite.</p> <p> TYPE: <code>Cell</code> DEFAULT: <code>SqueezeExcite</code> </p> RETURNS DESCRIPTION <p>Tensor</p> Source code in <code>mindcv\\models\\efficientnet.py</code> <pre><code>class MBConv(nn.Cell):\n    \"\"\"\n    MBConv Module.\n\n    Args:\n        cnf (MBConvConfig): The class which contains the parameters(in_channels, out_channels, nums_layers) and\n            the functions which help calculate the parameters after multipling the expand_ratio.\n        drop_path_prob: The drop path rate in MBConv. Default: 0.2.\n        norm (nn.Cell): The BatchNorm Method. Default: None.\n        se_layer (nn.Cell): The squeeze-excite Module. Default: SqueezeExcite.\n\n    Returns:\n        Tensor\n    \"\"\"\n\n    def __init__(\n        self,\n        cnf: MBConvConfig,\n        drop_path_prob: float = 0.2,\n        norm: Optional[nn.Cell] = None,\n        se_layer: Callable[..., nn.Cell] = SqueezeExcite,\n    ) -&gt; None:\n        super().__init__()\n\n        self.shortcut = cnf.stride == 1 and cnf.input_channels == cnf.out_channels\n\n        layers: List[nn.Cell] = []\n\n        # expand conv: the out_channels is cnf.expand_ratio times of the in_channels.\n        expanded_channels = cnf.adjust_channels(cnf.input_channels, cnf.expand_ratio)\n        if expanded_channels != cnf.input_channels:\n            layers.extend([\n                nn.Conv2d(cnf.input_channels, expanded_channels, kernel_size=1),\n                norm(expanded_channels),\n                Swish(),\n            ])\n\n        # depthwise conv: splits the filter into groups.\n        layers.extend([\n            nn.Conv2d(expanded_channels, expanded_channels, kernel_size=cnf.kernel_size,\n                      stride=cnf.stride, group=expanded_channels),\n            norm(expanded_channels),\n            Swish(),\n        ])\n\n        # squeeze and excitation\n        squeeze_channels = max(1, cnf.input_channels // 4)\n        layers.append(se_layer(in_channels=expanded_channels, rd_channels=squeeze_channels, act_layer=Swish))\n\n        # project\n        layers.extend([\n            nn.Conv2d(expanded_channels, cnf.out_channels, kernel_size=1),\n            norm(cnf.out_channels),\n        ])\n\n        self.block = nn.SequentialCell(layers)\n        self.dropout = DropPath(drop_path_prob)\n        self.out_channels = cnf.out_channels\n\n    def construct(self, x) -&gt; Tensor:\n        result = self.block(x)\n        if self.shortcut:\n            result = self.dropout(result)\n            result += x\n        return result\n</code></pre>"},{"location":"reference/models/#mindcv.models.efficientnet.MBConvConfig","title":"<code>mindcv.models.efficientnet.MBConvConfig</code>","text":"<p>The Parameters of MBConv which need to multiply the expand_ration.</p> PARAMETER DESCRIPTION <code>expand_ratio</code> <p>The Times of the num of out_channels with respect to in_channels.</p> <p> TYPE: <code>float</code> </p> <code>kernel_size</code> <p>The kernel size of the depthwise conv.</p> <p> TYPE: <code>int</code> </p> <code>stride</code> <p>The stride of the depthwise conv.</p> <p> TYPE: <code>int</code> </p> <code>in_chs</code> <p>The input_channels of the MBConv Module.</p> <p> TYPE: <code>int</code> </p> <code>out_chs</code> <p>The output_channels of the MBConv Module.</p> <p> TYPE: <code>int</code> </p> <code>num_layers</code> <p>The num of MBConv Module.</p> <p> TYPE: <code>int</code> </p> <code>width_cnf</code> <p>The ratio of the channel. Default: 1.0.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>depth_cnf</code> <p>The ratio of num_layers. Default: 1.0.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> RETURNS DESCRIPTION <p>None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; cnf = MBConvConfig(1, 3, 1, 32, 16, 1)\n&gt;&gt;&gt; print(cnf.input_channels)\n</code></pre> Source code in <code>mindcv\\models\\efficientnet.py</code> <pre><code>class MBConvConfig:\n    \"\"\"\n    The Parameters of MBConv which need to multiply the expand_ration.\n\n    Args:\n        expand_ratio (float): The Times of the num of out_channels with respect to in_channels.\n        kernel_size (int): The kernel size of the depthwise conv.\n        stride (int): The stride of the depthwise conv.\n        in_chs (int): The input_channels of the MBConv Module.\n        out_chs (int): The output_channels of the MBConv Module.\n        num_layers (int): The num of MBConv Module.\n        width_cnf: The ratio of the channel. Default: 1.0.\n        depth_cnf: The ratio of num_layers. Default: 1.0.\n\n    Returns:\n        None\n\n    Examples:\n        &gt;&gt;&gt; cnf = MBConvConfig(1, 3, 1, 32, 16, 1)\n        &gt;&gt;&gt; print(cnf.input_channels)\n    \"\"\"\n\n    def __init__(\n        self,\n        expand_ratio: float,\n        kernel_size: int,\n        stride: int,\n        in_chs: int,\n        out_chs: int,\n        num_layers: int,\n        width_cnf: float = 1.0,\n        depth_cnf: float = 1.0,\n    ) -&gt; None:\n        self.expand_ratio = expand_ratio\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.input_channels = self.adjust_channels(in_chs, width_cnf)\n        self.out_channels = self.adjust_channels(out_chs, width_cnf)\n        self.num_layers = self.adjust_depth(num_layers, depth_cnf)\n\n    @staticmethod\n    def adjust_channels(channels: int, width_cnf: float, min_value: Optional[int] = None) -&gt; int:\n        \"\"\"\n        Calculate the width of MBConv.\n\n        Args:\n            channels (int): The number of channel.\n            width_cnf (float): The ratio of channel.\n            min_value (int, optional): The minimum number of channel. Default: None.\n\n        Returns:\n            int, the width of MBConv.\n        \"\"\"\n\n        return make_divisible(channels * width_cnf, 8, min_value)\n\n    @staticmethod\n    def adjust_depth(num_layers: int, depth_cnf: float) -&gt; int:\n        \"\"\"\n        Calculate the depth of MBConv.\n\n        Args:\n            num_layers (int): The number of MBConv Module.\n            depth_cnf (float): The ratio of num_layers.\n\n        Returns:\n            int, the depth of MBConv.\n        \"\"\"\n\n        return int(math.ceil(num_layers * depth_cnf))\n</code></pre>"},{"location":"reference/models/#mindcv.models.efficientnet.MBConvConfig.adjust_channels","title":"<code>mindcv.models.efficientnet.MBConvConfig.adjust_channels(channels, width_cnf, min_value=None)</code>  <code>staticmethod</code>","text":"<p>Calculate the width of MBConv.</p> PARAMETER DESCRIPTION <code>channels</code> <p>The number of channel.</p> <p> TYPE: <code>int</code> </p> <code>width_cnf</code> <p>The ratio of channel.</p> <p> TYPE: <code>float</code> </p> <code>min_value</code> <p>The minimum number of channel. Default: None.</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>int</code> <p>int, the width of MBConv.</p> Source code in <code>mindcv\\models\\efficientnet.py</code> <pre><code>@staticmethod\ndef adjust_channels(channels: int, width_cnf: float, min_value: Optional[int] = None) -&gt; int:\n    \"\"\"\n    Calculate the width of MBConv.\n\n    Args:\n        channels (int): The number of channel.\n        width_cnf (float): The ratio of channel.\n        min_value (int, optional): The minimum number of channel. Default: None.\n\n    Returns:\n        int, the width of MBConv.\n    \"\"\"\n\n    return make_divisible(channels * width_cnf, 8, min_value)\n</code></pre>"},{"location":"reference/models/#mindcv.models.efficientnet.MBConvConfig.adjust_depth","title":"<code>mindcv.models.efficientnet.MBConvConfig.adjust_depth(num_layers, depth_cnf)</code>  <code>staticmethod</code>","text":"<p>Calculate the depth of MBConv.</p> PARAMETER DESCRIPTION <code>num_layers</code> <p>The number of MBConv Module.</p> <p> TYPE: <code>int</code> </p> <code>depth_cnf</code> <p>The ratio of num_layers.</p> <p> TYPE: <code>float</code> </p> RETURNS DESCRIPTION <code>int</code> <p>int, the depth of MBConv.</p> Source code in <code>mindcv\\models\\efficientnet.py</code> <pre><code>@staticmethod\ndef adjust_depth(num_layers: int, depth_cnf: float) -&gt; int:\n    \"\"\"\n    Calculate the depth of MBConv.\n\n    Args:\n        num_layers (int): The number of MBConv Module.\n        depth_cnf (float): The ratio of num_layers.\n\n    Returns:\n        int, the depth of MBConv.\n    \"\"\"\n\n    return int(math.ceil(num_layers * depth_cnf))\n</code></pre>"},{"location":"reference/models/#mindcv.models.efficientnet.efficientnet_b0","title":"<code>mindcv.models.efficientnet.efficientnet_b0(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Constructs a EfficientNet B0 architecture from <code>EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks &lt;https://arxiv.org/abs/1905.11946&gt;</code>_.</p> PARAMETER DESCRIPTION <code>pretrained</code> <p>If True, returns a model pretrained on IMAGENET. Default: False.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>num_classes</code> <p>The numbers of classes. Default: 1000.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1000</code> </p> <code>in_channels</code> <p>The input channels. Default: 1000.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> Inputs <ul> <li>x (Tensor) - Tensor of shape :math:<code>(N, C_{in}, H_{in}, W_{in})</code>.</li> </ul> Outputs <p>Tensor of shape :math:<code>(N, CLASSES_{out})</code>.</p> Source code in <code>mindcv\\models\\efficientnet.py</code> <pre><code>@register_model\ndef efficientnet_b0(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; EfficientNet:\n    \"\"\"\n    Constructs a EfficientNet B0 architecture from\n    `EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks &lt;https://arxiv.org/abs/1905.11946&gt;`_.\n\n    Args:\n        pretrained (bool): If True, returns a model pretrained on IMAGENET. Default: False.\n        num_classes (int): The numbers of classes. Default: 1000.\n        in_channels (int): The input channels. Default: 1000.\n\n    Inputs:\n        - **x** (Tensor) - Tensor of shape :math:`(N, C_{in}, H_{in}, W_{in})`.\n\n    Outputs:\n        Tensor of shape :math:`(N, CLASSES_{out})`.\n    \"\"\"\n    return _efficientnet(\"efficientnet_b0\", 1.0, 1.0, 0.2, in_channels, num_classes, pretrained, **kwargs)\n</code></pre>"},{"location":"reference/models/#mindcv.models.efficientnet.efficientnet_b1","title":"<code>mindcv.models.efficientnet.efficientnet_b1(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Constructs a EfficientNet B1 architecture from <code>EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks &lt;https://arxiv.org/abs/1905.11946&gt;</code>_.</p> PARAMETER DESCRIPTION <code>pretrained</code> <p>If True, returns a model pretrained on IMAGENET. Default: False.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>num_classes</code> <p>The numbers of classes. Default: 1000.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1000</code> </p> <code>in_channels</code> <p>The input channels. Default: 1000.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> Inputs <ul> <li>x (Tensor) - Tensor of shape :math:<code>(N, C_{in}, H_{in}, W_{in})</code>.</li> </ul> Outputs <p>Tensor of shape :math:<code>(N, CLASSES_{out})</code>.</p> Source code in <code>mindcv\\models\\efficientnet.py</code> <pre><code>@register_model\ndef efficientnet_b1(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; EfficientNet:\n    \"\"\"\n    Constructs a EfficientNet B1 architecture from\n    `EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks &lt;https://arxiv.org/abs/1905.11946&gt;`_.\n\n    Args:\n        pretrained (bool): If True, returns a model pretrained on IMAGENET. Default: False.\n        num_classes (int): The numbers of classes. Default: 1000.\n        in_channels (int): The input channels. Default: 1000.\n\n    Inputs:\n        - **x** (Tensor) - Tensor of shape :math:`(N, C_{in}, H_{in}, W_{in})`.\n\n    Outputs:\n        Tensor of shape :math:`(N, CLASSES_{out})`.\n    \"\"\"\n    return _efficientnet(\"efficientnet_b1\", 1.0, 1.1, 0.2, in_channels, num_classes, pretrained, **kwargs)\n</code></pre>"},{"location":"reference/models/#mindcv.models.efficientnet.efficientnet_b2","title":"<code>mindcv.models.efficientnet.efficientnet_b2(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Constructs a EfficientNet B2 architecture from <code>EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks &lt;https://arxiv.org/abs/1905.11946&gt;</code>_.</p> PARAMETER DESCRIPTION <code>pretrained</code> <p>If True, returns a model pretrained on IMAGENET. Default: False.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>num_classes</code> <p>The numbers of classes. Default: 1000.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1000</code> </p> <code>in_channels</code> <p>The input channels. Default: 1000.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> Inputs <ul> <li>x (Tensor) - Tensor of shape :math:<code>(N, C_{in}, H_{in}, W_{in})</code>.</li> </ul> Outputs <p>Tensor of shape :math:<code>(N, CLASSES_{out})</code>.</p> Source code in <code>mindcv\\models\\efficientnet.py</code> <pre><code>@register_model\ndef efficientnet_b2(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; EfficientNet:\n    \"\"\"\n    Constructs a EfficientNet B2 architecture from\n    `EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks &lt;https://arxiv.org/abs/1905.11946&gt;`_.\n\n    Args:\n        pretrained (bool): If True, returns a model pretrained on IMAGENET. Default: False.\n        num_classes (int): The numbers of classes. Default: 1000.\n        in_channels (int): The input channels. Default: 1000.\n\n    Inputs:\n        - **x** (Tensor) - Tensor of shape :math:`(N, C_{in}, H_{in}, W_{in})`.\n\n    Outputs:\n        Tensor of shape :math:`(N, CLASSES_{out})`.\n    \"\"\"\n    return _efficientnet(\"efficientnet_b2\", 1.1, 1.2, 0.3, in_channels, num_classes, pretrained, **kwargs)\n</code></pre>"},{"location":"reference/models/#mindcv.models.efficientnet.efficientnet_b3","title":"<code>mindcv.models.efficientnet.efficientnet_b3(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Constructs a EfficientNet B3 architecture from <code>EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks &lt;https://arxiv.org/abs/1905.11946&gt;</code>_.</p> PARAMETER DESCRIPTION <code>pretrained</code> <p>If True, returns a model pretrained on IMAGENET. Default: False.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>num_classes</code> <p>The numbers of classes. Default: 1000.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1000</code> </p> <code>in_channels</code> <p>The input channels. Default: 1000.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> Inputs <ul> <li>x (Tensor) - Tensor of shape :math:<code>(N, C_{in}, H_{in}, W_{in})</code>.</li> </ul> Outputs <p>Tensor of shape :math:<code>(N, CLASSES_{out})</code>.</p> Source code in <code>mindcv\\models\\efficientnet.py</code> <pre><code>@register_model\ndef efficientnet_b3(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; EfficientNet:\n    \"\"\"\n    Constructs a EfficientNet B3 architecture from\n    `EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks &lt;https://arxiv.org/abs/1905.11946&gt;`_.\n\n    Args:\n        pretrained (bool): If True, returns a model pretrained on IMAGENET. Default: False.\n        num_classes (int): The numbers of classes. Default: 1000.\n        in_channels (int): The input channels. Default: 1000.\n\n    Inputs:\n        - **x** (Tensor) - Tensor of shape :math:`(N, C_{in}, H_{in}, W_{in})`.\n\n    Outputs:\n        Tensor of shape :math:`(N, CLASSES_{out})`.\n    \"\"\"\n    return _efficientnet(\"efficientnet_b3\", 1.2, 1.4, 0.3, in_channels, num_classes, pretrained, **kwargs)\n</code></pre>"},{"location":"reference/models/#mindcv.models.efficientnet.efficientnet_b4","title":"<code>mindcv.models.efficientnet.efficientnet_b4(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Constructs a EfficientNet B4 architecture from <code>EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks &lt;https://arxiv.org/abs/1905.11946&gt;</code>_.</p> PARAMETER DESCRIPTION <code>pretrained</code> <p>If True, returns a model pretrained on IMAGENET. Default: False.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>num_classes</code> <p>The numbers of classes. Default: 1000.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1000</code> </p> <code>in_channels</code> <p>The input channels. Default: 1000.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> Inputs <ul> <li>x (Tensor) - Tensor of shape :math:<code>(N, C_{in}, H_{in}, W_{in})</code>.</li> </ul> Outputs <p>Tensor of shape :math:<code>(N, CLASSES_{out})</code>.</p> Source code in <code>mindcv\\models\\efficientnet.py</code> <pre><code>@register_model\ndef efficientnet_b4(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; EfficientNet:\n    \"\"\"\n    Constructs a EfficientNet B4 architecture from\n    `EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks &lt;https://arxiv.org/abs/1905.11946&gt;`_.\n\n    Args:\n        pretrained (bool): If True, returns a model pretrained on IMAGENET. Default: False.\n        num_classes (int): The numbers of classes. Default: 1000.\n        in_channels (int): The input channels. Default: 1000.\n\n    Inputs:\n        - **x** (Tensor) - Tensor of shape :math:`(N, C_{in}, H_{in}, W_{in})`.\n\n    Outputs:\n        Tensor of shape :math:`(N, CLASSES_{out})`.\n    \"\"\"\n    return _efficientnet(\"efficientnet_b4\", 1.4, 1.8, 0.4, in_channels, num_classes, pretrained, **kwargs)\n</code></pre>"},{"location":"reference/models/#mindcv.models.efficientnet.efficientnet_b5","title":"<code>mindcv.models.efficientnet.efficientnet_b5(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Constructs a EfficientNet B5 architecture from <code>EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks &lt;https://arxiv.org/abs/1905.11946&gt;</code>_.</p> PARAMETER DESCRIPTION <code>pretrained</code> <p>If True, returns a model pretrained on IMAGENET. Default: False.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>num_classes</code> <p>The numbers of classes. Default: 1000.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1000</code> </p> <code>in_channels</code> <p>The input channels. Default: 1000.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> Inputs <ul> <li>x (Tensor) - Tensor of shape :math:<code>(N, C_{in}, H_{in}, W_{in})</code>.</li> </ul> Outputs <p>Tensor of shape :math:<code>(N, CLASSES_{out})</code>.</p> Source code in <code>mindcv\\models\\efficientnet.py</code> <pre><code>@register_model\ndef efficientnet_b5(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; EfficientNet:\n    \"\"\"\n    Constructs a EfficientNet B5 architecture from\n    `EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks &lt;https://arxiv.org/abs/1905.11946&gt;`_.\n\n    Args:\n        pretrained (bool): If True, returns a model pretrained on IMAGENET. Default: False.\n        num_classes (int): The numbers of classes. Default: 1000.\n        in_channels (int): The input channels. Default: 1000.\n\n    Inputs:\n        - **x** (Tensor) - Tensor of shape :math:`(N, C_{in}, H_{in}, W_{in})`.\n\n    Outputs:\n        Tensor of shape :math:`(N, CLASSES_{out})`.\n    \"\"\"\n    return _efficientnet(\"efficientnet_b5\", 1.6, 2.2, 0.4, in_channels, num_classes, pretrained, **kwargs)\n</code></pre>"},{"location":"reference/models/#mindcv.models.efficientnet.efficientnet_b6","title":"<code>mindcv.models.efficientnet.efficientnet_b6(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Constructs a EfficientNet B6 architecture from <code>EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks &lt;https://arxiv.org/abs/1905.11946&gt;</code>_.</p> PARAMETER DESCRIPTION <code>pretrained</code> <p>If True, returns a model pretrained on IMAGENET. Default: False.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>num_classes</code> <p>The numbers of classes. Default: 1000.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1000</code> </p> <code>in_channels</code> <p>The input channels. Default: 1000.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> Inputs <ul> <li>x (Tensor) - Tensor of shape :math:<code>(N, C_{in}, H_{in}, W_{in})</code>.</li> </ul> Outputs <p>Tensor of shape :math:<code>(N, CLASSES_{out})</code>.</p> Source code in <code>mindcv\\models\\efficientnet.py</code> <pre><code>@register_model\ndef efficientnet_b6(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; EfficientNet:\n    \"\"\"\n    Constructs a EfficientNet B6 architecture from\n    `EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks &lt;https://arxiv.org/abs/1905.11946&gt;`_.\n\n    Args:\n        pretrained (bool): If True, returns a model pretrained on IMAGENET. Default: False.\n        num_classes (int): The numbers of classes. Default: 1000.\n        in_channels (int): The input channels. Default: 1000.\n\n    Inputs:\n        - **x** (Tensor) - Tensor of shape :math:`(N, C_{in}, H_{in}, W_{in})`.\n\n    Outputs:\n        Tensor of shape :math:`(N, CLASSES_{out})`.\n    \"\"\"\n    return _efficientnet(\"efficientnet_b6\", 1.8, 2.6, 0.5, in_channels, num_classes, pretrained, **kwargs)\n</code></pre>"},{"location":"reference/models/#mindcv.models.efficientnet.efficientnet_b7","title":"<code>mindcv.models.efficientnet.efficientnet_b7(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Constructs a EfficientNet B7 architecture from <code>EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks &lt;https://arxiv.org/abs/1905.11946&gt;</code>_.</p> PARAMETER DESCRIPTION <code>pretrained</code> <p>If True, returns a model pretrained on IMAGENET. Default: False.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>num_classes</code> <p>The numbers of classes. Default: 1000.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1000</code> </p> <code>in_channels</code> <p>The input channels. Default: 1000.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> Inputs <ul> <li>x (Tensor) - Tensor of shape :math:<code>(N, C_{in}, H_{in}, W_{in})</code>.</li> </ul> Outputs <p>Tensor of shape :math:<code>(N, CLASSES_{out})</code>.</p> Source code in <code>mindcv\\models\\efficientnet.py</code> <pre><code>@register_model\ndef efficientnet_b7(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; EfficientNet:\n    \"\"\"\n    Constructs a EfficientNet B7 architecture from\n    `EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks &lt;https://arxiv.org/abs/1905.11946&gt;`_.\n\n    Args:\n        pretrained (bool): If True, returns a model pretrained on IMAGENET. Default: False.\n        num_classes (int): The numbers of classes. Default: 1000.\n        in_channels (int): The input channels. Default: 1000.\n\n    Inputs:\n        - **x** (Tensor) - Tensor of shape :math:`(N, C_{in}, H_{in}, W_{in})`.\n\n    Outputs:\n        Tensor of shape :math:`(N, CLASSES_{out})`.\n    \"\"\"\n    return _efficientnet(\"efficientnet_b7\", 2.0, 3.1, 0.5, in_channels, num_classes, pretrained, **kwargs)\n</code></pre>"},{"location":"reference/models/#mindcv.models.efficientnet.efficientnet_v2_l","title":"<code>mindcv.models.efficientnet.efficientnet_v2_l(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Constructs a EfficientNet B4 architecture from <code>EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks &lt;https://arxiv.org/abs/1905.11946&gt;</code>_.</p> PARAMETER DESCRIPTION <code>pretrained</code> <p>If True, returns a model pretrained on IMAGENET. Default: False.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>num_classes</code> <p>The numbers of classes. Default: 1000.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1000</code> </p> <code>in_channels</code> <p>The input channels. Default: 1000.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> Inputs <ul> <li>x (Tensor) - Tensor of shape :math:<code>(N, C_{in}, H_{in}, W_{in})</code>.</li> </ul> Outputs <p>Tensor of shape :math:<code>(N, CLASSES_{out})</code>.</p> Source code in <code>mindcv\\models\\efficientnet.py</code> <pre><code>@register_model\ndef efficientnet_v2_l(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; EfficientNet:\n    \"\"\"\n    Constructs a EfficientNet B4 architecture from\n    `EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks &lt;https://arxiv.org/abs/1905.11946&gt;`_.\n\n    Args:\n        pretrained (bool): If True, returns a model pretrained on IMAGENET. Default: False.\n        num_classes (int): The numbers of classes. Default: 1000.\n        in_channels (int): The input channels. Default: 1000.\n\n    Inputs:\n        - **x** (Tensor) - Tensor of shape :math:`(N, C_{in}, H_{in}, W_{in})`.\n\n    Outputs:\n        Tensor of shape :math:`(N, CLASSES_{out})`.\n    \"\"\"\n    return _efficientnet(\"efficientnet_v2_l\", 1.0, 1.0, 0.2, in_channels, num_classes, pretrained, **kwargs)\n</code></pre>"},{"location":"reference/models/#mindcv.models.efficientnet.efficientnet_v2_m","title":"<code>mindcv.models.efficientnet.efficientnet_v2_m(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Constructs a EfficientNet B4 architecture from <code>EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks &lt;https://arxiv.org/abs/1905.11946&gt;</code>_.</p> PARAMETER DESCRIPTION <code>pretrained</code> <p>If True, returns a model pretrained on IMAGENET. Default: False.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>num_classes</code> <p>The numbers of classes. Default: 1000.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1000</code> </p> <code>in_channels</code> <p>The input channels. Default: 1000.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> Inputs <ul> <li>x (Tensor) - Tensor of shape :math:<code>(N, C_{in}, H_{in}, W_{in})</code>.</li> </ul> Outputs <p>Tensor of shape :math:<code>(N, CLASSES_{out})</code>.</p> Source code in <code>mindcv\\models\\efficientnet.py</code> <pre><code>@register_model\ndef efficientnet_v2_m(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; EfficientNet:\n    \"\"\"\n    Constructs a EfficientNet B4 architecture from\n    `EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks &lt;https://arxiv.org/abs/1905.11946&gt;`_.\n\n    Args:\n        pretrained (bool): If True, returns a model pretrained on IMAGENET. Default: False.\n        num_classes (int): The numbers of classes. Default: 1000.\n        in_channels (int): The input channels. Default: 1000.\n\n    Inputs:\n        - **x** (Tensor) - Tensor of shape :math:`(N, C_{in}, H_{in}, W_{in})`.\n\n    Outputs:\n        Tensor of shape :math:`(N, CLASSES_{out})`.\n    \"\"\"\n    return _efficientnet(\"efficientnet_v2_m\", 1.0, 1.0, 0.2, in_channels, num_classes, pretrained, **kwargs)\n</code></pre>"},{"location":"reference/models/#mindcv.models.efficientnet.efficientnet_v2_s","title":"<code>mindcv.models.efficientnet.efficientnet_v2_s(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Constructs a EfficientNet B4 architecture from <code>EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks &lt;https://arxiv.org/abs/1905.11946&gt;</code>_.</p> PARAMETER DESCRIPTION <code>pretrained</code> <p>If True, returns a model pretrained on IMAGENET. Default: False.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>num_classes</code> <p>The numbers of classes. Default: 1000.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1000</code> </p> <code>in_channels</code> <p>The input channels. Default: 1000.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> Inputs <ul> <li>x (Tensor) - Tensor of shape :math:<code>(N, C_{in}, H_{in}, W_{in})</code>.</li> </ul> Outputs <p>Tensor of shape :math:<code>(N, CLASSES_{out})</code>.</p> Source code in <code>mindcv\\models\\efficientnet.py</code> <pre><code>@register_model\ndef efficientnet_v2_s(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; EfficientNet:\n    \"\"\"\n    Constructs a EfficientNet B4 architecture from\n    `EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks &lt;https://arxiv.org/abs/1905.11946&gt;`_.\n\n    Args:\n        pretrained (bool): If True, returns a model pretrained on IMAGENET. Default: False.\n        num_classes (int): The numbers of classes. Default: 1000.\n        in_channels (int): The input channels. Default: 1000.\n\n    Inputs:\n        - **x** (Tensor) - Tensor of shape :math:`(N, C_{in}, H_{in}, W_{in})`.\n\n    Outputs:\n        Tensor of shape :math:`(N, CLASSES_{out})`.\n    \"\"\"\n    return _efficientnet(\"efficientnet_v2_s\", 1.0, 1.0, 0.2, in_channels, num_classes, pretrained, **kwargs)\n</code></pre>"},{"location":"reference/models/#mindcv.models.efficientnet.efficientnet_v2_xl","title":"<code>mindcv.models.efficientnet.efficientnet_v2_xl(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Constructs a EfficientNet B4 architecture from <code>EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks &lt;https://arxiv.org/abs/1905.11946&gt;</code>_.</p> PARAMETER DESCRIPTION <code>pretrained</code> <p>If True, returns a model pretrained on IMAGENET. Default: False.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>num_classes</code> <p>The numbers of classes. Default: 1000.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1000</code> </p> <code>in_channels</code> <p>The input channels. Default: 1000.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> Inputs <ul> <li>x (Tensor) - Tensor of shape :math:<code>(N, C_{in}, H_{in}, W_{in})</code>.</li> </ul> Outputs <p>Tensor of shape :math:<code>(N, CLASSES_{out})</code>.</p> Source code in <code>mindcv\\models\\efficientnet.py</code> <pre><code>@register_model\ndef efficientnet_v2_xl(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; EfficientNet:\n    \"\"\"\n    Constructs a EfficientNet B4 architecture from\n    `EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks &lt;https://arxiv.org/abs/1905.11946&gt;`_.\n\n    Args:\n        pretrained (bool): If True, returns a model pretrained on IMAGENET. Default: False.\n        num_classes (int): The numbers of classes. Default: 1000.\n        in_channels (int): The input channels. Default: 1000.\n\n    Inputs:\n        - **x** (Tensor) - Tensor of shape :math:`(N, C_{in}, H_{in}, W_{in})`.\n\n    Outputs:\n        Tensor of shape :math:`(N, CLASSES_{out})`.\n    \"\"\"\n    return _efficientnet(\"efficientnet_v2_xl\", 1.0, 1.0, 0.2, in_channels, num_classes, pretrained, **kwargs)\n</code></pre>"},{"location":"reference/models/#features","title":"features","text":""},{"location":"reference/models/#mindcv.models.features","title":"<code>mindcv.models.features</code>","text":""},{"location":"reference/models/#mindcv.models.features.FeatureExtractWrapper","title":"<code>mindcv.models.features.FeatureExtractWrapper</code>","text":"<p>               Bases: <code>Cell</code></p> <p>A wrapper of the original model, aims to perform the feature extraction at each stride. Basically, it performs 3 steps: 1. extract the return node name from the network's property <code>feature_info</code>; 2. partially flatten the network architecture if network's attribute <code>flatten_sequential</code> is True; 3. rebuild the forward steps and output the features based on the return node name.</p> <p>It also provide a property <code>out_channels</code> in the wrapped model, return the number of features at each output layer. This propery is usually used for the downstream tasks, which requires feature infomation at network build stage.</p> <p>It should be note that to apply this wrapper, there is a strong assumption that each of the outmost cell are registered in the same order as they are used. And there should be no reuse of each cell, even for the <code>ReLU</code> cell. Otherwise, the returned result may not be correct.</p> <p>And it should be also note that it basically rebuild the model. So the default checkpoint parameter cannot be loaded correctly once that model is wrapped. To use the pretrained weight, please load the weight first and then use this wrapper to rebuild the model.</p> PARAMETER DESCRIPTION <code>net</code> <p>The model need to be wrapped.</p> <p> TYPE: <code>Cell</code> </p> <code>out_indices</code> <p>The indicies of the output features. Default: [0, 1, 2, 3, 4]</p> <p> TYPE: <code>list[int]</code> DEFAULT: <code>[0, 1, 2, 3, 4]</code> </p> Source code in <code>mindcv\\models\\features.py</code> <pre><code>class FeatureExtractWrapper(nn.Cell):\n    \"\"\"A wrapper of the original model, aims to perform the feature extraction at each stride.\n    Basically, it performs 3 steps: 1. extract the return node name from the network's property\n    `feature_info`; 2. partially flatten the network architecture if network's attribute `flatten_sequential`\n    is True; 3. rebuild the forward steps and output the features based on the return node name.\n\n    It also provide a property `out_channels` in the wrapped model, return the number of features at each output\n    layer. This propery is usually used for the downstream tasks, which requires feature infomation at network\n    build stage.\n\n    It should be note that to apply this wrapper, there is a strong assumption that each of the outmost cell\n    are registered in the same order as they are used. And there should be no reuse of each cell, even for the `ReLU`\n    cell. Otherwise, the returned result may not be correct.\n\n    And it should be also note that it basically rebuild the model. So the default checkpoint parameter cannot be loaded\n    correctly once that model is wrapped. To use the pretrained weight, please load the weight first and then use this\n    wrapper to rebuild the model.\n\n    Args:\n        net (nn.Cell): The model need to be wrapped.\n        out_indices (list[int]): The indicies of the output features. Default: [0, 1, 2, 3, 4]\n    \"\"\"\n\n    def __init__(self, net: nn.Cell, out_indices: List[int] = [0, 1, 2, 3, 4]) -&gt; None:\n        super().__init__(auto_prefix=False)\n\n        feature_info = self._get_feature_info(net)\n        self.is_rewritten = getattr(net, \"is_rewritten\", False)\n        flatten_sequetial = getattr(net, \"flatten_sequential\", False)\n        return_layers = _get_return_layers(feature_info, out_indices)\n        self.return_index = list()\n\n        if not self.is_rewritten:\n            cells = _cell_list(net, flatten_sequential=flatten_sequetial)\n            self.net, updated_return_layers = self._create_net(cells, return_layers)\n\n            # calculate the return index\n            for i, name in enumerate(self.net.name_cells().keys()):\n                if name in updated_return_layers:\n                    self.return_index.append(i)\n        else:\n            self.net = net\n            self.return_index = out_indices\n\n        # calculate the out_channels\n        self._out_channels = list()\n        for i in return_layers.values():\n            self._out_channels.append(feature_info[i][\"chs\"])\n\n    @property\n    def out_channels(self):\n        \"\"\"The output channels of the model, filtered by the out_indices.\n        \"\"\"\n        return self._out_channels\n\n    def construct(self, x: Tensor) -&gt; List[Tensor]:\n        return self._collect(x)\n\n    def _get_feature_info(self, net: nn.Cell) -&gt; Dict[str, Any]:\n        try:\n            feature_info = getattr(net, \"feature_info\")\n        except AttributeError:\n            raise\n        return feature_info\n\n    def _create_net(\n        self, cells: Iterable[Tuple[str, str, nn.Cell]], return_layers: Dict[str, int]\n    ) -&gt; Tuple[nn.SequentialCell, Dict[str, int]]:\n        layers = OrderedDict()\n        updated_return_layers = dict()\n        remaining = set(return_layers.keys())\n        for new_name, old_name, module in cells:\n            layers[new_name] = module\n            if old_name in remaining:\n                updated_return_layers[new_name] = return_layers[old_name]\n                remaining.remove(old_name)\n            if not remaining:\n                break\n\n        net = nn.SequentialCell(layers)\n        return net, updated_return_layers\n\n    def _collect(self, x: Tensor) -&gt; List[Tensor]:\n        out = list()\n\n        if self.is_rewritten:\n            xs = self.net(x)\n\n            for i, x in enumerate(xs):\n                if i in self.return_index:\n                    out.append(x)\n        else:\n            for i, cell in enumerate(self.net.cell_list):\n                x = cell(x)\n                if i in self.return_index:\n                    out.append(x)\n\n        return out\n</code></pre>"},{"location":"reference/models/#mindcv.models.features.FeatureExtractWrapper.out_channels","title":"<code>mindcv.models.features.FeatureExtractWrapper.out_channels</code>  <code>property</code>","text":"<p>The output channels of the model, filtered by the out_indices.</p>"},{"location":"reference/models/#ghostnet","title":"ghostnet","text":""},{"location":"reference/models/#mindcv.models.ghostnet","title":"<code>mindcv.models.ghostnet</code>","text":"<p>MindSpore implementation of <code>GhostNet</code>. Refer to GhostNet: More Features from Cheap Operations.</p>"},{"location":"reference/models/#mindcv.models.ghostnet.GhostNet","title":"<code>mindcv.models.ghostnet.GhostNet</code>","text":"<p>               Bases: <code>Cell</code></p> <p>GhostNet model class, based on <code>\"GhostNet: More Features from Cheap Operations \" &lt;https://arxiv.org/abs/1911.11907&gt;</code>_. Args:     num_classes: number of classification classes. Default: 1000.     width: base width of hidden channel in blocks. Default: 1.0.     in_channels: number of input channels. Default: 3.     drop_rate: the probability of the features before classification. Default: 0.2.</p> Source code in <code>mindcv\\models\\ghostnet.py</code> <pre><code>class GhostNet(nn.Cell):\n    r\"\"\"GhostNet model class, based on\n    `\"GhostNet: More Features from Cheap Operations \" &lt;https://arxiv.org/abs/1911.11907&gt;`_.\n    Args:\n        num_classes: number of classification classes. Default: 1000.\n        width: base width of hidden channel in blocks. Default: 1.0.\n        in_channels: number of input channels. Default: 3.\n        drop_rate: the probability of the features before classification. Default: 0.2.\n    \"\"\"\n\n    def __init__(\n        self,\n        num_classes: int = 1000,\n        width: float = 1.0,\n        in_channels: int = 3,\n        drop_rate: float = 0.2,\n    ) -&gt; None:\n        super().__init__()\n        # setting of inverted residual blocks\n        self.num_classes = num_classes\n        self.drop_rate = drop_rate\n        self.cfgs = [\n            # k, t, c, SE, s\n            # stage1\n            [[3, 16, 16, 0, 1]],\n            # stage2\n            [[3, 48, 24, 0, 2]],\n            [[3, 72, 24, 0, 1]],\n            # stage3\n            [[5, 72, 40, 0.25, 2]],\n            [[5, 120, 40, 0.25, 1]],\n            # stage4\n            [[3, 240, 80, 0, 2]],\n            [[3, 200, 80, 0, 1],\n             [3, 184, 80, 0, 1],\n             [3, 184, 80, 0, 1],\n             [3, 480, 112, 0.25, 1],\n             [3, 672, 112, 0.25, 1]\n             ],\n            # stage5\n            [[5, 672, 160, 0.25, 2]],\n            [[5, 960, 160, 0, 1],\n             [5, 960, 160, 0.25, 1],\n             [5, 960, 160, 0, 1],\n             [5, 960, 160, 0.25, 1]\n             ]\n        ]\n\n        # building first layer\n        stem_chs = make_divisible(16 * width, 4)\n        self.conv_stem = nn.Conv2d(in_channels, stem_chs, 3, 2, pad_mode=\"pad\", padding=1, has_bias=False)\n        self.bn1 = nn.BatchNorm2d(stem_chs)\n        self.act1 = nn.ReLU()\n        prev_chs = stem_chs\n\n        # building inverted residual blocks\n        stages = []\n        for cfg in self.cfgs:\n            layers = []\n            for k, exp_size, c, se_ratio, s in cfg:\n                out_chs = make_divisible(c * width, 4)\n                mid_chs = make_divisible(exp_size * width, 4)\n                layers.append(GhostBottleneck(prev_chs, mid_chs, out_chs, k, s, se_ratio=se_ratio))\n                prev_chs = out_chs\n            stages.append(nn.SequentialCell(layers))\n\n        out_chs = make_divisible(exp_size * width, 4)\n        stages.append(ConvBnAct(prev_chs, out_chs, 1))\n        prev_chs = out_chs\n\n        self.blocks = nn.SequentialCell(stages)\n\n        # building last several layers\n        self.num_features = out_chs = 1280\n        self.global_pool = GlobalAvgPooling(keep_dims=True)\n        self.conv_head = nn.Conv2d(prev_chs, out_chs, 1, 1, pad_mode=\"pad\", padding=0, has_bias=True)\n        self.act2 = nn.ReLU()\n        self.flatten = nn.Flatten()\n        if self.drop_rate &gt; 0.0:\n            self.dropout = Dropout(p=drop_rate)\n        self.classifier = nn.Dense(out_chs, num_classes)\n        self._initialize_weights()\n\n    def _initialize_weights(self) -&gt; None:\n        \"\"\"Initialize weights for cells.\"\"\"\n        for _, cell in self.cells_and_names():\n            if isinstance(cell, nn.Conv2d):\n                cell.weight.set_data(init.initializer(init.HeUniform(), cell.weight.shape, cell.weight.dtype))\n                if cell.bias is not None:\n                    cell.bias.set_data(init.initializer(\"zeros\", cell.bias.shape, cell.bias.dtype))\n            elif isinstance(cell, nn.BatchNorm2d):\n                cell.gamma.set_data(init.initializer(\"ones\", cell.gamma.shape, cell.gamma.dtype))\n                cell.beta.set_data(init.initializer(\"zeros\", cell.beta.shape, cell.beta.dtype))\n            elif isinstance(cell, nn.Dense):\n                cell.weight.set_data(init.initializer(init.HeUniform(), cell.weight.shape, cell.weight.dtype))\n                if cell.bias is not None:\n                    cell.bias.set_data(init.initializer(\"zeros\", cell.bias.shape, cell.bias.dtype))\n\n    def forward_features(self, x: Tensor) -&gt; Tensor:\n        x = self.conv_stem(x)\n        x = self.bn1(x)\n        x = self.act1(x)\n        x = self.blocks(x)\n        return x\n\n    def forward_head(self, x: Tensor) -&gt; Tensor:\n        x = self.global_pool(x)\n        x = self.conv_head(x)\n        x = self.act2(x)\n        x = self.flatten(x)\n        if self.drop_rate &gt; 0.0:\n            x = self.dropout(x)\n        x = self.classifier(x)\n        return x\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n</code></pre>"},{"location":"reference/models/#mindcv.models.ghostnet.HardSigmoid","title":"<code>mindcv.models.ghostnet.HardSigmoid</code>","text":"<p>               Bases: <code>Cell</code></p> <p>Implementation for (relu6 + 3) / 6</p> Source code in <code>mindcv\\models\\ghostnet.py</code> <pre><code>class HardSigmoid(nn.Cell):\n    \"\"\"Implementation for (relu6 + 3) / 6\"\"\"\n\n    def __init__(self) -&gt; None:\n        super().__init__()\n        self.relu6 = nn.ReLU6()\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        return self.relu6(x + 3.0) / 6.0\n</code></pre>"},{"location":"reference/models/#mindcv.models.ghostnet.ghostnet_050","title":"<code>mindcv.models.ghostnet.ghostnet_050(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>GhostNet-0.5x</p> Source code in <code>mindcv\\models\\ghostnet.py</code> <pre><code>@register_model\ndef ghostnet_050(pretrained: bool = False, num_classes: int = 1000, in_channels: int = 3, **kwargs):\n    \"\"\" GhostNet-0.5x \"\"\"\n    default_cfg = default_cfgs[\"ghostnet_050\"]\n    model = GhostNet(width=0.5, in_channels=in_channels, num_classes=num_classes, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre>"},{"location":"reference/models/#mindcv.models.ghostnet.ghostnet_100","title":"<code>mindcv.models.ghostnet.ghostnet_100(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>GhostNet-1.0x</p> Source code in <code>mindcv\\models\\ghostnet.py</code> <pre><code>@register_model\ndef ghostnet_100(pretrained: bool = False, num_classes: int = 1000, in_channels: int = 3, **kwargs):\n    \"\"\" GhostNet-1.0x \"\"\"\n    default_cfg = default_cfgs[\"ghostnet_100\"]\n    model = GhostNet(width=1.0, in_channels=in_channels, num_classes=num_classes, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre>"},{"location":"reference/models/#mindcv.models.ghostnet.ghostnet_130","title":"<code>mindcv.models.ghostnet.ghostnet_130(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>GhostNet-1.3x</p> Source code in <code>mindcv\\models\\ghostnet.py</code> <pre><code>@register_model\ndef ghostnet_130(pretrained: bool = False, num_classes: int = 1000, in_channels: int = 3, **kwargs):\n    \"\"\" GhostNet-1.3x \"\"\"\n    default_cfg = default_cfgs[\"ghostnet_130\"]\n    model = GhostNet(width=1.3, in_channels=in_channels, num_classes=num_classes, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre>"},{"location":"reference/models/#halonet","title":"halonet","text":""},{"location":"reference/models/#mindcv.models.halonet","title":"<code>mindcv.models.halonet</code>","text":"<p>MindSpore implementation of <code>HaloNet</code>. Refer to Scaling Local Self-Attention for Parameter Effificient Visual Backbones.</p>"},{"location":"reference/models/#mindcv.models.halonet.ActLayer","title":"<code>mindcv.models.halonet.ActLayer</code>","text":"<p>               Bases: <code>Cell</code></p> <p>Build Activation Layer according to act type</p> Source code in <code>mindcv\\models\\halonet.py</code> <pre><code>class ActLayer(nn.Cell):\n    \"\"\" Build Activation Layer according to act type\n    \"\"\"\n    def __init__(self, act):\n        super().__init__()\n        if act == 'silu':\n            self.act = nn.SiLU()\n        elif act == 'relu':\n            self.act = nn.ReLU()\n        else:\n            self.act = Identity()\n\n    def construct(self, inputs):\n        out = self.act(inputs)\n        return out\n</code></pre>"},{"location":"reference/models/#mindcv.models.halonet.BatchNormAct2d","title":"<code>mindcv.models.halonet.BatchNormAct2d</code>","text":"<p>               Bases: <code>Cell</code></p> <p>Build layer contain: bn-act</p> Source code in <code>mindcv\\models\\halonet.py</code> <pre><code>class BatchNormAct2d(nn.Cell):\n    \"\"\" Build layer contain: bn-act\n    \"\"\"\n    def __init__(self, chs, act=None):\n        super().__init__()\n        self.bn = nn.BatchNorm2d(chs)\n        self.act = ActLayer(act)\n\n    def construct(self, inputs):\n        out = self.bn(inputs)\n        out = self.act(out)\n        return out\n</code></pre>"},{"location":"reference/models/#mindcv.models.halonet.BottleneckBlock","title":"<code>mindcv.models.halonet.BottleneckBlock</code>","text":"<p>               Bases: <code>Cell</code></p> <p>ResNet-like Bottleneck Block - 1x1 - kxk - 1x1</p> Source code in <code>mindcv\\models\\halonet.py</code> <pre><code>class BottleneckBlock(nn.Cell):\n    \"\"\" ResNet-like Bottleneck Block - 1x1 - kxk - 1x1\n    \"\"\"\n    def __init__(self,\n                 in_chs,\n                 out_chs,\n                 stride,\n                 act,\n                 downsample=None,\n                 shortcut=None,\n                 ):\n        super().__init__()\n        self.stride = stride\n        mid_chs = out_chs//4\n        self.conv1_1x1 = ConvBnAct(\n                                   in_chs,\n                                   mid_chs,\n                                   kernel_size=1,\n                                   stride=1,\n                                   padding=0,\n                                   act=act)\n        self.conv2_kxk = ConvBnAct(\n                                   mid_chs,\n                                   mid_chs,\n                                   kernel_size=3,\n                                   stride=self.stride,\n                                   padding=1,\n                                   act=act)\n        self.conv2b_kxk = Identity()\n        self.conv3_1x1 = ConvBnAct(\n                                   mid_chs,\n                                   out_chs,\n                                   kernel_size=1,\n                                   stride=1,\n                                   padding=0)\n        self.attn = Identity()\n        self.attn_last = Identity()\n        self.shortcut = shortcut\n        if self.shortcut:\n            if downsample:\n                self.creat_shortcut = ConvBnAct(\n                                                in_chs,\n                                                out_chs,\n                                                kernel_size=1,\n                                                stride=self.stride,\n                                                padding=0)\n            else:\n                self.creat_shortcut = ConvBnAct(\n                                                in_chs,\n                                                out_chs,\n                                                kernel_size=1,\n                                                stride=1,\n                                                padding=0)\n        self.Identity = Identity()\n        self.act = ActLayer(act)\n\n    def construct(self, x):\n        h = x\n        x = self.conv1_1x1(x)\n        x = self.conv2_kxk(x)\n        x = self.conv2b_kxk(x)\n        x = self.attn(x)\n        x = self.conv3_1x1(x)\n        out = self.attn_last(x)\n        if self.shortcut:\n            h = self.creat_shortcut(h)\n        else:\n            h = self.Identity(h)\n        out = out + h\n        out = self.act(out)\n        return out\n</code></pre>"},{"location":"reference/models/#mindcv.models.halonet.ConvBnAct","title":"<code>mindcv.models.halonet.ConvBnAct</code>","text":"<p>               Bases: <code>Cell</code></p> <p>Build layer contain: conv - bn - act</p> Source code in <code>mindcv\\models\\halonet.py</code> <pre><code>class ConvBnAct(nn.Cell):\n    \"\"\" Build layer contain: conv - bn - act\n    \"\"\"\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 kernel_size,\n                 stride,\n                 padding,\n                 act=None,\n                 bias_init=False\n                 ):\n        super().__init__()\n        self.conv = nn.Conv2d(in_channels=in_channels,\n                              out_channels=out_channels,\n                              kernel_size=kernel_size,\n                              stride=stride,\n                              pad_mode=\"pad\",\n                              padding=padding,\n                              weight_init=HeUniform(),\n                              has_bias=bias_init\n                              )\n        self.bn = nn.BatchNorm2d(out_channels)\n        self.act = ActLayer(act)\n\n    def construct(self, inputs):\n        out = self.conv(inputs)\n        out = self.bn(out)\n        out = self.act(out)\n        return out\n</code></pre>"},{"location":"reference/models/#mindcv.models.halonet.HaloAttention","title":"<code>mindcv.models.halonet.HaloAttention</code>","text":"<p>               Bases: <code>Cell</code></p> <p>The internal dimensions of the attention module are controlled by the interaction of several arguments. the output dimension : dim_out the value(v) dimension :  dim_out//num_heads the query(q) and key(k) dimensions are determined by :      * num_heads*dim_head      * num_heads*(dim_out*attn_ratio//num_heads) the ratio of q and k relative to the output : attn_ratio</p> PARAMETER DESCRIPTION <code>dim</code> <p>input dimension to the module</p> <p> TYPE: <code>int</code> </p> <code>dim_out</code> <p>output dimension of the module, same as dim if not set</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>feat_size</code> <p>size of input feature_map (not used, for arg compat with bottle/lambda)</p> <p> TYPE: <code>Tuple[int, int]</code> DEFAULT: <code>None</code> </p> <code>stride</code> <p>output stride of the module, query downscaled if &gt; 1 (default: 1).</p> <p> DEFAULT: <code>1</code> </p> <code>num_heads</code> <p>parallel attention heads (default: 8).</p> <p> DEFAULT: <code>8</code> </p> <code>dim_head</code> <p>dimension of query and key heads, calculated from dim_out * attn_ratio // num_heads if not set</p> <p> DEFAULT: <code>None</code> </p> <code>block_size</code> <p>size of blocks. (default: 8)</p> <p> TYPE: <code>int</code> DEFAULT: <code>8</code> </p> <code>halo_size</code> <p>size of halo overlap. (default: 3)</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>qk_ratio</code> <p>ratio of q and k dimensions to output dimension when dim_head not set. (default: 1.0)</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>qkv_bias</code> <p>add bias to q, k, and v projections</p> <p> TYPE: <code>bool) </code> DEFAULT: <code>False</code> </p> <code>avg_down</code> <p>use average pool downsample instead of strided query blocks</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>scale_pos_embed</code> <p>scale the position embedding as well as Q @ K</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Source code in <code>mindcv\\models\\halonet.py</code> <pre><code>class HaloAttention(nn.Cell):\n    \"\"\"\n    The internal dimensions of the attention module are controlled by\n    the interaction of several arguments.\n    the output dimension : dim_out\n    the value(v) dimension :  dim_out//num_heads\n    the query(q) and key(k) dimensions are determined by :\n         * num_heads*dim_head\n         * num_heads*(dim_out*attn_ratio//num_heads)\n    the ratio of q and k relative to the output : attn_ratio\n\n    Args:\n        dim (int): input dimension to the module\n        dim_out (int): output dimension of the module, same as dim if not set\n        feat_size (Tuple[int, int]): size of input feature_map (not used, for arg compat with bottle/lambda)\n        stride: output stride of the module, query downscaled if &gt; 1 (default: 1).\n        num_heads: parallel attention heads (default: 8).\n        dim_head: dimension of query and key heads, calculated from dim_out * attn_ratio // num_heads if not set\n        block_size (int): size of blocks. (default: 8)\n        halo_size (int): size of halo overlap. (default: 3)\n        qk_ratio (float): ratio of q and k dimensions to output dimension when dim_head not set. (default: 1.0)\n        qkv_bias (bool) : add bias to q, k, and v projections\n        avg_down (bool): use average pool downsample instead of strided query blocks\n        scale_pos_embed (bool): scale the position embedding as well as Q @ K\n    \"\"\"\n    def __init__(self,\n                 dim,\n                 dim_out=None,\n                 feat_size=None,\n                 stride=1,\n                 num_heads=8,\n                 dim_head=None,\n                 block_size=8,\n                 halo_size=3,\n                 qk_ratio=1.0,  # ratio of q and k dimensions to output dimension when dim_head not set.\n                 qkv_bias=False,\n                 avg_down=False,  # use average pool downsample instead of strided query blocks\n                 scale_pos_embed=False):  # scale the position embedding as well as Q @ K\n        super().__init__()\n        dim_out = dim_out or dim\n        assert dim_out % num_heads == 0\n        self.stride = stride\n        self.num_heads = num_heads  # 8\n        self.dim_head_qk = make_divisible(dim_out * qk_ratio, divisor=8) // num_heads\n        self.dim_head_v = dim_out // self.num_heads  # dimension of head\n        self.dim_out_qk = num_heads * self.dim_head_qk\n        self.dim_out_v = num_heads * self.dim_head_v  # dimension of dim_out_v\n        self.scale = self.dim_head_qk ** -0.5\n        self.scale_pos_embed = scale_pos_embed\n        self.block_size = self.block_size_ds = block_size\n        self.halo_size = halo_size\n        self.win_size = block_size + halo_size * 2  # neighbourhood window size\n        self.block_stride = stride\n        use_avg_pool = False\n        if stride &gt; 1:\n            use_avg_pool = avg_down or block_size % stride != 0\n            self.block_stride = stride\n            self.block_size_ds = self.block_size // self.block_stride\n        self.q = nn.Conv2d(dim,\n                           self.dim_out_qk,\n                           1,\n                           stride=self.block_stride,\n                           has_bias=qkv_bias,\n                           weight_init=HeUniform())\n        self.kv = nn.Conv2d(dim, self.dim_out_qk + self.dim_out_v, 1, has_bias=qkv_bias)\n        self.pos_embed = RelPosEmb(\n            block_size=self.block_size_ds, win_size=self.win_size, dim_head=self.dim_head_qk)\n        self.pool = nn.AvgPool2d(2, 2) if use_avg_pool else Identity()\n        self.softmax_fn = ops.Softmax(-1)\n        self.pad_kv = ops.Pad(\n            paddings=((0, 0), (0, 0), (self.halo_size, self.halo_size), (self.halo_size, self.halo_size))\n            )\n        self.kv_unfold = nn.Unfold(\n            ksizes=[1, self.win_size, self.win_size, 1],\n            strides=[1, self.block_size, self.block_size, 1],\n            rates=[1, 1, 1, 1],\n            padding='valid'\n            )\n\n    def construct(self, x):\n        B, C, H, W = x.shape\n        assert H % self.block_size == 0 and W % self.block_size == 0, 'fmap dimensions must be divisible'\n        num_h_blocks = H//self.block_size\n        num_w_blocks = W//self.block_size\n        num_blocks = num_h_blocks * num_w_blocks\n        q = self.q(x)\n        # unfold\n        q = ops.reshape(q, (-1, self.dim_head_qk, num_h_blocks, self.block_size_ds, num_w_blocks, self.block_size_ds))\n        q = ops.transpose(q, (0, 1, 3, 5, 2, 4))\n        q = ops.reshape(q, (B*self.num_heads, self.dim_head_qk, -1, num_blocks))\n        q = ops.transpose(q, (0, 3, 2, 1))  # B*num_heads,num_blocks,block_size**2, dim_head\n        kv = self.kv(x)  # [bs,dim_out,H,W]\n        kv = self.pad_kv(kv)\n        kv = self.kv_unfold(kv)  # B, C_kh_kw, _, _\n        kv = ops.reshape(kv, (B * self.num_heads, self.dim_head_qk + self.dim_head_v, -1, num_blocks))\n        kv = ops.transpose(kv, (0, 3, 2, 1))  # [B * self.num_heads, num_blocks, -1, self.dim_head_qk + self.dim_head_v]\n        k = kv[..., :self.dim_head_qk]\n        v = kv[..., self.dim_head_qk:(self.dim_head_qk + self.dim_head_v)]\n        k = ops.transpose(k, (0, 1, 3, 2))  # [B * self.num_heads, num_blocks, self.dim_head_qk, -1]\n        if self.scale_pos_embed:\n            attn = (ops.matmul(q, k) + self.pos_embed(q)) * self.scale\n        else:\n            pos_embed_q = self.pos_embed(q)\n            part_1 = (ops.matmul(q, k)) * self.scale\n            attn = part_1 + pos_embed_q\n        # attn: B * num_heads, num_blocks, block_size ** 2, win_size ** 2\n        attn = self.softmax_fn(attn)\n        # attn = attn @ v\n        attn = ops.matmul(attn, v)  # attn: B * num_heads, num_blocks, block_size ** 2, dim_head_v\n        out = ops.transpose(attn, (0, 3, 2, 1))  # B * num_heads, dim_head_v, block_size ** 2, num_blocks\n        # fold\n        out = ops.reshape(out, (-1, self.block_size_ds, self.block_size_ds, num_h_blocks, num_w_blocks))\n        # -1, num_h_blocks, self.block_size_ds, num_w_blocks, self.block_size_ds\n        out = ops.transpose(out, (0, 3, 1, 4, 2))\n        out = ops.reshape(out, (B, self.dim_out_v, H // self.block_stride, W // self.block_stride))\n        # B, dim_out, H // block_stride, W // block_stride\n        out = self.pool(out)\n        return out\n</code></pre>"},{"location":"reference/models/#mindcv.models.halonet.HaloNet","title":"<code>mindcv.models.halonet.HaloNet</code>","text":"<p>               Bases: <code>Cell</code></p> <p>Define main structure of HaloNet: stem - blocks - head</p> Source code in <code>mindcv\\models\\halonet.py</code> <pre><code>class HaloNet(nn.Cell):\n    \"\"\" Define main structure of HaloNet: stem - blocks - head\n    \"\"\"\n    def __init__(self,\n                 depth_list,\n                 block_size,\n                 halo_size,\n                 stage1_block,\n                 stage2_block,\n                 stage3_block,\n                 stage4_block,\n                 chs_list,\n                 num_heads,\n                 num_classes,\n                 stride_list,\n                 hidden_chs,\n                 act,\n                 ):\n        super().__init__()\n        self.stem = Stem(act)\n        self.stage1 = HaloStage(\n                                block_types=stage1_block,\n                                block_size=block_size,\n                                halo_size=halo_size,\n                                depth=depth_list[0],\n                                channel=chs_list[0],\n                                out_channel=chs_list[1],\n                                stride=stride_list[0],\n                                num_head=num_heads[0],\n                                hidden_chs=hidden_chs,\n                                act=act,\n                                )\n        self.stage2 = HaloStage(\n                                block_types=stage2_block,\n                                block_size=block_size,\n                                halo_size=halo_size,\n                                depth=depth_list[1],\n                                channel=chs_list[1],\n                                out_channel=chs_list[2],\n                                stride=stride_list[1],\n                                num_head=num_heads[1],\n                                hidden_chs=hidden_chs,\n                                act=act,\n                                downsample=True)\n        self.stage3 = HaloStage(\n                                block_types=stage3_block,\n                                block_size=block_size,\n                                halo_size=halo_size,\n                                depth=depth_list[2],\n                                channel=chs_list[2],\n                                out_channel=chs_list[3],\n                                stride=stride_list[2],\n                                num_head=num_heads[2],\n                                hidden_chs=hidden_chs,\n                                act=act,\n                                downsample=True)\n        self.stage4 = HaloStage(\n                                block_types=stage4_block,\n                                block_size=block_size,\n                                halo_size=halo_size,\n                                depth=depth_list[3],\n                                channel=chs_list[3],\n                                out_channel=chs_list[4],\n                                stride=stride_list[3],\n                                num_head=num_heads[3],\n                                hidden_chs=hidden_chs,\n                                act=act,\n                                downsample=True)\n        self.classifier = nn.SequentialCell([\n            SelectAdaptivePool2d(flatten=True),\n            nn.Dense(chs_list[4], num_classes, TruncatedNormal(.02), bias_init='zeros'),\n            Identity()]\n        )\n        self._initialize_weights()\n\n    def _initialize_weights(self) -&gt; None:\n        \"\"\"Initialize weights for cells.\"\"\"\n        for _, cell in self.cells_and_names():\n            if isinstance(cell, nn.Conv2d):\n                cell.weight.set_data(init.initializer(init.HeUniform(), cell.weight.shape, cell.weight.dtype))\n                if cell.bias is not None:\n                    cell.bias.set_data(init.initializer(\"zeros\", cell.bias.shape, cell.bias.dtype))\n            elif isinstance(cell, nn.BatchNorm2d):\n                cell.gamma.set_data(init.initializer(\"ones\", cell.gamma.shape, cell.gamma.dtype))\n                cell.beta.set_data(init.initializer(\"zeros\", cell.beta.shape, cell.beta.dtype))\n            elif isinstance(cell, nn.Dense):\n                cell.weight.set_data(init.initializer(init.HeUniform(), cell.weight.shape, cell.weight.dtype))\n                if cell.bias is not None:\n                    cell.bias.set_data(init.initializer(\"zeros\", cell.bias.shape, cell.bias.dtype))\n\n    def construct(self, x):\n        x = self.stem(x)\n        out_stage1 = self.stage1(x)\n        out_stage2 = self.stage2(out_stage1)\n        out_stage3 = self.stage3(out_stage2)\n        out_stage4 = self.stage4(out_stage3)\n        out = self.classifier(out_stage4)\n        return out\n</code></pre>"},{"location":"reference/models/#mindcv.models.halonet.HaloStage","title":"<code>mindcv.models.halonet.HaloStage</code>","text":"<p>               Bases: <code>Cell</code></p> <p>Stage layers for HaloNet. Stage layers contains a number of Blocks.</p> Source code in <code>mindcv\\models\\halonet.py</code> <pre><code>class HaloStage(nn.Cell):\n    \"\"\" Stage layers for HaloNet. Stage layers contains a number of Blocks.\n    \"\"\"\n    def __init__(self,\n                 block_types,\n                 block_size,\n                 halo_size,\n                 depth,\n                 channel,\n                 out_channel,\n                 stride,\n                 num_head,\n                 act,\n                 hidden_chs=None,\n                 downsample=None,\n                 ):\n        super().__init__()\n        self.depth = depth\n        blocks = []\n        for idx in range(depth):\n            if idx == 0:\n                shortcut = True\n                in_channel = channel\n                if downsample is None:\n                    self.down = False\n                else:\n                    self.down = downsample\n                block_stride = stride\n                self.hidden = hidden_chs\n            else:\n                stride = 1\n                shortcut = False\n                in_channel = out_channel\n                self.down = False\n                block_stride = 1\n                self.hidden = None\n\n            block_type = block_types[idx]\n            if block_type == 'bottle':\n                blocks.append(\n                    BottleneckBlock(\n                        in_chs=in_channel,\n                        out_chs=out_channel,\n                        stride=block_stride,\n                        shortcut=shortcut,\n                        downsample=self.down,\n                        act=act,\n                    )\n                )\n            if block_type == 'attn':\n                if num_head &gt; 0:\n                    blocks.append(\n                        SelfAttnBlock(\n                            chs=out_channel,\n                            stride=stride,\n                            num_heads=num_head,\n                            block_size=block_size,\n                            halo_size=halo_size,\n                            hidden_chs=self.hidden,\n                            shortcut=shortcut,\n                            act=act,\n                        )\n                    )\n        self.blocks = nn.CellList(blocks)\n\n    def construct(self, x):\n        for stage in self.blocks:\n            x = stage(x)\n        return x\n</code></pre>"},{"location":"reference/models/#mindcv.models.halonet.RelPosEmb","title":"<code>mindcv.models.halonet.RelPosEmb</code>","text":"<p>               Bases: <code>Cell</code></p> <p>Relative Position Embedding</p> Source code in <code>mindcv\\models\\halonet.py</code> <pre><code>class RelPosEmb(nn.Cell):\n    \"\"\" Relative Position Embedding\n    \"\"\"\n    def __init__(\n            self,\n            block_size,\n            win_size,\n            dim_head,\n            ):\n        \"\"\"\n        :param block_size (int): block size\n        :param win_size (int): neighbourhood window size\n        :param dim_head (int): attention head dim\n        :param scale (float): scale factor (for init)\n        \"\"\"\n        super().__init__()\n        self.block_size = block_size\n        self.dim_head = dim_head\n        tensor1 = Tensor(shape=((2 * win_size - 1), dim_head), dtype=ms.float32, init=TruncatedNormal(sigma=.02))\n        self.rel_height = Parameter(tensor1)\n        tensor2 = Tensor(shape=((2 * win_size - 1), dim_head), dtype=ms.float32, init=TruncatedNormal(sigma=.02))\n        self.rel_width = Parameter(tensor2)\n\n    def construct(self, q):\n        B, BB, HW, _ = q.shape\n        # relative logits in width dimension\n        q = ops.reshape(q, (-1, self.block_size, self.block_size, self.dim_head))\n        rel_logits_w = rel_logits_1d(q, self.rel_width, permute_mask=(0, 1, 3, 2, 4))\n        # relative logits in height dimension\n        q = ops.transpose(q, (0, 2, 1, 3))\n        rel_logits_h = rel_logits_1d(q, self.rel_height, permute_mask=(0, 3, 1, 4, 2))\n        rel_logits = rel_logits_h+rel_logits_w\n        rel_logits = ops.reshape(rel_logits, (B, BB, HW, -1))\n        return rel_logits\n</code></pre>"},{"location":"reference/models/#mindcv.models.halonet.RelPosEmb.__init__","title":"<code>mindcv.models.halonet.RelPosEmb.__init__(block_size, win_size, dim_head)</code>","text":"<p>:param block_size (int): block size :param win_size (int): neighbourhood window size :param dim_head (int): attention head dim :param scale (float): scale factor (for init)</p> Source code in <code>mindcv\\models\\halonet.py</code> <pre><code>def __init__(\n        self,\n        block_size,\n        win_size,\n        dim_head,\n        ):\n    \"\"\"\n    :param block_size (int): block size\n    :param win_size (int): neighbourhood window size\n    :param dim_head (int): attention head dim\n    :param scale (float): scale factor (for init)\n    \"\"\"\n    super().__init__()\n    self.block_size = block_size\n    self.dim_head = dim_head\n    tensor1 = Tensor(shape=((2 * win_size - 1), dim_head), dtype=ms.float32, init=TruncatedNormal(sigma=.02))\n    self.rel_height = Parameter(tensor1)\n    tensor2 = Tensor(shape=((2 * win_size - 1), dim_head), dtype=ms.float32, init=TruncatedNormal(sigma=.02))\n    self.rel_width = Parameter(tensor2)\n</code></pre>"},{"location":"reference/models/#mindcv.models.halonet.SelectAdaptivePool2d","title":"<code>mindcv.models.halonet.SelectAdaptivePool2d</code>","text":"<p>               Bases: <code>Cell</code></p> <p>Selectable global pooling layer with dynamic input kernel size</p> Source code in <code>mindcv\\models\\halonet.py</code> <pre><code>class SelectAdaptivePool2d(nn.Cell):\n    \"\"\" Selectable global pooling layer with dynamic input kernel size\n    \"\"\"\n    def __init__(self, pool_type='avg', flatten=False):\n        super().__init__()\n        # convert other false values to empty string for consistent TS typing\n        self.pool_type = pool_type or ''\n        self.flatten = nn.Flatten() if flatten else Identity()\n        if pool_type == '':\n            self.pool = Identity()\n        elif pool_type == 'avg':\n            self.pool = ops.ReduceMean(keep_dims=True)\n        else:\n            assert False, 'Invalid pool type: %s' % pool_type\n\n    def construct(self, inputs):\n        out = self.pool(inputs, (2, 3))\n        out = self.flatten(out)\n        return out\n</code></pre>"},{"location":"reference/models/#mindcv.models.halonet.SelfAttnBlock","title":"<code>mindcv.models.halonet.SelfAttnBlock</code>","text":"<p>               Bases: <code>Cell</code></p> <p>ResNet-like Bottleneck Block - 1x1 -kxk - self attn -1x1</p> Source code in <code>mindcv\\models\\halonet.py</code> <pre><code>class SelfAttnBlock(nn.Cell):\n    \"\"\" ResNet-like Bottleneck Block - 1x1 -kxk - self attn -1x1\n    \"\"\"\n    def __init__(self,\n                 chs,\n                 num_heads,\n                 block_size,\n                 halo_size,\n                 act,\n                 stride=None,\n                 shortcut=None,\n                 hidden_chs=None,\n                 ):\n        super().__init__()\n        mid_chs = chs//4\n        if hidden_chs is None:\n            out_chs = chs\n        else:\n            out_chs = hidden_chs\n\n        if stride is None:\n            self.stride = 1\n        else:\n            self.stride = stride\n        self.conv1_1x1 = ConvBnAct(out_chs, mid_chs, kernel_size=1, stride=1, padding=0, act=act)\n        self.conv2_kxk = Identity()\n        self.conv3_1x1 = ConvBnAct(mid_chs, chs, kernel_size=1, stride=1, padding=0)\n        self.self_attn = HaloAttention(mid_chs,\n                                       dim_out=mid_chs,\n                                       block_size=block_size,\n                                       halo_size=halo_size,\n                                       num_heads=num_heads,\n                                       stride=self.stride)\n        self.post_attn = BatchNormAct2d(mid_chs, act=act)\n        self.shortcut = shortcut\n        if self.shortcut:\n            self.creat_shortcut = ConvBnAct(\n                                            out_chs,\n                                            chs,\n                                            kernel_size=1,\n                                            stride=self.stride,\n                                            padding=0)\n        self.Identity = Identity()\n        self.act = ActLayer(act=act)\n\n    def construct(self, x):\n        h = x\n        out = self.conv1_1x1(x)\n        out = self.self_attn(out)\n        out = self.post_attn(out)\n        out = self.conv3_1x1(out)\n        if self.shortcut:\n            h = self.creat_shortcut(h)\n        else:\n            h = self.Identity(h)\n        out = out + h\n        out = self.act(out)\n        return out\n</code></pre>"},{"location":"reference/models/#mindcv.models.halonet.halonet_50t","title":"<code>mindcv.models.halonet.halonet_50t(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get HaloNet model. Refer to the base class <code>models.HaloNet</code> for more details.</p> Source code in <code>mindcv\\models\\halonet.py</code> <pre><code>@register_model\ndef halonet_50t(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs):\n    \"\"\"Get HaloNet model.\n    Refer to the base class `models.HaloNet` for more details.\"\"\"\n    default_cfg = default_cfgs[\"halonet_50t\"]\n    model = HaloNet(\n        depth_list=[3, 4, 6, 3],\n        stage1_block=['bottle', 'bottle', 'bottle'],\n        stage2_block=['bottle', 'bottle', 'bottle', 'attn'],\n        stage3_block=['bottle', 'attn', 'bottle', 'attn', 'bottle', 'attn'],\n        stage4_block=['bottle', 'attn', 'bottle'],\n        chs_list=[64, 256, 512, 1024, 2048],\n        num_heads=[0, 4, 8, 8],\n        num_classes=num_classes,\n        stride_list=[1, 2, 2, 2],\n        block_size=8,\n        halo_size=3,\n        hidden_chs=None,\n        act='silu',\n        **kwargs\n    )\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n    return model\n</code></pre>"},{"location":"reference/models/#mindcv.models.halonet.rel_logits_1d","title":"<code>mindcv.models.halonet.rel_logits_1d(q, rel_k, permute_mask)</code>","text":"<p>Compute relative logits along one dimension :param q: [batch,H,W,dim] :param rel_k: [2*window-1,dim] :param permute_mask: permute output axis according to this</p> Source code in <code>mindcv\\models\\halonet.py</code> <pre><code>def rel_logits_1d(q, rel_k, permute_mask):\n    \"\"\" Compute relative logits along one dimension\n    :param q: [batch,H,W,dim]\n    :param rel_k: [2*window-1,dim]\n    :param permute_mask: permute output axis according to this\n    \"\"\"\n    B, H, W, _ = q.shape\n    rel_size = rel_k.shape[0]\n    win_size = (rel_size+1)//2\n    rel_k = ops.transpose(rel_k, (1, 0))\n    x = msnp.tensordot(q, rel_k, axes=1)\n    x = ops.reshape(x, (-1, W, rel_size))\n    # pad to shift from relative to absolute indexing\n    x_pad = ops.pad(x, paddings=((0, 0), (0, 0), (0, 1)))\n    x_pad = ops.flatten(x_pad)\n    x_pad = ops.expand_dims(x_pad, 1)\n    x_pad = ops.pad(x_pad, paddings=((0, 0), (0, 0), (0, rel_size - W)))\n    x_pad = ops.squeeze(x_pad, axis=())\n    # reshape adn slice out the padded elements\n    x_pad = ops.reshape(x_pad, (-1, W+1, rel_size))\n    x = x_pad[:, :W, win_size-1:]\n    # reshape and tile\n    x = ops.reshape(x, (B, H, 1, W, win_size))\n    x = ops.broadcast_to(x, (B, H, win_size, W, win_size))\n    x = ops.transpose(x, permute_mask)\n    return x\n</code></pre>"},{"location":"reference/models/#hrnet","title":"hrnet","text":""},{"location":"reference/models/#mindcv.models.hrnet","title":"<code>mindcv.models.hrnet</code>","text":"<p>MindSpore implementation of <code>HRNet</code>. Refer to Deep High-Resolution Representation Learning for Visual Recognition</p>"},{"location":"reference/models/#mindcv.models.hrnet.BasicBlock","title":"<code>mindcv.models.hrnet.BasicBlock</code>","text":"<p>               Bases: <code>Cell</code></p> <p>Basic block of HRNet</p> Source code in <code>mindcv\\models\\hrnet.py</code> <pre><code>class BasicBlock(nn.Cell):\n    \"\"\"Basic block of HRNet\"\"\"\n\n    expansion: int = 1\n\n    def __init__(\n        self,\n        in_channels: int,\n        channels: int,\n        stride: int = 1,\n        groups: int = 1,\n        base_width: int = 64,\n        norm: Optional[nn.Cell] = None,\n        down_sample: Optional[nn.Cell] = None,\n    ) -&gt; None:\n        super().__init__()\n        if norm is None:\n            norm = nn.BatchNorm2d\n        assert groups == 1, \"BasicBlock only supports groups=1\"\n        assert base_width == 64, \"BasicBlock only supports base_width=64\"\n\n        self.conv1 = nn.Conv2d(\n            in_channels,\n            channels,\n            kernel_size=3,\n            stride=stride,\n            padding=1,\n            pad_mode=\"pad\",\n        )\n        self.bn1 = norm(channels)\n        self.relu = nn.ReLU()\n        self.conv2 = nn.Conv2d(\n            channels, channels, kernel_size=3, stride=1, padding=1, pad_mode=\"pad\"\n        )\n        self.bn2 = norm(channels)\n        self.down_sample = down_sample\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.down_sample is not None:\n            identity = self.down_sample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n</code></pre>"},{"location":"reference/models/#mindcv.models.hrnet.Bottleneck","title":"<code>mindcv.models.hrnet.Bottleneck</code>","text":"<p>               Bases: <code>Cell</code></p> <p>Bottleneck block of HRNet</p> Source code in <code>mindcv\\models\\hrnet.py</code> <pre><code>class Bottleneck(nn.Cell):\n    \"\"\"Bottleneck block of HRNet\"\"\"\n\n    expansion: int = 4\n\n    def __init__(\n        self,\n        in_channels: int,\n        channels: int,\n        stride: int = 1,\n        groups: int = 1,\n        base_width: int = 64,\n        norm: Optional[nn.Cell] = None,\n        down_sample: Optional[nn.Cell] = None,\n    ) -&gt; None:\n        super().__init__()\n        if norm is None:\n            norm = nn.BatchNorm2d\n\n        width = int(channels * (base_width / 64.0)) * groups\n\n        self.conv1 = nn.Conv2d(in_channels, width, kernel_size=1, stride=1)\n        self.bn1 = norm(width)\n        self.conv2 = nn.Conv2d(\n            width,\n            width,\n            kernel_size=3,\n            stride=stride,\n            padding=1,\n            pad_mode=\"pad\",\n            group=groups,\n        )\n        self.bn2 = norm(width)\n        self.conv3 = nn.Conv2d(\n            width, channels * self.expansion, kernel_size=1, stride=1\n        )\n        self.bn3 = norm(channels * self.expansion)\n        self.relu = nn.ReLU()\n        self.down_sample = down_sample\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.down_sample is not None:\n            identity = self.down_sample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n</code></pre>"},{"location":"reference/models/#mindcv.models.hrnet.HRModule","title":"<code>mindcv.models.hrnet.HRModule</code>","text":"<p>               Bases: <code>Cell</code></p> <p>High-Resolution Module for HRNet. In this module, every branch has 4 BasicBlocks/Bottlenecks. Fusion/Exchange is in this module.</p> Source code in <code>mindcv\\models\\hrnet.py</code> <pre><code>class HRModule(nn.Cell):\n    \"\"\"High-Resolution Module for HRNet.\n    In this module, every branch has 4 BasicBlocks/Bottlenecks. Fusion/Exchange\n    is in this module.\n    \"\"\"\n\n    def __init__(\n        self,\n        num_branches: int,\n        block: Type[Union[BasicBlock, Bottleneck]],\n        num_blocks: List[int],\n        num_inchannels: List[int],\n        num_channels: List[int],\n        multi_scale_output: bool = True,\n    ) -&gt; None:\n        super().__init__()\n        self._check_branches(num_branches, num_blocks, num_inchannels, num_channels)\n\n        self.num_inchannels = num_inchannels\n        self.num_branches = num_branches\n\n        self.multi_scale_output = multi_scale_output\n\n        self.branches = self._make_branches(\n            num_branches, block, num_blocks, num_channels\n        )\n        self.fuse_layers = self._make_fuse_layers()\n        self.relu = nn.ReLU()\n\n    @staticmethod\n    def _check_branches(\n        num_branches: int,\n        num_blocks: List[int],\n        num_inchannels: List[int],\n        num_channels: List[int],\n    ) -&gt; None:\n        \"\"\"Check input to avoid ValueError.\"\"\"\n        if num_branches != len(num_blocks):\n            error_msg = f\"NUM_BRANCHES({num_branches})!= NUM_BLOCKS({len(num_blocks)})\"\n            raise ValueError(error_msg)\n\n        if num_branches != len(num_channels):\n            error_msg = (\n                f\"NUM_BRANCHES({num_branches})!= NUM_CHANNELS({len(num_channels)})\"\n            )\n            raise ValueError(error_msg)\n\n        if num_branches != len(num_inchannels):\n            error_msg = (\n                f\"NUM_BRANCHES({num_branches}) != NUM_INCHANNELS({len(num_inchannels)})\"\n            )\n            raise ValueError(error_msg)\n\n    def _make_one_branch(\n        self,\n        branch_index: int,\n        block: Type[Union[BasicBlock, Bottleneck]],\n        num_blocks: List[int],\n        num_channels: List[int],\n        stride: int = 1,\n    ) -&gt; nn.SequentialCell:\n        downsample = None\n        if stride != 1 or self.num_inchannels[branch_index] != num_channels[branch_index] * block.expansion:\n            downsample = nn.SequentialCell(\n                nn.Conv2d(\n                    self.num_inchannels[branch_index],\n                    num_channels[branch_index] * block.expansion,\n                    kernel_size=1,\n                    stride=stride,\n                ),\n                nn.BatchNorm2d(num_channels[branch_index] * block.expansion),\n            )\n\n        layers = []\n        layers.append(\n            block(\n                self.num_inchannels[branch_index],\n                num_channels[branch_index],\n                stride,\n                down_sample=downsample,\n            )\n        )\n        self.num_inchannels[branch_index] = num_channels[branch_index] * block.expansion\n        for _ in range(1, num_blocks[branch_index]):\n            layers.append(\n                block(self.num_inchannels[branch_index], num_channels[branch_index])\n            )\n\n        return nn.SequentialCell(layers)\n\n    def _make_branches(\n        self,\n        num_branches: int,\n        block: Type[Union[BasicBlock, Bottleneck]],\n        num_blocks: List[int],\n        num_channels: List[int],\n    ) -&gt; nn.CellList:\n        \"\"\"Make branches.\"\"\"\n        branches = []\n\n        for i in range(num_branches):\n            branches.append(self._make_one_branch(i, block, num_blocks, num_channels))\n\n        return nn.CellList(branches)\n\n    def _make_fuse_layers(self) -&gt; nn.CellList:\n        if self.num_branches == 1:\n            return None\n\n        num_branches = self.num_branches\n        num_inchannels = self.num_inchannels\n        fuse_layers = []\n        for i in range(num_branches if self.multi_scale_output else 1):\n            fuse_layer = []\n            for j in range(num_branches):\n                if j &gt; i:\n                    fuse_layer.append(\n                        nn.SequentialCell(\n                            nn.Conv2d(\n                                num_inchannels[j], num_inchannels[i], kernel_size=1\n                            ),\n                            nn.BatchNorm2d(num_inchannels[i]),\n                        )\n                    )\n                elif j == i:\n                    fuse_layer.append(IdentityCell())\n                else:\n                    conv3x3s = []\n                    for k in range(i - j):\n                        if k == i - j - 1:\n                            num_outchannels_conv3x3 = num_inchannels[i]\n                            conv3x3s.append(\n                                nn.SequentialCell(\n                                    nn.Conv2d(\n                                        num_inchannels[j],\n                                        num_outchannels_conv3x3,\n                                        kernel_size=3,\n                                        stride=2,\n                                        padding=1,\n                                        pad_mode=\"pad\",\n                                    ),\n                                    nn.BatchNorm2d(num_outchannels_conv3x3),\n                                )\n                            )\n                        else:\n                            num_outchannels_conv3x3 = num_inchannels[j]\n                            conv3x3s.append(\n                                nn.SequentialCell(\n                                    nn.Conv2d(\n                                        num_inchannels[j],\n                                        num_outchannels_conv3x3,\n                                        kernel_size=3,\n                                        stride=2,\n                                        padding=1,\n                                        pad_mode=\"pad\",\n                                    ),\n                                    nn.BatchNorm2d(num_outchannels_conv3x3),\n                                    nn.ReLU(),\n                                )\n                            )\n                    fuse_layer.append(nn.SequentialCell(conv3x3s))\n            fuse_layers.append(nn.CellList(fuse_layer))\n\n        return nn.CellList(fuse_layers)\n\n    def construct(self, x: List[Tensor]) -&gt; List[Tensor]:\n        if self.num_branches == 1:\n            return [self.branches[0](x[0])]\n\n        x2 = []\n        for i in range(self.num_branches):\n            x2.append(self.branches[i](x[i]))\n\n        x_fuse = []\n\n        for i in range(len(self.fuse_layers)):\n            y = x2[0] if i == 0 else self.fuse_layers[i][0](x2[0])\n            for j in range(1, self.num_branches):\n                if i == j:\n                    y = y + x2[j]\n                elif j &gt; i:\n                    _, _, height, width = x2[i].shape\n                    t = self.fuse_layers[i][j](x2[j])\n                    t = ops.cast(t, ms.float32)\n                    t = ops.ResizeNearestNeighbor((height, width))(t)\n                    t = ops.cast(t, ms.float16)\n                    y = y + t\n                else:\n                    y = y + self.fuse_layers[i][j](x2[j])\n            x_fuse.append(self.relu(y))\n\n        if not self.multi_scale_output:\n            x_fuse = x_fuse[0]\n\n        return x_fuse\n</code></pre>"},{"location":"reference/models/#mindcv.models.hrnet.HRNet","title":"<code>mindcv.models.hrnet.HRNet</code>","text":"<p>               Bases: <code>Cell</code></p> <p>HRNet Backbone, based on <code>\"Deep High-Resolution Representation Learning for Visual Recognition\" &lt;https://arxiv.org/abs/1908.07919&gt;</code>_.</p> PARAMETER DESCRIPTION <code>stage_cfg</code> <p>Configuration of the extra blocks. It accepts a dictionay storing the detail config of each block. which include <code>num_modules</code>, <code>num_branches</code>, <code>block</code>, <code>num_blocks</code>, <code>num_channels</code>. For detail example, please check the implementation of <code>hrnet_w32</code> and <code>hrnet_w48</code>.</p> <p> TYPE: <code>Dict[str, Dict[str, int]]</code> </p> <code>num_classes</code> <p>number of classification classes. Default: 1000.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1000</code> </p> <code>in_channels</code> <p>Number the channels of the input. Default: 3.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> Source code in <code>mindcv\\models\\hrnet.py</code> <pre><code>class HRNet(nn.Cell):\n    r\"\"\"HRNet Backbone, based on\n    `\"Deep High-Resolution Representation Learning for Visual Recognition\"\n    &lt;https://arxiv.org/abs/1908.07919&gt;`_.\n\n    Args:\n        stage_cfg: Configuration of the extra blocks. It accepts a dictionay\n            storing the detail config of each block. which include `num_modules`,\n            `num_branches`, `block`, `num_blocks`, `num_channels`. For detail example,\n            please check the implementation of `hrnet_w32` and `hrnet_w48`.\n        num_classes: number of classification classes. Default: 1000.\n        in_channels: Number the channels of the input. Default: 3.\n    \"\"\"\n\n    blocks_dict = {\"BASIC\": BasicBlock, \"BOTTLENECK\": Bottleneck}\n\n    def __init__(\n        self,\n        stage_cfg: Dict[str, Dict[str, int]],\n        num_classes: int = 1000,\n        in_channels: int = 3,\n    ) -&gt; None:\n        super().__init__()\n\n        self.stage_cfg = stage_cfg\n        # stem net\n        self.conv1 = nn.Conv2d(\n            in_channels, 64, kernel_size=3, stride=2, padding=1, pad_mode=\"pad\"\n        )\n        self.bn1 = nn.BatchNorm2d(64)\n        self.conv2 = nn.Conv2d(\n            64, 64, kernel_size=3, stride=2, padding=1, pad_mode=\"pad\"\n        )\n        self.bn2 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU()\n\n        # stage 1\n        self.stage1_cfg = self.stage_cfg[\"stage1\"]\n        num_channels = self.stage1_cfg[\"num_channels\"][0]\n        num_blocks = self.stage1_cfg[\"num_blocks\"][0]\n        block = self.blocks_dict[self.stage1_cfg[\"block\"]]\n        self.layer1 = self._make_layer(block, 64, num_channels, num_blocks)\n\n        # stage 2\n        self.stage2_cfg = self.stage_cfg[\"stage2\"]\n        num_channels = self.stage2_cfg[\"num_channels\"]\n        block = self.blocks_dict[self.stage2_cfg[\"block\"]]\n        num_channels = [\n            num_channels[i] * block.expansion for i in range(len(num_channels))\n        ]\n\n        self.transition1, self.transition1_flags = self._make_transition_layer(\n            [256], num_channels\n        )\n        self.stage2, pre_stage_channels = self._make_stage(\n            self.stage2_cfg, num_channels\n        )\n\n        # stage 3\n        self.stage3_cfg = self.stage_cfg[\"stage3\"]\n        num_channels = self.stage3_cfg[\"num_channels\"]\n        block = self.blocks_dict[self.stage3_cfg[\"block\"]]\n        num_channels = [\n            num_channels[i] * block.expansion for i in range(len(num_channels))\n        ]\n\n        self.transition2, self.transition2_flags = self._make_transition_layer(\n            pre_stage_channels, num_channels\n        )\n        self.stage3, pre_stage_channels = self._make_stage(\n            self.stage3_cfg, num_channels\n        )\n\n        # stage 4\n        self.stage4_cfg = self.stage_cfg[\"stage4\"]\n        num_channels = self.stage4_cfg[\"num_channels\"]\n        block = self.blocks_dict[self.stage4_cfg[\"block\"]]\n        num_channels = [\n            num_channels[i] * block.expansion for i in range(len(num_channels))\n        ]\n        self.transition3, self.transition3_flags = self._make_transition_layer(\n            pre_stage_channels, num_channels\n        )\n        self.stage4, pre_stage_channels = self._make_stage(\n            self.stage4_cfg, num_channels\n        )\n\n        # head\n        self.pool = GlobalAvgPooling()\n        self.incre_modules, self.downsample_modules, self.final_layer = self._make_head(\n            pre_stage_channels\n        )\n        self.classifier = nn.Dense(2048, num_classes)\n\n    def _make_head(self, pre_stage_channels: List[int]):\n        head_block = Bottleneck\n        head_channels = [32, 64, 128, 256]\n\n        # increase the #channesl on each resolution\n        # from C, 2C, 4C, 8C to 128, 256, 512, 1024\n        incre_modules = list()\n        for i, channels in enumerate(pre_stage_channels):\n            incre_module = self._make_layer(\n                head_block, channels, head_channels[i], 1, stride=1\n            )\n            incre_modules.append(incre_module)\n        incre_modules = nn.CellList(incre_modules)\n\n        # downsample modules\n        downsamp_modules = []\n        for i in range(len(pre_stage_channels) - 1):\n            in_channels = head_channels[i] * head_block.expansion\n            out_channels = head_channels[i + 1] * head_block.expansion\n\n            downsamp_module = nn.SequentialCell(\n                nn.Conv2d(\n                    in_channels=in_channels,\n                    out_channels=out_channels,\n                    kernel_size=3,\n                    stride=2,\n                    pad_mode=\"pad\",\n                    padding=1,\n                ),\n                nn.BatchNorm2d(out_channels),\n                nn.ReLU(),\n            )\n\n            downsamp_modules.append(downsamp_module)\n        downsamp_modules = nn.CellList(downsamp_modules)\n\n        final_layer = nn.SequentialCell(\n            nn.Conv2d(\n                in_channels=head_channels[3] * head_block.expansion,\n                out_channels=2048,\n                kernel_size=1,\n                stride=1,\n                padding=0,\n            ),\n            nn.BatchNorm2d(2048),\n            nn.ReLU(),\n        )\n\n        return incre_modules, downsamp_modules, final_layer\n\n    def _make_transition_layer(\n        self, num_channels_pre_layer: List[int], num_channels_cur_layer: List[int]\n    ) -&gt; Tuple[nn.CellList, List[bool]]:\n        num_branches_cur = len(num_channels_cur_layer)\n        num_branches_pre = len(num_channels_pre_layer)\n\n        transition_layers = []\n        transition_layers_flags = []\n        for i in range(num_branches_cur):\n            if i &lt; num_branches_pre:\n                if num_channels_cur_layer[i] != num_channels_pre_layer[i]:\n                    transition_layers.append(\n                        nn.SequentialCell(\n                            nn.Conv2d(\n                                num_channels_pre_layer[i],\n                                num_channels_cur_layer[i],\n                                kernel_size=3,\n                                padding=1,\n                                pad_mode=\"pad\",\n                            ),\n                            nn.BatchNorm2d(num_channels_cur_layer[i]),\n                            nn.ReLU(),\n                        )\n                    )\n                    transition_layers_flags.append(True)\n                else:\n                    transition_layers.append(IdentityCell())\n                    transition_layers_flags.append(False)\n            else:\n                conv3x3s = []\n                for j in range(i + 1 - num_branches_pre):\n                    inchannels = num_channels_pre_layer[-1]\n                    outchannels = (\n                        num_channels_cur_layer[i]\n                        if j == i - num_branches_pre\n                        else inchannels\n                    )\n                    conv3x3s.append(\n                        nn.SequentialCell(\n                            [\n                                nn.Conv2d(\n                                    inchannels,\n                                    outchannels,\n                                    kernel_size=3,\n                                    stride=2,\n                                    padding=1,\n                                    pad_mode=\"pad\",\n                                ),\n                                nn.BatchNorm2d(outchannels),\n                                nn.ReLU(),\n                            ]\n                        )\n                    )\n                transition_layers.append(nn.SequentialCell(conv3x3s))\n                transition_layers_flags.append(True)\n\n        return nn.CellList(transition_layers), transition_layers_flags\n\n    def _make_layer(\n        self,\n        block: Type[Union[BasicBlock, Bottleneck]],\n        in_channels: int,\n        out_channels: int,\n        blocks: int,\n        stride: int = 1,\n    ) -&gt; nn.SequentialCell:\n        downsample = None\n        if stride != 1 or in_channels != out_channels * block.expansion:\n            downsample = nn.SequentialCell(\n                nn.Conv2d(\n                    in_channels,\n                    out_channels * block.expansion,\n                    kernel_size=1,\n                    stride=stride,\n                ),\n                nn.BatchNorm2d(out_channels * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(in_channels, out_channels, stride, down_sample=downsample))\n        for _ in range(1, blocks):\n            layers.append(block(out_channels * block.expansion, out_channels))\n\n        return nn.SequentialCell(layers)\n\n    def _make_stage(\n        self,\n        layer_config: Dict[str, int],\n        num_inchannels: int,\n        multi_scale_output: bool = True,\n    ) -&gt; Tuple[nn.SequentialCell, List[int]]:\n        num_modules = layer_config[\"num_modules\"]\n        num_branches = layer_config[\"num_branches\"]\n        num_blocks = layer_config[\"num_blocks\"]\n        num_channels = layer_config[\"num_channels\"]\n        block = self.blocks_dict[layer_config[\"block\"]]\n\n        modules = []\n        for i in range(num_modules):\n            # multi_scale_output is only used last module\n            if not multi_scale_output and i == num_modules - 1:\n                reset_multi_scale_output = False\n            else:\n                reset_multi_scale_output = True\n\n            modules.append(\n                HRModule(\n                    num_branches,\n                    block,\n                    num_blocks,\n                    num_inchannels,\n                    num_channels,\n                    reset_multi_scale_output,\n                )\n            )\n            num_inchannels = modules[-1].num_inchannels\n\n        return nn.SequentialCell(modules), num_inchannels\n\n    def forward_features(self, x: Tensor) -&gt; List[Tensor]:\n        \"\"\"Perform the feature extraction.\n\n        Args:\n            x: Tensor\n\n        Returns:\n            Extracted feature\n        \"\"\"\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.conv2(x)\n        x = self.bn2(x)\n        x = self.relu(x)\n\n        # stage 1\n        x = self.layer1(x)\n\n        # stage 2\n        x_list = []\n        for i in range(self.stage2_cfg[\"num_branches\"]):\n            if self.transition1_flags[i]:\n                x_list.append(self.transition1[i](x))\n            else:\n                x_list.append(x)\n        y_list = self.stage2(x_list)\n\n        # stage 3\n        x_list = []\n        for i in range(self.stage3_cfg[\"num_branches\"]):\n            if self.transition2_flags[i]:\n                x_list.append(self.transition2[i](y_list[-1]))\n            else:\n                x_list.append(y_list[i])\n        y_list = self.stage3(x_list)\n\n        # stage 4\n        x_list = []\n        for i in range(self.stage4_cfg[\"num_branches\"]):\n            if self.transition3_flags[i]:\n                x_list.append(self.transition3[i](y_list[-1]))\n            else:\n                x_list.append(y_list[i])\n        y = self.stage4(x_list)\n\n        return y\n\n    def forward_head(self, x: List[Tensor]) -&gt; Tensor:\n        y = self.incre_modules[0](x[0])\n        for i in range(len(self.downsample_modules)):\n            y = self.incre_modules[i + 1](x[i + 1]) + self.downsample_modules[i](y)\n\n        y = self.final_layer(y)\n        y = self.pool(y)\n        y = self.classifier(y)\n        return y\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n</code></pre>"},{"location":"reference/models/#mindcv.models.hrnet.HRNet.forward_features","title":"<code>mindcv.models.hrnet.HRNet.forward_features(x)</code>","text":"<p>Perform the feature extraction.</p> PARAMETER DESCRIPTION <code>x</code> <p>Tensor</p> <p> TYPE: <code>Tensor</code> </p> RETURNS DESCRIPTION <code>List[Tensor]</code> <p>Extracted feature</p> Source code in <code>mindcv\\models\\hrnet.py</code> <pre><code>def forward_features(self, x: Tensor) -&gt; List[Tensor]:\n    \"\"\"Perform the feature extraction.\n\n    Args:\n        x: Tensor\n\n    Returns:\n        Extracted feature\n    \"\"\"\n    x = self.conv1(x)\n    x = self.bn1(x)\n    x = self.relu(x)\n    x = self.conv2(x)\n    x = self.bn2(x)\n    x = self.relu(x)\n\n    # stage 1\n    x = self.layer1(x)\n\n    # stage 2\n    x_list = []\n    for i in range(self.stage2_cfg[\"num_branches\"]):\n        if self.transition1_flags[i]:\n            x_list.append(self.transition1[i](x))\n        else:\n            x_list.append(x)\n    y_list = self.stage2(x_list)\n\n    # stage 3\n    x_list = []\n    for i in range(self.stage3_cfg[\"num_branches\"]):\n        if self.transition2_flags[i]:\n            x_list.append(self.transition2[i](y_list[-1]))\n        else:\n            x_list.append(y_list[i])\n    y_list = self.stage3(x_list)\n\n    # stage 4\n    x_list = []\n    for i in range(self.stage4_cfg[\"num_branches\"]):\n        if self.transition3_flags[i]:\n            x_list.append(self.transition3[i](y_list[-1]))\n        else:\n            x_list.append(y_list[i])\n    y = self.stage4(x_list)\n\n    return y\n</code></pre>"},{"location":"reference/models/#mindcv.models.hrnet.HRNetFeatures","title":"<code>mindcv.models.hrnet.HRNetFeatures</code>","text":"<p>               Bases: <code>HRNet</code></p> <p>The feature extraction version of HRNet</p> Source code in <code>mindcv\\models\\hrnet.py</code> <pre><code>class HRNetFeatures(HRNet):\n    \"\"\"\n    The feature extraction version of HRNet\n    \"\"\"\n    def __init__(self, **kwargs) -&gt; None:\n        super(HRNetFeatures, self).__init__(**kwargs)\n        head_channels = [32, 64, 128, 256]\n        curr_stride = 2\n        self.feature_info = [dict(chs=64, reduction=curr_stride, name=\"stem\")]\n\n        for i, c in enumerate(head_channels):\n            curr_stride *= 2\n            c = c * 4\n            self.feature_info += [dict(chs=c, reduction=curr_stride, name=f'stage{i + 1}')]\n\n        self.is_rewritten = True\n\n    def construct(self, x: Tensor) -&gt; List[Tensor]:\n        out = []\n\n        # stem\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        out.append(x)\n        x = self.conv2(x)\n        x = self.bn2(x)\n        x = self.relu(x)\n\n        # stage 1\n        x = self.layer1(x)\n\n        # stage 2\n        x_list = []\n        for i in range(self.stage2_cfg[\"num_branches\"]):\n            if self.transition1_flags[i]:\n                x_list.append(self.transition1[i](x))\n            else:\n                x_list.append(x)\n        y_list = self.stage2(x_list)\n\n        # stage 3\n        x_list = []\n        for i in range(self.stage3_cfg[\"num_branches\"]):\n            if self.transition2_flags[i]:\n                x_list.append(self.transition2[i](y_list[-1]))\n            else:\n                x_list.append(y_list[i])\n        y_list = self.stage3(x_list)\n\n        # stage 4\n        x_list = []\n        for i in range(self.stage4_cfg[\"num_branches\"]):\n            if self.transition3_flags[i]:\n                x_list.append(self.transition3[i](y_list[-1]))\n            else:\n                x_list.append(y_list[i])\n        y_list = self.stage4(x_list)\n\n        for f, incre in zip(y_list, self.incre_modules):\n            out.append(incre(f))\n\n        return out\n</code></pre>"},{"location":"reference/models/#mindcv.models.hrnet.IdentityCell","title":"<code>mindcv.models.hrnet.IdentityCell</code>","text":"<p>               Bases: <code>Cell</code></p> <p>Identity Cell</p> Source code in <code>mindcv\\models\\hrnet.py</code> <pre><code>class IdentityCell(nn.Cell):\n    \"\"\"Identity Cell\"\"\"\n\n    def __init__(self) -&gt; None:\n        super().__init__()\n\n    def construct(self, x: Any) -&gt; Any:\n        return x\n</code></pre>"},{"location":"reference/models/#mindcv.models.hrnet.hrnet_w32","title":"<code>mindcv.models.hrnet.hrnet_w32(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get HRNet with width=32 model. Refer to the base class <code>models.HRNet</code> for more details.</p> PARAMETER DESCRIPTION <code>pretrained</code> <p>Whether the model is pretrained. Default: False</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>num_classes</code> <p>number of classification classes. Default: 1000</p> <p> TYPE: <code>int</code> DEFAULT: <code>1000</code> </p> <code>in_channels</code> <p>Number of input channels. Default: 3</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> RETURNS DESCRIPTION <code>Union[HRNet, HRNetFeatures]</code> <p>HRNet model</p> Source code in <code>mindcv\\models\\hrnet.py</code> <pre><code>@register_model\ndef hrnet_w32(\n    pretrained: bool = False, num_classes: int = 1000, in_channels: int = 3, **kwargs\n) -&gt; Union[HRNet, HRNetFeatures]:\n    \"\"\"Get HRNet with width=32 model.\n    Refer to the base class `models.HRNet` for more details.\n\n    Args:\n        pretrained: Whether the model is pretrained. Default: False\n        num_classes: number of classification classes. Default: 1000\n        in_channels: Number of input channels. Default: 3\n\n    Returns:\n        HRNet model\n    \"\"\"\n    default_cfg = default_cfgs[\"hrnet_w32\"]\n    stage_cfg = dict(\n        stage1=dict(\n            num_modules=1,\n            num_branches=1,\n            block=\"BOTTLENECK\",\n            num_blocks=[4],\n            num_channels=[64],\n        ),\n        stage2=dict(\n            num_modules=1,\n            num_branches=2,\n            block=\"BASIC\",\n            num_blocks=[4, 4],\n            num_channels=[32, 64],\n        ),\n        stage3=dict(\n            num_modules=4,\n            num_branches=3,\n            block=\"BASIC\",\n            num_blocks=[4, 4, 4],\n            num_channels=[32, 64, 128],\n        ),\n        stage4=dict(\n            num_modules=3,\n            num_branches=4,\n            block=\"BASIC\",\n            num_blocks=[4, 4, 4, 4],\n            num_channels=[32, 64, 128, 256],\n        ),\n    )\n    model_args = dict(stage_cfg=stage_cfg, num_classes=num_classes, in_channels=in_channels, **kwargs)\n    return _create_hrnet(pretrained, **dict(default_cfg=default_cfg, **model_args))\n</code></pre>"},{"location":"reference/models/#mindcv.models.hrnet.hrnet_w48","title":"<code>mindcv.models.hrnet.hrnet_w48(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get HRNet with width=48 model. Refer to the base class <code>models.HRNet</code> for more details.</p> PARAMETER DESCRIPTION <code>pretrained</code> <p>Whether the model is pretrained. Default: False</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>num_classes</code> <p>number of classification classes. Default: 1000</p> <p> TYPE: <code>int</code> DEFAULT: <code>1000</code> </p> <code>in_channels</code> <p>Number of input channels. Default: 3</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> RETURNS DESCRIPTION <code>Union[HRNet, HRNetFeatures]</code> <p>HRNet model</p> Source code in <code>mindcv\\models\\hrnet.py</code> <pre><code>@register_model\ndef hrnet_w48(\n    pretrained: bool = False, num_classes: int = 1000, in_channels: int = 3, **kwargs\n) -&gt; Union[HRNet, HRNetFeatures]:\n    \"\"\"Get HRNet with width=48 model.\n    Refer to the base class `models.HRNet` for more details.\n\n    Args:\n        pretrained: Whether the model is pretrained. Default: False\n        num_classes: number of classification classes. Default: 1000\n        in_channels: Number of input channels. Default: 3\n\n    Returns:\n        HRNet model\n    \"\"\"\n    default_cfg = default_cfgs[\"hrnet_w48\"]\n    stage_cfg = dict(\n        stage1=dict(\n            num_modules=1,\n            num_branches=1,\n            block=\"BOTTLENECK\",\n            num_blocks=[4],\n            num_channels=[64],\n        ),\n        stage2=dict(\n            num_modules=1,\n            num_branches=2,\n            block=\"BASIC\",\n            num_blocks=[4, 4],\n            num_channels=[48, 96],\n        ),\n        stage3=dict(\n            num_modules=4,\n            num_branches=3,\n            block=\"BASIC\",\n            num_blocks=[4, 4, 4],\n            num_channels=[48, 96, 192],\n        ),\n        stage4=dict(\n            num_modules=3,\n            num_branches=4,\n            block=\"BASIC\",\n            num_blocks=[4, 4, 4, 4],\n            num_channels=[48, 96, 192, 384],\n        ),\n    )\n    model_args = dict(stage_cfg=stage_cfg, num_classes=num_classes, in_channels=in_channels, **kwargs)\n    return _create_hrnet(pretrained, **dict(default_cfg=default_cfg, **model_args))\n</code></pre>"},{"location":"reference/models/#inceptionv3","title":"inceptionv3","text":""},{"location":"reference/models/#mindcv.models.inceptionv3","title":"<code>mindcv.models.inceptionv3</code>","text":"<p>MindSpore implementation of <code>InceptionV3</code>. Refer to Rethinking the Inception Architecture for Computer Vision.</p>"},{"location":"reference/models/#mindcv.models.inceptionv3.BasicConv2d","title":"<code>mindcv.models.inceptionv3.BasicConv2d</code>","text":"<p>               Bases: <code>Cell</code></p> <p>A block for conv bn and relu</p> Source code in <code>mindcv\\models\\inceptionv3.py</code> <pre><code>class BasicConv2d(nn.Cell):\n    \"\"\"A block for conv bn and relu\"\"\"\n\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        kernel_size: Union[int, Tuple] = 1,\n        stride: int = 1,\n        padding: int = 0,\n        pad_mode: str = \"same\",\n    ) -&gt; None:\n        super().__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride,\n                              padding=padding, pad_mode=pad_mode)\n        self.bn = nn.BatchNorm2d(out_channels, eps=0.001, momentum=0.9997)\n        self.relu = nn.ReLU()\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        x = self.conv(x)\n        x = self.bn(x)\n        x = self.relu(x)\n        return x\n</code></pre>"},{"location":"reference/models/#mindcv.models.inceptionv3.InceptionAux","title":"<code>mindcv.models.inceptionv3.InceptionAux</code>","text":"<p>               Bases: <code>Cell</code></p> <p>Inception module for the aux classifier head</p> Source code in <code>mindcv\\models\\inceptionv3.py</code> <pre><code>class InceptionAux(nn.Cell):\n    \"\"\"Inception module for the aux classifier head\"\"\"\n\n    def __init__(\n        self,\n        in_channels: int,\n        num_classes: int,\n    ) -&gt; None:\n        super().__init__()\n        self.avg_pool = nn.AvgPool2d(5, stride=3, pad_mode=\"valid\")\n        self.conv0 = BasicConv2d(in_channels, 128, kernel_size=1)\n        self.conv1 = BasicConv2d(128, 768, kernel_size=5, pad_mode=\"valid\")\n        self.flatten = nn.Flatten()\n        self.fc = nn.Dense(in_channels, num_classes)\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        x = self.avg_pool(x)\n        x = self.conv0(x)\n        x = self.conv1(x)\n        x = self.flatten(x)\n        x = self.fc(x)\n        return x\n</code></pre>"},{"location":"reference/models/#mindcv.models.inceptionv3.InceptionV3","title":"<code>mindcv.models.inceptionv3.InceptionV3</code>","text":"<p>               Bases: <code>Cell</code></p> <p>Inception v3 model architecture from <code>\"Rethinking the Inception Architecture for Computer Vision\" &lt;https://arxiv.org/abs/1512.00567&gt;</code>_.</p> <p>.. note::     Important: In contrast to the other models the inception_v3 expects tensors with a size of     N x 3 x 299 x 299, so ensure your images are sized accordingly.</p> PARAMETER DESCRIPTION <code>num_classes</code> <p>number of classification classes. Default: 1000.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1000</code> </p> <code>aux_logits</code> <p>use auxiliary classifier or not. Default: False.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>in_channels</code> <p>number the channels of the input. Default: 3.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>drop_rate</code> <p>dropout rate of the layer before main classifier. Default: 0.2.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.2</code> </p> Source code in <code>mindcv\\models\\inceptionv3.py</code> <pre><code>class InceptionV3(nn.Cell):\n    r\"\"\"Inception v3 model architecture from\n    `\"Rethinking the Inception Architecture for Computer Vision\" &lt;https://arxiv.org/abs/1512.00567&gt;`_.\n\n    .. note::\n        **Important**: In contrast to the other models the inception_v3 expects tensors with a size of\n        N x 3 x 299 x 299, so ensure your images are sized accordingly.\n\n    Args:\n        num_classes: number of classification classes. Default: 1000.\n        aux_logits: use auxiliary classifier or not. Default: False.\n        in_channels: number the channels of the input. Default: 3.\n        drop_rate: dropout rate of the layer before main classifier. Default: 0.2.\n    \"\"\"\n\n    def __init__(\n        self,\n        num_classes: int = 1000,\n        aux_logits: bool = True,\n        in_channels: int = 3,\n        drop_rate: float = 0.2,\n    ) -&gt; None:\n        super().__init__()\n        self.aux_logits = aux_logits\n        self.conv1a = BasicConv2d(in_channels, 32, kernel_size=3, stride=2, pad_mode=\"valid\")\n        self.conv2a = BasicConv2d(32, 32, kernel_size=3, stride=1, pad_mode=\"valid\")\n        self.conv2b = BasicConv2d(32, 64, kernel_size=3, stride=1)\n        self.maxpool1 = nn.MaxPool2d(kernel_size=3, stride=2)\n        self.conv3b = BasicConv2d(64, 80, kernel_size=1)\n        self.conv4a = BasicConv2d(80, 192, kernel_size=3, pad_mode=\"valid\")\n        self.maxpool2 = nn.MaxPool2d(kernel_size=3, stride=2)\n        self.inception5b = InceptionA(192, pool_features=32)\n        self.inception5c = InceptionA(256, pool_features=64)\n        self.inception5d = InceptionA(288, pool_features=64)\n        self.inception6a = InceptionB(288)\n        self.inception6b = InceptionC(768, channels_7x7=128)\n        self.inception6c = InceptionC(768, channels_7x7=160)\n        self.inception6d = InceptionC(768, channels_7x7=160)\n        self.inception6e = InceptionC(768, channels_7x7=192)\n        if self.aux_logits:\n            self.aux = InceptionAux(768, num_classes)\n        self.inception7a = InceptionD(768)\n        self.inception7b = InceptionE(1280)\n        self.inception7c = InceptionE(2048)\n\n        self.pool = GlobalAvgPooling()\n        self.dropout = Dropout(p=drop_rate)\n        self.num_features = 2048\n        self.classifier = nn.Dense(self.num_features, num_classes)\n        self._initialize_weights()\n\n    def _initialize_weights(self) -&gt; None:\n        \"\"\"Initialize weights for cells.\"\"\"\n        for _, cell in self.cells_and_names():\n            if isinstance(cell, nn.Conv2d):\n                cell.weight.set_data(\n                    init.initializer(init.XavierUniform(), cell.weight.shape, cell.weight.dtype))\n\n    def forward_preaux(self, x: Tensor) -&gt; Tensor:\n        x = self.conv1a(x)\n        x = self.conv2a(x)\n        x = self.conv2b(x)\n        x = self.maxpool1(x)\n        x = self.conv3b(x)\n        x = self.conv4a(x)\n        x = self.maxpool2(x)\n        x = self.inception5b(x)\n        x = self.inception5c(x)\n        x = self.inception5d(x)\n        x = self.inception6a(x)\n        x = self.inception6b(x)\n        x = self.inception6c(x)\n        x = self.inception6d(x)\n        x = self.inception6e(x)\n        return x\n\n    def forward_postaux(self, x: Tensor) -&gt; Tensor:\n        x = self.inception7a(x)\n        x = self.inception7b(x)\n        x = self.inception7c(x)\n        return x\n\n    def forward_features(self, x: Tensor) -&gt; Tensor:\n        x = self.forward_preaux(x)\n        x = self.forward_postaux(x)\n        return x\n\n    def construct(self, x: Tensor) -&gt; Union[Tensor, Tuple[Tensor, Tensor]]:\n        x = self.forward_preaux(x)\n        if self.training and self.aux_logits:\n            aux = self.aux(x)\n        else:\n            aux = None\n        x = self.forward_postaux(x)\n\n        x = self.pool(x)\n        x = self.dropout(x)\n        x = self.classifier(x)\n\n        if self.training and self.aux_logits:\n            return x, aux\n        return x\n</code></pre>"},{"location":"reference/models/#mindcv.models.inceptionv3.inception_v3","title":"<code>mindcv.models.inceptionv3.inception_v3(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get InceptionV3 model. Refer to the base class <code>models.InceptionV3</code> for more details.</p> Source code in <code>mindcv\\models\\inceptionv3.py</code> <pre><code>@register_model\ndef inception_v3(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; InceptionV3:\n    \"\"\"Get InceptionV3 model.\n    Refer to the base class `models.InceptionV3` for more details.\"\"\"\n    default_cfg = default_cfgs[\"inception_v3\"]\n    model = InceptionV3(num_classes=num_classes, aux_logits=True, in_channels=in_channels, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre>"},{"location":"reference/models/#inceptionv4","title":"inceptionv4","text":""},{"location":"reference/models/#mindcv.models.inceptionv4","title":"<code>mindcv.models.inceptionv4</code>","text":"<p>MindSpore implementation of <code>InceptionV4</code>. Refer to Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning.</p>"},{"location":"reference/models/#mindcv.models.inceptionv4.BasicConv2d","title":"<code>mindcv.models.inceptionv4.BasicConv2d</code>","text":"<p>               Bases: <code>Cell</code></p> <p>A block for conv bn and relu</p> Source code in <code>mindcv\\models\\inceptionv4.py</code> <pre><code>class BasicConv2d(nn.Cell):\n    \"\"\"A block for conv bn and relu\"\"\"\n\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        kernel_size: Union[int, Tuple] = 1,\n        stride: int = 1,\n        padding: int = 0,\n        pad_mode: str = \"same\",\n    ) -&gt; None:\n        super().__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride,\n                              padding=padding, pad_mode=pad_mode)\n        self.bn = nn.BatchNorm2d(out_channels, eps=0.001, momentum=0.9997)\n        self.relu = nn.ReLU()\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        x = self.conv(x)\n        x = self.bn(x)\n        x = self.relu(x)\n        return x\n</code></pre>"},{"location":"reference/models/#mindcv.models.inceptionv4.InceptionA","title":"<code>mindcv.models.inceptionv4.InceptionA</code>","text":"<p>               Bases: <code>Cell</code></p> <p>Inception V4 model basic architecture</p> Source code in <code>mindcv\\models\\inceptionv4.py</code> <pre><code>class InceptionA(nn.Cell):\n    \"\"\"Inception V4 model basic architecture\"\"\"\n\n    def __init__(self) -&gt; None:\n        super().__init__()\n        self.branch_0 = BasicConv2d(384, 96, kernel_size=1, stride=1)\n        self.branch_1 = nn.SequentialCell([\n            BasicConv2d(384, 64, kernel_size=1, stride=1),\n            BasicConv2d(64, 96, kernel_size=3, stride=1, pad_mode=\"pad\", padding=1)\n        ])\n        self.branch_2 = nn.SequentialCell([\n            BasicConv2d(384, 64, kernel_size=1, stride=1),\n            BasicConv2d(64, 96, kernel_size=3, stride=1, pad_mode=\"pad\", padding=1),\n            BasicConv2d(96, 96, kernel_size=3, stride=1, pad_mode=\"pad\", padding=1)\n        ])\n        self.branch_3 = nn.SequentialCell([\n            nn.AvgPool2d(kernel_size=3, stride=1, pad_mode=\"same\"),\n            BasicConv2d(384, 96, kernel_size=1, stride=1)\n        ])\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        x0 = self.branch_0(x)\n        x1 = self.branch_1(x)\n        x2 = self.branch_2(x)\n        x3 = self.branch_3(x)\n        x4 = ops.concat((x0, x1, x2, x3), axis=1)\n        return x4\n</code></pre>"},{"location":"reference/models/#mindcv.models.inceptionv4.InceptionB","title":"<code>mindcv.models.inceptionv4.InceptionB</code>","text":"<p>               Bases: <code>Cell</code></p> <p>Inception V4 model basic architecture</p> Source code in <code>mindcv\\models\\inceptionv4.py</code> <pre><code>class InceptionB(nn.Cell):\n    \"\"\"Inception V4 model basic architecture\"\"\"\n\n    def __init__(self) -&gt; None:\n        super().__init__()\n        self.branch_0 = BasicConv2d(1024, 384, kernel_size=1, stride=1)\n        self.branch_1 = nn.SequentialCell([\n            BasicConv2d(1024, 192, kernel_size=1, stride=1),\n            BasicConv2d(192, 224, kernel_size=(1, 7), stride=1),\n            BasicConv2d(224, 256, kernel_size=(7, 1), stride=1),\n        ])\n        self.branch_2 = nn.SequentialCell([\n            BasicConv2d(1024, 192, kernel_size=1, stride=1),\n            BasicConv2d(192, 192, kernel_size=(7, 1), stride=1),\n            BasicConv2d(192, 224, kernel_size=(1, 7), stride=1),\n            BasicConv2d(224, 224, kernel_size=(7, 1), stride=1),\n            BasicConv2d(224, 256, kernel_size=(1, 7), stride=1)\n        ])\n        self.branch_3 = nn.SequentialCell([\n            nn.AvgPool2d(kernel_size=3, stride=1, pad_mode=\"same\"),\n            BasicConv2d(1024, 128, kernel_size=1, stride=1)\n        ])\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        x0 = self.branch_0(x)\n        x1 = self.branch_1(x)\n        x2 = self.branch_2(x)\n        x3 = self.branch_3(x)\n        x4 = ops.concat((x0, x1, x2, x3), axis=1)\n        return x4\n</code></pre>"},{"location":"reference/models/#mindcv.models.inceptionv4.InceptionC","title":"<code>mindcv.models.inceptionv4.InceptionC</code>","text":"<p>               Bases: <code>Cell</code></p> <p>Inception V4 model basic architecture</p> Source code in <code>mindcv\\models\\inceptionv4.py</code> <pre><code>class InceptionC(nn.Cell):\n    \"\"\"Inception V4 model basic architecture\"\"\"\n\n    def __init__(self) -&gt; None:\n        super().__init__()\n        self.branch_0 = BasicConv2d(1536, 256, kernel_size=1, stride=1)\n\n        self.branch_1 = BasicConv2d(1536, 384, kernel_size=1, stride=1)\n        self.branch_1_1 = BasicConv2d(384, 256, kernel_size=(1, 3), stride=1)\n        self.branch_1_2 = BasicConv2d(384, 256, kernel_size=(3, 1), stride=1)\n\n        self.branch_2 = nn.SequentialCell([\n            BasicConv2d(1536, 384, kernel_size=1, stride=1),\n            BasicConv2d(384, 448, kernel_size=(3, 1), stride=1),\n            BasicConv2d(448, 512, kernel_size=(1, 3), stride=1),\n        ])\n        self.branch_2_1 = BasicConv2d(512, 256, kernel_size=(1, 3), stride=1)\n        self.branch_2_2 = BasicConv2d(512, 256, kernel_size=(3, 1), stride=1)\n\n        self.branch_3 = nn.SequentialCell([\n            nn.AvgPool2d(kernel_size=3, stride=1, pad_mode=\"same\"),\n            BasicConv2d(1536, 256, kernel_size=1, stride=1)\n        ])\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        x0 = self.branch_0(x)\n        x1 = self.branch_1(x)\n        x1_1 = self.branch_1_1(x1)\n        x1_2 = self.branch_1_2(x1)\n        x1 = ops.concat((x1_1, x1_2), axis=1)\n        x2 = self.branch_2(x)\n        x2_1 = self.branch_2_1(x2)\n        x2_2 = self.branch_2_2(x2)\n        x2 = ops.concat((x2_1, x2_2), axis=1)\n        x3 = self.branch_3(x)\n        return ops.concat((x0, x1, x2, x3), axis=1)\n</code></pre>"},{"location":"reference/models/#mindcv.models.inceptionv4.InceptionV4","title":"<code>mindcv.models.inceptionv4.InceptionV4</code>","text":"<p>               Bases: <code>Cell</code></p> <p>Inception v4 model architecture from <code>\"Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning\" &lt;https://arxiv.org/abs/1602.07261&gt;</code>_.  # noqa: E501</p> PARAMETER DESCRIPTION <code>num_classes</code> <p>number of classification classes. Default: 1000.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1000</code> </p> <code>in_channels</code> <p>number the channels of the input. Default: 3.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>drop_rate</code> <p>dropout rate of the layer before main classifier. Default: 0.2.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.2</code> </p> Source code in <code>mindcv\\models\\inceptionv4.py</code> <pre><code>class InceptionV4(nn.Cell):\n    r\"\"\"Inception v4 model architecture from\n    `\"Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning\" &lt;https://arxiv.org/abs/1602.07261&gt;`_.  # noqa: E501\n\n    Args:\n        num_classes: number of classification classes. Default: 1000.\n        in_channels: number the channels of the input. Default: 3.\n        drop_rate: dropout rate of the layer before main classifier. Default: 0.2.\n    \"\"\"\n\n    def __init__(\n        self,\n        num_classes: int = 1000,\n        in_channels: int = 3,\n        drop_rate: float = 0.2,\n    ) -&gt; None:\n        super().__init__()\n        blocks = [Stem(in_channels)]\n        for _ in range(4):\n            blocks.append(InceptionA())\n        blocks.append(ReductionA())\n        for _ in range(7):\n            blocks.append(InceptionB())\n        blocks.append(ReductionB())\n        for _ in range(3):\n            blocks.append(InceptionC())\n        self.features = nn.SequentialCell(blocks)\n\n        self.pool = GlobalAvgPooling()\n        self.dropout = Dropout(p=drop_rate)\n        self.num_features = 1536\n        self.classifier = nn.Dense(self.num_features, num_classes)\n        self._initialize_weights()\n\n    def _initialize_weights(self) -&gt; None:\n        \"\"\"Initialize weights for cells.\"\"\"\n        for _, cell in self.cells_and_names():\n            if isinstance(cell, nn.Conv2d):\n                cell.weight.set_data(\n                    init.initializer(init.XavierUniform(), cell.weight.shape, cell.weight.dtype))\n\n    def forward_features(self, x: Tensor) -&gt; Tensor:\n        x = self.features(x)\n        return x\n\n    def forward_head(self, x: Tensor) -&gt; Tensor:\n        x = self.pool(x)\n        x = self.dropout(x)\n        x = self.classifier(x)\n        return x\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n</code></pre>"},{"location":"reference/models/#mindcv.models.inceptionv4.ReductionA","title":"<code>mindcv.models.inceptionv4.ReductionA</code>","text":"<p>               Bases: <code>Cell</code></p> <p>Inception V4 model Residual Connections</p> Source code in <code>mindcv\\models\\inceptionv4.py</code> <pre><code>class ReductionA(nn.Cell):\n    \"\"\"Inception V4 model Residual Connections\"\"\"\n\n    def __init__(self) -&gt; None:\n        super().__init__()\n        self.branch_0 = BasicConv2d(384, 384, kernel_size=3, stride=2, pad_mode=\"valid\")\n        self.branch_1 = nn.SequentialCell([\n            BasicConv2d(384, 192, kernel_size=1, stride=1),\n            BasicConv2d(192, 224, kernel_size=3, stride=1, pad_mode=\"pad\", padding=1),\n            BasicConv2d(224, 256, kernel_size=3, stride=2, pad_mode=\"valid\"),\n        ])\n        self.branch_2 = nn.MaxPool2d(3, stride=2)\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        x0 = self.branch_0(x)\n        x1 = self.branch_1(x)\n        x2 = self.branch_2(x)\n        x3 = ops.concat((x0, x1, x2), axis=1)\n        return x3\n</code></pre>"},{"location":"reference/models/#mindcv.models.inceptionv4.ReductionB","title":"<code>mindcv.models.inceptionv4.ReductionB</code>","text":"<p>               Bases: <code>Cell</code></p> <p>Inception V4 model Residual Connections</p> Source code in <code>mindcv\\models\\inceptionv4.py</code> <pre><code>class ReductionB(nn.Cell):\n    \"\"\"Inception V4 model Residual Connections\"\"\"\n\n    def __init__(self) -&gt; None:\n        super().__init__()\n        self.branch_0 = nn.SequentialCell([\n            BasicConv2d(1024, 192, kernel_size=1, stride=1),\n            BasicConv2d(192, 192, kernel_size=3, stride=2, pad_mode=\"valid\"),\n        ])\n        self.branch_1 = nn.SequentialCell([\n            BasicConv2d(1024, 256, kernel_size=1, stride=1),\n            BasicConv2d(256, 256, kernel_size=(1, 7), stride=1),\n            BasicConv2d(256, 320, kernel_size=(7, 1), stride=1),\n            BasicConv2d(320, 320, kernel_size=3, stride=2, pad_mode=\"valid\")\n        ])\n        self.branch_2 = nn.MaxPool2d(3, stride=2)\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        x0 = self.branch_0(x)\n        x1 = self.branch_1(x)\n        x2 = self.branch_2(x)\n        x3 = ops.concat((x0, x1, x2), axis=1)\n        return x3  # 8 x 8 x 1536\n</code></pre>"},{"location":"reference/models/#mindcv.models.inceptionv4.Stem","title":"<code>mindcv.models.inceptionv4.Stem</code>","text":"<p>               Bases: <code>Cell</code></p> <p>Inception V4 model blocks.</p> Source code in <code>mindcv\\models\\inceptionv4.py</code> <pre><code>class Stem(nn.Cell):\n    \"\"\"Inception V4 model blocks.\"\"\"\n\n    def __init__(self, in_channels: int) -&gt; None:\n        super().__init__()\n        self.conv2d_1a_3x3 = BasicConv2d(in_channels, 32, kernel_size=3, stride=2, pad_mode=\"valid\")\n        self.conv2d_2a_3x3 = BasicConv2d(32, 32, kernel_size=3, stride=1, pad_mode=\"valid\")\n        self.conv2d_2b_3x3 = BasicConv2d(32, 64, kernel_size=3, stride=1, pad_mode=\"pad\", padding=1)\n\n        self.mixed_3a_branch_0 = nn.MaxPool2d(3, stride=2)\n        self.mixed_3a_branch_1 = BasicConv2d(64, 96, kernel_size=3, stride=2, pad_mode=\"valid\")\n\n        self.mixed_4a_branch_0 = nn.SequentialCell([\n            BasicConv2d(160, 64, kernel_size=1, stride=1),\n            BasicConv2d(64, 96, kernel_size=3, stride=1, pad_mode=\"valid\")\n        ])\n\n        self.mixed_4a_branch_1 = nn.SequentialCell([\n            BasicConv2d(160, 64, kernel_size=1, stride=1),\n            BasicConv2d(64, 64, kernel_size=(1, 7), stride=1),\n            BasicConv2d(64, 64, kernel_size=(7, 1), stride=1),\n            BasicConv2d(64, 96, kernel_size=3, stride=1, pad_mode=\"valid\")\n        ])\n\n        self.mixed_5a_branch_0 = BasicConv2d(192, 192, kernel_size=3, stride=2, pad_mode=\"valid\")\n        self.mixed_5a_branch_1 = nn.MaxPool2d(3, stride=2)\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        x = self.conv2d_1a_3x3(x)  # 149 x 149 x 32\n        x = self.conv2d_2a_3x3(x)  # 147 x 147 x 32\n        x = self.conv2d_2b_3x3(x)  # 147 x 147 x 64\n\n        x0 = self.mixed_3a_branch_0(x)\n        x1 = self.mixed_3a_branch_1(x)\n        x = ops.concat((x0, x1), axis=1)  # 73 x 73 x 160\n\n        x0 = self.mixed_4a_branch_0(x)\n        x1 = self.mixed_4a_branch_1(x)\n        x = ops.concat((x0, x1), axis=1)  # 71 x 71 x 192\n\n        x0 = self.mixed_5a_branch_0(x)\n        x1 = self.mixed_5a_branch_1(x)\n        x = ops.concat((x0, x1), axis=1)  # 35 x 35 x 384\n        return x\n</code></pre>"},{"location":"reference/models/#mindcv.models.inceptionv4.inception_v4","title":"<code>mindcv.models.inceptionv4.inception_v4(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get InceptionV4 model. Refer to the base class <code>models.InceptionV4</code> for more details.</p> Source code in <code>mindcv\\models\\inceptionv4.py</code> <pre><code>@register_model\ndef inception_v4(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; InceptionV4:\n    \"\"\"Get InceptionV4 model.\n    Refer to the base class `models.InceptionV4` for more details.\"\"\"\n    default_cfg = default_cfgs[\"inception_v4\"]\n    model = InceptionV4(num_classes=num_classes, in_channels=in_channels, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre>"},{"location":"reference/models/#mae","title":"mae","text":""},{"location":"reference/models/#mindcv.models.mae","title":"<code>mindcv.models.mae</code>","text":""},{"location":"reference/models/#mindcv.models.mae.MAEForPretrain","title":"<code>mindcv.models.mae.MAEForPretrain</code>","text":"<p>               Bases: <code>Cell</code></p> Source code in <code>mindcv\\models\\mae.py</code> <pre><code>class MAEForPretrain(nn.Cell):\n    def __init__(\n        self,\n        image_size: int = 224,\n        patch_size: int = 16,\n        in_channels: int = 3,\n        embed_dim: int = 1024,\n        depth: int = 24,\n        num_heads: int = 16,\n        mlp_ratio: float = 4.,\n        decoder_embed_dim: int = 512,\n        decoder_depth: int = 8,\n        decoder_num_heads: int = 16,\n        qkv_bias: bool = True,\n        qk_norm: bool = False,\n        proj_drop_rate: float = 0.,\n        attn_drop_rate: float = 0.,\n        drop_path_rate: float = 0.,\n        init_values: Optional[float] = None,\n        act_layer: nn.Cell = nn.GELU,\n        norm_layer: nn.Cell = nn.LayerNorm,\n        mlp_layer: Callable = Mlp,\n        norm_pix_loss: bool = True,\n        mask_ratio: float = 0.75,\n        **kwargs,\n    ):\n        super(MAEForPretrain, self).__init__()\n        self.patch_embed = PatchEmbed(image_size=image_size, patch_size=patch_size,\n                                      in_chans=in_channels, embed_dim=embed_dim)\n        self.num_patches = self.patch_embed.num_patches\n        dpr = [x.item() for x in np.linspace(0, drop_path_rate, depth)]\n        self.blocks = nn.CellList([\n            Block(\n                dim=embed_dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_norm=qk_norm,\n                attn_drop=attn_drop_rate, proj_drop=proj_drop_rate,\n                mlp_ratio=mlp_ratio, drop_path=dpr[i], init_values=init_values,\n                act_layer=act_layer, norm_layer=norm_layer, mlp_layer=mlp_layer,\n            ) for i in range(depth)\n        ])\n\n        self.cls_token = Parameter(initializer(Normal(sigma=0.02), (1, 1, embed_dim)))\n\n        self.unmask_len = int(np.floor(self.num_patches * (1 - mask_ratio)))\n\n        encoder_pos_emb = Tensor(get_2d_sincos_pos_embed(\n            embed_dim, int(self.num_patches ** 0.5), cls_token=True), ms.float32\n        )\n        encoder_pos_emb = ops.expand_dims(encoder_pos_emb, axis=0)\n        self.pos_embed = Parameter(encoder_pos_emb, requires_grad=False)\n        self.norm = norm_layer((embed_dim,))\n\n        self.decoder_embed = nn.Dense(embed_dim, decoder_embed_dim)\n        self.mask_token = Parameter(initializer(Normal(sigma=0.02), (1, 1, decoder_embed_dim)))\n\n        decoder_pos_emb = Tensor(get_2d_sincos_pos_embed(\n            decoder_embed_dim, int(self.num_patches ** 0.5), cls_token=True), ms.float32\n        )\n        decoder_pos_emb = ops.expand_dims(decoder_pos_emb, axis=0)\n        self.decoder_pos_embed = Parameter(decoder_pos_emb, requires_grad=False)\n\n        self.decoder_blocks = nn.CellList([\n            Block(\n                dim=decoder_embed_dim, num_heads=decoder_num_heads, qkv_bias=qkv_bias, qk_norm=qk_norm,\n                attn_drop=attn_drop_rate, proj_drop=proj_drop_rate,\n                mlp_ratio=mlp_ratio, drop_path=dpr[i], init_values=init_values,\n                act_layer=act_layer, norm_layer=norm_layer, mlp_layer=mlp_layer,\n            ) for i in range(decoder_depth)\n        ])\n        self.decoder_norm = norm_layer((decoder_embed_dim,))\n        self.decoder_pred = nn.Dense(decoder_embed_dim, patch_size ** 2 * in_channels)\n\n        self.sort = ops.Sort()\n\n        self.norm_pix_loss = norm_pix_loss\n        self._init_weights()\n\n    def _init_weights(self):\n        for name, cell in self.cells_and_names():\n            if isinstance(cell, nn.Dense):\n                cell.weight.set_data(\n                    initializer(\"xavier_uniform\", cell.weight.shape, cell.weight.dtype)\n                )\n                if cell.bias is not None:\n                    cell.bias.set_data(\n                        initializer('zeros', cell.bias.shape, cell.bias.dtype)\n                    )\n\n            elif isinstance(cell, nn.LayerNorm):\n                cell.gamma.set_data(\n                    initializer('ones', cell.gamma.shape, cell.gamma.dtype)\n                )\n                cell.beta.set_data(\n                    initializer('zeros', cell.beta.shape, cell.beta.dtype)\n                )\n            if name == \"patch_embed.proj\":\n                cell.weight.set_data(\n                    initializer(\"xavier_uniform\", cell.weight.shape, cell.weight.dtype)\n                )\n\n    def patchify(self, imgs):\n        \"\"\"\n        imgs: (N, 3, H, W)\n        x: (N, L, patch_size ** 2 * 3)\n        \"\"\"\n        N, _, H, W = imgs.shape\n        p = self.patch_embed.patch_size[0]\n        assert H == W and H % p == 0\n        h = w = H // p\n\n        x = ops.reshape(imgs, (N, 3, h, p, w, p))\n        x = ops.transpose(x, (0, 2, 4, 3, 5, 1))\n        x = ops.reshape(x, (N, h * w, p ** 2 * 3))\n        return x\n\n    def unpatchify(self, x):\n        \"\"\"\n        x: (N, L, patch_size ** 2 * 3)\n        imgs: (N, 3, H, W)\n        \"\"\"\n        N, L, _ = x.shape\n        p = self.patch_embed.patch_size[0]\n        h = w = int(L ** 0.5)\n        assert h * w == L\n\n        imgs = ops.reshape(x, (N, h, w, p, p, 3))\n        imgs = ops.transpose(imgs, (0, 5, 1, 3, 2, 4))\n        imgs = ops.reshape(imgs, (N, 3, h * p, w * p))\n        return imgs\n\n    def apply_masking(self, x, mask):\n        D = x.shape[2]\n        _, ids_shuffle = self.sort(mask.astype(ms.float32))\n        _, ids_restore = self.sort(ids_shuffle.astype(ms.float32))\n\n        ids_keep = ids_shuffle[:, :self.unmask_len]\n        ids_keep = ops.broadcast_to(ops.expand_dims(ids_keep, axis=-1), (-1, -1, D))\n        x_unmasked = ops.gather_elements(x, dim=1, index=ids_keep)\n\n        return x_unmasked, ids_restore\n\n    def forward_features(self, x, mask):\n        x = self.patch_embed(x)\n        bsz = x.shape[0]\n\n        x = x + self.pos_embed[:, 1:, :]\n        x, ids_restore = self.apply_masking(x, mask)\n\n        cls_token = self.cls_token + self.pos_embed[:, :1, :]\n        cls_token = ops.broadcast_to(cls_token, (bsz, -1, -1))\n        cls_token = cls_token.astype(x.dtype)\n        x = ops.concat((cls_token, x), axis=1)\n\n        for blk in self.blocks:\n            x = blk(x)\n\n        x = self.norm(x)\n        return x, ids_restore\n\n    def forward_decoder(self, x, ids_restore):\n        x = self.decoder_embed(x)\n        bsz, L, D = x.shape\n\n        mask_len = self.num_patches + 1 - L\n        mask_tokens = ops.broadcast_to(self.mask_token, (bsz, mask_len, -1))\n        mask_tokens = mask_tokens.astype(x.dtype)\n\n        x_ = ops.concat((x[:, 1:, :], mask_tokens), axis=1)\n        ids_restore = ops.broadcast_to(ops.expand_dims(ids_restore, axis=-1), (-1, -1, D))\n        x_ = ops.gather_elements(x_, dim=1, index=ids_restore)\n        x = ops.concat((x[:, :1, :], x_), axis=1)\n\n        x = x + self.decoder_pos_embed\n\n        for blk in self.decoder_blocks:\n            x = blk(x)\n\n        x = self.decoder_norm(x)\n        x = self.decoder_pred(x)\n\n        return x[:, 1:, :]\n\n    def forward_loss(self, imgs, pred, mask):\n        target = self.patchify(imgs)\n        if self.norm_pix_loss:\n            mean = target.mean(axis=-1, keep_dims=True)\n            std = target.std(axis=-1, keepdims=True)\n            target = (target - mean) / std\n\n        loss = (pred - target) ** 2\n        loss = loss.mean(axis=-1)\n\n        mask = mask.astype(loss.dtype)\n        loss = (loss * mask).sum() / mask.sum()\n        return loss\n\n    def construct(self, imgs, mask):\n        bsz = imgs.shape[0]\n        mask = ops.reshape(mask, (bsz, -1))\n        features, ids_restore = self.forward_features(imgs, mask)\n        pred = self.forward_decoder(features, ids_restore)\n        loss = self.forward_loss(imgs, pred, mask)\n        return loss\n\n    def get_num_layers(self):\n        return len(self.blocks)\n\n    def no_weight_decay(self):\n        return {'pos_embed', 'cls_token'}\n</code></pre>"},{"location":"reference/models/#mindcv.models.mae.MAEForPretrain.patchify","title":"<code>mindcv.models.mae.MAEForPretrain.patchify(imgs)</code>","text":"Source code in <code>mindcv\\models\\mae.py</code> <pre><code>def patchify(self, imgs):\n    \"\"\"\n    imgs: (N, 3, H, W)\n    x: (N, L, patch_size ** 2 * 3)\n    \"\"\"\n    N, _, H, W = imgs.shape\n    p = self.patch_embed.patch_size[0]\n    assert H == W and H % p == 0\n    h = w = H // p\n\n    x = ops.reshape(imgs, (N, 3, h, p, w, p))\n    x = ops.transpose(x, (0, 2, 4, 3, 5, 1))\n    x = ops.reshape(x, (N, h * w, p ** 2 * 3))\n    return x\n</code></pre>"},{"location":"reference/models/#mindcv.models.mae.MAEForPretrain.unpatchify","title":"<code>mindcv.models.mae.MAEForPretrain.unpatchify(x)</code>","text":"Source code in <code>mindcv\\models\\mae.py</code> <pre><code>def unpatchify(self, x):\n    \"\"\"\n    x: (N, L, patch_size ** 2 * 3)\n    imgs: (N, 3, H, W)\n    \"\"\"\n    N, L, _ = x.shape\n    p = self.patch_embed.patch_size[0]\n    h = w = int(L ** 0.5)\n    assert h * w == L\n\n    imgs = ops.reshape(x, (N, h, w, p, p, 3))\n    imgs = ops.transpose(imgs, (0, 5, 1, 3, 2, 4))\n    imgs = ops.reshape(imgs, (N, 3, h * p, w * p))\n    return imgs\n</code></pre>"},{"location":"reference/models/#mindcv.models.mae.get_1d_sincos_pos_embed_from_grid","title":"<code>mindcv.models.mae.get_1d_sincos_pos_embed_from_grid(embed_dim, pos)</code>","text":"Source code in <code>mindcv\\models\\mae.py</code> <pre><code>def get_1d_sincos_pos_embed_from_grid(embed_dim, pos):\n    \"\"\"\n    embed_dim: output dimension for each position\n    pos: a list of positions to be encoded: size (M,)\n    out: (M, D)\n    \"\"\"\n    assert embed_dim % 2 == 0\n    omega = np.arange(embed_dim // 2, dtype=np.float32)\n    omega /= embed_dim / 2.\n    omega = 1. / 10000 ** omega  # (D/2,)\n\n    pos = pos.reshape(-1)  # (M,)\n    out = np.einsum('m,d-&gt;md', pos, omega)  # (M, D/2), outer product\n\n    emb_sin = np.sin(out)  # (M, D/2)\n    emb_cos = np.cos(out)  # (M, D/2)\n\n    emb = np.concatenate([emb_sin, emb_cos], axis=1)  # (M, D)\n    return emb\n</code></pre>"},{"location":"reference/models/#mindcv.models.mae.get_2d_sincos_pos_embed","title":"<code>mindcv.models.mae.get_2d_sincos_pos_embed(embed_dim, grid_size, cls_token=False)</code>","text":"Source code in <code>mindcv\\models\\mae.py</code> <pre><code>def get_2d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n    \"\"\"\n    grid_size: int of the grid height and width\n    return:\n    pos_embed: [grid_size*grid_size, embed_dim] or [1+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)\n    \"\"\"\n    grid_h = np.arange(grid_size, dtype=np.float32)\n    grid_w = np.arange(grid_size, dtype=np.float32)\n    grid = np.meshgrid(grid_w, grid_h)  # here w goes first\n    grid = np.stack(grid, axis=0)\n\n    grid = grid.reshape([2, 1, grid_size, grid_size])\n    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\n    if cls_token:\n        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n    return pos_embed\n</code></pre>"},{"location":"reference/models/#mixnet","title":"mixnet","text":""},{"location":"reference/models/#mindcv.models.mixnet","title":"<code>mindcv.models.mixnet</code>","text":"<p>MindSpore implementation of <code>MixNet</code>. Refer to MixConv: Mixed Depthwise Convolutional Kernels</p>"},{"location":"reference/models/#mindcv.models.mixnet.MDConv","title":"<code>mindcv.models.mixnet.MDConv</code>","text":"<p>               Bases: <code>Cell</code></p> <p>Mixed Depth-wise Convolution</p> Source code in <code>mindcv\\models\\mixnet.py</code> <pre><code>class MDConv(nn.Cell):\n    \"\"\"Mixed Depth-wise Convolution\"\"\"\n\n    def __init__(self, channels: int, kernel_size: list, stride: int) -&gt; None:\n        super(MDConv, self).__init__()\n        self.num_groups = len(kernel_size)\n\n        if self.num_groups == 1:\n            self.mixed_depthwise_conv = nn.Conv2d(\n                channels,\n                channels,\n                kernel_size[0],\n                stride=stride,\n                pad_mode=\"pad\",\n                padding=kernel_size[0] // 2,\n                group=channels,\n                has_bias=False\n            )\n        else:\n            self.split_channels = _splitchannels(channels, self.num_groups)\n\n            self.mixed_depthwise_conv = nn.CellList()\n            for i in range(self.num_groups):\n                self.mixed_depthwise_conv.append(nn.Conv2d(\n                    self.split_channels[i],\n                    self.split_channels[i],\n                    kernel_size[i],\n                    stride=stride,\n                    pad_mode=\"pad\",\n                    padding=kernel_size[i] // 2,\n                    group=self.split_channels[i],\n                    has_bias=False\n                ))\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        if self.num_groups == 1:\n            return self.mixed_depthwise_conv(x)\n\n        output = []\n        start, end = 0, 0\n        for i in range(self.num_groups):\n            start, end = end, end + self.split_channels[i]\n            x_split = x[:, start:end]\n\n            conv = self.mixed_depthwise_conv[i]\n            output.append(conv(x_split))\n\n        return ops.concat(output, axis=1)\n</code></pre>"},{"location":"reference/models/#mindcv.models.mixnet.MixNet","title":"<code>mindcv.models.mixnet.MixNet</code>","text":"<p>               Bases: <code>Cell</code></p> <p>MixNet model class, based on <code>\"MixConv: Mixed Depthwise Convolutional Kernels\" &lt;https://arxiv.org/abs/1907.09595&gt;</code>_</p> PARAMETER DESCRIPTION <code>arch</code> <p>size of the architecture. \"small\", \"medium\" or \"large\". Default: \"small\".</p> <p> TYPE: <code>str</code> DEFAULT: <code>'small'</code> </p> <code>num_classes</code> <p>number of classification classes. Default: 1000.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1000</code> </p> <code>in_channels</code> <p>number of the channels of the input. Default: 3.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>feature_size</code> <p>numbet of the channels of the output features. Default: 1536.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1536</code> </p> <code>drop_rate</code> <p>rate of dropout for classifier. Default: 0.2.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.2</code> </p> <code>depth_multiplier</code> <p>expansion coefficient of channels. Default: 1.0.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> Source code in <code>mindcv\\models\\mixnet.py</code> <pre><code>class MixNet(nn.Cell):\n    r\"\"\"MixNet model class, based on\n    `\"MixConv: Mixed Depthwise Convolutional Kernels\" &lt;https://arxiv.org/abs/1907.09595&gt;`_\n\n    Args:\n        arch: size of the architecture. \"small\", \"medium\" or \"large\". Default: \"small\".\n        num_classes: number of classification classes. Default: 1000.\n        in_channels: number of the channels of the input. Default: 3.\n        feature_size: numbet of the channels of the output features. Default: 1536.\n        drop_rate: rate of dropout for classifier. Default: 0.2.\n        depth_multiplier: expansion coefficient of channels. Default: 1.0.\n    \"\"\"\n\n    def __init__(\n        self,\n        arch: str = \"small\",\n        num_classes: int = 1000,\n        in_channels: int = 3,\n        feature_size: int = 1536,\n        drop_rate: float = 0.2,\n        depth_multiplier: float = 1.0\n    ) -&gt; None:\n        super(MixNet, self).__init__()\n        if arch == \"small\":\n            block_configs = [\n                [16, 16, [3], [1], [1], 1, 1, \"ReLU\", 0.0],\n                [16, 24, [3], [1, 1], [1, 1], 2, 6, \"ReLU\", 0.0],\n                [24, 24, [3], [1, 1], [1, 1], 1, 3, \"ReLU\", 0.0],\n                [24, 40, [3, 5, 7], [1], [1], 2, 6, \"Swish\", 0.5],\n                [40, 40, [3, 5], [1, 1], [1, 1], 1, 6, \"Swish\", 0.5],\n                [40, 40, [3, 5], [1, 1], [1, 1], 1, 6, \"Swish\", 0.5],\n                [40, 40, [3, 5], [1, 1], [1, 1], 1, 6, \"Swish\", 0.5],\n                [40, 80, [3, 5, 7], [1], [1, 1], 2, 6, \"Swish\", 0.25],\n                [80, 80, [3, 5], [1], [1, 1], 1, 6, \"Swish\", 0.25],\n                [80, 80, [3, 5], [1], [1, 1], 1, 6, \"Swish\", 0.25],\n                [80, 120, [3, 5, 7], [1, 1], [1, 1], 1, 6, \"Swish\", 0.5],\n                [120, 120, [3, 5, 7, 9], [1, 1], [1, 1], 1, 3, \"Swish\", 0.5],\n                [120, 120, [3, 5, 7, 9], [1, 1], [1, 1], 1, 3, \"Swish\", 0.5],\n                [120, 200, [3, 5, 7, 9, 11], [1], [1], 2, 6, \"Swish\", 0.5],\n                [200, 200, [3, 5, 7, 9], [1], [1, 1], 1, 6, \"Swish\", 0.5],\n                [200, 200, [3, 5, 7, 9], [1], [1, 1], 1, 6, \"Swish\", 0.5]\n            ]\n            stem_channels = 16\n            drop_rate = drop_rate\n        else:\n            block_configs = [\n                [24, 24, [3], [1], [1], 1, 1, \"ReLU\", 0.0],\n                [24, 32, [3, 5, 7], [1, 1], [1, 1], 2, 6, \"ReLU\", 0.0],\n                [32, 32, [3], [1, 1], [1, 1], 1, 3, \"ReLU\", 0.0],\n                [32, 40, [3, 5, 7, 9], [1], [1], 2, 6, \"Swish\", 0.5],\n                [40, 40, [3, 5], [1, 1], [1, 1], 1, 6, \"Swish\", 0.5],\n                [40, 40, [3, 5], [1, 1], [1, 1], 1, 6, \"Swish\", 0.5],\n                [40, 40, [3, 5], [1, 1], [1, 1], 1, 6, \"Swish\", 0.5],\n                [40, 80, [3, 5, 7], [1], [1], 2, 6, \"Swish\", 0.25],\n                [80, 80, [3, 5, 7, 9], [1, 1], [1, 1], 1, 6, \"Swish\", 0.25],\n                [80, 80, [3, 5, 7, 9], [1, 1], [1, 1], 1, 6, \"Swish\", 0.25],\n                [80, 80, [3, 5, 7, 9], [1, 1], [1, 1], 1, 6, \"Swish\", 0.25],\n                [80, 120, [3], [1], [1], 1, 6, \"Swish\", 0.5],\n                [120, 120, [3, 5, 7, 9], [1, 1], [1, 1], 1, 3, \"Swish\", 0.5],\n                [120, 120, [3, 5, 7, 9], [1, 1], [1, 1], 1, 3, \"Swish\", 0.5],\n                [120, 120, [3, 5, 7, 9], [1, 1], [1, 1], 1, 3, \"Swish\", 0.5],\n                [120, 200, [3, 5, 7, 9], [1], [1], 2, 6, \"Swish\", 0.5],\n                [200, 200, [3, 5, 7, 9], [1], [1, 1], 1, 6, \"Swish\", 0.5],\n                [200, 200, [3, 5, 7, 9], [1], [1, 1], 1, 6, \"Swish\", 0.5],\n                [200, 200, [3, 5, 7, 9], [1], [1, 1], 1, 6, \"Swish\", 0.5]\n            ]\n            if arch == \"medium\":\n                stem_channels = 24\n                drop_rate = drop_rate\n            elif arch == \"large\":\n                stem_channels = 24\n                depth_multiplier *= 1.3\n                drop_rate = drop_rate\n            else:\n                raise ValueError(f\"Unsupported model type {arch}\")\n\n        if depth_multiplier != 1.0:\n            stem_channels = _roundchannels(stem_channels * depth_multiplier)\n\n            for i, conf in enumerate(block_configs):\n                conf_ls = list(conf)\n                conf_ls[0] = _roundchannels(conf_ls[0] * depth_multiplier)\n                conf_ls[1] = _roundchannels(conf_ls[1] * depth_multiplier)\n                block_configs[i] = tuple(conf_ls)\n\n        # stem convolution\n        self.stem_conv = nn.SequentialCell([\n            nn.Conv2d(in_channels, stem_channels, 3, stride=2, pad_mode=\"pad\", padding=1),\n            nn.BatchNorm2d(stem_channels),\n            nn.ReLU()\n        ])\n\n        # building MixNet blocks\n        layers = []\n        for inc, outc, k, ek, pk, s, er, ac, se in block_configs:\n            layers.append(MixNetBlock(\n                inc,\n                outc,\n                kernel_size=k,\n                expand_ksize=ek,\n                project_ksize=pk,\n                stride=s,\n                expand_ratio=er,\n                activation=ac,\n                se_ratio=se\n            ))\n        self.layers = nn.SequentialCell(layers)\n\n        # head\n        self.head_conv = nn.SequentialCell([\n            nn.Conv2d(block_configs[-1][1], feature_size, 1, pad_mode=\"pad\", padding=0),\n            nn.BatchNorm2d(feature_size),\n            nn.ReLU()\n        ])\n\n        self.pool = GlobalAvgPooling()\n        self.dropout = Dropout(p=drop_rate)\n        self.classifier = nn.Dense(feature_size, num_classes)\n\n        self._initialize_weights()\n\n    def _initialize_weights(self) -&gt; None:\n        \"\"\"Initialize weights for cells.\"\"\"\n        for _, cell in self.cells_and_names():\n            if isinstance(cell, nn.Conv2d):\n                fan_out = cell.kernel_size[0] * cell.kernel_size[1] * cell.out_channels\n                cell.weight.set_data(\n                    init.initializer(init.Normal(math.sqrt(2.0 / fan_out)),\n                                     cell.weight.shape, cell.weight.dtype))\n                if cell.bias is not None:\n                    cell.bias.set_data(\n                        init.initializer(\"zeros\", cell.bias.shape, cell.bias.dtype))\n            elif isinstance(cell, nn.BatchNorm2d):\n                cell.gamma.set_data(init.initializer(\"ones\", cell.gamma.shape, cell.gamma.dtype))\n                cell.beta.set_data(init.initializer(\"zeros\", cell.beta.shape, cell.beta.dtype))\n            elif isinstance(cell, nn.Dense):\n                cell.weight.set_data(\n                    init.initializer(init.Uniform(1.0 / math.sqrt(cell.weight.shape[0])),\n                                     cell.weight.shape, cell.weight.dtype))\n                if cell.bias is not None:\n                    cell.bias.set_data(init.initializer(\"zeros\", cell.bias.shape, cell.bias.dtype))\n\n    def forward_features(self, x: Tensor) -&gt; Tensor:\n        x = self.stem_conv(x)\n        x = self.layers(x)\n        x = self.head_conv(x)\n        return x\n\n    def forward_head(self, x: Tensor) -&gt; Tensor:\n        x = self.pool(x)\n        x = self.dropout(x)\n        x = self.classifier(x)\n        return x\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n</code></pre>"},{"location":"reference/models/#mindcv.models.mixnet.MixNetBlock","title":"<code>mindcv.models.mixnet.MixNetBlock</code>","text":"<p>               Bases: <code>Cell</code></p> <p>Basic Block of MixNet</p> Source code in <code>mindcv\\models\\mixnet.py</code> <pre><code>class MixNetBlock(nn.Cell):\n    \"\"\"Basic Block of MixNet\"\"\"\n\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        kernel_size: list = [3],\n        expand_ksize: list = [1],\n        project_ksize: list = [1],\n        stride: int = 1,\n        expand_ratio: int = 1,\n        activation: str = \"ReLU\",\n        se_ratio: float = 0.0,\n    ) -&gt; None:\n        super(MixNetBlock, self).__init__()\n        assert activation in [\"ReLU\", \"Swish\"]\n        self.activation = Swish if activation == \"Swish\" else nn.ReLU\n\n        expand_channels = in_channels * expand_ratio\n        self.residual_connection = (stride == 1 and in_channels == out_channels)\n\n        conv = []\n        if expand_ratio != 1:\n            # expand\n            conv.extend([\n                GroupedConv2d(in_channels, expand_channels, expand_ksize),\n                nn.BatchNorm2d(expand_channels),\n                self.activation()\n            ])\n\n        # depthwise\n        conv.extend([\n            MDConv(expand_channels, kernel_size, stride),\n            nn.BatchNorm2d(expand_channels),\n            self.activation()\n        ])\n\n        if se_ratio &gt; 0:\n            squeeze_channels = int(in_channels * se_ratio)\n            squeeze_excite = SqueezeExcite(expand_channels, rd_channels=squeeze_channels)\n            conv.append(squeeze_excite)\n\n        # projection phase\n        conv.extend([\n            GroupedConv2d(expand_channels, out_channels, project_ksize),\n            nn.BatchNorm2d(out_channels)\n        ])\n\n        self.convs = nn.SequentialCell(conv)\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        if self.residual_connection:\n            return x + self.convs(x)\n        else:\n            return self.convs(x)\n</code></pre>"},{"location":"reference/models/#mlpmixer","title":"mlpmixer","text":""},{"location":"reference/models/#mindcv.models.mlpmixer","title":"<code>mindcv.models.mlpmixer</code>","text":"<p>MindSpore implementation of <code>MLP-Mixer</code>. Refer to MLP-Mixer: An all-MLP Architecture for Vision.</p>"},{"location":"reference/models/#mindcv.models.mlpmixer.FeedForward","title":"<code>mindcv.models.mlpmixer.FeedForward</code>","text":"<p>               Bases: <code>Cell</code></p> <p>Feed Forward Block. MLP Layer. FC -&gt; GELU -&gt; FC</p> Source code in <code>mindcv\\models\\mlpmixer.py</code> <pre><code>class FeedForward(nn.Cell):\n    \"\"\"Feed Forward Block. MLP Layer. FC -&gt; GELU -&gt; FC\"\"\"\n\n    def __init__(self, dim, hidden_dim, dropout=0.):\n        super(FeedForward, self).__init__()\n        self.net = nn.SequentialCell(\n            nn.Dense(dim, hidden_dim),\n            nn.GELU(),\n            Dropout(p=dropout),\n            nn.Dense(hidden_dim, dim),\n            Dropout(p=dropout)\n        )\n\n    def construct(self, x):\n        return self.net(x)\n</code></pre>"},{"location":"reference/models/#mindcv.models.mlpmixer.MLPMixer","title":"<code>mindcv.models.mlpmixer.MLPMixer</code>","text":"<p>               Bases: <code>Cell</code></p> <p>MLP-Mixer model class, based on <code>\"MLP-Mixer: An all-MLP Architecture for Vision\" &lt;https://arxiv.org/abs/2105.01601&gt;</code>_</p> PARAMETER DESCRIPTION <code>depth</code> <p>number of MixerBlocks.</p> <p> TYPE: <code>int) </code> </p> <code>patch_size</code> <p>size of a single image patch.</p> <p> TYPE: <code>int or tuple) </code> </p> <code>n_patches</code> <p>number of patches.</p> <p> TYPE: <code>int) </code> </p> <code>n_channels</code> <p>channels(dimension) of a single embedded patch.</p> <p> TYPE: <code>int) </code> </p> <code>token_dim</code> <p>hidden dim of token-mixing MLP.</p> <p> TYPE: <code>int) </code> </p> <code>channel_dim</code> <p>hidden dim of channel-mixing MLP.</p> <p> TYPE: <code>int) </code> </p> <code>num_classes</code> <p>number of classification classes.</p> <p> TYPE: <code>int) </code> DEFAULT: <code>1000</code> </p> <code>in_channels</code> <p>number the channels of the input. Default: 3.</p> <p> DEFAULT: <code>3</code> </p> Source code in <code>mindcv\\models\\mlpmixer.py</code> <pre><code>class MLPMixer(nn.Cell):\n    r\"\"\"MLP-Mixer model class, based on\n    `\"MLP-Mixer: An all-MLP Architecture for Vision\" &lt;https://arxiv.org/abs/2105.01601&gt;`_\n\n    Args:\n        depth (int) : number of MixerBlocks.\n        patch_size (int or tuple) : size of a single image patch.\n        n_patches (int) : number of patches.\n        n_channels (int) : channels(dimension) of a single embedded patch.\n        token_dim (int) : hidden dim of token-mixing MLP.\n        channel_dim (int) : hidden dim of channel-mixing MLP.\n        num_classes (int) : number of classification classes.\n        in_channels: number the channels of the input. Default: 3.\n    \"\"\"\n\n    def __init__(self, depth, patch_size, n_patches, n_channels, token_dim, channel_dim, num_classes=1000,\n                 in_channels=3):\n        super().__init__()\n        self.n_patches = n_patches\n        self.n_channels = n_channels\n        # patch with shape of (3, patch_size, patch_size) is embedded to n_channels dim feature.\n        self.to_patch_embedding = nn.SequentialCell(\n            nn.Conv2d(in_channels, n_channels, patch_size, patch_size, pad_mode=\"pad\", padding=0),\n            TransPose(permutation=(0, 2, 1), embedding=True),\n        )\n        self.mixer_blocks = nn.SequentialCell()\n        for _ in range(depth):\n            self.mixer_blocks.append(MixerBlock(n_patches, n_channels, token_dim, channel_dim))\n        self.layer_norm = nn.LayerNorm((n_channels,))\n        self.mlp_head = nn.Dense(n_channels, num_classes)\n        self.mean = ops.ReduceMean()\n        self._initialize_weights()\n\n    def construct(self, x):\n        x = self.to_patch_embedding(x)\n        x = self.mixer_blocks(x)\n        x = self.layer_norm(x)\n        x = self.mean(x, 1)\n        return self.mlp_head(x)\n\n    def _initialize_weights(self):\n        # todo: implement weights init\n        pass\n</code></pre>"},{"location":"reference/models/#mindcv.models.mlpmixer.MixerBlock","title":"<code>mindcv.models.mlpmixer.MixerBlock</code>","text":"<p>               Bases: <code>Cell</code></p> <p>Mixer Layer with token-mixing MLP and channel-mixing MLP</p> Source code in <code>mindcv\\models\\mlpmixer.py</code> <pre><code>class MixerBlock(nn.Cell):\n    \"\"\"Mixer Layer with token-mixing MLP and channel-mixing MLP\"\"\"\n\n    def __init__(self, n_patches, n_channels, token_dim, channel_dim, dropout=0.):\n        super().__init__()\n        self.token_mix = nn.SequentialCell(\n            nn.LayerNorm((n_channels,)),\n            TransPose((0, 2, 1)),\n            FeedForward(n_patches, token_dim, dropout),\n            TransPose((0, 2, 1))\n        )\n        self.channel_mix = nn.SequentialCell(\n            nn.LayerNorm((n_channels,)),\n            FeedForward(n_channels, channel_dim, dropout),\n        )\n\n    def construct(self, x):\n        x = x + self.token_mix(x)\n        x = x + self.channel_mix(x)\n        return x\n</code></pre>"},{"location":"reference/models/#mindcv.models.mlpmixer.TransPose","title":"<code>mindcv.models.mlpmixer.TransPose</code>","text":"<p>               Bases: <code>Cell</code></p> <p>TransPose Layer. Wrap operator Transpose for easy integration in nn.SequentialCell</p> Source code in <code>mindcv\\models\\mlpmixer.py</code> <pre><code>class TransPose(nn.Cell):\n    \"\"\"TransPose Layer. Wrap operator Transpose for easy integration in nn.SequentialCell\"\"\"\n\n    def __init__(self, permutation=(0, 2, 1), embedding=False):\n        super(TransPose, self).__init__()\n        self.permutation = permutation\n        self.embedding = embedding\n        if embedding:\n            self.reshape = ops.Reshape()\n        self.transpose = ops.Transpose()\n\n    def construct(self, x):\n        if self.embedding:\n            b, c, h, w = x.shape\n            x = self.reshape(x, (b, c, h * w))\n        x = self.transpose(x, self.permutation)\n        return x\n</code></pre>"},{"location":"reference/models/#mnasnet","title":"mnasnet","text":""},{"location":"reference/models/#mindcv.models.mnasnet","title":"<code>mindcv.models.mnasnet</code>","text":"<p>MindSpore implementation of <code>MnasNet</code>. Refer to MnasNet: Platform-Aware Neural Architecture Search for Mobile.</p>"},{"location":"reference/models/#mindcv.models.mnasnet.Mnasnet","title":"<code>mindcv.models.mnasnet.Mnasnet</code>","text":"<p>               Bases: <code>Cell</code></p> <p>MnasNet model architecture from <code>\"MnasNet: Platform-Aware Neural Architecture Search for Mobile\" &lt;https://arxiv.org/abs/1807.11626&gt;</code>_.</p> PARAMETER DESCRIPTION <code>alpha</code> <p>scale factor of model width.</p> <p> TYPE: <code>float</code> </p> <code>in_channels</code> <p>number the channels of the input. Default: 3.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>num_classes</code> <p>number of classification classes. Default: 1000.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1000</code> </p> <code>drop_rate</code> <p>dropout rate of the layer before main classifier. Default: 0.2.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.2</code> </p> Source code in <code>mindcv\\models\\mnasnet.py</code> <pre><code>class Mnasnet(nn.Cell):\n    r\"\"\"MnasNet model architecture from\n    `\"MnasNet: Platform-Aware Neural Architecture Search for Mobile\" &lt;https://arxiv.org/abs/1807.11626&gt;`_.\n\n    Args:\n        alpha: scale factor of model width.\n        in_channels: number the channels of the input. Default: 3.\n        num_classes: number of classification classes. Default: 1000.\n        drop_rate: dropout rate of the layer before main classifier. Default: 0.2.\n    \"\"\"\n\n    def __init__(\n        self,\n        alpha: float,\n        in_channels: int = 3,\n        num_classes: int = 1000,\n        drop_rate: float = 0.2,\n    ):\n        super().__init__()\n\n        inverted_residual_setting = [\n            # t, c, n, s, k\n            [3, 24, 3, 2, 3],  # -&gt; 56x56\n            [3, 40, 3, 2, 5],  # -&gt; 28x28\n            [6, 80, 3, 2, 5],  # -&gt; 14x14\n            [6, 96, 2, 1, 3],  # -&gt; 14x14\n            [6, 192, 4, 2, 5],  # -&gt; 7x7\n            [6, 320, 1, 1, 3],  # -&gt; 7x7\n        ]\n\n        mid_channels = make_divisible(32 * alpha, 8)\n        input_channels = make_divisible(16 * alpha, 8)\n\n        features: List[nn.Cell] = [\n            nn.Conv2d(in_channels, mid_channels, kernel_size=3, stride=2, pad_mode=\"pad\", padding=1),\n            nn.BatchNorm2d(mid_channels, momentum=0.99, eps=1e-3),\n            nn.ReLU(),\n            nn.Conv2d(mid_channels, mid_channels, kernel_size=3, stride=1, pad_mode=\"pad\", padding=1,\n                      group=mid_channels),\n            nn.BatchNorm2d(mid_channels, momentum=0.99, eps=1e-3),\n            nn.ReLU(),\n            nn.Conv2d(mid_channels, input_channels, kernel_size=1, stride=1),\n            nn.BatchNorm2d(input_channels, momentum=0.99, eps=1e-3),\n        ]\n\n        for t, c, n, s, k in inverted_residual_setting:\n            output_channels = make_divisible(c * alpha, 8)\n            for i in range(n):\n                stride = s if i == 0 else 1\n                features.append(InvertedResidual(input_channels, output_channels,\n                                                 stride=stride, kernel_size=k, expand_ratio=t))\n                input_channels = output_channels\n\n        features.extend([\n            nn.Conv2d(input_channels, 1280, kernel_size=1, stride=1),\n            nn.BatchNorm2d(1280, momentum=0.99, eps=1e-3),\n            nn.ReLU(),\n        ])\n        self.features = nn.SequentialCell(features)\n        self.pool = GlobalAvgPooling()\n        self.dropout = Dropout(p=drop_rate)\n        self.classifier = nn.Dense(1280, num_classes)\n        self._initialize_weights()\n\n    def _initialize_weights(self) -&gt; None:\n        \"\"\"Initialize weights for cells.\"\"\"\n        for _, cell in self.cells_and_names():\n            if isinstance(cell, nn.Conv2d):\n                cell.weight.set_data(\n                    init.initializer(init.HeNormal(mode=\"fan_out\", nonlinearity=\"relu\"),\n                                     cell.weight.shape, cell.weight.dtype))\n                if cell.bias is not None:\n                    cell.bias.set_data(init.initializer(\"zeros\", cell.bias.shape, cell.bias.dtype))\n            elif isinstance(cell, nn.BatchNorm2d):\n                cell.gamma.set_data(init.initializer(\"ones\", cell.gamma.shape, cell.gamma.dtype))\n                cell.beta.set_data(init.initializer(\"zeros\", cell.beta.shape, cell.beta.dtype))\n            elif isinstance(cell, nn.Dense):\n                cell.weight.set_data(\n                    init.initializer(init.HeUniform(mode=\"fan_out\", nonlinearity=\"sigmoid\"),\n                                     cell.weight.shape, cell.weight.dtype))\n                if cell.bias is not None:\n                    cell.bias.set_data(init.initializer(\"zeros\", cell.bias.shape, cell.bias.dtype))\n\n    def forward_features(self, x: Tensor) -&gt; Tensor:\n        x = self.features(x)\n        return x\n\n    def forward_head(self, x: Tensor) -&gt; Tensor:\n        x = self.pool(x)\n        x = self.dropout(x)\n        x = self.classifier(x)\n        return x\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n</code></pre>"},{"location":"reference/models/#mindcv.models.mnasnet.mnasnet_050","title":"<code>mindcv.models.mnasnet.mnasnet_050(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get MnasNet model with width scaled by 0.5. Refer to the base class <code>models.Mnasnet</code> for more details.</p> Source code in <code>mindcv\\models\\mnasnet.py</code> <pre><code>@register_model\ndef mnasnet_050(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; Mnasnet:\n    \"\"\"Get MnasNet model with width scaled by 0.5.\n    Refer to the base class `models.Mnasnet` for more details.\"\"\"\n    default_cfg = default_cfgs[\"mnasnet_050\"]\n    model = Mnasnet(alpha=0.5, in_channels=in_channels, num_classes=num_classes, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre>"},{"location":"reference/models/#mindcv.models.mnasnet.mnasnet_075","title":"<code>mindcv.models.mnasnet.mnasnet_075(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get MnasNet model with width scaled by 0.75. Refer to the base class <code>models.Mnasnet</code> for more details.</p> Source code in <code>mindcv\\models\\mnasnet.py</code> <pre><code>@register_model\ndef mnasnet_075(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; Mnasnet:\n    \"\"\"Get MnasNet model with width scaled by 0.75.\n    Refer to the base class `models.Mnasnet` for more details.\"\"\"\n    default_cfg = default_cfgs[\"mnasnet_075\"]\n    model = Mnasnet(alpha=0.75, in_channels=in_channels, num_classes=num_classes, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre>"},{"location":"reference/models/#mindcv.models.mnasnet.mnasnet_100","title":"<code>mindcv.models.mnasnet.mnasnet_100(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get MnasNet model with width scaled by 1.0. Refer to the base class <code>models.Mnasnet</code> for more details.</p> Source code in <code>mindcv\\models\\mnasnet.py</code> <pre><code>@register_model\ndef mnasnet_100(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; Mnasnet:\n    \"\"\"Get MnasNet model with width scaled by 1.0.\n    Refer to the base class `models.Mnasnet` for more details.\"\"\"\n    default_cfg = default_cfgs[\"mnasnet_100\"]\n    model = Mnasnet(alpha=1.0, in_channels=in_channels, num_classes=num_classes, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre>"},{"location":"reference/models/#mindcv.models.mnasnet.mnasnet_130","title":"<code>mindcv.models.mnasnet.mnasnet_130(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get MnasNet model with width scaled by 1.3. Refer to the base class <code>models.Mnasnet</code> for more details.</p> Source code in <code>mindcv\\models\\mnasnet.py</code> <pre><code>@register_model\ndef mnasnet_130(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; Mnasnet:\n    \"\"\"Get MnasNet model with width scaled by 1.3.\n    Refer to the base class `models.Mnasnet` for more details.\"\"\"\n    default_cfg = default_cfgs[\"mnasnet_130\"]\n    model = Mnasnet(alpha=1.3, in_channels=in_channels, num_classes=num_classes, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre>"},{"location":"reference/models/#mindcv.models.mnasnet.mnasnet_140","title":"<code>mindcv.models.mnasnet.mnasnet_140(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get MnasNet model with width scaled by 1.4. Refer to the base class <code>models.Mnasnet</code> for more details.</p> Source code in <code>mindcv\\models\\mnasnet.py</code> <pre><code>@register_model\ndef mnasnet_140(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; Mnasnet:\n    \"\"\"Get MnasNet model with width scaled by 1.4.\n    Refer to the base class `models.Mnasnet` for more details.\"\"\"\n    default_cfg = default_cfgs[\"mnasnet_140\"]\n    model = Mnasnet(alpha=1.4, in_channels=in_channels, num_classes=num_classes, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre>"},{"location":"reference/models/#mobilenetv1","title":"mobilenetv1","text":""},{"location":"reference/models/#mindcv.models.mobilenetv1","title":"<code>mindcv.models.mobilenetv1</code>","text":"<p>MindSpore implementation of <code>MobileNetV1</code>. Refer to MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications.</p>"},{"location":"reference/models/#mindcv.models.mobilenetv1.MobileNetV1","title":"<code>mindcv.models.mobilenetv1.MobileNetV1</code>","text":"<p>               Bases: <code>Cell</code></p> <p>MobileNetV1 model class, based on <code>\"MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications\" &lt;https://arxiv.org/abs/1704.04861&gt;</code>_  # noqa: E501</p> PARAMETER DESCRIPTION <code>alpha</code> <p>scale factor of model width. Default: 1.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>in_channels</code> <p>number the channels of the input. Default: 3.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>num_classes</code> <p>number of classification classes. Default: 1000.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1000</code> </p> Source code in <code>mindcv\\models\\mobilenetv1.py</code> <pre><code>class MobileNetV1(nn.Cell):\n    r\"\"\"MobileNetV1 model class, based on\n    `\"MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications\" &lt;https://arxiv.org/abs/1704.04861&gt;`_  # noqa: E501\n\n    Args:\n        alpha: scale factor of model width. Default: 1.\n        in_channels: number the channels of the input. Default: 3.\n        num_classes: number of classification classes. Default: 1000.\n    \"\"\"\n\n    def __init__(\n        self,\n        alpha: float = 1.0,\n        in_channels: int = 3,\n        num_classes: int = 1000,\n    ) -&gt; None:\n        super().__init__()\n        input_channels = int(32 * alpha)\n        # Setting of depth-wise separable conv\n        # c: number of output channel\n        # s: stride of depth-wise conv\n        block_setting = [\n            # c, s\n            [64, 1],\n            [128, 2],\n            [128, 1],\n            [256, 2],\n            [256, 1],\n            [512, 2],\n            [512, 1],\n            [512, 1],\n            [512, 1],\n            [512, 1],\n            [512, 1],\n            [1024, 2],\n            [1024, 1],\n        ]\n\n        features = [\n            nn.Conv2d(in_channels, input_channels, 3, 2, pad_mode=\"pad\", padding=1, has_bias=False),\n            nn.BatchNorm2d(input_channels),\n            nn.ReLU(),\n        ]\n        for c, s in block_setting:\n            output_channel = int(c * alpha)\n            features.append(depthwise_separable_conv(input_channels, output_channel, s))\n            input_channels = output_channel\n        self.features = nn.SequentialCell(features)\n\n        self.pool = GlobalAvgPooling()\n        self.classifier = nn.Dense(input_channels, num_classes)\n        self._initialize_weights()\n\n    def _initialize_weights(self) -&gt; None:\n        \"\"\"Initialize weights for cells.\"\"\"\n        for _, cell in self.cells_and_names():\n            if isinstance(cell, nn.Conv2d):\n                cell.weight.set_data(init.initializer(init.XavierUniform(), cell.weight.shape, cell.weight.dtype))\n            if isinstance(cell, nn.Dense):\n                cell.weight.set_data(init.initializer(init.TruncatedNormal(), cell.weight.shape, cell.weight.dtype))\n\n    def forward_features(self, x: Tensor) -&gt; Tensor:\n        x = self.features(x)\n        return x\n\n    def forward_head(self, x: Tensor) -&gt; Tensor:\n        x = self.pool(x)\n        x = self.classifier(x)\n        return x\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n</code></pre>"},{"location":"reference/models/#mindcv.models.mobilenetv1.mobilenet_v1_025","title":"<code>mindcv.models.mobilenetv1.mobilenet_v1_025(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get MobileNetV1 model with width scaled by 0.25. Refer to the base class <code>models.MobileNetV1</code> for more details.</p> Source code in <code>mindcv\\models\\mobilenetv1.py</code> <pre><code>@register_model\ndef mobilenet_v1_025(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; MobileNetV1:\n    \"\"\"Get MobileNetV1 model with width scaled by 0.25.\n    Refer to the base class `models.MobileNetV1` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"mobilenet_v1_025\"]\n    model = MobileNetV1(alpha=0.25, in_channels=in_channels, num_classes=num_classes, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre>"},{"location":"reference/models/#mindcv.models.mobilenetv1.mobilenet_v1_050","title":"<code>mindcv.models.mobilenetv1.mobilenet_v1_050(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get MobileNetV1 model with width scaled by 0.5. Refer to the base class <code>models.MobileNetV1</code> for more details.</p> Source code in <code>mindcv\\models\\mobilenetv1.py</code> <pre><code>@register_model\ndef mobilenet_v1_050(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; MobileNetV1:\n    \"\"\"Get MobileNetV1 model with width scaled by 0.5.\n    Refer to the base class `models.MobileNetV1` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"mobilenet_v1_050\"]\n    model = MobileNetV1(alpha=0.5, in_channels=in_channels, num_classes=num_classes, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre>"},{"location":"reference/models/#mindcv.models.mobilenetv1.mobilenet_v1_075","title":"<code>mindcv.models.mobilenetv1.mobilenet_v1_075(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get MobileNetV1 model with width scaled by 0.75. Refer to the base class <code>models.MobileNetV1</code> for more details.</p> Source code in <code>mindcv\\models\\mobilenetv1.py</code> <pre><code>@register_model\ndef mobilenet_v1_075(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; MobileNetV1:\n    \"\"\"Get MobileNetV1 model with width scaled by 0.75.\n    Refer to the base class `models.MobileNetV1` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"mobilenet_v1_075\"]\n    model = MobileNetV1(alpha=0.75, in_channels=in_channels, num_classes=num_classes, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre>"},{"location":"reference/models/#mindcv.models.mobilenetv1.mobilenet_v1_100","title":"<code>mindcv.models.mobilenetv1.mobilenet_v1_100(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get MobileNetV1 model without width scaling. Refer to the base class <code>models.MobileNetV1</code> for more details.</p> Source code in <code>mindcv\\models\\mobilenetv1.py</code> <pre><code>@register_model\ndef mobilenet_v1_100(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; MobileNetV1:\n    \"\"\"Get MobileNetV1 model without width scaling.\n    Refer to the base class `models.MobileNetV1` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"mobilenet_v1_100\"]\n    model = MobileNetV1(alpha=1.0, in_channels=in_channels, num_classes=num_classes, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre>"},{"location":"reference/models/#mobilenetv2","title":"mobilenetv2","text":""},{"location":"reference/models/#mindcv.models.mobilenetv2","title":"<code>mindcv.models.mobilenetv2</code>","text":"<p>MindSpore implementation of <code>MobileNetV2</code>. Refer to MobileNetV2: Inverted Residuals and Linear Bottlenecks.</p>"},{"location":"reference/models/#mindcv.models.mobilenetv2.InvertedResidual","title":"<code>mindcv.models.mobilenetv2.InvertedResidual</code>","text":"<p>               Bases: <code>Cell</code></p> <p>Inverted Residual Block of MobileNetV2</p> Source code in <code>mindcv\\models\\mobilenetv2.py</code> <pre><code>class InvertedResidual(nn.Cell):\n    \"\"\"Inverted Residual Block of MobileNetV2\"\"\"\n\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        stride: int,\n        expand_ratio: int,\n    ) -&gt; None:\n        super().__init__()\n        assert stride in [1, 2]\n        hidden_dim = round(in_channels * expand_ratio)\n        self.use_res_connect = stride == 1 and in_channels == out_channels\n\n        layers = []\n        if expand_ratio != 1:\n            # pw\n            layers.extend([\n                nn.Conv2d(in_channels, hidden_dim, 1, 1, pad_mode=\"pad\", padding=0, has_bias=False),\n                nn.BatchNorm2d(hidden_dim),\n                nn.ReLU6()\n            ])\n        layers.extend([\n            # dw\n            nn.Conv2d(hidden_dim, hidden_dim, 3, stride, pad_mode=\"pad\", padding=1, group=hidden_dim, has_bias=False),\n            nn.BatchNorm2d(hidden_dim),\n            nn.ReLU6(),\n            # pw-linear\n            nn.Conv2d(hidden_dim, out_channels, 1, 1, pad_mode=\"pad\", padding=0, has_bias=False),\n            nn.BatchNorm2d(out_channels),\n        ])\n        self.layers = nn.SequentialCell(layers)\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        if self.use_res_connect:\n            return x + self.layers(x)\n        return self.layers(x)\n</code></pre>"},{"location":"reference/models/#mindcv.models.mobilenetv2.MobileNetV2","title":"<code>mindcv.models.mobilenetv2.MobileNetV2</code>","text":"<p>               Bases: <code>Cell</code></p> <p>MobileNetV2 model class, based on <code>\"MobileNetV2: Inverted Residuals and Linear Bottlenecks\" &lt;https://arxiv.org/abs/1801.04381&gt;</code>_</p> PARAMETER DESCRIPTION <code>alpha</code> <p>scale factor of model width. Default: 1.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>round_nearest</code> <p>divisor of make divisible function. Default: 8.</p> <p> TYPE: <code>int</code> DEFAULT: <code>8</code> </p> <code>in_channels</code> <p>number the channels of the input. Default: 3.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>num_classes</code> <p>number of classification classes. Default: 1000.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1000</code> </p> Source code in <code>mindcv\\models\\mobilenetv2.py</code> <pre><code>class MobileNetV2(nn.Cell):\n    r\"\"\"MobileNetV2 model class, based on\n    `\"MobileNetV2: Inverted Residuals and Linear Bottlenecks\" &lt;https://arxiv.org/abs/1801.04381&gt;`_\n\n    Args:\n        alpha: scale factor of model width. Default: 1.\n        round_nearest: divisor of make divisible function. Default: 8.\n        in_channels: number the channels of the input. Default: 3.\n        num_classes: number of classification classes. Default: 1000.\n    \"\"\"\n\n    def __init__(\n        self,\n        alpha: float = 1.0,\n        round_nearest: int = 8,\n        in_channels: int = 3,\n        num_classes: int = 1000,\n    ) -&gt; None:\n        super().__init__()\n        input_channels = make_divisible(32 * alpha, round_nearest)\n        # Setting of inverted residual blocks.\n        # t: The expansion factor.\n        # c: Number of output channel.\n        # n: Number of block.\n        # s: First block stride.\n        inverted_residual_setting = [\n            # t, c, n, s\n            [1, 16, 1, 1],\n            [6, 24, 2, 2],\n            [6, 32, 3, 2],\n            [6, 64, 4, 2],\n            [6, 96, 3, 1],\n            [6, 160, 3, 2],\n            [6, 320, 1, 1],\n        ]\n        last_channels = make_divisible(1280 * max(1.0, alpha), round_nearest)\n\n        # Building stem conv layer.\n        features = [\n            nn.Conv2d(in_channels, input_channels, 3, 2, pad_mode=\"pad\", padding=1, has_bias=False),\n            nn.BatchNorm2d(input_channels),\n            nn.ReLU6(),\n        ]\n\n        total_reduction = 2\n        self.feature_info = []\n        self.flatten_sequential = True\n        self.feature_info.append(dict(chs=input_channels, reduction=total_reduction,\n                                      name=f'features.{len(features) - 1}'))\n\n        # Building inverted residual blocks.\n        for t, c, n, s in inverted_residual_setting:\n            output_channel = make_divisible(c * alpha, round_nearest)\n            for i in range(n):\n                stride = s if i == 0 else 1\n                features.append(InvertedResidual(input_channels, output_channel, stride, expand_ratio=t))\n                input_channels = output_channel\n\n                total_reduction *= stride\n                self.feature_info.append(dict(chs=output_channel, reduction=total_reduction,\n                                              name=f'features.{len(features) - 1}'))\n\n        # Building last point-wise layers.\n        features.extend([\n            nn.Conv2d(input_channels, last_channels, 1, 1, pad_mode=\"pad\", padding=0, has_bias=False),\n            nn.BatchNorm2d(last_channels),\n            nn.ReLU6(),\n        ])\n\n        self.feature_info.append(dict(chs=last_channels, reduction=total_reduction,\n                                      name=f'features.{len(features) - 1}'))\n\n        self.features = nn.SequentialCell(features)\n\n        self.pool = GlobalAvgPooling()\n        self.classifier = nn.SequentialCell([\n            Dropout(p=0.2),  # confirmed by paper authors\n            nn.Dense(last_channels, num_classes),\n        ])\n        self._initialize_weights()\n\n    def _initialize_weights(self) -&gt; None:\n        \"\"\"Initialize weights for cells.\"\"\"\n        for _, cell in self.cells_and_names():\n            if isinstance(cell, nn.Conv2d):\n                n = cell.kernel_size[0] * cell.kernel_size[1] * cell.out_channels\n                cell.weight.set_data(\n                    init.initializer(init.Normal(sigma=math.sqrt(2. / n), mean=0.0),\n                                     cell.weight.shape, cell.weight.dtype))\n                if cell.bias is not None:\n                    cell.bias.set_data(init.initializer(\"zeros\", cell.bias.shape, cell.bias.dtype))\n            elif isinstance(cell, nn.BatchNorm2d):\n                cell.gamma.set_data(init.initializer(\"ones\", cell.gamma.shape, cell.gamma.dtype))\n                cell.beta.set_data(init.initializer(\"zeros\", cell.beta.shape, cell.beta.dtype))\n            elif isinstance(cell, nn.Dense):\n                cell.weight.set_data(\n                    init.initializer(init.Normal(sigma=0.01, mean=0.0), cell.weight.shape, cell.weight.dtype))\n                if cell.bias is not None:\n                    cell.bias.set_data(init.initializer(\"zeros\", cell.bias.shape, cell.bias.dtype))\n\n    def forward_features(self, x: Tensor) -&gt; Tensor:\n        x = self.features(x)\n        return x\n\n    def forward_head(self, x: Tensor) -&gt; Tensor:\n        x = self.pool(x)\n        x = self.classifier(x)\n        return x\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n</code></pre>"},{"location":"reference/models/#mindcv.models.mobilenetv2.mobilenet_v2_035_128","title":"<code>mindcv.models.mobilenetv2.mobilenet_v2_035_128(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get MobileNetV2 model with width scaled by 0.35 and input image size of 128. Refer to the base class <code>models.MobileNetV2</code> for more details.</p> Source code in <code>mindcv\\models\\mobilenetv2.py</code> <pre><code>@register_model\ndef mobilenet_v2_035_128(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; MobileNetV2:\n    \"\"\"Get MobileNetV2 model with width scaled by 0.35 and input image size of 128.\n    Refer to the base class `models.MobileNetV2` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"mobilenet_v2_035_128\"]\n    model_args = dict(alpha=0.35, num_classes=num_classes, in_channels=in_channels, **kwargs)\n    return _create_mobilenet_v2(pretrained, **dict(default_cfg=default_cfg, **model_args))\n</code></pre>"},{"location":"reference/models/#mindcv.models.mobilenetv2.mobilenet_v2_035_160","title":"<code>mindcv.models.mobilenetv2.mobilenet_v2_035_160(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get MobileNetV2 model with width scaled by 0.35 and input image size of 160. Refer to the base class <code>models.MobileNetV2</code> for more details.</p> Source code in <code>mindcv\\models\\mobilenetv2.py</code> <pre><code>@register_model\ndef mobilenet_v2_035_160(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; MobileNetV2:\n    \"\"\"Get MobileNetV2 model with width scaled by 0.35 and input image size of 160.\n    Refer to the base class `models.MobileNetV2` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"mobilenet_v2_035_160\"]\n    model_args = dict(alpha=0.35, num_classes=num_classes, in_channels=in_channels, **kwargs)\n    return _create_mobilenet_v2(pretrained, **dict(default_cfg=default_cfg, **model_args))\n</code></pre>"},{"location":"reference/models/#mindcv.models.mobilenetv2.mobilenet_v2_035_192","title":"<code>mindcv.models.mobilenetv2.mobilenet_v2_035_192(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get MobileNetV2 model with width scaled by 0.35 and input image size of 192. Refer to the base class <code>models.MobileNetV2</code> for more details.</p> Source code in <code>mindcv\\models\\mobilenetv2.py</code> <pre><code>@register_model\ndef mobilenet_v2_035_192(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; MobileNetV2:\n    \"\"\"Get MobileNetV2 model with width scaled by 0.35 and input image size of 192.\n    Refer to the base class `models.MobileNetV2` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"mobilenet_v2_035_192\"]\n    model_args = dict(alpha=0.35, num_classes=num_classes, in_channels=in_channels, **kwargs)\n    return _create_mobilenet_v2(pretrained, **dict(default_cfg=default_cfg, **model_args))\n</code></pre>"},{"location":"reference/models/#mindcv.models.mobilenetv2.mobilenet_v2_035_224","title":"<code>mindcv.models.mobilenetv2.mobilenet_v2_035_224(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get MobileNetV2 model with width scaled by 0.35 and input image size of 224. Refer to the base class <code>models.MobileNetV2</code> for more details.</p> Source code in <code>mindcv\\models\\mobilenetv2.py</code> <pre><code>@register_model\ndef mobilenet_v2_035_224(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; MobileNetV2:\n    \"\"\"Get MobileNetV2 model with width scaled by 0.35 and input image size of 224.\n    Refer to the base class `models.MobileNetV2` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"mobilenet_v2_035_224\"]\n    model_args = dict(alpha=0.35, num_classes=num_classes, in_channels=in_channels, **kwargs)\n    return _create_mobilenet_v2(pretrained, **dict(default_cfg=default_cfg, **model_args))\n</code></pre>"},{"location":"reference/models/#mindcv.models.mobilenetv2.mobilenet_v2_035_96","title":"<code>mindcv.models.mobilenetv2.mobilenet_v2_035_96(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get MobileNetV2 model with width scaled by 0.35 and input image size of 96. Refer to the base class <code>models.MobileNetV2</code> for more details.</p> Source code in <code>mindcv\\models\\mobilenetv2.py</code> <pre><code>@register_model\ndef mobilenet_v2_035_96(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; MobileNetV2:\n    \"\"\"Get MobileNetV2 model with width scaled by 0.35 and input image size of 96.\n    Refer to the base class `models.MobileNetV2` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"mobilenet_v2_035_96\"]\n    model_args = dict(alpha=0.35, num_classes=num_classes, in_channels=in_channels, **kwargs)\n    return _create_mobilenet_v2(pretrained, **dict(default_cfg=default_cfg, **model_args))\n</code></pre>"},{"location":"reference/models/#mindcv.models.mobilenetv2.mobilenet_v2_050_128","title":"<code>mindcv.models.mobilenetv2.mobilenet_v2_050_128(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get MobileNetV2 model with width scaled by 0.5 and input image size of 128. Refer to the base class <code>models.MobileNetV2</code> for more details.</p> Source code in <code>mindcv\\models\\mobilenetv2.py</code> <pre><code>@register_model\ndef mobilenet_v2_050_128(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; MobileNetV2:\n    \"\"\"Get MobileNetV2 model with width scaled by 0.5 and input image size of 128.\n    Refer to the base class `models.MobileNetV2` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"mobilenet_v2_050_128\"]\n    model_args = dict(alpha=0.5, num_classes=num_classes, in_channels=in_channels, **kwargs)\n    return _create_mobilenet_v2(pretrained, **dict(default_cfg=default_cfg, **model_args))\n</code></pre>"},{"location":"reference/models/#mindcv.models.mobilenetv2.mobilenet_v2_050_160","title":"<code>mindcv.models.mobilenetv2.mobilenet_v2_050_160(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get MobileNetV2 model with width scaled by 0.5 and input image size of 160. Refer to the base class <code>models.MobileNetV2</code> for more details.</p> Source code in <code>mindcv\\models\\mobilenetv2.py</code> <pre><code>@register_model\ndef mobilenet_v2_050_160(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; MobileNetV2:\n    \"\"\"Get MobileNetV2 model with width scaled by 0.5 and input image size of 160.\n    Refer to the base class `models.MobileNetV2` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"mobilenet_v2_050_160\"]\n    model_args = dict(alpha=0.5, num_classes=num_classes, in_channels=in_channels, **kwargs)\n    return _create_mobilenet_v2(pretrained, **dict(default_cfg=default_cfg, **model_args))\n</code></pre>"},{"location":"reference/models/#mindcv.models.mobilenetv2.mobilenet_v2_050_192","title":"<code>mindcv.models.mobilenetv2.mobilenet_v2_050_192(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get MobileNetV2 model with width scaled by 0.5 and input image size of 192. Refer to the base class <code>models.MobileNetV2</code> for more details.</p> Source code in <code>mindcv\\models\\mobilenetv2.py</code> <pre><code>@register_model\ndef mobilenet_v2_050_192(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; MobileNetV2:\n    \"\"\"Get MobileNetV2 model with width scaled by 0.5 and input image size of 192.\n    Refer to the base class `models.MobileNetV2` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"mobilenet_v2_050_192\"]\n    model_args = dict(alpha=0.5, num_classes=num_classes, in_channels=in_channels, **kwargs)\n    return _create_mobilenet_v2(pretrained, **dict(default_cfg=default_cfg, **model_args))\n</code></pre>"},{"location":"reference/models/#mindcv.models.mobilenetv2.mobilenet_v2_050_224","title":"<code>mindcv.models.mobilenetv2.mobilenet_v2_050_224(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get MobileNetV2 model with width scaled by 0.5 and input image size of 224. Refer to the base class <code>models.MobileNetV2</code> for more details.</p> Source code in <code>mindcv\\models\\mobilenetv2.py</code> <pre><code>@register_model\ndef mobilenet_v2_050_224(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; MobileNetV2:\n    \"\"\"Get MobileNetV2 model with width scaled by 0.5 and input image size of 224.\n    Refer to the base class `models.MobileNetV2` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"mobilenet_v2_050_224\"]\n    model_args = dict(alpha=0.5, num_classes=num_classes, in_channels=in_channels, **kwargs)\n    return _create_mobilenet_v2(pretrained, **dict(default_cfg=default_cfg, **model_args))\n</code></pre>"},{"location":"reference/models/#mindcv.models.mobilenetv2.mobilenet_v2_050_96","title":"<code>mindcv.models.mobilenetv2.mobilenet_v2_050_96(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get MobileNetV2 model with width scaled by 0.5 and input image size of 96. Refer to the base class <code>models.MobileNetV2</code> for more details.</p> Source code in <code>mindcv\\models\\mobilenetv2.py</code> <pre><code>@register_model\ndef mobilenet_v2_050_96(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; MobileNetV2:\n    \"\"\"Get MobileNetV2 model with width scaled by 0.5 and input image size of 96.\n    Refer to the base class `models.MobileNetV2` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"mobilenet_v2_050_96\"]\n    model_args = dict(alpha=0.5, num_classes=num_classes, in_channels=in_channels, **kwargs)\n    return _create_mobilenet_v2(pretrained, **dict(default_cfg=default_cfg, **model_args))\n</code></pre>"},{"location":"reference/models/#mindcv.models.mobilenetv2.mobilenet_v2_075","title":"<code>mindcv.models.mobilenetv2.mobilenet_v2_075(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get MobileNetV2 model with width scaled by 0.75 and input image size of 224. Refer to the base class <code>models.MobileNetV2</code> for more details.</p> Source code in <code>mindcv\\models\\mobilenetv2.py</code> <pre><code>@register_model\ndef mobilenet_v2_075(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; MobileNetV2:\n    \"\"\"Get MobileNetV2 model with width scaled by 0.75 and input image size of 224.\n    Refer to the base class `models.MobileNetV2` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"mobilenet_v2_075\"]\n    model_args = dict(alpha=0.75, num_classes=num_classes, in_channels=in_channels, **kwargs)\n    return _create_mobilenet_v2(pretrained, **dict(default_cfg=default_cfg, **model_args))\n</code></pre>"},{"location":"reference/models/#mindcv.models.mobilenetv2.mobilenet_v2_075_128","title":"<code>mindcv.models.mobilenetv2.mobilenet_v2_075_128(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get MobileNetV2 model with width scaled by 0.75 and input image size of 128. Refer to the base class <code>models.MobileNetV2</code> for more details.</p> Source code in <code>mindcv\\models\\mobilenetv2.py</code> <pre><code>@register_model\ndef mobilenet_v2_075_128(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; MobileNetV2:\n    \"\"\"Get MobileNetV2 model with width scaled by 0.75 and input image size of 128.\n    Refer to the base class `models.MobileNetV2` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"mobilenet_v2_075_128\"]\n    model_args = dict(alpha=0.75, num_classes=num_classes, in_channels=in_channels, **kwargs)\n    return _create_mobilenet_v2(pretrained, **dict(default_cfg=default_cfg, **model_args))\n</code></pre>"},{"location":"reference/models/#mindcv.models.mobilenetv2.mobilenet_v2_075_160","title":"<code>mindcv.models.mobilenetv2.mobilenet_v2_075_160(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get MobileNetV2 model with width scaled by 0.75 and input image size of 160. Refer to the base class <code>models.MobileNetV2</code> for more details.</p> Source code in <code>mindcv\\models\\mobilenetv2.py</code> <pre><code>@register_model\ndef mobilenet_v2_075_160(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; MobileNetV2:\n    \"\"\"Get MobileNetV2 model with width scaled by 0.75 and input image size of 160.\n    Refer to the base class `models.MobileNetV2` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"mobilenet_v2_075_160\"]\n    model_args = dict(alpha=0.75, num_classes=num_classes, in_channels=in_channels, **kwargs)\n    return _create_mobilenet_v2(pretrained, **dict(default_cfg=default_cfg, **model_args))\n</code></pre>"},{"location":"reference/models/#mindcv.models.mobilenetv2.mobilenet_v2_075_192","title":"<code>mindcv.models.mobilenetv2.mobilenet_v2_075_192(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get MobileNetV2 model with width scaled by 0.75 and input image size of 192. Refer to the base class <code>models.MobileNetV2</code> for more details.</p> Source code in <code>mindcv\\models\\mobilenetv2.py</code> <pre><code>@register_model\ndef mobilenet_v2_075_192(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; MobileNetV2:\n    \"\"\"Get MobileNetV2 model with width scaled by 0.75 and input image size of 192.\n    Refer to the base class `models.MobileNetV2` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"mobilenet_v2_075_192\"]\n    model_args = dict(alpha=0.75, num_classes=num_classes, in_channels=in_channels, **kwargs)\n    return _create_mobilenet_v2(pretrained, **dict(default_cfg=default_cfg, **model_args))\n</code></pre>"},{"location":"reference/models/#mindcv.models.mobilenetv2.mobilenet_v2_075_96","title":"<code>mindcv.models.mobilenetv2.mobilenet_v2_075_96(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get MobileNetV2 model with width scaled by 0.75 and input image size of 96. Refer to the base class <code>models.MobileNetV2</code> for more details.</p> Source code in <code>mindcv\\models\\mobilenetv2.py</code> <pre><code>@register_model\ndef mobilenet_v2_075_96(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; MobileNetV2:\n    \"\"\"Get MobileNetV2 model with width scaled by 0.75 and input image size of 96.\n    Refer to the base class `models.MobileNetV2` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"mobilenet_v2_075_96\"]\n    model_args = dict(alpha=0.75, num_classes=num_classes, in_channels=in_channels, **kwargs)\n    return _create_mobilenet_v2(pretrained, **dict(default_cfg=default_cfg, **model_args))\n</code></pre>"},{"location":"reference/models/#mindcv.models.mobilenetv2.mobilenet_v2_100","title":"<code>mindcv.models.mobilenetv2.mobilenet_v2_100(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get MobileNetV2 model without width scaling and input image size of 224. Refer to the base class <code>models.MobileNetV2</code> for more details.</p> Source code in <code>mindcv\\models\\mobilenetv2.py</code> <pre><code>@register_model\ndef mobilenet_v2_100(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; MobileNetV2:\n    \"\"\"Get MobileNetV2 model without width scaling and input image size of 224.\n    Refer to the base class `models.MobileNetV2` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"mobilenet_v2_100\"]\n    model_args = dict(alpha=1.0, num_classes=num_classes, in_channels=in_channels, **kwargs)\n    return _create_mobilenet_v2(pretrained, **dict(default_cfg=default_cfg, **model_args))\n</code></pre>"},{"location":"reference/models/#mindcv.models.mobilenetv2.mobilenet_v2_100_128","title":"<code>mindcv.models.mobilenetv2.mobilenet_v2_100_128(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get MobileNetV2 model without width scaling and input image size of 128. Refer to the base class <code>models.MobileNetV2</code> for more details.</p> Source code in <code>mindcv\\models\\mobilenetv2.py</code> <pre><code>@register_model\ndef mobilenet_v2_100_128(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; MobileNetV2:\n    \"\"\"Get MobileNetV2 model without width scaling and input image size of 128.\n    Refer to the base class `models.MobileNetV2` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"mobilenet_v2_100_128\"]\n    model_args = dict(alpha=1.0, num_classes=num_classes, in_channels=in_channels, **kwargs)\n    return _create_mobilenet_v2(pretrained, **dict(default_cfg=default_cfg, **model_args))\n</code></pre>"},{"location":"reference/models/#mindcv.models.mobilenetv2.mobilenet_v2_100_160","title":"<code>mindcv.models.mobilenetv2.mobilenet_v2_100_160(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get MobileNetV2 model without width scaling and input image size of 160. Refer to the base class <code>models.MobileNetV2</code> for more details.</p> Source code in <code>mindcv\\models\\mobilenetv2.py</code> <pre><code>@register_model\ndef mobilenet_v2_100_160(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; MobileNetV2:\n    \"\"\"Get MobileNetV2 model without width scaling and input image size of 160.\n    Refer to the base class `models.MobileNetV2` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"mobilenet_v2_100_160\"]\n    model_args = dict(alpha=1.0, num_classes=num_classes, in_channels=in_channels, **kwargs)\n    return _create_mobilenet_v2(pretrained, **dict(default_cfg=default_cfg, **model_args))\n</code></pre>"},{"location":"reference/models/#mindcv.models.mobilenetv2.mobilenet_v2_100_192","title":"<code>mindcv.models.mobilenetv2.mobilenet_v2_100_192(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get MobileNetV2 model without width scaling and input image size of 192. Refer to the base class <code>models.MobileNetV2</code> for more details.</p> Source code in <code>mindcv\\models\\mobilenetv2.py</code> <pre><code>@register_model\ndef mobilenet_v2_100_192(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; MobileNetV2:\n    \"\"\"Get MobileNetV2 model without width scaling and input image size of 192.\n    Refer to the base class `models.MobileNetV2` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"mobilenet_v2_100_192\"]\n    model_args = dict(alpha=1.0, num_classes=num_classes, in_channels=in_channels, **kwargs)\n    return _create_mobilenet_v2(pretrained, **dict(default_cfg=default_cfg, **model_args))\n</code></pre>"},{"location":"reference/models/#mindcv.models.mobilenetv2.mobilenet_v2_100_96","title":"<code>mindcv.models.mobilenetv2.mobilenet_v2_100_96(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get MobileNetV2 model without width scaling and input image size of 96. Refer to the base class <code>models.MobileNetV2</code> for more details.</p> Source code in <code>mindcv\\models\\mobilenetv2.py</code> <pre><code>@register_model\ndef mobilenet_v2_100_96(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; MobileNetV2:\n    \"\"\"Get MobileNetV2 model without width scaling and input image size of 96.\n    Refer to the base class `models.MobileNetV2` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"mobilenet_v2_100_96\"]\n    model_args = dict(alpha=1.0, num_classes=num_classes, in_channels=in_channels, **kwargs)\n    return _create_mobilenet_v2(pretrained, **dict(default_cfg=default_cfg, **model_args))\n</code></pre>"},{"location":"reference/models/#mindcv.models.mobilenetv2.mobilenet_v2_130_224","title":"<code>mindcv.models.mobilenetv2.mobilenet_v2_130_224(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get MobileNetV2 model with width scaled by 1.3 and input image size of 224. Refer to the base class <code>models.MobileNetV2</code> for more details.</p> Source code in <code>mindcv\\models\\mobilenetv2.py</code> <pre><code>@register_model\ndef mobilenet_v2_130_224(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; MobileNetV2:\n    \"\"\"Get MobileNetV2 model with width scaled by 1.3 and input image size of 224.\n    Refer to the base class `models.MobileNetV2` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"mobilenet_v2_130_224\"]\n    model_args = dict(alpha=1.3, num_classes=num_classes, in_channels=in_channels, **kwargs)\n    return _create_mobilenet_v2(pretrained, **dict(default_cfg=default_cfg, **model_args))\n</code></pre>"},{"location":"reference/models/#mindcv.models.mobilenetv2.mobilenet_v2_140","title":"<code>mindcv.models.mobilenetv2.mobilenet_v2_140(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get MobileNetV2 model with width scaled by 1.4 and input image size of 224. Refer to the base class <code>models.MobileNetV2</code> for more details.</p> Source code in <code>mindcv\\models\\mobilenetv2.py</code> <pre><code>@register_model\ndef mobilenet_v2_140(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; MobileNetV2:\n    \"\"\"Get MobileNetV2 model with width scaled by 1.4 and input image size of 224.\n    Refer to the base class `models.MobileNetV2` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"mobilenet_v2_140\"]\n    model_args = dict(alpha=1.4, num_classes=num_classes, in_channels=in_channels, **kwargs)\n    return _create_mobilenet_v2(pretrained, **dict(default_cfg=default_cfg, **model_args))\n</code></pre>"},{"location":"reference/models/#mobilenetv3","title":"mobilenetv3","text":""},{"location":"reference/models/#mindcv.models.mobilenetv3","title":"<code>mindcv.models.mobilenetv3</code>","text":"<p>MindSpore implementation of <code>MobileNetV3</code>. Refer to Searching for MobileNetV3.</p>"},{"location":"reference/models/#mindcv.models.mobilenetv3.Bottleneck","title":"<code>mindcv.models.mobilenetv3.Bottleneck</code>","text":"<p>               Bases: <code>Cell</code></p> <p>Bottleneck Block of MobilenetV3. depth-wise separable convolutions + inverted residual + squeeze excitation</p> Source code in <code>mindcv\\models\\mobilenetv3.py</code> <pre><code>class Bottleneck(nn.Cell):\n    \"\"\"Bottleneck Block of MobilenetV3. depth-wise separable convolutions + inverted residual + squeeze excitation\"\"\"\n\n    def __init__(\n        self,\n        in_channels: int,\n        mid_channels: int,\n        out_channels: int,\n        kernel_size: int,\n        stride: int = 1,\n        activation: str = \"relu\",\n        use_se: bool = False,\n    ) -&gt; None:\n        super().__init__()\n        self.use_se = use_se\n        self.use_res_connect = stride == 1 and in_channels == out_channels\n        assert activation in [\"relu\", \"hswish\"]\n        self.activation = nn.HSwish if activation == \"hswish\" else nn.ReLU\n\n        layers = []\n        # Expand.\n        if in_channels != mid_channels:\n            layers.extend([\n                nn.Conv2d(in_channels, mid_channels, 1, 1, pad_mode=\"pad\", padding=0, has_bias=False),\n                nn.BatchNorm2d(mid_channels),\n                self.activation(),\n            ])\n        # DepthWise.\n        layers.extend([\n            nn.Conv2d(mid_channels, mid_channels, kernel_size, stride,\n                      pad_mode=\"same\", group=mid_channels, has_bias=False),\n            nn.BatchNorm2d(mid_channels),\n            self.activation(),\n        ])\n        # SqueezeExcitation.\n        if use_se:\n            layers.append(\n                SqueezeExcite(mid_channels, 1.0 / 4, act_layer=nn.ReLU, gate_layer=nn.HSigmoid)\n            )\n        # Project.\n        layers.extend([\n            nn.Conv2d(mid_channels, out_channels, 1, 1, pad_mode=\"pad\", padding=0, has_bias=False),\n            nn.BatchNorm2d(out_channels),\n        ])\n        self.layers = nn.SequentialCell(layers)\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        if self.use_res_connect:\n            return x + self.layers(x)\n        return self.layers(x)\n</code></pre>"},{"location":"reference/models/#mindcv.models.mobilenetv3.MobileNetV3","title":"<code>mindcv.models.mobilenetv3.MobileNetV3</code>","text":"<p>               Bases: <code>Cell</code></p> <p>MobileNetV3 model class, based on <code>\"Searching for MobileNetV3\" &lt;https://arxiv.org/abs/1905.02244&gt;</code>_</p> PARAMETER DESCRIPTION <code>arch</code> <p>size of the architecture. 'small' or 'large'.</p> <p> TYPE: <code>str</code> </p> <code>alpha</code> <p>scale factor of model width. Default: 1.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>round_nearest</code> <p>divisor of make divisible function. Default: 8.</p> <p> TYPE: <code>int</code> DEFAULT: <code>8</code> </p> <code>in_channels</code> <p>number the channels of the input. Default: 3.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>num_classes</code> <p>number of classification classes. Default: 1000.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1000</code> </p> Source code in <code>mindcv\\models\\mobilenetv3.py</code> <pre><code>class MobileNetV3(nn.Cell):\n    r\"\"\"MobileNetV3 model class, based on\n    `\"Searching for MobileNetV3\" &lt;https://arxiv.org/abs/1905.02244&gt;`_\n\n    Args:\n        arch: size of the architecture. 'small' or 'large'.\n        alpha: scale factor of model width. Default: 1.\n        round_nearest: divisor of make divisible function. Default: 8.\n        in_channels: number the channels of the input. Default: 3.\n        num_classes: number of classification classes. Default: 1000.\n    \"\"\"\n\n    def __init__(\n        self,\n        arch: str,\n        alpha: float = 1.0,\n        round_nearest: int = 8,\n        in_channels: int = 3,\n        num_classes: int = 1000,\n    ) -&gt; None:\n        super().__init__()\n        input_channels = make_divisible(16 * alpha, round_nearest)\n        # Setting of bottleneck blocks. ex: [k, e, c, se, nl, s]\n        # k: kernel size of depth-wise conv\n        # e: expansion size\n        # c: number of output channel\n        # se: whether there is a Squeeze-And-Excite in that block\n        # nl: type of non-linearity used\n        # s: stride of depth-wise conv\n        if arch == \"large\":\n            bottleneck_setting = [\n                [3, 16, 16, False, \"relu\", 1],\n                [3, 64, 24, False, \"relu\", 2],\n                [3, 72, 24, False, \"relu\", 1],\n                [5, 72, 40, True, \"relu\", 2],\n                [5, 120, 40, True, \"relu\", 1],\n                [5, 120, 40, True, \"relu\", 1],\n                [3, 240, 80, False, \"hswish\", 2],\n                [3, 200, 80, False, \"hswish\", 1],\n                [3, 184, 80, False, \"hswish\", 1],\n                [3, 184, 80, False, \"hswish\", 1],\n                [3, 480, 112, True, \"hswish\", 1],\n                [3, 672, 112, True, \"hswish\", 1],\n                [5, 672, 160, True, \"hswish\", 2],\n                [5, 960, 160, True, \"hswish\", 1],\n                [5, 960, 160, True, \"hswish\", 1],\n            ]\n            last_channels = make_divisible(alpha * 1280, round_nearest)\n        elif arch == \"small\":\n            bottleneck_setting = [\n                [3, 16, 16, True, \"relu\", 2],\n                [3, 72, 24, False, \"relu\", 2],\n                [3, 88, 24, False, \"relu\", 1],\n                [5, 96, 40, True, \"hswish\", 2],\n                [5, 240, 40, True, \"hswish\", 1],\n                [5, 240, 40, True, \"hswish\", 1],\n                [5, 120, 48, True, \"hswish\", 1],\n                [5, 144, 48, True, \"hswish\", 1],\n                [5, 288, 96, True, \"hswish\", 2],\n                [5, 576, 96, True, \"hswish\", 1],\n                [5, 576, 96, True, \"hswish\", 1],\n            ]\n            last_channels = make_divisible(alpha * 1024, round_nearest)\n        else:\n            raise ValueError(f\"Unsupported model type {arch}\")\n\n        # Building stem conv layer.\n        features = [\n            nn.Conv2d(in_channels, input_channels, 3, 2, pad_mode=\"pad\", padding=1, has_bias=False),\n            nn.BatchNorm2d(input_channels),\n            nn.HSwish(),\n        ]\n\n        total_reduction = 2\n        self.feature_info = [dict(chs=input_channels, reduction=total_reduction, name=f'features.{len(features) - 1}')]\n\n        # Building bottleneck blocks.\n        for k, e, c, se, nl, s in bottleneck_setting:\n            exp_channels = make_divisible(alpha * e, round_nearest)\n            output_channels = make_divisible(alpha * c, round_nearest)\n            features.append(Bottleneck(input_channels, exp_channels, output_channels,\n                                       kernel_size=k, stride=s, activation=nl, use_se=se))\n            input_channels = output_channels\n\n            total_reduction *= s\n            self.feature_info.append(dict(chs=input_channels, reduction=total_reduction,\n                                          name=f'features.{len(features) - 1}'))\n\n        # Building last point-wise conv layers.\n        output_channels = input_channels * 6\n        features.extend([\n            nn.Conv2d(input_channels, output_channels, 1, 1, pad_mode=\"pad\", padding=0, has_bias=False),\n            nn.BatchNorm2d(output_channels),\n            nn.HSwish(),\n        ])\n\n        self.feature_info.append(dict(chs=output_channels, reduction=total_reduction,\n                                      name=f'features.{len(features) - 1}'))\n        self.flatten_sequential = True\n\n        self.features = nn.SequentialCell(features)\n\n        self.pool = GlobalAvgPooling()\n        self.classifier = nn.SequentialCell([\n            nn.Dense(output_channels, last_channels),\n            nn.HSwish(),\n            Dropout(p=0.2),\n            nn.Dense(last_channels, num_classes),\n        ])\n        self._initialize_weights()\n\n    def _initialize_weights(self) -&gt; None:\n        \"\"\"Initialize weights for cells.\"\"\"\n        for _, cell in self.cells_and_names():\n            if isinstance(cell, nn.Conv2d):\n                n = cell.kernel_size[0] * cell.kernel_size[1] * cell.out_channels\n                cell.weight.set_data(\n                    init.initializer(init.Normal(sigma=math.sqrt(2. / n), mean=0.0),\n                                     cell.weight.shape, cell.weight.dtype))\n                if cell.bias is not None:\n                    cell.bias.set_data(init.initializer(\"zeros\", cell.bias.shape, cell.bias.dtype))\n            elif isinstance(cell, nn.BatchNorm2d):\n                cell.gamma.set_data(init.initializer(\"ones\", cell.gamma.shape, cell.gamma.dtype))\n                cell.beta.set_data(init.initializer(\"zeros\", cell.beta.shape, cell.beta.dtype))\n            elif isinstance(cell, nn.Dense):\n                cell.weight.set_data(\n                    init.initializer(init.Normal(sigma=0.01, mean=0.0), cell.weight.shape, cell.weight.dtype))\n                if cell.bias is not None:\n                    cell.bias.set_data(init.initializer(\"zeros\", cell.bias.shape, cell.bias.dtype))\n\n    def forward_features(self, x: Tensor) -&gt; Tensor:\n        x = self.features(x)\n        return x\n\n    def forward_head(self, x: Tensor) -&gt; Tensor:\n        x = self.pool(x)\n        x = self.classifier(x)\n        return x\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n</code></pre>"},{"location":"reference/models/#mindcv.models.mobilenetv3.mobilenet_v3_large_075","title":"<code>mindcv.models.mobilenetv3.mobilenet_v3_large_075(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get large MobileNetV3 model with width scaled by 0.75. Refer to the base class <code>models.MobileNetV3</code> for more details.</p> Source code in <code>mindcv\\models\\mobilenetv3.py</code> <pre><code>@register_model\ndef mobilenet_v3_large_075(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; MobileNetV3:\n    \"\"\"Get large MobileNetV3 model with width scaled by 0.75.\n    Refer to the base class `models.MobileNetV3` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"mobilenet_v3_large_075\"]\n    model_args = dict(arch=\"large\", alpha=0.75, in_channels=in_channels, num_classes=num_classes, **kwargs)\n    return _create_mobilenet_v3(pretrained, **dict(default_cfg=default_cfg, **model_args))\n</code></pre>"},{"location":"reference/models/#mindcv.models.mobilenetv3.mobilenet_v3_large_100","title":"<code>mindcv.models.mobilenetv3.mobilenet_v3_large_100(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get large MobileNetV3 model without width scaling. Refer to the base class <code>models.MobileNetV3</code> for more details.</p> Source code in <code>mindcv\\models\\mobilenetv3.py</code> <pre><code>@register_model\ndef mobilenet_v3_large_100(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; MobileNetV3:\n    \"\"\"Get large MobileNetV3 model without width scaling.\n    Refer to the base class `models.MobileNetV3` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"mobilenet_v3_large_100\"]\n    model_args = dict(arch=\"large\", alpha=1.0, in_channels=in_channels, num_classes=num_classes, **kwargs)\n    return _create_mobilenet_v3(pretrained, **dict(default_cfg=default_cfg, **model_args))\n</code></pre>"},{"location":"reference/models/#mindcv.models.mobilenetv3.mobilenet_v3_small_075","title":"<code>mindcv.models.mobilenetv3.mobilenet_v3_small_075(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get small MobileNetV3 model with width scaled by 0.75. Refer to the base class <code>models.MobileNetV3</code> for more details.</p> Source code in <code>mindcv\\models\\mobilenetv3.py</code> <pre><code>@register_model\ndef mobilenet_v3_small_075(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; MobileNetV3:\n    \"\"\"Get small MobileNetV3 model with width scaled by 0.75.\n    Refer to the base class `models.MobileNetV3` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"mobilenet_v3_small_075\"]\n    model_args = dict(arch=\"small\", alpha=0.75, in_channels=in_channels, num_classes=num_classes, **kwargs)\n    return _create_mobilenet_v3(pretrained, **dict(default_cfg=default_cfg, **model_args))\n</code></pre>"},{"location":"reference/models/#mindcv.models.mobilenetv3.mobilenet_v3_small_100","title":"<code>mindcv.models.mobilenetv3.mobilenet_v3_small_100(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get small MobileNetV3 model without width scaling. Refer to the base class <code>models.MobileNetV3</code> for more details.</p> Source code in <code>mindcv\\models\\mobilenetv3.py</code> <pre><code>@register_model\ndef mobilenet_v3_small_100(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; MobileNetV3:\n    \"\"\"Get small MobileNetV3 model without width scaling.\n    Refer to the base class `models.MobileNetV3` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"mobilenet_v3_small_100\"]\n    model_args = dict(arch=\"small\", alpha=1.0, in_channels=in_channels, num_classes=num_classes, **kwargs)\n    return _create_mobilenet_v3(pretrained, **dict(default_cfg=default_cfg, **model_args))\n</code></pre>"},{"location":"reference/models/#mobilevit","title":"mobilevit","text":""},{"location":"reference/models/#mindcv.models.mobilevit","title":"<code>mindcv.models.mobilevit</code>","text":"<p>MindSpore implementation of <code>MobileViT</code>. Refer to MobileViT\uff1aLight-weight, General-purpose, and Mobile-friendly Vision Transformer.</p>"},{"location":"reference/models/#mindcv.models.mobilevit.ConvLayer","title":"<code>mindcv.models.mobilevit.ConvLayer</code>","text":"<p>               Bases: <code>Cell</code></p> <p>Conv2d + BN + Act</p> Source code in <code>mindcv\\models\\mobilevit.py</code> <pre><code>class ConvLayer(nn.Cell):\n    \"\"\" Conv2d + BN + Act\"\"\"\n    def __init__(self,\n                 in_channels: int,\n                 out_channels: int,\n                 kernel_size: int = 3,\n                 stride: int = 1,\n                 pad_mode: str = \"pad\",\n                 padding: Optional[int] = None,\n                 dilation: int = 1,\n                 groups: int = 1,\n                 norm: Optional[nn.Cell] = nn.BatchNorm2d,\n                 activation: Optional[nn.Cell] = nn.SiLU,\n                 has_bias: Optional[bool] = False) -&gt; None:\n        super().__init__()\n\n        if pad_mode == \"pad\":\n            if padding is None:\n                padding = ((stride - 1) + dilation * (kernel_size - 1)) // 2\n        else:\n            padding = 0\n\n        if has_bias is None:\n            has_bias = norm is None\n\n        layers = [\n            nn.Conv2d(\n                in_channels,\n                out_channels,\n                kernel_size,\n                stride,\n                pad_mode=pad_mode,\n                padding=padding,\n                dilation=dilation,\n                group=groups,\n                has_bias=has_bias)\n        ]\n\n        if norm:\n            layers.append(norm(out_channels, momentum=0.9))\n        if activation:\n            layers.append(activation())\n\n        self.features = nn.SequentialCell(layers)\n\n    def construct(self, x):\n        output = self.features(x)\n        return output\n</code></pre>"},{"location":"reference/models/#mindcv.models.mobilevit.InvertedResidual","title":"<code>mindcv.models.mobilevit.InvertedResidual</code>","text":"<p>               Bases: <code>Cell</code></p> <p>This class implements the inverted residual block, as described in <code>MobileNetv2 &lt;https://arxiv.org/abs/1801.04381&gt;</code>_ paper</p> PARAMETER DESCRIPTION <code>in_channels</code> <p>:math:<code>C_{in}</code> from an expected input of size :math:<code>(N, C_{in}, H_{in}, W_{in})</code></p> <p> TYPE: <code>int</code> </p> <code>out_channels</code> <p>:math:<code>C_{out}</code> from an expected output of size :math:<code>(N, C_{out}, H_{out}, W_{out)</code></p> <p> TYPE: <code>int</code> </p> <code>stride</code> <p>Use convolutions with a stride. Default: 1</p> <p> TYPE: <code>int</code> </p> <code>expand_ratio</code> <p>Expand the input channels by this factor in depth-wise conv</p> <p> TYPE: <code>Union[int, float]</code> </p> <code>skip_connection</code> <p>Use skip-connection. Default: True</p> <p> TYPE: <code>Optional[bool]</code> DEFAULT: <code>True</code> </p> Shape <ul> <li>Input: :math:<code>(N, C_{in}, H_{in}, W_{in})</code></li> <li>Output: :math:<code>(N, C_{out}, H_{out}, W_{out})</code></li> </ul> <p>.. note::     If <code>in_channels =! out_channels</code> and <code>stride &gt; 1</code>, we set <code>skip_connection=False</code></p> Source code in <code>mindcv\\models\\mobilevit.py</code> <pre><code>class InvertedResidual(nn.Cell):\n    \"\"\"\n    This class implements the inverted residual block, as described in\n    `MobileNetv2 &lt;https://arxiv.org/abs/1801.04381&gt;`_ paper\n\n    Args:\n        in_channels (int): :math:`C_{in}` from an expected input of size :math:`(N, C_{in}, H_{in}, W_{in})`\n        out_channels (int): :math:`C_{out}` from an expected output of size :math:`(N, C_{out}, H_{out}, W_{out)`\n        stride (int): Use convolutions with a stride. Default: 1\n        expand_ratio (Union[int, float]): Expand the input channels by this factor in depth-wise conv\n        skip_connection (Optional[bool]): Use skip-connection. Default: True\n\n    Shape:\n        - Input: :math:`(N, C_{in}, H_{in}, W_{in})`\n        - Output: :math:`(N, C_{out}, H_{out}, W_{out})`\n\n    .. note::\n        If `in_channels =! out_channels` and `stride &gt; 1`, we set `skip_connection=False`\n\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        stride: int,\n        expand_ratio: Union[int, float],\n        skip_connection: Optional[bool] = True,\n    ) -&gt; None:\n        assert stride in [1, 2]\n        hidden_dim = make_divisible(int(round(in_channels * expand_ratio)), 8)\n\n        super().__init__()\n\n        block = []\n        if expand_ratio != 1:\n            block.append(\n                ConvLayer(\n                    in_channels=in_channels,\n                    out_channels=hidden_dim,\n                    kernel_size=1,\n                    norm=nn.BatchNorm2d,\n                    activation=nn.SiLU\n                ),\n            )\n\n        block.append(\n            ConvLayer(\n                in_channels=hidden_dim,\n                out_channels=hidden_dim,\n                kernel_size=3,\n                stride=stride,\n                groups=hidden_dim,\n                norm=nn.BatchNorm2d,\n                activation=nn.SiLU\n            ),\n        )\n\n        block.append(\n           ConvLayer(\n                in_channels=hidden_dim,\n                out_channels=out_channels,\n                kernel_size=1,\n                has_bias=False,\n                activation=None\n            ),\n        )\n\n        self.block = nn.SequentialCell(block)\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.exp = expand_ratio\n        self.stride = stride\n        self.use_res_connect = (\n            self.stride == 1 and in_channels == out_channels and skip_connection\n        )\n\n    def construct(self, x: Tensor, *args, **kwargs) -&gt; Tensor:\n        if self.use_res_connect:\n            return x + self.block(x)\n        else:\n            return self.block(x)\n</code></pre>"},{"location":"reference/models/#mindcv.models.mobilevit.MobileViT","title":"<code>mindcv.models.mobilevit.MobileViT</code>","text":"<p>               Bases: <code>Cell</code></p> <p>This class implements the <code>MobileViT architecture &lt;https://arxiv.org/abs/2110.02178?context=cs.LG&gt;</code>_</p> Source code in <code>mindcv\\models\\mobilevit.py</code> <pre><code>class MobileViT(nn.Cell):\n    \"\"\"\n    This class implements the `MobileViT architecture &lt;https://arxiv.org/abs/2110.02178?context=cs.LG&gt;`_\n    \"\"\"\n    def __init__(self, model_cfg: Dict, num_classes: int = 1000):\n        super().__init__()\n\n        image_channels = 3\n        out_channels = 16\n\n        self.conv_1 = ConvLayer(\n            in_channels=image_channels,\n            out_channels=out_channels,\n            kernel_size=3,\n            stride=2\n        )\n\n        self.layer_1, out_channels = self._make_layer(input_channel=out_channels, cfg=model_cfg[\"layer1\"])\n        self.layer_2, out_channels = self._make_layer(input_channel=out_channels, cfg=model_cfg[\"layer2\"])\n        self.layer_3, out_channels = self._make_layer(input_channel=out_channels, cfg=model_cfg[\"layer3\"])\n        self.layer_4, out_channels = self._make_layer(input_channel=out_channels, cfg=model_cfg[\"layer4\"])\n        self.layer_5, out_channels = self._make_layer(input_channel=out_channels, cfg=model_cfg[\"layer5\"])\n\n        exp_channels = min(model_cfg[\"last_layer_exp_factor\"] * out_channels, 960)\n        self.conv_1x1_exp = ConvLayer(\n            in_channels=out_channels,\n            out_channels=exp_channels,\n            kernel_size=1\n        )\n\n        classifier = []\n        classifier.append(GlobalAvgPooling())\n        classifier.append(nn.Flatten())\n        if 0.0 &lt; model_cfg[\"cls_dropout\"] &lt; 1.0:\n            classifier.append(Dropout(p=model_cfg[\"cls_dropout\"]))\n        classifier.append(nn.Dense(in_channels=exp_channels, out_channels=num_classes))\n        self.classifier = nn.SequentialCell(classifier)\n        self._initialize_weights()\n\n    def _make_layer(self, input_channel, cfg: Dict) -&gt; Tuple[nn.SequentialCell, int]:\n        block_type = cfg.get(\"block_type\", \"mobilevit\")\n        if block_type.lower() == \"mobilevit\":\n            return self._make_mit_layer(input_channel=input_channel, cfg=cfg)\n        else:\n            return self._make_mobilenet_layer(input_channel=input_channel, cfg=cfg)\n\n    @staticmethod\n    def _make_mobilenet_layer(input_channel: int, cfg: Dict) -&gt; Tuple[nn.SequentialCell, int]:\n        output_channels = cfg.get(\"out_channels\")\n        num_blocks = cfg.get(\"num_blocks\", 2)\n        expand_ratio = cfg.get(\"expand_ratio\", 4)\n        block = []\n\n        for i in range(num_blocks):\n            stride = cfg.get(\"stride\", 1) if i == 0 else 1\n\n            layer = InvertedResidual(\n                in_channels=input_channel,\n                out_channels=output_channels,\n                stride=stride,\n                expand_ratio=expand_ratio\n            )\n            block.append(layer)\n            input_channel = output_channels\n\n        return nn.SequentialCell(block), input_channel\n\n    @staticmethod\n    def _make_mit_layer(input_channel: int, cfg: Dict) -&gt; [nn.SequentialCell, int]:\n        stride = cfg.get(\"stride\", 1)\n        block = []\n\n        if stride == 2:\n            layer = InvertedResidual(\n                in_channels=input_channel,\n                out_channels=cfg.get(\"out_channels\"),\n                stride=stride,\n                expand_ratio=cfg.get(\"mv_expand_ratio\", 4)\n            )\n\n            block.append(layer)\n            input_channel = cfg.get(\"out_channels\")\n\n        transformer_dim = cfg[\"transformer_channels\"]\n        ffn_dim = cfg.get(\"ffn_dim\")\n        num_heads = cfg.get(\"num_heads\", 4)\n        head_dim = transformer_dim // num_heads\n\n        if transformer_dim % head_dim != 0:\n            raise ValueError(\"Transformer input dimension should be divisible by head dimension. \"\n                             \"Got {} and {}.\".format(transformer_dim, head_dim))\n\n        block.append(MobileViTBlock(\n            in_channels=input_channel,\n            out_channels=cfg.get(\"out_channels\"),\n            transformer_dim=transformer_dim,\n            ffn_dim=ffn_dim,\n            n_transformer_blocks=cfg.get(\"transformer_blocks\", 1),\n            patch_h=cfg.get(\"patch_h\", 2),\n            patch_w=cfg.get(\"patch_w\", 2),\n            dropout=cfg.get(\"dropout\", 0.1),\n            ffn_dropout=cfg.get(\"ffn_dropout\", 0.0),\n            attn_dropout=cfg.get(\"attn_dropout\", 0.1),\n            head_dim=head_dim,\n            conv_ksize=3\n        ))\n\n        return nn.SequentialCell(block), input_channel\n\n    def _initialize_weights(self) -&gt; None:\n        \"\"\"Initialize weights for cells.\"\"\"\n        for _, cell in self.cells_and_names():\n            if isinstance(cell, nn.Dense):\n                if cell.weight is not None:\n                    cell.weight.set_data(init.initializer(init.TruncatedNormal(sigma=.02), cell.weight.shape,\n                                                          cell.weight.dtype))\n                if cell.bias is not None:\n                    cell.bias.set_data(init.initializer('zeros', cell.bias.shape, cell.bias.dtype))\n            elif isinstance(cell, (nn.LayerNorm, nn.BatchNorm2d)):\n                if cell.gamma is not None:\n                    cell.gamma.set_data(init.initializer('ones', cell.gamma.shape, cell.gamma.dtype))\n                if cell.beta is not None:\n                    cell.beta.set_data(init.initializer('zeros', cell.beta.shape, cell.beta.dtype))\n            elif isinstance(cell, nn.Conv2d):\n                if cell.weight is not None:\n                    cell.weight.set_data(init.initializer(init.HeNormal(mode='fan_out', nonlinearity='leaky_relu'),\n                                         cell.weight.shape, cell.weight.dtype))\n                if cell.bias is not None:\n                    cell.bias.set_data(init.initializer('zeros', cell.bias.shape, cell.bias.dtype))\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        x = self.conv_1(x)\n        x = self.layer_1(x)\n        x = self.layer_2(x)\n        x = self.layer_3(x)\n        x = self.layer_4(x)\n        x = self.layer_5(x)\n        x = self.conv_1x1_exp(x)\n        x = self.classifier(x)\n        return x\n</code></pre>"},{"location":"reference/models/#mindcv.models.mobilevit.MobileViTBlock","title":"<code>mindcv.models.mobilevit.MobileViTBlock</code>","text":"<p>               Bases: <code>Cell</code></p> <p>This class defines the <code>MobileViT block &lt;https://arxiv.org/abs/2110.02178?context=cs.LG&gt;</code>_</p> PARAMETER DESCRIPTION <code>opts</code> <p>command line arguments</p> <p> </p> <code>in_channels</code> <p>:math:<code>C_{in}</code> from an expected input of size :math:<code>(N, C_{in}, H, W)</code></p> <p> TYPE: <code>int</code> </p> <code>transformer_dim</code> <p>Input dimension to the transformer unit</p> <p> TYPE: <code>int</code> </p> <code>ffn_dim</code> <p>Dimension of the FFN block</p> <p> TYPE: <code>int</code> </p> <code>n_transformer_blocks</code> <p>Number of transformer blocks. Default: 2</p> <p> TYPE: <code>int</code> DEFAULT: <code>2</code> </p> <code>head_dim</code> <p>Head dimension in the multi-head attention. Default: 32</p> <p> TYPE: <code>int</code> DEFAULT: <code>32</code> </p> <code>attn_dropout</code> <p>Dropout in multi-head attention. Default: 0.0</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>dropout</code> <p>Dropout rate. Default: 0.0</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>ffn_dropout</code> <p>Dropout between FFN layers in transformer. Default: 0.0</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>patch_h</code> <p>Patch height for unfolding operation. Default: 8</p> <p> TYPE: <code>int</code> DEFAULT: <code>8</code> </p> <code>patch_w</code> <p>Patch width for unfolding operation. Default: 8</p> <p> TYPE: <code>int</code> DEFAULT: <code>8</code> </p> <code>transformer_norm_layer</code> <p>Normalization layer in the transformer block. Default: layer_norm</p> <p> TYPE: <code>Optional[str]</code> </p> <code>conv_ksize</code> <p>Kernel size to learn local representations in MobileViT block. Default: 3</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>no_fusion</code> <p>Do not combine the input and output feature maps. Default: False</p> <p> TYPE: <code>Optional[bool]</code> </p> Source code in <code>mindcv\\models\\mobilevit.py</code> <pre><code>class MobileViTBlock(nn.Cell):\n    \"\"\"\n    This class defines the `MobileViT block &lt;https://arxiv.org/abs/2110.02178?context=cs.LG&gt;`_\n\n    Args:\n        opts: command line arguments\n        in_channels (int): :math:`C_{in}` from an expected input of size :math:`(N, C_{in}, H, W)`\n        transformer_dim (int): Input dimension to the transformer unit\n        ffn_dim (int): Dimension of the FFN block\n        n_transformer_blocks (int): Number of transformer blocks. Default: 2\n        head_dim (int): Head dimension in the multi-head attention. Default: 32\n        attn_dropout (float): Dropout in multi-head attention. Default: 0.0\n        dropout (float): Dropout rate. Default: 0.0\n        ffn_dropout (float): Dropout between FFN layers in transformer. Default: 0.0\n        patch_h (int): Patch height for unfolding operation. Default: 8\n        patch_w (int): Patch width for unfolding operation. Default: 8\n        transformer_norm_layer (Optional[str]): Normalization layer in the transformer block. Default: layer_norm\n        conv_ksize (int): Kernel size to learn local representations in MobileViT block. Default: 3\n        no_fusion (Optional[bool]): Do not combine the input and output feature maps. Default: False\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        transformer_dim: int,\n        ffn_dim: int,\n        n_transformer_blocks: int = 2,\n        head_dim: int = 32,\n        attn_dropout: float = 0.0,\n        dropout: float = 0.0,\n        ffn_dropout: float = 0.0,\n        patch_h: int = 8,\n        patch_w: int = 8,\n        conv_ksize: Optional[int] = 3,\n        *args,\n        **kwargs\n    ) -&gt; None:\n        super().__init__()\n\n        conv_3x3_in = ConvLayer(\n            in_channels=in_channels,\n            out_channels=in_channels,\n            kernel_size=conv_ksize,\n            stride=1\n        )\n        conv_1x1_in = ConvLayer(\n            in_channels=in_channels,\n            out_channels=transformer_dim,\n            kernel_size=1,\n            stride=1\n        )\n\n        conv_1x1_out = ConvLayer(\n            in_channels=transformer_dim,\n            out_channels=in_channels,\n            kernel_size=1,\n            stride=1\n        )\n        conv_3x3_out = ConvLayer(\n            in_channels=2 * in_channels,\n            out_channels=out_channels,\n            kernel_size=conv_ksize,\n            stride=1,\n            pad_mode=\"pad\",\n            padding=1\n        )\n\n        local_rep = []\n        local_rep.append(conv_3x3_in)\n        local_rep.append(conv_1x1_in)\n        self.local_rep = nn.SequentialCell(local_rep)\n\n        assert transformer_dim % head_dim == 0\n        num_heads = transformer_dim // head_dim\n\n        self.global_rep = [\n            TransformerEncoder(\n                embed_dim=transformer_dim,\n                ffn_latent_dim=ffn_dim,\n                num_heads=num_heads,\n                attn_dropout=attn_dropout,\n                dropout=dropout,\n                ffn_dropout=ffn_dropout\n            )\n            for _ in range(n_transformer_blocks)\n        ]\n        self.global_rep.append(nn.LayerNorm((transformer_dim,)))\n        self.global_rep = nn.CellList(self.global_rep)\n\n        self.conv_proj = conv_1x1_out\n        self.fusion = conv_3x3_out\n\n        self.patch_h = patch_h\n        self.patch_w = patch_w\n        self.patch_area = self.patch_w * self.patch_h\n\n        self.cnn_in_dim = in_channels\n        self.cnn_out_dim = transformer_dim\n        self.n_heads = num_heads\n        self.ffn_dim = ffn_dim\n        self.dropout = dropout\n        self.attn_dropout = attn_dropout\n        self.ffn_dropout = ffn_dropout\n        self.n_blocks = n_transformer_blocks\n        self.conv_ksize = conv_ksize\n        self.interpolate = Interpolate(mode=\"bilinear\", align_corners=True)\n\n    def unfolding(self, x: Tensor) -&gt; Tuple[Tensor, Dict]:\n        patch_w, patch_h = self.patch_w, self.patch_h\n        patch_area = patch_w * patch_h\n        batch_size, in_channels, orig_h, orig_w = x.shape\n\n        new_h = int(math.ceil(orig_h / self.patch_h) * self.patch_h)\n        new_w = int(math.ceil(orig_w / self.patch_w) * self.patch_w)\n\n        interpolate = False\n        if new_w != orig_w or new_h != orig_h:\n            # Note: Padding can be done, but then it needs to be handled in attention function.\n            x = self.interpolate(x, size=(new_h, new_w))\n            interpolate = True\n\n        # number of patches along width and height\n        num_patch_w = new_w // patch_w  # n_w\n        num_patch_h = new_h // patch_h  # n_h\n        num_patches = num_patch_h * num_patch_w  # N\n\n        # [B, C, H, W] -&gt; [B * C * n_h, p_h, n_w, p_w]\n        x = ops.reshape(x, (batch_size * in_channels * num_patch_h, patch_h, num_patch_w, patch_w))\n        # [B * C * n_h, p_h, n_w, p_w] -&gt; [B * C * n_h, n_w, p_h, p_w]\n        x = ops.transpose(x, (0, 2, 1, 3))\n        # [B * C * n_h, n_w, p_h, p_w] -&gt; [B, C, N, P] where P = p_h * p_w and N = n_h * n_w\n        x = ops.reshape(x, (batch_size, in_channels, num_patches, patch_area))\n        # [B, C, N, P] -&gt; [B, P, N, C]\n        x = ops.transpose(x, (0, 3, 2, 1))\n        # [B, P, N, C] -&gt; [BP, N, C]\n        x = ops.reshape(x, (batch_size * patch_area, num_patches, -1))\n\n        info_dict = {\n            \"orig_size\": (orig_h, orig_w),\n            \"batch_size\": batch_size,\n            \"interpolate\": interpolate,\n            \"total_patches\": num_patches,\n            \"num_patches_w\": num_patch_w,\n            \"num_patches_h\": num_patch_h,\n        }\n\n        return x, info_dict\n\n    def folding(self, x: Tensor, info_dict: Dict) -&gt; Tensor:\n        n_dim = ops.rank(x)\n        assert n_dim == 3, \"Tensor should be of shape BPxNxC. Got: {}\".format(\n            x.shape\n        )\n        # [BP, N, C] --&gt; [B, P, N, C]\n        x = x.view(\n            info_dict[\"batch_size\"], self.patch_area, info_dict[\"total_patches\"], -1\n        )\n\n        batch_size, pixels, num_patches, channels = x.shape\n        num_patch_h = info_dict[\"num_patches_h\"]\n        num_patch_w = info_dict[\"num_patches_w\"]\n\n        # [B, P, N, C] -&gt; [B, C, N, P]\n        x = ops.transpose(x, (0, 3, 2, 1))\n        # [B, C, N, P] -&gt; [B*C*n_h, n_w, p_h, p_w]\n        x = ops.reshape(x, (batch_size * channels * num_patch_h, num_patch_w, self.patch_h, self.patch_w))\n        # [B*C*n_h, n_w, p_h, p_w] -&gt; [B*C*n_h, p_h, n_w, p_w]\n        x = ops.transpose(x, (0, 2, 1, 3))\n        # [B*C*n_h, p_h, n_w, p_w] -&gt; [B, C, H, W]\n        x = ops.reshape(x, (batch_size, channels, num_patch_h * self.patch_h, num_patch_w * self.patch_w))\n        if info_dict[\"interpolate\"]:\n            x = self.interpolate(x, size=info_dict[\"orig_size\"])\n        return x\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        res = x\n        fm = self.local_rep(x)\n        # convert feature map to patches\n        patches, info_dict = self.unfolding(fm)\n        # learn global representations\n        for transformer_layer in self.global_rep:\n            patches = transformer_layer(patches)\n        # [B x Patch x Patches x C] -&gt; [B x C x Patches x Patch]\n        fm = self.folding(x=patches, info_dict=info_dict)\n        fm = self.conv_proj(fm)\n        fm = self.fusion(ops.concat((res, fm), 1))\n        return fm\n</code></pre>"},{"location":"reference/models/#mindcv.models.mobilevit.MultiHeadAttention","title":"<code>mindcv.models.mobilevit.MultiHeadAttention</code>","text":"<p>               Bases: <code>Cell</code></p> <p>This layer applies a multi-head self- or cross-attention as described in <code>Attention is all you need &lt;https://arxiv.org/abs/1706.03762&gt;</code>_ paper</p> PARAMETER DESCRIPTION <code>embed_dim</code> <p>:math:<code>C_{in}</code> from an expected input of size :math:<code>(N, P, C_{in})</code></p> <p> TYPE: <code>int</code> </p> <code>num_heads</code> <p>Number of heads in multi-head attention</p> <p> TYPE: <code>int</code> </p> <code>attn_dropout</code> <p>Attention dropout. Default: 0.0</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>bias</code> <p>Use bias or not. Default: <code>True</code></p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> Shape <ul> <li>Input: :math:<code>(N, P, C_{in})</code> where :math:<code>N</code> is batch size, :math:<code>P</code> is number of patches, and :math:<code>C_{in}</code> is input embedding dim</li> <li>Output: same shape as the input</li> </ul> Source code in <code>mindcv\\models\\mobilevit.py</code> <pre><code>class MultiHeadAttention(nn.Cell):\n    \"\"\"\n    This layer applies a multi-head self- or cross-attention as described in\n    `Attention is all you need &lt;https://arxiv.org/abs/1706.03762&gt;`_ paper\n\n    Args:\n        embed_dim (int): :math:`C_{in}` from an expected input of size :math:`(N, P, C_{in})`\n        num_heads (int): Number of heads in multi-head attention\n        attn_dropout (float): Attention dropout. Default: 0.0\n        bias (bool): Use bias or not. Default: ``True``\n\n    Shape:\n        - Input: :math:`(N, P, C_{in})` where :math:`N` is batch size, :math:`P` is number of patches,\n        and :math:`C_{in}` is input embedding dim\n        - Output: same shape as the input\n\n    \"\"\"\n\n    def __init__(\n        self,\n        embed_dim: int,\n        num_heads: int,\n        attn_dropout: float = 0.0,\n        bias: bool = True,\n        *args,\n        **kwargs\n    ) -&gt; None:\n        super().__init__()\n        if embed_dim % num_heads != 0:\n            raise ValueError(\n                \"Embedding dim must be divisible by number of heads in {}. Got: embed_dim={} and num_heads={}\".format(\n                    self.__class__.__name__, embed_dim, num_heads\n                )\n            )\n\n        self.qkv_proj = nn.Dense(in_channels=embed_dim, out_channels=embed_dim * 3, has_bias=bias)\n\n        self.attn_dropout = Dropout(p=attn_dropout)\n        self.out_proj = nn.Dense(in_channels=embed_dim, out_channels=embed_dim, has_bias=bias)\n\n        self.head_dim = embed_dim // num_heads\n        self.scaling = self.head_dim ** -0.5\n        self.softmax = nn.Softmax(axis=-1)\n        self.num_heads = num_heads\n        self.embed_dim = embed_dim\n        self.batch_matmul = ops.BatchMatMul()\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        B, N, C = x.shape\n        qkv = self.qkv_proj(x)\n        qkv = ops.reshape(qkv, (B, N, 3, self.num_heads, C // self.num_heads))\n        qkv = ops.transpose(qkv, (2, 0, 3, 1, 4))\n        q, k, v = qkv[0], qkv[1], qkv[2]\n\n        attn = ops.BatchMatMul(transpose_b=True)(q, k) * self.scaling\n        attn = nn.Softmax(axis=-1)(attn)\n        attn = self.attn_dropout(attn)\n\n        x = ops.transpose(ops.BatchMatMul()(attn, v), (0, 2, 1, 3))\n        x = ops.reshape(x, (B, N, C))\n        x = self.out_proj(x)\n        return x\n</code></pre>"},{"location":"reference/models/#mindcv.models.mobilevit.TransformerEncoder","title":"<code>mindcv.models.mobilevit.TransformerEncoder</code>","text":"<p>               Bases: <code>Cell</code></p> <p>This class defines the pre-norm <code>Transformer encoder &lt;https://arxiv.org/abs/1706.03762&gt;</code>_ Args:     embed_dim (int): :math:<code>C_{in}</code> from an expected input of size :math:<code>(N, P, C_{in})</code>     ffn_latent_dim (int): Inner dimension of the FFN     num_heads (int) : Number of heads in multi-head attention. Default: 8     attn_dropout (float): Dropout rate for attention in multi-head attention. Default: 0.0     dropout (float): Dropout rate. Default: 0.0     ffn_dropout (float): Dropout between FFN layers. Default: 0.0</p> Shape <ul> <li>Input: :math:<code>(N, P, C_{in})</code> where :math:<code>N</code> is batch size, :math:<code>P</code> is number of patches, and :math:<code>C_{in}</code> is input embedding dim</li> <li>Output: same shape as the input</li> </ul> Source code in <code>mindcv\\models\\mobilevit.py</code> <pre><code>class TransformerEncoder(nn.Cell):\n    \"\"\"\n    This class defines the pre-norm `Transformer encoder &lt;https://arxiv.org/abs/1706.03762&gt;`_\n    Args:\n        embed_dim (int): :math:`C_{in}` from an expected input of size :math:`(N, P, C_{in})`\n        ffn_latent_dim (int): Inner dimension of the FFN\n        num_heads (int) : Number of heads in multi-head attention. Default: 8\n        attn_dropout (float): Dropout rate for attention in multi-head attention. Default: 0.0\n        dropout (float): Dropout rate. Default: 0.0\n        ffn_dropout (float): Dropout between FFN layers. Default: 0.0\n\n    Shape:\n        - Input: :math:`(N, P, C_{in})` where :math:`N` is batch size, :math:`P` is number of patches,\n        and :math:`C_{in}` is input embedding dim\n        - Output: same shape as the input\n    \"\"\"\n\n    def __init__(\n        self,\n        embed_dim: int,\n        ffn_latent_dim: int,\n        num_heads: Optional[int] = 8,\n        attn_dropout: Optional[float] = 0.0,\n        dropout: Optional[float] = 0.0,\n        ffn_dropout: Optional[float] = 0.0,\n        *args,\n        **kwargs\n    ) -&gt; None:\n\n        super().__init__()\n\n        attn_unit = MultiHeadAttention(\n            embed_dim,\n            num_heads,\n            attn_dropout=attn_dropout,\n            bias=True\n        )\n\n        self.pre_norm_mha = nn.SequentialCell(\n            nn.LayerNorm((embed_dim,)),\n            attn_unit,\n            Dropout(p=dropout)\n        )\n\n        self.pre_norm_ffn = nn.SequentialCell(\n            nn.LayerNorm((embed_dim,)),\n            nn.Dense(in_channels=embed_dim, out_channels=ffn_latent_dim, has_bias=True),\n            nn.SiLU(),\n            Dropout(p=ffn_dropout),\n            nn.Dense(in_channels=ffn_latent_dim, out_channels=embed_dim, has_bias=True),\n            Dropout(p=dropout)\n        )\n        self.embed_dim = embed_dim\n        self.ffn_dim = ffn_latent_dim\n        self.ffn_dropout = ffn_dropout\n        self.std_dropout = dropout\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        # multi-head attention\n        res = x\n        x = self.pre_norm_mha(x)\n        x = x + res\n\n        # feed forward network\n        x = x + self.pre_norm_ffn(x)\n        return x\n</code></pre>"},{"location":"reference/models/#nasnet","title":"nasnet","text":""},{"location":"reference/models/#mindcv.models.nasnet","title":"<code>mindcv.models.nasnet</code>","text":"<p>MindSpore implementation of <code>NasNet</code>. Refer to: Learning Transferable Architectures for Scalable Image Recognition</p>"},{"location":"reference/models/#mindcv.models.nasnet.BranchSeparables","title":"<code>mindcv.models.nasnet.BranchSeparables</code>","text":"<p>               Bases: <code>Cell</code></p> <p>NasNet model basic architecture</p> Source code in <code>mindcv\\models\\nasnet.py</code> <pre><code>class BranchSeparables(nn.Cell):\n    \"\"\"NasNet model basic architecture\"\"\"\n\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        kernel_size: int,\n        stride: int,\n        padding: int,\n        bias: bool = False,\n    ) -&gt; None:\n        super().__init__()\n        self.relu = nn.ReLU()\n        self.separable_1 = SeparableConv2d(\n            in_channels, in_channels, kernel_size, stride, padding, bias=bias\n        )\n        self.bn_sep_1 = nn.BatchNorm2d(num_features=in_channels, eps=0.001, momentum=0.9, affine=True)\n        self.relu1 = nn.ReLU()\n        self.separable_2 = SeparableConv2d(\n            in_channels, out_channels, kernel_size, 1, padding, bias=bias\n        )\n        self.bn_sep_2 = nn.BatchNorm2d(num_features=out_channels, eps=0.001, momentum=0.9, affine=True)\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        x = self.relu(x)\n        x = self.separable_1(x)\n        x = self.bn_sep_1(x)\n        x = self.relu1(x)\n        x = self.separable_2(x)\n        x = self.bn_sep_2(x)\n        return x\n</code></pre>"},{"location":"reference/models/#mindcv.models.nasnet.BranchSeparablesReduction","title":"<code>mindcv.models.nasnet.BranchSeparablesReduction</code>","text":"<p>               Bases: <code>BranchSeparables</code></p> <p>NasNet model Residual Connections</p> Source code in <code>mindcv\\models\\nasnet.py</code> <pre><code>class BranchSeparablesReduction(BranchSeparables):\n    \"\"\"NasNet model Residual Connections\"\"\"\n\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        kernel_size: int,\n        stride: int,\n        padding: int,\n        z_padding: int = 1,\n        bias: bool = False,\n    ) -&gt; None:\n        BranchSeparables.__init__(\n            self, in_channels, out_channels, kernel_size, stride, padding, bias\n        )\n        self.padding = nn.Pad(paddings=((0, 0), (0, 0), (z_padding, 0), (z_padding, 0)), mode=\"CONSTANT\")\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        x = self.relu(x)\n        x = self.padding(x)\n        x = self.separable_1(x)\n        x = x[:, :, 1:, 1:]\n        x = self.bn_sep_1(x)\n        x = self.relu1(x)\n        x = self.separable_2(x)\n        x = self.bn_sep_2(x)\n        return x\n</code></pre>"},{"location":"reference/models/#mindcv.models.nasnet.BranchSeparablesStem","title":"<code>mindcv.models.nasnet.BranchSeparablesStem</code>","text":"<p>               Bases: <code>Cell</code></p> <p>NasNet model basic architecture</p> Source code in <code>mindcv\\models\\nasnet.py</code> <pre><code>class BranchSeparablesStem(nn.Cell):\n    \"\"\"NasNet model basic architecture\"\"\"\n\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        kernel_size: int,\n        stride: int,\n        padding: int,\n        bias: bool = False,\n    ) -&gt; None:\n        super().__init__()\n        self.relu = nn.ReLU()\n        self.separable_1 = SeparableConv2d(\n            in_channels, out_channels, kernel_size, stride, padding, bias=bias\n        )\n        self.bn_sep_1 = nn.BatchNorm2d(num_features=out_channels, eps=0.001, momentum=0.9, affine=True)\n        self.relu1 = nn.ReLU()\n        self.separable_2 = SeparableConv2d(\n            out_channels, out_channels, kernel_size, 1, padding, bias=bias\n        )\n        self.bn_sep_2 = nn.BatchNorm2d(num_features=out_channels, eps=0.001, momentum=0.9, affine=True)\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        x = self.relu(x)\n        x = self.separable_1(x)\n        x = self.bn_sep_1(x)\n        x = self.relu1(x)\n        x = self.separable_2(x)\n        x = self.bn_sep_2(x)\n        return x\n</code></pre>"},{"location":"reference/models/#mindcv.models.nasnet.CellStem0","title":"<code>mindcv.models.nasnet.CellStem0</code>","text":"<p>               Bases: <code>Cell</code></p> <p>NasNet model basic architecture</p> Source code in <code>mindcv\\models\\nasnet.py</code> <pre><code>class CellStem0(nn.Cell):\n    \"\"\"NasNet model basic architecture\"\"\"\n\n    def __init__(\n        self,\n        stem_filters: int,\n        num_filters: int = 42,\n    ) -&gt; None:\n        super().__init__()\n        self.num_filters = num_filters\n        self.stem_filters = stem_filters\n        self.conv_1x1 = nn.SequentialCell([\n            nn.ReLU(),\n            nn.Conv2d(in_channels=self.stem_filters, out_channels=self.num_filters, kernel_size=1, stride=1,\n                      pad_mode=\"pad\", has_bias=False),\n            nn.BatchNorm2d(num_features=self.num_filters, eps=0.001, momentum=0.9, affine=True)\n        ])\n\n        self.comb_iter_0_left = BranchSeparables(\n            self.num_filters, self.num_filters, 5, 2, 2\n        )\n        self.comb_iter_0_right = BranchSeparablesStem(\n            self.stem_filters, self.num_filters, 7, 2, 3, bias=False\n        )\n\n        self.comb_iter_1_left = nn.MaxPool2d(kernel_size=3, stride=2, pad_mode=\"same\")\n        self.comb_iter_1_right = BranchSeparablesStem(\n            self.stem_filters, self.num_filters, 7, 2, 3, bias=False\n        )\n\n        self.comb_iter_2_left = nn.AvgPool2d(kernel_size=3, stride=2, pad_mode=\"same\")\n        self.comb_iter_2_right = BranchSeparablesStem(\n            self.stem_filters, self.num_filters, 5, 2, 2, bias=False\n        )\n\n        self.comb_iter_3_right = nn.AvgPool2d(kernel_size=3, stride=1, pad_mode=\"same\")\n\n        self.comb_iter_4_left = BranchSeparables(\n            self.num_filters, self.num_filters, 3, 1, 1, bias=False\n        )\n        self.comb_iter_4_right = nn.MaxPool2d(kernel_size=3, stride=2, pad_mode=\"same\")\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        x1 = self.conv_1x1(x)\n\n        x_comb_iter_0_left = self.comb_iter_0_left(x1)\n        x_comb_iter_0_right = self.comb_iter_0_right(x)\n        x_comb_iter_0 = x_comb_iter_0_left + x_comb_iter_0_right\n\n        x_comb_iter_1_left = self.comb_iter_1_left(x1)\n        x_comb_iter_1_right = self.comb_iter_1_right(x)\n        x_comb_iter_1 = x_comb_iter_1_left + x_comb_iter_1_right\n\n        x_comb_iter_2_left = self.comb_iter_2_left(x1)\n        x_comb_iter_2_right = self.comb_iter_2_right(x)\n        x_comb_iter_2 = x_comb_iter_2_left + x_comb_iter_2_right\n\n        x_comb_iter_3_right = self.comb_iter_3_right(x_comb_iter_0)\n        x_comb_iter_3 = x_comb_iter_3_right + x_comb_iter_1\n\n        x_comb_iter_4_left = self.comb_iter_4_left(x_comb_iter_0)\n        x_comb_iter_4_right = self.comb_iter_4_right(x1)\n        x_comb_iter_4 = x_comb_iter_4_left + x_comb_iter_4_right\n\n        x_out = ops.concat((x_comb_iter_1, x_comb_iter_2, x_comb_iter_3, x_comb_iter_4), axis=1)\n        return x_out\n</code></pre>"},{"location":"reference/models/#mindcv.models.nasnet.CellStem1","title":"<code>mindcv.models.nasnet.CellStem1</code>","text":"<p>               Bases: <code>Cell</code></p> <p>NasNet model basic architecture</p> Source code in <code>mindcv\\models\\nasnet.py</code> <pre><code>class CellStem1(nn.Cell):\n    \"\"\"NasNet model basic architecture\"\"\"\n\n    def __init__(\n        self,\n        stem_filters: int,\n        num_filters: int,\n    ) -&gt; None:\n        super().__init__()\n        self.num_filters = num_filters\n        self.stem_filters = stem_filters\n        self.conv_1x1 = nn.SequentialCell([\n            nn.ReLU(),\n            nn.Conv2d(in_channels=2 * self.num_filters, out_channels=self.num_filters, kernel_size=1, stride=1,\n                      pad_mode=\"pad\", has_bias=False),\n            nn.BatchNorm2d(num_features=self.num_filters, eps=0.001, momentum=0.9, affine=True)])\n\n        self.relu = nn.ReLU()\n        self.path_1 = nn.SequentialCell([\n            nn.AvgPool2d(kernel_size=1, stride=2, pad_mode=\"valid\"),\n            nn.Conv2d(in_channels=self.stem_filters, out_channels=self.num_filters // 2, kernel_size=1, stride=1,\n                      pad_mode=\"pad\", has_bias=False)])\n\n        self.path_2 = nn.CellList([])\n        self.path_2.append(nn.Pad(paddings=((0, 0), (0, 0), (0, 1), (0, 1)), mode=\"CONSTANT\"))\n        self.path_2.append(\n            nn.AvgPool2d(kernel_size=1, stride=2, pad_mode=\"valid\")\n        )\n        self.path_2.append(\n            nn.Conv2d(in_channels=self.stem_filters, out_channels=self.num_filters // 2, kernel_size=1, stride=1,\n                      pad_mode=\"pad\", has_bias=False)\n        )\n\n        self.final_path_bn = nn.BatchNorm2d(num_features=self.num_filters, eps=0.001, momentum=0.9, affine=True)\n\n        self.comb_iter_0_left = BranchSeparables(\n            self.num_filters,\n            self.num_filters,\n            5,\n            2,\n            2,\n            bias=False\n        )\n        self.comb_iter_0_right = BranchSeparables(\n            self.num_filters,\n            self.num_filters,\n            7,\n            2,\n            3,\n            bias=False\n        )\n\n        self.comb_iter_1_left = nn.MaxPool2d(3, stride=2, pad_mode=\"same\")\n        self.comb_iter_1_right = BranchSeparables(\n            self.num_filters,\n            self.num_filters,\n            7,\n            2,\n            3,\n            bias=False\n        )\n\n        self.comb_iter_2_left = nn.AvgPool2d(3, stride=2, pad_mode=\"same\")\n        self.comb_iter_2_right = BranchSeparables(\n            self.num_filters,\n            self.num_filters,\n            5,\n            2,\n            2,\n            bias=False\n        )\n\n        self.comb_iter_3_right = nn.AvgPool2d(kernel_size=3, stride=1, pad_mode=\"same\")\n\n        self.comb_iter_4_left = BranchSeparables(\n            self.num_filters,\n            self.num_filters,\n            3,\n            1,\n            1,\n            bias=False\n        )\n        self.comb_iter_4_right = nn.MaxPool2d(3, stride=2, pad_mode=\"same\")\n\n    def construct(self, x_conv0: Tensor, x_stem_0: Tensor) -&gt; Tensor:\n        x_left = self.conv_1x1(x_stem_0)\n        x_relu = self.relu(x_conv0)\n        # path 1\n        x_path1 = self.path_1(x_relu)\n        # path 2\n        x_path2 = self.path_2[0](x_relu)\n        x_path2 = x_path2[:, :, 1:, 1:]\n        x_path2 = self.path_2[1](x_path2)\n        x_path2 = self.path_2[2](x_path2)\n        # final path\n        x_right = self.final_path_bn(ops.concat((x_path1, x_path2), axis=1))\n\n        x_comb_iter_0_left = self.comb_iter_0_left(x_left)\n        x_comb_iter_0_right = self.comb_iter_0_right(x_right)\n        x_comb_iter_0 = x_comb_iter_0_left + x_comb_iter_0_right\n\n        x_comb_iter_1_left = self.comb_iter_1_left(x_left)\n        x_comb_iter_1_right = self.comb_iter_1_right(x_right)\n        x_comb_iter_1 = x_comb_iter_1_left + x_comb_iter_1_right\n\n        x_comb_iter_2_left = self.comb_iter_2_left(x_left)\n        x_comb_iter_2_right = self.comb_iter_2_right(x_right)\n        x_comb_iter_2 = x_comb_iter_2_left + x_comb_iter_2_right\n\n        x_comb_iter_3_right = self.comb_iter_3_right(x_comb_iter_0)\n        x_comb_iter_3 = x_comb_iter_3_right + x_comb_iter_1\n\n        x_comb_iter_4_left = self.comb_iter_4_left(x_comb_iter_0)\n        x_comb_iter_4_right = self.comb_iter_4_right(x_left)\n        x_comb_iter_4 = x_comb_iter_4_left + x_comb_iter_4_right\n\n        x_out = ops.concat((x_comb_iter_1, x_comb_iter_2, x_comb_iter_3, x_comb_iter_4), axis=1)\n        return x_out\n</code></pre>"},{"location":"reference/models/#mindcv.models.nasnet.FirstCell","title":"<code>mindcv.models.nasnet.FirstCell</code>","text":"<p>               Bases: <code>Cell</code></p> <p>NasNet model basic architecture</p> Source code in <code>mindcv\\models\\nasnet.py</code> <pre><code>class FirstCell(nn.Cell):\n    \"\"\"NasNet model basic architecture\"\"\"\n\n    def __init__(\n        self,\n        in_channels_left: int,\n        out_channels_left: int,\n        in_channels_right: int,\n        out_channels_right: int,\n    ) -&gt; None:\n        super().__init__()\n        self.conv_1x1 = nn.SequentialCell([\n            nn.ReLU(),\n            nn.Conv2d(in_channels=in_channels_right, out_channels=out_channels_right, kernel_size=1, stride=1,\n                      pad_mode=\"pad\", has_bias=False),\n            nn.BatchNorm2d(num_features=out_channels_right, eps=0.001, momentum=0.9, affine=True)])\n\n        self.relu = nn.ReLU()\n        self.path_1 = nn.SequentialCell([\n            nn.AvgPool2d(kernel_size=1, stride=2, pad_mode=\"valid\"),\n            nn.Conv2d(in_channels=in_channels_left, out_channels=out_channels_left, kernel_size=1, stride=1,\n                      pad_mode=\"pad\", has_bias=False)])\n\n        self.path_2 = nn.CellList([])\n        self.path_2.append(nn.Pad(paddings=((0, 0), (0, 0), (0, 1), (0, 1)), mode=\"CONSTANT\"))\n        self.path_2.append(\n            nn.AvgPool2d(kernel_size=1, stride=2, pad_mode=\"valid\")\n        )\n        self.path_2.append(\n            nn.Conv2d(in_channels=in_channels_left, out_channels=out_channels_left, kernel_size=1, stride=1,\n                      pad_mode=\"pad\", has_bias=False)\n        )\n\n        self.final_path_bn = nn.BatchNorm2d(num_features=out_channels_left * 2, eps=0.001, momentum=0.9, affine=True)\n\n        self.comb_iter_0_left = BranchSeparables(\n            out_channels_right, out_channels_right, 5, 1, 2, bias=False\n        )\n        self.comb_iter_0_right = BranchSeparables(\n            out_channels_right, out_channels_right, 3, 1, 1, bias=False\n        )\n\n        self.comb_iter_1_left = BranchSeparables(\n            out_channels_right, out_channels_right, 5, 1, 2, bias=False\n        )\n        self.comb_iter_1_right = BranchSeparables(\n            out_channels_right, out_channels_right, 3, 1, 1, bias=False\n        )\n\n        self.comb_iter_2_left = nn.AvgPool2d(kernel_size=3, stride=1, pad_mode=\"same\")\n\n        self.comb_iter_3_left = nn.AvgPool2d(kernel_size=3, stride=1, pad_mode=\"same\")\n        self.comb_iter_3_right = nn.AvgPool2d(kernel_size=3, stride=1, pad_mode=\"same\")\n\n        self.comb_iter_4_left = BranchSeparables(\n            out_channels_right, out_channels_right, 3, 1, 1, bias=False\n        )\n\n    def construct(self, x: Tensor, x_prev: Tensor) -&gt; Tensor:\n        x_relu = self.relu(x_prev)\n        x_path1 = self.path_1(x_relu)\n        x_path2 = self.path_2[0](x_relu)\n        x_path2 = x_path2[:, :, 1:, 1:]\n        x_path2 = self.path_2[1](x_path2)\n        x_path2 = self.path_2[2](x_path2)\n        # final path\n        x_left = self.final_path_bn(ops.concat((x_path1, x_path2), axis=1))\n\n        x_right = self.conv_1x1(x)\n\n        x_comb_iter_0_left = self.comb_iter_0_left(x_right)\n        x_comb_iter_0_right = self.comb_iter_0_right(x_left)\n        x_comb_iter_0 = x_comb_iter_0_left + x_comb_iter_0_right\n\n        x_comb_iter_1_left = self.comb_iter_1_left(x_left)\n        x_comb_iter_1_right = self.comb_iter_1_right(x_left)\n        x_comb_iter_1 = x_comb_iter_1_left + x_comb_iter_1_right\n\n        x_comb_iter_2_left = self.comb_iter_2_left(x_right)\n        x_comb_iter_2 = x_comb_iter_2_left + x_left\n\n        x_comb_iter_3_left = self.comb_iter_3_left(x_left)\n        x_comb_iter_3_right = self.comb_iter_3_right(x_left)\n        x_comb_iter_3 = x_comb_iter_3_left + x_comb_iter_3_right\n\n        x_comb_iter_4_left = self.comb_iter_4_left(x_right)\n        x_comb_iter_4 = x_comb_iter_4_left + x_right\n\n        x_out = ops.concat((x_left, x_comb_iter_0, x_comb_iter_1, x_comb_iter_2, x_comb_iter_3, x_comb_iter_4), axis=1)\n        return x_out\n</code></pre>"},{"location":"reference/models/#mindcv.models.nasnet.NASNetAMobile","title":"<code>mindcv.models.nasnet.NASNetAMobile</code>","text":"<p>               Bases: <code>Cell</code></p> <p>NasNet model class, based on <code>\"Learning Transferable Architectures for Scalable Image Recognition\" &lt;https://arxiv.org/pdf/1707.07012v4.pdf&gt;</code>_ Args:     num_classes: number of classification classes.     stem_filters: number of stem filters. Default: 32.     penultimate_filters: number of penultimate filters. Default: 1056.     filters_multiplier: size of filters multiplier. Default: 2.</p> Source code in <code>mindcv\\models\\nasnet.py</code> <pre><code>class NASNetAMobile(nn.Cell):\n    r\"\"\"NasNet model class, based on\n    `\"Learning Transferable Architectures for Scalable Image Recognition\" &lt;https://arxiv.org/pdf/1707.07012v4.pdf&gt;`_\n    Args:\n        num_classes: number of classification classes.\n        stem_filters: number of stem filters. Default: 32.\n        penultimate_filters: number of penultimate filters. Default: 1056.\n        filters_multiplier: size of filters multiplier. Default: 2.\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels: int = 3,\n        num_classes: int = 1000,\n        stem_filters: int = 32,\n        penultimate_filters: int = 1056,\n        filters_multiplier: int = 2,\n    ) -&gt; None:\n        super().__init__()\n        self.stem_filters = stem_filters\n        self.penultimate_filters = penultimate_filters\n        self.filters_multiplier = filters_multiplier\n\n        filters = self.penultimate_filters // 24\n        # 24 is default value for the architecture\n\n        self.conv0 = nn.SequentialCell([\n            nn.Conv2d(in_channels=in_channels, out_channels=self.stem_filters, kernel_size=3, stride=2, pad_mode=\"pad\",\n                      padding=0,\n                      has_bias=False),\n            nn.BatchNorm2d(num_features=self.stem_filters, eps=0.001, momentum=0.9, affine=True)\n        ])\n\n        self.cell_stem_0 = CellStem0(\n            self.stem_filters, num_filters=filters // (filters_multiplier ** 2)\n        )\n        self.cell_stem_1 = CellStem1(\n            self.stem_filters, num_filters=filters // filters_multiplier\n        )\n\n        self.cell_0 = FirstCell(\n            in_channels_left=filters,\n            out_channels_left=filters // 2,  # 1, 0.5\n            in_channels_right=2 * filters,\n            out_channels_right=filters,\n        )  # 2, 1\n        self.cell_1 = NormalCell(\n            in_channels_left=2 * filters,\n            out_channels_left=filters,  # 2, 1\n            in_channels_right=6 * filters,\n            out_channels_right=filters,\n        )  # 6, 1\n        self.cell_2 = NormalCell(\n            in_channels_left=6 * filters,\n            out_channels_left=filters,  # 6, 1\n            in_channels_right=6 * filters,\n            out_channels_right=filters,\n        )  # 6, 1\n        self.cell_3 = NormalCell(\n            in_channels_left=6 * filters,\n            out_channels_left=filters,  # 6, 1\n            in_channels_right=6 * filters,\n            out_channels_right=filters,\n        )  # 6, 1\n\n        self.reduction_cell_0 = ReductionCell0(\n            in_channels_left=6 * filters,\n            out_channels_left=2 * filters,  # 6, 2\n            in_channels_right=6 * filters,\n            out_channels_right=2 * filters,\n        )  # 6, 2\n\n        self.cell_6 = FirstCell(\n            in_channels_left=6 * filters,\n            out_channels_left=filters,  # 6, 1\n            in_channels_right=8 * filters,\n            out_channels_right=2 * filters,\n        )  # 8, 2\n        self.cell_7 = NormalCell(\n            in_channels_left=8 * filters,\n            out_channels_left=2 * filters,  # 8, 2\n            in_channels_right=12 * filters,\n            out_channels_right=2 * filters,\n        )  # 12, 2\n        self.cell_8 = NormalCell(\n            in_channels_left=12 * filters,\n            out_channels_left=2 * filters,  # 12, 2\n            in_channels_right=12 * filters,\n            out_channels_right=2 * filters,\n        )  # 12, 2\n        self.cell_9 = NormalCell(\n            in_channels_left=12 * filters,\n            out_channels_left=2 * filters,  # 12, 2\n            in_channels_right=12 * filters,\n            out_channels_right=2 * filters,\n        )  # 12, 2\n\n        self.reduction_cell_1 = ReductionCell1(\n            in_channels_left=12 * filters,\n            out_channels_left=4 * filters,  # 12, 4\n            in_channels_right=12 * filters,\n            out_channels_right=4 * filters,\n        )  # 12, 4\n\n        self.cell_12 = FirstCell(\n            in_channels_left=12 * filters,\n            out_channels_left=2 * filters,  # 12, 2\n            in_channels_right=16 * filters,\n            out_channels_right=4 * filters,\n        )  # 16, 4\n        self.cell_13 = NormalCell(\n            in_channels_left=16 * filters,\n            out_channels_left=4 * filters,  # 16, 4\n            in_channels_right=24 * filters,\n            out_channels_right=4 * filters,\n        )  # 24, 4\n        self.cell_14 = NormalCell(\n            in_channels_left=24 * filters,\n            out_channels_left=4 * filters,  # 24, 4\n            in_channels_right=24 * filters,\n            out_channels_right=4 * filters,\n        )  # 24, 4\n        self.cell_15 = NormalCell(\n            in_channels_left=24 * filters,\n            out_channels_left=4 * filters,  # 24, 4\n            in_channels_right=24 * filters,\n            out_channels_right=4 * filters,\n        )  # 24, 4\n\n        self.relu = nn.ReLU()\n        self.dropout = Dropout(p=0.5)\n        self.classifier = nn.Dense(in_channels=24 * filters, out_channels=num_classes)\n        self.pool = GlobalAvgPooling()\n        self._initialize_weights()\n\n    def _initialize_weights(self):\n        \"\"\"Initialize weights for cells.\"\"\"\n        self.init_parameters_data()\n        for _, cell in self.cells_and_names():\n            if isinstance(cell, nn.Conv2d):\n                n = cell.kernel_size[0] * cell.kernel_size[1] * cell.out_channels\n                cell.weight.set_data(init.initializer(init.Normal(math.sqrt(2. / n), 0),\n                                                      cell.weight.shape, cell.weight.dtype))\n                if cell.bias is not None:\n                    cell.bias.set_data(init.initializer(init.Zero(), cell.bias.shape, cell.bias.dtype))\n            elif isinstance(cell, nn.BatchNorm2d):\n                cell.gamma.set_data(init.initializer(init.One(), cell.gamma.shape, cell.gamma.dtype))\n                cell.beta.set_data(init.initializer(init.Zero(), cell.beta.shape, cell.beta.dtype))\n            elif isinstance(cell, nn.Dense):\n                cell.weight.set_data(init.initializer(init.Normal(0.01, 0), cell.weight.shape, cell.weight.dtype))\n                if cell.bias is not None:\n                    cell.bias.set_data(init.initializer(init.Zero(), cell.bias.shape, cell.bias.dtype))\n\n    def forward_features(self, x: Tensor) -&gt; Tensor:\n        \"\"\"Network forward feature extraction.\"\"\"\n        x_conv0 = self.conv0(x)\n        x_stem_0 = self.cell_stem_0(x_conv0)\n        x_stem_1 = self.cell_stem_1(x_conv0, x_stem_0)\n\n        x_cell_0 = self.cell_0(x_stem_1, x_stem_0)\n        x_cell_1 = self.cell_1(x_cell_0, x_stem_1)\n        x_cell_2 = self.cell_2(x_cell_1, x_cell_0)\n        x_cell_3 = self.cell_3(x_cell_2, x_cell_1)\n\n        x_reduction_cell_0 = self.reduction_cell_0(x_cell_3, x_cell_2)\n\n        x_cell_6 = self.cell_6(x_reduction_cell_0, x_cell_3)\n        x_cell_7 = self.cell_7(x_cell_6, x_reduction_cell_0)\n        x_cell_8 = self.cell_8(x_cell_7, x_cell_6)\n        x_cell_9 = self.cell_9(x_cell_8, x_cell_7)\n\n        x_reduction_cell_1 = self.reduction_cell_1(x_cell_9, x_cell_8)\n\n        x_cell_12 = self.cell_12(x_reduction_cell_1, x_cell_9)\n        x_cell_13 = self.cell_13(x_cell_12, x_reduction_cell_1)\n        x_cell_14 = self.cell_14(x_cell_13, x_cell_12)\n        x_cell_15 = self.cell_15(x_cell_14, x_cell_13)\n\n        x_cell_15 = self.relu(x_cell_15)\n        return x_cell_15\n\n    def forward_head(self, x: Tensor) -&gt; Tensor:\n        x = self.pool(x)  # global average pool\n        x = self.dropout(x)\n        x = self.classifier(x)\n        return x\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n</code></pre>"},{"location":"reference/models/#mindcv.models.nasnet.NASNetAMobile.forward_features","title":"<code>mindcv.models.nasnet.NASNetAMobile.forward_features(x)</code>","text":"<p>Network forward feature extraction.</p> Source code in <code>mindcv\\models\\nasnet.py</code> <pre><code>def forward_features(self, x: Tensor) -&gt; Tensor:\n    \"\"\"Network forward feature extraction.\"\"\"\n    x_conv0 = self.conv0(x)\n    x_stem_0 = self.cell_stem_0(x_conv0)\n    x_stem_1 = self.cell_stem_1(x_conv0, x_stem_0)\n\n    x_cell_0 = self.cell_0(x_stem_1, x_stem_0)\n    x_cell_1 = self.cell_1(x_cell_0, x_stem_1)\n    x_cell_2 = self.cell_2(x_cell_1, x_cell_0)\n    x_cell_3 = self.cell_3(x_cell_2, x_cell_1)\n\n    x_reduction_cell_0 = self.reduction_cell_0(x_cell_3, x_cell_2)\n\n    x_cell_6 = self.cell_6(x_reduction_cell_0, x_cell_3)\n    x_cell_7 = self.cell_7(x_cell_6, x_reduction_cell_0)\n    x_cell_8 = self.cell_8(x_cell_7, x_cell_6)\n    x_cell_9 = self.cell_9(x_cell_8, x_cell_7)\n\n    x_reduction_cell_1 = self.reduction_cell_1(x_cell_9, x_cell_8)\n\n    x_cell_12 = self.cell_12(x_reduction_cell_1, x_cell_9)\n    x_cell_13 = self.cell_13(x_cell_12, x_reduction_cell_1)\n    x_cell_14 = self.cell_14(x_cell_13, x_cell_12)\n    x_cell_15 = self.cell_15(x_cell_14, x_cell_13)\n\n    x_cell_15 = self.relu(x_cell_15)\n    return x_cell_15\n</code></pre>"},{"location":"reference/models/#mindcv.models.nasnet.NormalCell","title":"<code>mindcv.models.nasnet.NormalCell</code>","text":"<p>               Bases: <code>Cell</code></p> <p>NasNet model basic architecture</p> Source code in <code>mindcv\\models\\nasnet.py</code> <pre><code>class NormalCell(nn.Cell):\n    \"\"\"NasNet model basic architecture\"\"\"\n    def __init__(self,\n                 in_channels_left: int,\n                 out_channels_left: int,\n                 in_channels_right: int,\n                 out_channels_right: int) -&gt; None:\n        super().__init__()\n        self.conv_prev_1x1 = nn.SequentialCell([\n            nn.ReLU(),\n            nn.Conv2d(in_channels=in_channels_left, out_channels=out_channels_left, kernel_size=1, stride=1,\n                      pad_mode=\"pad\", has_bias=False),\n            nn.BatchNorm2d(num_features=out_channels_left, eps=0.001, momentum=0.9, affine=True)])\n\n        self.conv_1x1 = nn.SequentialCell([\n            nn.ReLU(),\n            nn.Conv2d(in_channels=in_channels_right, out_channels=out_channels_right, kernel_size=1, stride=1,\n                      pad_mode=\"pad\", has_bias=False),\n            nn.BatchNorm2d(num_features=out_channels_right, eps=0.001, momentum=0.9, affine=True)])\n\n        self.comb_iter_0_left = BranchSeparables(\n            out_channels_right, out_channels_right, 5, 1, 2, bias=False\n        )\n        self.comb_iter_0_right = BranchSeparables(\n            out_channels_left, out_channels_left, 3, 1, 1, bias=False\n        )\n\n        self.comb_iter_1_left = BranchSeparables(\n            out_channels_left, out_channels_left, 5, 1, 2, bias=False\n        )\n        self.comb_iter_1_right = BranchSeparables(\n            out_channels_left, out_channels_left, 3, 1, 1, bias=False\n        )\n\n        self.comb_iter_2_left = nn.AvgPool2d(kernel_size=3, stride=1, pad_mode=\"same\")\n\n        self.comb_iter_3_left = nn.AvgPool2d(kernel_size=3, stride=1, pad_mode=\"same\")\n        self.comb_iter_3_right = nn.AvgPool2d(kernel_size=3, stride=1, pad_mode=\"same\")\n\n        self.comb_iter_4_left = BranchSeparables(\n            out_channels_right, out_channels_right, 3, 1, 1, bias=False\n        )\n\n    def construct(self, x: Tensor, x_prev: Tensor) -&gt; Tensor:\n        x_left = self.conv_prev_1x1(x_prev)\n        x_right = self.conv_1x1(x)\n\n        x_comb_iter_0_left = self.comb_iter_0_left(x_right)\n        x_comb_iter_0_right = self.comb_iter_0_right(x_left)\n        x_comb_iter_0 = x_comb_iter_0_left + x_comb_iter_0_right\n\n        x_comb_iter_1_left = self.comb_iter_1_left(x_left)\n        x_comb_iter_1_right = self.comb_iter_1_right(x_left)\n        x_comb_iter_1 = x_comb_iter_1_left + x_comb_iter_1_right\n\n        x_comb_iter_2_left = self.comb_iter_2_left(x_right)\n        x_comb_iter_2 = x_comb_iter_2_left + x_left\n\n        x_comb_iter_3_left = self.comb_iter_3_left(x_left)\n        x_comb_iter_3_right = self.comb_iter_3_right(x_left)\n        x_comb_iter_3 = x_comb_iter_3_left + x_comb_iter_3_right\n\n        x_comb_iter_4_left = self.comb_iter_4_left(x_right)\n        x_comb_iter_4 = x_comb_iter_4_left + x_right\n\n        x_out = ops.concat((x_left, x_comb_iter_0, x_comb_iter_1, x_comb_iter_2, x_comb_iter_3, x_comb_iter_4), axis=1)\n        return x_out\n</code></pre>"},{"location":"reference/models/#mindcv.models.nasnet.ReductionCell0","title":"<code>mindcv.models.nasnet.ReductionCell0</code>","text":"<p>               Bases: <code>Cell</code></p> <p>NasNet model Residual Connections</p> Source code in <code>mindcv\\models\\nasnet.py</code> <pre><code>class ReductionCell0(nn.Cell):\n    \"\"\"NasNet model Residual Connections\"\"\"\n\n    def __init__(\n        self,\n        in_channels_left: int,\n        out_channels_left: int,\n        in_channels_right: int,\n        out_channels_right: int,\n    ) -&gt; None:\n        super().__init__()\n        self.conv_prev_1x1 = nn.SequentialCell([\n            nn.ReLU(),\n            nn.Conv2d(in_channels=in_channels_left, out_channels=out_channels_left, kernel_size=1, stride=1,\n                      pad_mode=\"pad\", has_bias=False),\n            nn.BatchNorm2d(num_features=out_channels_left, eps=0.001, momentum=0.9, affine=True)])\n\n        self.conv_1x1 = nn.SequentialCell([\n            nn.ReLU(),\n            nn.Conv2d(in_channels=in_channels_right, out_channels=out_channels_right, kernel_size=1, stride=1,\n                      pad_mode=\"pad\", has_bias=False),\n            nn.BatchNorm2d(num_features=out_channels_right, eps=0.001, momentum=0.9, affine=True)])\n\n        self.comb_iter_0_left = BranchSeparablesReduction(\n            out_channels_right, out_channels_right, 5, 2, 2, bias=False\n        )\n        self.comb_iter_0_right = BranchSeparablesReduction(\n            out_channels_right, out_channels_right, 7, 2, 3, bias=False\n        )\n\n        self.comb_iter_1_left = nn.MaxPool2d(3, stride=2, pad_mode=\"same\")\n        self.comb_iter_1_right = BranchSeparablesReduction(\n            out_channels_right, out_channels_right, 7, 2, 3, bias=False\n        )\n\n        self.comb_iter_2_left = nn.AvgPool2d(3, stride=2, pad_mode=\"same\")\n        self.comb_iter_2_right = BranchSeparablesReduction(\n            out_channels_right, out_channels_right, 5, 2, 2, bias=False\n        )\n\n        self.comb_iter_3_right = nn.AvgPool2d(kernel_size=3, stride=1, pad_mode=\"same\")\n\n        self.comb_iter_4_left = BranchSeparablesReduction(\n            out_channels_right, out_channels_right, 3, 1, 1, bias=False\n        )\n        self.comb_iter_4_right = nn.MaxPool2d(3, stride=2, pad_mode=\"same\")\n\n    def construct(self, x: Tensor, x_prev: Tensor) -&gt; Tensor:\n        x_left = self.conv_prev_1x1(x_prev)\n        x_right = self.conv_1x1(x)\n\n        x_comb_iter_0_left = self.comb_iter_0_left(x_right)\n        x_comb_iter_0_right = self.comb_iter_0_right(x_left)\n        x_comb_iter_0 = x_comb_iter_0_left + x_comb_iter_0_right\n\n        x_comb_iter_1_left = self.comb_iter_1_left(x_right)\n        x_comb_iter_1_right = self.comb_iter_1_right(x_left)\n        x_comb_iter_1 = x_comb_iter_1_left + x_comb_iter_1_right\n\n        x_comb_iter_2_left = self.comb_iter_2_left(x_right)\n        x_comb_iter_2_right = self.comb_iter_2_right(x_left)\n        x_comb_iter_2 = x_comb_iter_2_left + x_comb_iter_2_right\n\n        x_comb_iter_3_right = self.comb_iter_3_right(x_comb_iter_0)\n        x_comb_iter_3 = x_comb_iter_3_right + x_comb_iter_1\n\n        x_comb_iter_4_left = self.comb_iter_4_left(x_comb_iter_0)\n        x_comb_iter_4_right = self.comb_iter_4_right(x_right)\n        x_comb_iter_4 = x_comb_iter_4_left + x_comb_iter_4_right\n\n        x_out = ops.concat((x_comb_iter_1, x_comb_iter_2, x_comb_iter_3, x_comb_iter_4), axis=1)\n        return x_out\n</code></pre>"},{"location":"reference/models/#mindcv.models.nasnet.ReductionCell1","title":"<code>mindcv.models.nasnet.ReductionCell1</code>","text":"<p>               Bases: <code>Cell</code></p> <p>NasNet model Residual Connections</p> Source code in <code>mindcv\\models\\nasnet.py</code> <pre><code>class ReductionCell1(nn.Cell):\n    \"\"\"NasNet model Residual Connections\"\"\"\n\n    def __init__(\n        self,\n        in_channels_left: int,\n        out_channels_left: int,\n        in_channels_right: int,\n        out_channels_right: int,\n    ) -&gt; None:\n        super().__init__()\n        self.conv_prev_1x1 = nn.SequentialCell([\n            nn.ReLU(),\n            nn.Conv2d(in_channels=in_channels_left, out_channels=out_channels_left, kernel_size=1, stride=1,\n                      pad_mode=\"pad\", has_bias=False),\n            nn.BatchNorm2d(num_features=out_channels_left, eps=0.001, momentum=0.9, affine=True)])\n\n        self.conv_1x1 = nn.SequentialCell([\n            nn.ReLU(),\n            nn.Conv2d(in_channels=in_channels_right, out_channels=out_channels_right, kernel_size=1, stride=1,\n                      pad_mode=\"pad\", has_bias=False),\n            nn.BatchNorm2d(num_features=out_channels_right, eps=0.001, momentum=0.9, affine=True)])\n\n        self.comb_iter_0_left = BranchSeparables(\n            out_channels_right,\n            out_channels_right,\n            5,\n            2,\n            2,\n            bias=False\n        )\n        self.comb_iter_0_right = BranchSeparables(\n            out_channels_right,\n            out_channels_right,\n            7,\n            2,\n            3,\n            bias=False\n        )\n\n        self.comb_iter_1_left = nn.MaxPool2d(3, stride=2, pad_mode=\"same\")\n        self.comb_iter_1_right = BranchSeparables(\n            out_channels_right,\n            out_channels_right,\n            7,\n            2,\n            3,\n            bias=False\n        )\n\n        self.comb_iter_2_left = nn.AvgPool2d(3, stride=2, pad_mode=\"same\")\n        self.comb_iter_2_right = BranchSeparables(\n            out_channels_right,\n            out_channels_right,\n            5,\n            2,\n            2,\n            bias=False\n        )\n\n        self.comb_iter_3_right = nn.AvgPool2d(kernel_size=3, stride=1, pad_mode=\"same\")\n\n        self.comb_iter_4_left = BranchSeparables(\n            out_channels_right,\n            out_channels_right,\n            3,\n            1,\n            1,\n            bias=False\n        )\n        self.comb_iter_4_right = nn.MaxPool2d(3, stride=2, pad_mode=\"same\")\n\n    def construct(self, x: Tensor, x_prev: Tensor) -&gt; Tensor:\n        x_left = self.conv_prev_1x1(x_prev)\n        x_right = self.conv_1x1(x)\n\n        x_comb_iter_0_left = self.comb_iter_0_left(x_right)\n        x_comb_iter_0_right = self.comb_iter_0_right(x_left)\n        x_comb_iter_0 = x_comb_iter_0_left + x_comb_iter_0_right\n\n        x_comb_iter_1_left = self.comb_iter_1_left(x_right)\n        x_comb_iter_1_right = self.comb_iter_1_right(x_left)\n        x_comb_iter_1 = x_comb_iter_1_left + x_comb_iter_1_right\n\n        x_comb_iter_2_left = self.comb_iter_2_left(x_right)\n        x_comb_iter_2_right = self.comb_iter_2_right(x_left)\n        x_comb_iter_2 = x_comb_iter_2_left + x_comb_iter_2_right\n\n        x_comb_iter_3_right = self.comb_iter_3_right(x_comb_iter_0)\n        x_comb_iter_3 = x_comb_iter_3_right + x_comb_iter_1\n\n        x_comb_iter_4_left = self.comb_iter_4_left(x_comb_iter_0)\n        x_comb_iter_4_right = self.comb_iter_4_right(x_right)\n        x_comb_iter_4 = x_comb_iter_4_left + x_comb_iter_4_right\n\n        x_out = ops.concat((x_comb_iter_1, x_comb_iter_2, x_comb_iter_3, x_comb_iter_4), axis=1)\n        return x_out\n</code></pre>"},{"location":"reference/models/#mindcv.models.nasnet.SeparableConv2d","title":"<code>mindcv.models.nasnet.SeparableConv2d</code>","text":"<p>               Bases: <code>Cell</code></p> <p>depth-wise convolutions + point-wise convolutions</p> Source code in <code>mindcv\\models\\nasnet.py</code> <pre><code>class SeparableConv2d(nn.Cell):\n    \"\"\"depth-wise convolutions + point-wise convolutions\"\"\"\n\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        dw_kernel: int,\n        dw_stride: int,\n        dw_padding: int,\n        bias: bool = False,\n    ) -&gt; None:\n        super().__init__()\n        self.depthwise_conv2d = nn.Conv2d(in_channels=in_channels, out_channels=in_channels, kernel_size=dw_kernel,\n                                          stride=dw_stride, pad_mode=\"pad\", padding=dw_padding, group=in_channels,\n                                          has_bias=bias)\n        self.pointwise_conv2d = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=1, stride=1,\n                                          pad_mode=\"pad\", has_bias=bias)\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        x = self.depthwise_conv2d(x)\n        x = self.pointwise_conv2d(x)\n        return x\n</code></pre>"},{"location":"reference/models/#mindcv.models.nasnet.nasnet_a_4x1056","title":"<code>mindcv.models.nasnet.nasnet_a_4x1056(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get NasNet model. Refer to the base class <code>models.NASNetAMobile</code> for more details.</p> Source code in <code>mindcv\\models\\nasnet.py</code> <pre><code>@register_model\ndef nasnet_a_4x1056(pretrained: bool = False, num_classes: int = 1000, in_channels: int = 3, **kwargs) -&gt; NASNetAMobile:\n    \"\"\"Get NasNet model.\n    Refer to the base class `models.NASNetAMobile` for more details.\"\"\"\n    default_cfg = default_cfgs[\"nasnet_a_4x1056\"]\n    model = NASNetAMobile(in_channels=in_channels, num_classes=num_classes, **kwargs)\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n    return model\n</code></pre>"},{"location":"reference/models/#pit","title":"pit","text":""},{"location":"reference/models/#mindcv.models.pit","title":"<code>mindcv.models.pit</code>","text":"<p>MindSpore implementation of <code>PiT</code>. Refer to Rethinking Spatial Dimensions of Vision Transformers.</p>"},{"location":"reference/models/#mindcv.models.pit.Attention","title":"<code>mindcv.models.pit.Attention</code>","text":"<p>               Bases: <code>Cell</code></p> <p>define multi-head self attention block</p> Source code in <code>mindcv\\models\\pit.py</code> <pre><code>class Attention(nn.Cell):\n    \"\"\"define multi-head self attention block\"\"\"\n\n    def __init__(\n        self,\n        dim: int,\n        num_heads: int = 8,\n        qkv_bias: bool = False,\n        attn_drop: float = 0.0,\n        proj_drop: float = 0.0,\n    ) -&gt; None:\n        super().__init__()\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        self.scale = head_dim**-0.5\n        # get pair-wise relative position index for each token inside the window\n        self.q = nn.Dense(in_channels=dim, out_channels=dim, has_bias=qkv_bias)\n        self.k = nn.Dense(in_channels=dim, out_channels=dim, has_bias=qkv_bias)\n        self.v = nn.Dense(in_channels=dim, out_channels=dim, has_bias=qkv_bias)\n        self.attn_drop = Dropout(p=attn_drop)\n        self.proj = nn.Dense(dim, dim)\n        self.proj_drop = Dropout(p=proj_drop)\n        self.softmax = nn.Softmax(axis=-1)\n\n        self.batchmatmul = ops.BatchMatMul()\n\n    def construct(self, x):\n        B, N, C = x.shape\n        q = ops.reshape(self.q(x), (B, N, self.num_heads, C // self.num_heads)) * self.scale\n        q = ops.transpose(q, (0, 2, 1, 3))\n        k = ops.reshape(self.k(x), (B, N, self.num_heads, C // self.num_heads))\n        k = ops.transpose(k, (0, 2, 3, 1))\n        v = ops.reshape(self.v(x), (B, N, self.num_heads, C // self.num_heads))\n        v = ops.transpose(v, (0, 2, 1, 3))\n\n        attn = self.batchmatmul(q, k)\n        attn = self.softmax(attn)\n        attn = self.attn_drop(attn)\n\n        x = self.batchmatmul(attn, v)\n        x = ops.reshape(ops.transpose(x, (0, 2, 1, 3)), (B, N, C))\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n</code></pre>"},{"location":"reference/models/#mindcv.models.pit.Block","title":"<code>mindcv.models.pit.Block</code>","text":"<p>               Bases: <code>Cell</code></p> <p>define the basic block of PiT</p> Source code in <code>mindcv\\models\\pit.py</code> <pre><code>class Block(nn.Cell):\n    \"\"\"define the basic block of PiT\"\"\"\n\n    def __init__(\n        self,\n        dim: int,\n        num_heads: int,\n        mlp_ratio: float = 4.0,\n        qkv_bias: bool = False,\n        drop: float = 0.0,\n        attn_drop: float = 0.0,\n        drop_path: float = 0.0,\n        act_layer: nn.cell = nn.GELU,\n        norm_layer: nn.cell = nn.LayerNorm,\n    ) -&gt; None:\n        super().__init__()\n        self.norm1 = norm_layer((dim,), epsilon=1e-6)\n        self.attn = Attention(dim, num_heads=num_heads, qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=drop)\n        self.drop_path = DropPath(drop_path) if drop_path &gt; 0.0 else Identity()\n        self.norm2 = norm_layer((dim,), epsilon=1e-6)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n\n    def construct(self, x):\n        x = x + self.drop_path(self.attn(self.norm1(x)))\n        x = x + self.drop_path(self.mlp(self.norm2(x)))\n        return x\n</code></pre>"},{"location":"reference/models/#mindcv.models.pit.Mlp","title":"<code>mindcv.models.pit.Mlp</code>","text":"<p>               Bases: <code>Cell</code></p> <p>MLP as used in Vision Transformer, MLP-Mixer and related networks</p> Source code in <code>mindcv\\models\\pit.py</code> <pre><code>class Mlp(nn.Cell):\n    \"\"\"MLP as used in Vision Transformer, MLP-Mixer and related networks\"\"\"\n\n    def __init__(\n        self,\n        in_features: int,\n        hidden_features: int = None,\n        out_features: int = None,\n        act_layer: nn.cell = nn.GELU,\n        drop: float = 0.0,\n    ) -&gt; None:\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Dense(in_channels=in_features, out_channels=hidden_features, has_bias=True)\n        self.act = act_layer()\n        self.fc2 = nn.Dense(in_channels=hidden_features, out_channels=out_features, has_bias=True)\n        self.drop = Dropout(p=drop)\n\n    def construct(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.drop(x)\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x\n</code></pre>"},{"location":"reference/models/#mindcv.models.pit.PoolingTransformer","title":"<code>mindcv.models.pit.PoolingTransformer</code>","text":"<p>               Bases: <code>Cell</code></p> <p>PiT model class, based on <code>\"Rethinking Spatial Dimensions of Vision Transformers\" &lt;https://arxiv.org/abs/2103.16302&gt;</code> Args:     image_size (int) : images input size.     patch_size (int) : image patch size.     stride (int) : stride of the depthwise conv.     base_dims (List[int]) : middle dim of each layer.     depth (List[int]) : model block depth of each layer.     heads (List[int]) : number of heads of multi-head attention of each layer     mlp_ratio (float) : ratio of hidden features in Mlp.     num_classes (int) : number of classification classes. Default: 1000.     in_chans (int) : number the channels of the input. Default: 3.     attn_drop_rate (float) : attention layers dropout rate. Default: 0.     drop_rate (float) : dropout rate. Default: 0.     drop_path_rate (float) : drop path rate. Default: 0.</p> Source code in <code>mindcv\\models\\pit.py</code> <pre><code>class PoolingTransformer(nn.Cell):\n    r\"\"\"PiT model class, based on\n    `\"Rethinking Spatial Dimensions of Vision Transformers\"\n    &lt;https://arxiv.org/abs/2103.16302&gt;`\n    Args:\n        image_size (int) : images input size.\n        patch_size (int) : image patch size.\n        stride (int) : stride of the depthwise conv.\n        base_dims (List[int]) : middle dim of each layer.\n        depth (List[int]) : model block depth of each layer.\n        heads (List[int]) : number of heads of multi-head attention of each layer\n        mlp_ratio (float) : ratio of hidden features in Mlp.\n        num_classes (int) : number of classification classes. Default: 1000.\n        in_chans (int) : number the channels of the input. Default: 3.\n        attn_drop_rate (float) : attention layers dropout rate. Default: 0.\n        drop_rate (float) : dropout rate. Default: 0.\n        drop_path_rate (float) : drop path rate. Default: 0.\n    \"\"\"\n\n    def __init__(\n        self,\n        image_size: int,\n        patch_size: int,\n        stride: int,\n        base_dims: List[int],\n        depth: List[int],\n        heads: List[int],\n        mlp_ratio: float,\n        num_classes: int = 1000,\n        in_chans: int = 3,\n        attn_drop_rate: float = 0.0,\n        drop_rate: float = 0.0,\n        drop_path_rate: float = 0.0,\n    ) -&gt; None:\n        super().__init__()\n\n        total_block = sum(depth)\n        padding = 0\n        block_idx = 0\n\n        width = math.floor((image_size + 2 * padding - patch_size) / stride + 1)\n\n        self.base_dims = base_dims\n        self.heads = heads\n        self.num_classes = num_classes\n\n        self.patch_size = patch_size\n        self.pos_embed = Parameter(Tensor(np.random.randn(1, base_dims[0] * heads[0], width, width), mstype.float32))\n        self.patch_embed = conv_embedding(in_chans, base_dims[0] * heads[0], patch_size, stride, padding)\n        self.cls_token = Parameter(Tensor(np.random.randn(1, 1, base_dims[0] * heads[0]), mstype.float32))\n\n        self.pos_drop = Dropout(p=drop_rate)\n        self.tile = ops.Tile()\n\n        self.transformers = nn.CellList([])\n        self.pools = nn.CellList([])\n\n        for stage in range(len(depth)):\n            drop_path_prob = [drop_path_rate * i / total_block for i in range(block_idx, block_idx + depth[stage])]\n            block_idx += depth[stage]\n            self.transformers.append(\n                Transformer(\n                    base_dims[stage], depth[stage], heads[stage], mlp_ratio, drop_rate, attn_drop_rate, drop_path_prob\n                )\n            )\n            if stage &lt; len(heads) - 1:\n                self.pools.append(\n                    conv_head_pooling(\n                        base_dims[stage] * heads[stage], base_dims[stage + 1] * heads[stage + 1], stride=2\n                    )\n                )\n\n        self.norm = nn.LayerNorm((base_dims[-1] * heads[-1],), epsilon=1e-6)\n\n        self.embed_dim = base_dims[-1] * heads[-1]\n\n        # Classifier head\n        if num_classes &gt; 0:\n            self.head = nn.Dense(in_channels=base_dims[-1] * heads[-1], out_channels=num_classes, has_bias=True)\n        else:\n            self.head = Identity()\n\n        self.pos_embed.set_data(\n            init.initializer(init.TruncatedNormal(sigma=0.02), self.pos_embed.shape, self.pos_embed.dtype)\n        )\n        self.cls_token.set_data(\n            init.initializer(init.TruncatedNormal(sigma=0.02), self.cls_token.shape, self.cls_token.dtype)\n        )\n        self._initialize_weights()\n\n    def _initialize_weights(self) -&gt; None:\n        \"\"\"init_weights\"\"\"\n        for _, cell in self.cells_and_names():\n            if isinstance(cell, nn.LayerNorm):\n                cell.gamma.set_data(init.initializer(init.One(), cell.gamma.shape, cell.gamma.dtype))\n                cell.beta.set_data(init.initializer(init.Zero(), cell.beta.shape, cell.beta.dtype))\n            if isinstance(cell, nn.Conv2d):\n                n = cell.kernel_size[0] * cell.kernel_size[1] * cell.in_channels\n                cell.weight.set_data(\n                    init.initializer(init.Uniform(math.sqrt(1.0 / n)), cell.weight.shape, cell.weight.dtype)\n                )\n                if cell.bias is not None:\n                    cell.bias.set_data(\n                        init.initializer(init.Uniform(math.sqrt(1.0 / n)), cell.bias.shape, cell.bias.dtype)\n                    )\n            if isinstance(cell, nn.Dense):\n                init_range = 1.0 / np.sqrt(cell.weight.shape[0])\n                cell.weight.set_data(init.initializer(init.Uniform(init_range), cell.weight.shape, cell.weight.dtype))\n                if cell.bias is not None:\n                    cell.bias.set_data(init.initializer(init.Uniform(init_range), cell.bias.shape, cell.bias.dtype))\n\n    def forward_features(self, x: Tensor) -&gt; Tensor:\n        x = self.patch_embed(x)\n\n        pos_embed = self.pos_embed\n        x = self.pos_drop(x + pos_embed)\n\n        cls_tokens = self.tile(self.cls_token, (x.shape[0], 1, 1))\n\n        for stage in range(len(self.pools)):\n            x, cls_tokens = self.transformers[stage](x, cls_tokens)\n            x, cls_tokens = self.pools[stage](x, cls_tokens)\n        x, cls_tokens = self.transformers[-1](x, cls_tokens)\n\n        cls_tokens = self.norm(cls_tokens)\n\n        return cls_tokens\n\n    def forward_head(self, x: Tensor) -&gt; Tensor:\n        cls_token = self.head(x[:, 0])\n        return cls_token\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        cls_token = self.forward_features(x)\n        cls_token = self.forward_head(cls_token)\n        return cls_token\n</code></pre>"},{"location":"reference/models/#mindcv.models.pit.Transformer","title":"<code>mindcv.models.pit.Transformer</code>","text":"<p>               Bases: <code>Cell</code></p> <p>define the transformer block of PiT</p> Source code in <code>mindcv\\models\\pit.py</code> <pre><code>class Transformer(nn.Cell):\n    \"\"\"define the transformer block of PiT\"\"\"\n\n    def __init__(\n        self,\n        base_dim: List[int],\n        depth: List[int],\n        heads: List[int],\n        mlp_ratio: float,\n        drop_rate: float = 0.0,\n        attn_drop_rate: float = 0.0,\n        drop_path_prob: float = None,\n    ) -&gt; None:\n        super().__init__()\n        self.layers = nn.CellList([])\n        embed_dim = base_dim * heads\n\n        if drop_path_prob is None:\n            drop_path_prob = [0.0 for _ in range(depth)]\n\n        self.blocks = nn.CellList(\n            [\n                Block(\n                    dim=embed_dim,\n                    num_heads=heads,\n                    mlp_ratio=mlp_ratio,\n                    qkv_bias=True,\n                    drop=drop_rate,\n                    attn_drop=attn_drop_rate,\n                    drop_path=drop_path_prob[i],\n                    norm_layer=nn.LayerNorm,\n                )\n                for i in range(depth)\n            ]\n        )\n\n    def construct(self, x, cls_tokens):\n        h, w = x.shape[2:4]\n        x = ops.reshape(x, (x.shape[0], x.shape[1], h * w))\n        x = ops.transpose(x, (0, 2, 1))\n        token_length = cls_tokens.shape[1]\n        x = ops.concat((cls_tokens, x), axis=1)\n        for blk in self.blocks:\n            x = blk(x)\n\n        cls_tokens = x[:, :token_length]\n        x = x[:, token_length:]\n        x = ops.transpose(x, (0, 2, 1))\n        x = ops.reshape(x, (x.shape[0], x.shape[1], h, w))\n        return x, cls_tokens\n</code></pre>"},{"location":"reference/models/#mindcv.models.pit.conv_embedding","title":"<code>mindcv.models.pit.conv_embedding</code>","text":"<p>               Bases: <code>Cell</code></p> <p>define embedding layer using conv2d</p> Source code in <code>mindcv\\models\\pit.py</code> <pre><code>class conv_embedding(nn.Cell):\n    \"\"\"define embedding layer using conv2d\"\"\"\n\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        patch_size: int,\n        stride: int,\n        padding: int,\n    ) -&gt; None:\n        super().__init__()\n        self.conv = nn.Conv2d(\n            in_channels,\n            out_channels,\n            kernel_size=patch_size,\n            stride=stride,\n            pad_mode=\"pad\",\n            padding=padding,\n            has_bias=True,\n        )\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        x = self.conv(x)\n        return x\n</code></pre>"},{"location":"reference/models/#mindcv.models.pit.conv_head_pooling","title":"<code>mindcv.models.pit.conv_head_pooling</code>","text":"<p>               Bases: <code>Cell</code></p> <p>define pooling layer using conv in spatial tokens with an additional fully-connected layer (to adjust the channel size to match the spatial tokens)</p> Source code in <code>mindcv\\models\\pit.py</code> <pre><code>class conv_head_pooling(nn.Cell):\n    \"\"\"define pooling layer using conv in spatial tokens with an additional fully-connected layer\n    (to adjust the channel size to match the spatial tokens)\"\"\"\n\n    def __init__(\n        self,\n        in_feature: int,\n        out_feature: int,\n        stride: int,\n        pad_mode: str = \"pad\",\n    ) -&gt; None:\n        super().__init__()\n        self.conv = nn.Conv2d(\n            in_feature,\n            out_feature,\n            kernel_size=stride + 1,\n            padding=stride // 2,\n            stride=stride,\n            pad_mode=pad_mode,\n            group=in_feature,\n            has_bias=True,\n        )\n        self.fc = nn.Dense(in_channels=in_feature, out_channels=out_feature, has_bias=True)\n\n    def construct(self, x, cls_token):\n        x = self.conv(x)\n        cls_token = self.fc(cls_token)\n\n        return x, cls_token\n</code></pre>"},{"location":"reference/models/#mindcv.models.pit.pit_b","title":"<code>mindcv.models.pit.pit_b(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get PiT-B model. Refer to the base class <code>models.PoolingTransformer</code> for more details.</p> Source code in <code>mindcv\\models\\pit.py</code> <pre><code>@register_model\ndef pit_b(pretrained: bool = False, num_classes: int = 1000, in_channels: int = 3, **kwargs) -&gt; PoolingTransformer:\n    \"\"\"Get PiT-B model.\n    Refer to the base class `models.PoolingTransformer` for more details.\"\"\"\n    default_cfg = default_cfgs[\"pit_b\"]\n    model = PoolingTransformer(\n        image_size=224,\n        patch_size=14,\n        stride=7,\n        base_dims=[64, 64, 64],\n        depth=[3, 6, 4],\n        heads=[4, 8, 16],\n        mlp_ratio=4.0,\n        num_classes=num_classes,\n        in_chans=in_channels,\n        **kwargs\n    )\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre>"},{"location":"reference/models/#mindcv.models.pit.pit_s","title":"<code>mindcv.models.pit.pit_s(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get PiT-S model. Refer to the base class <code>models.PoolingTransformer</code> for more details.</p> Source code in <code>mindcv\\models\\pit.py</code> <pre><code>@register_model\ndef pit_s(pretrained: bool = False, num_classes: int = 1000, in_channels: int = 3, **kwargs) -&gt; PoolingTransformer:\n    \"\"\"Get PiT-S model.\n    Refer to the base class `models.PoolingTransformer` for more details.\"\"\"\n    default_cfg = default_cfgs[\"pit_s\"]\n    model = PoolingTransformer(\n        image_size=224,\n        patch_size=16,\n        stride=8,\n        base_dims=[48, 48, 48],\n        depth=[2, 6, 4],\n        heads=[3, 6, 12],\n        mlp_ratio=4.0,\n        num_classes=num_classes,\n        in_chans=in_channels,\n        **kwargs\n    )\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre>"},{"location":"reference/models/#mindcv.models.pit.pit_ti","title":"<code>mindcv.models.pit.pit_ti(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get PiT-Ti model. Refer to the base class <code>models.PoolingTransformer</code> for more details.</p> Source code in <code>mindcv\\models\\pit.py</code> <pre><code>@register_model\ndef pit_ti(pretrained: bool = False, num_classes: int = 1000, in_channels: int = 3, **kwargs) -&gt; PoolingTransformer:\n    \"\"\"Get PiT-Ti model.\n    Refer to the base class `models.PoolingTransformer` for more details.\"\"\"\n    default_cfg = default_cfgs[\"pit_ti\"]\n    model = PoolingTransformer(\n        image_size=224,\n        patch_size=16,\n        stride=8,\n        base_dims=[32, 32, 32],\n        depth=[2, 6, 4],\n        heads=[2, 4, 8],\n        mlp_ratio=4.0,\n        num_classes=num_classes,\n        in_chans=in_channels,\n        **kwargs\n    )\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre>"},{"location":"reference/models/#mindcv.models.pit.pit_xs","title":"<code>mindcv.models.pit.pit_xs(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get PiT-XS model. Refer to the base class <code>models.PoolingTransformer</code> for more details.</p> Source code in <code>mindcv\\models\\pit.py</code> <pre><code>@register_model\ndef pit_xs(pretrained: bool = False, num_classes: int = 1000, in_channels: int = 3, **kwargs) -&gt; PoolingTransformer:\n    \"\"\"Get PiT-XS model.\n    Refer to the base class `models.PoolingTransformer` for more details.\"\"\"\n    default_cfg = default_cfgs[\"pit_xs\"]\n    model = PoolingTransformer(\n        image_size=224,\n        patch_size=16,\n        stride=8,\n        base_dims=[48, 48, 48],\n        depth=[2, 6, 4],\n        heads=[2, 4, 8],\n        mlp_ratio=4.0,\n        num_classes=num_classes,\n        in_chans=in_channels,\n        **kwargs\n    )\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre>"},{"location":"reference/models/#poolformer","title":"poolformer","text":""},{"location":"reference/models/#mindcv.models.poolformer","title":"<code>mindcv.models.poolformer</code>","text":"<p>MindSpore implementation of <code>poolformer</code>. Refer to PoolFormer: MetaFormer Is Actually What You Need for Vision.</p>"},{"location":"reference/models/#mindcv.models.poolformer.ConvMlp","title":"<code>mindcv.models.poolformer.ConvMlp</code>","text":"<p>               Bases: <code>Cell</code></p> <p>MLP using 1x1 convs that keeps spatial dims</p> Source code in <code>mindcv\\models\\poolformer.py</code> <pre><code>class ConvMlp(nn.Cell):\n    \"\"\"MLP using 1x1 convs that keeps spatial dims\"\"\"\n\n    def __init__(\n        self,\n        in_features,\n        hidden_features=None,\n        out_features=None,\n        act_layer=nn.GELU,\n        norm_layer=None,\n        bias=True,\n        drop=0.0,\n    ):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        bias = to_2tuple(bias)\n\n        self.fc1 = nn.Conv2d(in_features, hidden_features, kernel_size=1, has_bias=bias[0])\n        self.norm = norm_layer(hidden_features) if norm_layer else Identity()\n        self.act = act_layer(approximate=False)\n        self.drop = Dropout(p=drop)\n        self.fc2 = nn.Conv2d(hidden_features, out_features, kernel_size=1, has_bias=bias[1])\n        self.cls_init_weights()\n\n    def cls_init_weights(self):\n        \"\"\"Initialize weights for cells.\"\"\"\n        for name, m in self.cells_and_names():\n            if isinstance(m, nn.Conv2d):\n                m.weight.set_data(\n                    init.initializer(init.TruncatedNormal(sigma=.02), m.weight.shape, m.weight.dtype))\n                if m.bias is not None:\n                    m.bias.set_data(\n                        init.initializer(init.Constant(0), m.bias.shape, m.bias.dtype))\n\n    def construct(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.drop(x)\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x\n</code></pre>"},{"location":"reference/models/#mindcv.models.poolformer.ConvMlp.cls_init_weights","title":"<code>mindcv.models.poolformer.ConvMlp.cls_init_weights()</code>","text":"<p>Initialize weights for cells.</p> Source code in <code>mindcv\\models\\poolformer.py</code> <pre><code>def cls_init_weights(self):\n    \"\"\"Initialize weights for cells.\"\"\"\n    for name, m in self.cells_and_names():\n        if isinstance(m, nn.Conv2d):\n            m.weight.set_data(\n                init.initializer(init.TruncatedNormal(sigma=.02), m.weight.shape, m.weight.dtype))\n            if m.bias is not None:\n                m.bias.set_data(\n                    init.initializer(init.Constant(0), m.bias.shape, m.bias.dtype))\n</code></pre>"},{"location":"reference/models/#mindcv.models.poolformer.PatchEmbed","title":"<code>mindcv.models.poolformer.PatchEmbed</code>","text":"<p>               Bases: <code>Cell</code></p> <p>Patch Embedding that is implemented by a layer of conv. Input: tensor in shape [B, C, H, W] Output: tensor in shape [B, C, H/stride, W/stride]</p> Source code in <code>mindcv\\models\\poolformer.py</code> <pre><code>class PatchEmbed(nn.Cell):\n    \"\"\"Patch Embedding that is implemented by a layer of conv.\n    Input: tensor in shape [B, C, H, W]\n    Output: tensor in shape [B, C, H/stride, W/stride]\"\"\"\n\n    def __init__(self, in_chs=3, embed_dim=768, patch_size=16, stride=16, padding=0, norm_layer=None):\n        super().__init__()\n        patch_size = to_2tuple(patch_size)\n        stride = to_2tuple(stride)\n        # padding = to_2tuple(padding)\n        self.proj = nn.Conv2d(in_chs, embed_dim, kernel_size=patch_size, stride=stride, padding=padding, pad_mode=\"pad\",\n                              has_bias=True)\n        self.norm = norm_layer(embed_dim) if norm_layer else Identity()\n\n    def construct(self, x):\n        x = self.proj(x)\n        x = self.norm(x)\n        return x\n</code></pre>"},{"location":"reference/models/#mindcv.models.poolformer.PoolFormer","title":"<code>mindcv.models.poolformer.PoolFormer</code>","text":"<p>               Bases: <code>Cell</code></p> <p>PoolFormer model class, based on <code>\"MetaFormer Is Actually What You Need for Vision\" &lt;https://arxiv.org/pdf/2111.11418v3.pdf&gt;</code>_</p> PARAMETER DESCRIPTION <code>layers</code> <p>number of blocks for the 4 stages</p> <p> </p> <code>embed_dims</code> <p>the embedding dims for the 4 stages. Default: (64, 128, 320, 512)</p> <p> DEFAULT: <code>(64, 128, 320, 512)</code> </p> <code>mlp_ratios</code> <p>mlp ratios for the 4 stages. Default: (4, 4, 4, 4)</p> <p> DEFAULT: <code>(4, 4, 4, 4)</code> </p> <code>downsamples</code> <p>flags to apply downsampling or not. Default: (True, True, True, True)</p> <p> DEFAULT: <code>(True, True, True, True)</code> </p> <code>pool_size</code> <p>the pooling size for the 4 stages. Default: 3</p> <p> DEFAULT: <code>3</code> </p> <code>in_chans</code> <p>number of input channels. Default: 3</p> <p> DEFAULT: <code>3</code> </p> <code>num_classes</code> <p>number of classes for the image classification. Default: 1000</p> <p> DEFAULT: <code>1000</code> </p> <code>global_pool</code> <p>define the types of pooling layer. Default: avg</p> <p> DEFAULT: <code>'avg'</code> </p> <code>norm_layer</code> <p>define the types of normalization. Default: nn.GroupNorm</p> <p> DEFAULT: <code>GroupNorm</code> </p> <code>act_layer</code> <p>define the types of activation. Default: nn.GELU</p> <p> DEFAULT: <code>GELU</code> </p> <code>in_patch_size</code> <p>specify the patch embedding for the input image. Default: 7</p> <p> DEFAULT: <code>7</code> </p> <code>in_stride</code> <p>specify the stride for the input image. Default: 4.</p> <p> DEFAULT: <code>4</code> </p> <code>in_pad</code> <p>specify the pad for the input image. Default: 2.</p> <p> DEFAULT: <code>2</code> </p> <code>down_patch_size</code> <p>specify the downsample. Default: 3.</p> <p> DEFAULT: <code>3</code> </p> <code>down_stride</code> <p>specify the downsample (patch embed.). Default: 2.</p> <p> DEFAULT: <code>2</code> </p> <code>down_pad</code> <p>specify the downsample (patch embed.). Default: 1.</p> <p> DEFAULT: <code>1</code> </p> <code>drop_rate</code> <p>dropout rate of the layer before main classifier. Default: 0.</p> <p> DEFAULT: <code>0.0</code> </p> <code>drop_path_rate</code> <p>Stochastic Depth. Default: 0.</p> <p> DEFAULT: <code>0.0</code> </p> <code>layer_scale_init_value</code> <p>LayerScale. Default: 1e-5.</p> <p> DEFAULT: <code>1e-05</code> </p> <code>fork_feat</code> <p>whether output features of the 4 stages, for dense prediction. Default: False.</p> <p> DEFAULT: <code>False</code> </p> Source code in <code>mindcv\\models\\poolformer.py</code> <pre><code>class PoolFormer(nn.Cell):\n    r\"\"\"PoolFormer model class, based on\n    `\"MetaFormer Is Actually What You Need for Vision\" &lt;https://arxiv.org/pdf/2111.11418v3.pdf&gt;`_\n\n    Args:\n        layers: number of blocks for the 4 stages\n        embed_dims: the embedding dims for the 4 stages. Default: (64, 128, 320, 512)\n        mlp_ratios: mlp ratios for the 4 stages. Default: (4, 4, 4, 4)\n        downsamples: flags to apply downsampling or not. Default: (True, True, True, True)\n        pool_size: the pooling size for the 4 stages. Default: 3\n        in_chans: number of input channels. Default: 3\n        num_classes: number of classes for the image classification. Default: 1000\n        global_pool: define the types of pooling layer. Default: avg\n        norm_layer: define the types of normalization. Default: nn.GroupNorm\n        act_layer: define the types of activation. Default: nn.GELU\n        in_patch_size: specify the patch embedding for the input image. Default: 7\n        in_stride: specify the stride for the input image. Default: 4.\n        in_pad: specify the pad for the input image. Default: 2.\n        down_patch_size: specify the downsample. Default: 3.\n        down_stride: specify the downsample (patch embed.). Default: 2.\n        down_pad: specify the downsample (patch embed.). Default: 1.\n        drop_rate: dropout rate of the layer before main classifier. Default: 0.\n        drop_path_rate: Stochastic Depth. Default: 0.\n        layer_scale_init_value: LayerScale. Default: 1e-5.\n        fork_feat: whether output features of the 4 stages, for dense prediction. Default: False.\n    \"\"\"\n\n    def __init__(\n        self,\n        layers,\n        embed_dims=(64, 128, 320, 512),\n        mlp_ratios=(4, 4, 4, 4),\n        downsamples=(True, True, True, True),\n        pool_size=3,\n        in_chans=3,\n        num_classes=1000,\n        global_pool=\"avg\",\n        norm_layer=nn.GroupNorm,\n        act_layer=nn.GELU,\n        in_patch_size=7,\n        in_stride=4,\n        in_pad=2,\n        down_patch_size=3,\n        down_stride=2,\n        down_pad=1,\n        drop_rate=0.0,\n        drop_path_rate=0.0,\n        layer_scale_init_value=1e-5,\n        fork_feat=False,\n    ):\n        super().__init__()\n\n        if not fork_feat:\n            self.num_classes = num_classes\n        self.fork_feat = fork_feat\n\n        self.global_pool = global_pool\n        self.num_features = embed_dims[-1]\n        self.grad_checkpointing = False\n\n        self.patch_embed = PatchEmbed(\n            patch_size=in_patch_size, stride=in_stride, padding=in_pad,\n            in_chs=in_chans, embed_dim=embed_dims[0])\n\n        # set the main block in network\n        network = []\n        for i in range(len(layers)):\n            network.append(basic_blocks(\n                embed_dims[i], i, layers,\n                pool_size=pool_size, mlp_ratio=mlp_ratios[i],\n                act_layer=act_layer, norm_layer=norm_layer,\n                drop_rate=drop_rate, drop_path_rate=drop_path_rate,\n                layer_scale_init_value=layer_scale_init_value)\n            )\n            if i &lt; len(layers) - 1 and (downsamples[i] or embed_dims[i] != embed_dims[i + 1]):\n                # downsampling between stages\n                network.append(PatchEmbed(\n                    in_chs=embed_dims[i], embed_dim=embed_dims[i + 1],\n                    patch_size=down_patch_size, stride=down_stride, padding=down_pad)\n                )\n\n        self.network = nn.SequentialCell(*network)\n        self.norm = norm_layer(1, embed_dims[-1])\n        self.head = nn.Dense(embed_dims[-1], num_classes, has_bias=True) if num_classes &gt; 0 else Identity()\n        # self._initialize_weights()\n        self.cls_init_weights()\n\n    def cls_init_weights(self):\n        \"\"\"Initialize weights for cells.\"\"\"\n        for name, m in self.cells_and_names():\n            if isinstance(m, nn.Dense):\n                m.weight.set_data(\n                    init.initializer(init.TruncatedNormal(sigma=.02), m.weight.shape, m.weight.dtype))\n                if m.bias is not None:\n                    m.bias.set_data(\n                        init.initializer(init.Constant(0), m.bias.shape, m.bias.dtype))\n\n    def reset_classifier(self, num_classes, global_pool=None):\n        self.num_classes = num_classes\n        if global_pool is not None:\n            self.global_pool = global_pool\n        self.head = nn.Dense(self.num_features, num_classes) if num_classes &gt; 0 else Identity()\n\n    def forward_features(self, x: Tensor) -&gt; Tensor:\n        x = self.patch_embed(x)\n        x = self.network(x)\n        if self.fork_feat:\n            # otuput features of four stages for dense prediction\n            return x\n        x = self.norm(x)\n        return x\n\n    def forward_head(self, x: Tensor) -&gt; Tensor:\n        return self.head(x.mean([-2, -1]))\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        x = self.forward_features(x)\n        return self.forward_head(x)\n</code></pre>"},{"location":"reference/models/#mindcv.models.poolformer.PoolFormer.cls_init_weights","title":"<code>mindcv.models.poolformer.PoolFormer.cls_init_weights()</code>","text":"<p>Initialize weights for cells.</p> Source code in <code>mindcv\\models\\poolformer.py</code> <pre><code>def cls_init_weights(self):\n    \"\"\"Initialize weights for cells.\"\"\"\n    for name, m in self.cells_and_names():\n        if isinstance(m, nn.Dense):\n            m.weight.set_data(\n                init.initializer(init.TruncatedNormal(sigma=.02), m.weight.shape, m.weight.dtype))\n            if m.bias is not None:\n                m.bias.set_data(\n                    init.initializer(init.Constant(0), m.bias.shape, m.bias.dtype))\n</code></pre>"},{"location":"reference/models/#mindcv.models.poolformer.PoolFormerBlock","title":"<code>mindcv.models.poolformer.PoolFormerBlock</code>","text":"<p>               Bases: <code>Cell</code></p> <p>Implementation of one PoolFormer block.</p> Source code in <code>mindcv\\models\\poolformer.py</code> <pre><code>class PoolFormerBlock(nn.Cell):\n    \"\"\"Implementation of one PoolFormer block.\"\"\"\n\n    def __init__(\n        self,\n        dim,\n        pool_size=3,\n        mlp_ratio=4.0,\n        act_layer=nn.GELU,\n        norm_layer=nn.GroupNorm,\n        drop=0.0,\n        drop_path=0.0,\n        layer_scale_init_value=1e-5,\n    ):\n        super().__init__()\n        self.norm1 = norm_layer(1, dim)\n        self.token_mixer = Pooling(pool_size=pool_size)\n        self.drop_path = DropPath(drop_path) if drop_path &gt; 0.0 else Identity()\n        self.norm2 = norm_layer(1, dim)\n        self.mlp = ConvMlp(dim, hidden_features=int(dim * mlp_ratio), act_layer=act_layer, drop=drop)\n\n        if layer_scale_init_value:\n            layer_scale_init_tensor = Tensor(layer_scale_init_value * np.ones([dim]).astype(np.float32))\n            self.layer_scale_1 = mindspore.Parameter(layer_scale_init_tensor)\n            self.layer_scale_2 = mindspore.Parameter(layer_scale_init_tensor)\n        else:\n            self.layer_scale_1 = None\n            self.layer_scale_2 = None\n        self.expand_dims = ops.ExpandDims()\n\n    def construct(self, x):\n        if self.layer_scale_1 is not None:\n            x = x + self.drop_path(\n                self.expand_dims(self.expand_dims(self.layer_scale_1, -1), -1) * self.token_mixer(self.norm1(x)))\n            x = x + self.drop_path(\n                self.expand_dims(self.expand_dims(self.layer_scale_2, -1), -1) * self.mlp(self.norm2(x)))\n        else:\n            x = x + self.drop_path(self.token_mixer(self.norm1(x)))\n            x = x + self.drop_path(self.mlp(self.norm2(x)))\n        return x\n</code></pre>"},{"location":"reference/models/#mindcv.models.poolformer.basic_blocks","title":"<code>mindcv.models.poolformer.basic_blocks(dim, index, layers, pool_size=3, mlp_ratio=4.0, act_layer=nn.GELU, norm_layer=nn.GroupNorm, drop_rate=0.0, drop_path_rate=0.0, layer_scale_init_value=1e-05)</code>","text":"<p>generate PoolFormer blocks for a stage</p> Source code in <code>mindcv\\models\\poolformer.py</code> <pre><code>def basic_blocks(\n    dim,\n    index,\n    layers,\n    pool_size=3,\n    mlp_ratio=4.0,\n    act_layer=nn.GELU,\n    norm_layer=nn.GroupNorm,\n    drop_rate=0.0,\n    drop_path_rate=0.0,\n    layer_scale_init_value=1e-5,\n):\n    \"\"\"generate PoolFormer blocks for a stage\"\"\"\n    blocks = []\n    for block_idx in range(layers[index]):\n        block_dpr = drop_path_rate * (block_idx + sum(layers[:index])) / (sum(layers) - 1)\n        blocks.append(PoolFormerBlock(\n            dim, pool_size=pool_size, mlp_ratio=mlp_ratio,\n            act_layer=act_layer, norm_layer=norm_layer,\n            drop=drop_rate, drop_path=block_dpr,\n            layer_scale_init_value=layer_scale_init_value,\n        ))\n    blocks = nn.SequentialCell(*blocks)\n    return blocks\n</code></pre>"},{"location":"reference/models/#mindcv.models.poolformer.poolformer_m36","title":"<code>mindcv.models.poolformer.poolformer_m36(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get poolformer_m36 model. Refer to the base class <code>models.PoolFormer</code> for more details.</p> Source code in <code>mindcv\\models\\poolformer.py</code> <pre><code>@register_model\ndef poolformer_m36(pretrained: bool = False, num_classes: int = 1000, in_channels: int = 3, **kwargs) -&gt; PoolFormer:\n    \"\"\"Get poolformer_m36 model.\n    Refer to the base class `models.PoolFormer` for more details.\"\"\"\n    default_cfg = default_cfgs[\"poolformer_m36\"]\n    layers = (6, 6, 18, 6)\n    embed_dims = (96, 192, 384, 768)\n    model = PoolFormer(\n        in_chans=in_channels,\n        num_classes=num_classes,\n        layers=layers,\n        layer_scale_init_value=1e-6,\n        embed_dims=embed_dims,\n        **kwargs\n    )\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n    return model\n</code></pre>"},{"location":"reference/models/#mindcv.models.poolformer.poolformer_m48","title":"<code>mindcv.models.poolformer.poolformer_m48(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get poolformer_m48 model. Refer to the base class <code>models.PoolFormer</code> for more details.</p> Source code in <code>mindcv\\models\\poolformer.py</code> <pre><code>@register_model\ndef poolformer_m48(pretrained: bool = False, num_classes: int = 1000, in_channels: int = 3, **kwargs) -&gt; PoolFormer:\n    \"\"\"Get poolformer_m48 model.\n    Refer to the base class `models.PoolFormer` for more details.\"\"\"\n    default_cfg = default_cfgs[\"poolformer_m48\"]\n    layers = (8, 8, 24, 8)\n    embed_dims = (96, 192, 384, 768)\n    model = PoolFormer(\n        in_chans=in_channels,\n        num_classes=num_classes,\n        layers=layers,\n        layer_scale_init_value=1e-6,\n        embed_dims=embed_dims,\n        **kwargs\n    )\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n    return model\n</code></pre>"},{"location":"reference/models/#mindcv.models.poolformer.poolformer_s12","title":"<code>mindcv.models.poolformer.poolformer_s12(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get poolformer_s12 model. Refer to the base class <code>models.PoolFormer</code> for more details.</p> Source code in <code>mindcv\\models\\poolformer.py</code> <pre><code>@register_model\ndef poolformer_s12(pretrained: bool = False, num_classes: int = 1000, in_channels: int = 3, **kwargs) -&gt; PoolFormer:\n    \"\"\"Get poolformer_s12 model.\n    Refer to the base class `models.PoolFormer` for more details.\"\"\"\n    default_cfg = default_cfgs[\"poolformer_s12\"]\n    model = PoolFormer(in_chans=in_channels, num_classes=num_classes, layers=(2, 2, 6, 2), **kwargs)\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n    return model\n</code></pre>"},{"location":"reference/models/#mindcv.models.poolformer.poolformer_s24","title":"<code>mindcv.models.poolformer.poolformer_s24(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get poolformer_s24 model. Refer to the base class <code>models.PoolFormer</code> for more details.</p> Source code in <code>mindcv\\models\\poolformer.py</code> <pre><code>@register_model\ndef poolformer_s24(pretrained: bool = False, num_classes: int = 1000, in_channels: int = 3, **kwargs) -&gt; PoolFormer:\n    \"\"\"Get poolformer_s24 model.\n    Refer to the base class `models.PoolFormer` for more details.\"\"\"\n    default_cfg = default_cfgs[\"poolformer_s24\"]\n    model = PoolFormer(in_chans=in_channels, num_classes=num_classes, layers=(4, 4, 12, 4), **kwargs)\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n    return model\n</code></pre>"},{"location":"reference/models/#mindcv.models.poolformer.poolformer_s36","title":"<code>mindcv.models.poolformer.poolformer_s36(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get poolformer_s36 model. Refer to the base class <code>models.PoolFormer</code> for more details.</p> Source code in <code>mindcv\\models\\poolformer.py</code> <pre><code>@register_model\ndef poolformer_s36(pretrained: bool = False, num_classes: int = 1000, in_channels: int = 3, **kwargs) -&gt; PoolFormer:\n    \"\"\"Get poolformer_s36 model.\n    Refer to the base class `models.PoolFormer` for more details.\"\"\"\n    default_cfg = default_cfgs[\"poolformer_s36\"]\n    model = PoolFormer(\n        in_chans=in_channels, num_classes=num_classes, layers=(6, 6, 18, 6), layer_scale_init_value=1e-6, **kwargs\n    )\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n    return model\n</code></pre>"},{"location":"reference/models/#pvt","title":"pvt","text":""},{"location":"reference/models/#mindcv.models.pvt","title":"<code>mindcv.models.pvt</code>","text":"<p>MindSpore implementation of <code>PVT</code>. Refer to PVT: Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions</p>"},{"location":"reference/models/#mindcv.models.pvt.Attention","title":"<code>mindcv.models.pvt.Attention</code>","text":"<p>               Bases: <code>Cell</code></p> <p>spatial-reduction attention (SRA)</p> Source code in <code>mindcv\\models\\pvt.py</code> <pre><code>class Attention(nn.Cell):\n    \"\"\"spatial-reduction attention (SRA)\"\"\"\n\n    def __init__(\n        self,\n        dim: int,\n        num_heads: int = 8,\n        qkv_bias: bool = False,\n        qk_scale: Optional[float] = None,\n        attn_drop: float = 0.0,\n        proj_drop: float = 0.0,\n        sr_ratio: int = 1,\n    ):\n        super(Attention, self).__init__()\n        assert dim % num_heads == 0, f\"dim {dim} should be divided by num_heads {num_heads}.\"\n\n        self.dim = dim\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        self.scale = qk_scale or head_dim**-0.5\n\n        self.q = nn.Dense(dim, dim, has_bias=qkv_bias)\n        self.kv = nn.Dense(dim, dim * 2, has_bias=qkv_bias)\n        self.attn_drop = Dropout(p=attn_drop)\n        self.proj = nn.Dense(dim, dim)\n        self.proj_drop = Dropout(p=proj_drop)\n        self.qk_batmatmul = ops.BatchMatMul(transpose_b=True)\n        self.batmatmul = ops.BatchMatMul()\n        self.softmax = nn.Softmax(axis=-1)\n        self.reshape = ops.reshape\n        self.transpose = ops.transpose\n\n        self.sr_ratio = sr_ratio\n        if sr_ratio &gt; 1:\n            self.sr = nn.Conv2d(dim, dim, kernel_size=sr_ratio, stride=sr_ratio, has_bias=True)\n            self.norm = nn.LayerNorm([dim])\n\n    def construct(self, x, H, W):\n        B, N, C = x.shape\n        q = self.q(x)\n        q = self.reshape(q, (B, N, self.num_heads, C // self.num_heads))\n        q = self.transpose(q, (0, 2, 1, 3))\n        if self.sr_ratio &gt; 1:\n            x_ = self.reshape(self.transpose(x, (0, 2, 1)), (B, C, H, W))\n\n            x_ = self.transpose(self.reshape(self.sr(x_), (B, C, -1)), (0, 2, 1))\n            x_ = self.norm(x_)\n            kv = self.kv(x_)\n\n            kv = self.transpose(self.reshape(kv, (B, -1, 2, self.num_heads, C // self.num_heads)), (2, 0, 3, 1, 4))\n        else:\n            kv = self.kv(x)\n            kv = self.transpose(self.reshape(kv, (B, -1, 2, self.num_heads, C // self.num_heads)), (2, 0, 3, 1, 4))\n        k, v = kv[0], kv[1]\n        attn = self.qk_batmatmul(q, k) * self.scale\n        attn = self.softmax(attn)\n        attn = self.attn_drop(attn)\n        x = self.batmatmul(attn, v)\n        x = self.reshape(self.transpose(x, (0, 2, 1, 3)), (B, N, C))\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n</code></pre>"},{"location":"reference/models/#mindcv.models.pvt.Block","title":"<code>mindcv.models.pvt.Block</code>","text":"<p>               Bases: <code>Cell</code></p> <p>Block with spatial-reduction attention (SRA) and feed forward</p> Source code in <code>mindcv\\models\\pvt.py</code> <pre><code>class Block(nn.Cell):\n    \"\"\" Block with spatial-reduction attention (SRA) and feed forward\"\"\"\n    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm, sr_ratio=1):\n        super(Block, self).__init__()\n        self.norm1 = norm_layer([dim], epsilon=1e-5)\n        self.attn = Attention(\n            dim,\n            num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale,\n            attn_drop=attn_drop, proj_drop=drop, sr_ratio=sr_ratio)\n        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n        self.drop_path = DropPath(drop_path) if drop_path &gt; 0.0 else Identity()\n        self.norm2 = norm_layer([dim])\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n\n    def construct(self, x, H, W):\n        x1 = self.norm1(x)\n        x1 = self.attn(x1, H, W)\n        x = x + self.drop_path(x1)\n        x = x + self.drop_path(self.mlp(self.norm2(x)))\n\n        return x\n</code></pre>"},{"location":"reference/models/#mindcv.models.pvt.PatchEmbed","title":"<code>mindcv.models.pvt.PatchEmbed</code>","text":"<p>               Bases: <code>Cell</code></p> <p>Image to Patch Embedding</p> Source code in <code>mindcv\\models\\pvt.py</code> <pre><code>class PatchEmbed(nn.Cell):\n    \"\"\"Image to Patch Embedding\"\"\"\n\n    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n        super().__init__()\n\n        img_size = (img_size, img_size)\n        patch_size = (patch_size, patch_size)\n\n        self.img_size = img_size\n        self.patch_size = patch_size\n\n        self.H, self.W = img_size[0] // patch_size[0], img_size[1] // patch_size[1]\n        self.num_patches = self.H * self.W\n        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size, has_bias=True)\n        self.norm = nn.LayerNorm([embed_dim], epsilon=1e-5)\n        self.reshape = ops.reshape\n        self.transpose = ops.transpose\n\n    def construct(self, x):\n        B, C, H, W = x.shape\n\n        x = self.proj(x)\n        b, c, h, w = x.shape\n        x = self.reshape(x, (b, c, h * w))\n        x = self.transpose(x, (0, 2, 1))\n        x = self.norm(x)\n        H, W = H // self.patch_size[0], W // self.patch_size[1]\n\n        return x, (H, W)\n</code></pre>"},{"location":"reference/models/#mindcv.models.pvt.PyramidVisionTransformer","title":"<code>mindcv.models.pvt.PyramidVisionTransformer</code>","text":"<p>               Bases: <code>Cell</code></p> <p>Pyramid Vision Transformer model class, based on <code>\"Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions\" &lt;https://arxiv.org/abs/2102.12122&gt;</code>_  # noqa: E501</p> PARAMETER DESCRIPTION <code>img_size(int)</code> <p>size of a input image.</p> <p> </p> <code>patch_size</code> <p>size of a single image patch.</p> <p> TYPE: <code>int) </code> DEFAULT: <code>4</code> </p> <code>in_chans</code> <p>number the channels of the input. Default: 3.</p> <p> TYPE: <code>int) </code> DEFAULT: <code>3</code> </p> <code>num_classes</code> <p>number of classification classes. Default: 1000.</p> <p> TYPE: <code>int) </code> DEFAULT: <code>1000</code> </p> <code>embed_dims</code> <p>how many hidden dim in each PatchEmbed.</p> <p> TYPE: <code>list) </code> DEFAULT: <code>[64, 128, 320, 512]</code> </p> <code>num_heads</code> <p>number of attention head in each stage.</p> <p> TYPE: <code>list) </code> DEFAULT: <code>[1, 2, 5, 8]</code> </p> <code>mlp_ratios</code> <p>ratios of MLP hidden dims in each stage.</p> <p> TYPE: <code>list</code> DEFAULT: <code>[8, 8, 4, 4]</code> </p> <code>qkv_bias(bool)</code> <p>use bias in attention.</p> <p> </p> <code>qk_scale(float)</code> <p>Scale multiplied by qk in attention(if not none), otherwise head_dim ** -0.5.</p> <p> </p> <code>drop_rate(float)</code> <p>The drop rate for each block. Default: 0.0.</p> <p> </p> <code>attn_drop_rate(float)</code> <p>The drop rate for attention. Default: 0.0.</p> <p> </p> <code>drop_path_rate(float)</code> <p>The drop rate for drop path. Default: 0.0.</p> <p> </p> <code>norm_layer(nn.Cell)</code> <p>Norm layer that will be used in blocks. Default: nn.LayerNorm.</p> <p> </p> <code>depths</code> <p>number of Blocks.</p> <p> TYPE: <code>list) </code> DEFAULT: <code>[2, 2, 2, 2]</code> </p> <code>sr_ratios(list)</code> <p>stride and kernel size of each attention.</p> <p> </p> <code>num_stages(int)</code> <p>number of stage. Default: 4.</p> <p> </p> Source code in <code>mindcv\\models\\pvt.py</code> <pre><code>class PyramidVisionTransformer(nn.Cell):\n    r\"\"\"Pyramid Vision Transformer model class, based on\n    `\"Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions\" &lt;https://arxiv.org/abs/2102.12122&gt;`_  # noqa: E501\n\n    Args:\n        img_size(int) : size of a input image.\n        patch_size (int) : size of a single image patch.\n        in_chans (int) : number the channels of the input. Default: 3.\n        num_classes (int) : number of classification classes. Default: 1000.\n        embed_dims (list) : how many hidden dim in each PatchEmbed.\n        num_heads (list) : number of attention head in each stage.\n        mlp_ratios (list): ratios of MLP hidden dims in each stage.\n        qkv_bias(bool) : use bias in attention.\n        qk_scale(float) : Scale multiplied by qk in attention(if not none), otherwise head_dim ** -0.5.\n        drop_rate(float) : The drop rate for each block. Default: 0.0.\n        attn_drop_rate(float) : The drop rate for attention. Default: 0.0.\n        drop_path_rate(float) : The drop rate for drop path. Default: 0.0.\n        norm_layer(nn.Cell) : Norm layer that will be used in blocks. Default: nn.LayerNorm.\n        depths (list) : number of Blocks.\n        sr_ratios(list) : stride and kernel size of each attention.\n        num_stages(int) : number of stage. Default: 4.\n    \"\"\"\n\n    def __init__(self, img_size=224, patch_size=4, in_chans=3, num_classes=1000, embed_dims=[64, 128, 320, 512],\n                 num_heads=[1, 2, 5, 8], mlp_ratios=[8, 8, 4, 4], qkv_bias=True, qk_scale=None, drop_rate=0.0,\n                 attn_drop_rate=0.0, drop_path_rate=0.0, norm_layer=nn.LayerNorm,\n                 depths=[2, 2, 2, 2], sr_ratios=[8, 4, 2, 1], num_stages=4):\n        super(PyramidVisionTransformer, self).__init__()\n        self.num_classes = num_classes\n        self.depths = depths\n        self.num_stages = num_stages\n        dpr = [x.item() for x in np.linspace(0, drop_path_rate, sum(depths))]  # stochastic depth decay rule\n        cur = 0\n        b_list = []\n        self.pos_embed = []\n        self.pos_drop = Dropout(p=drop_rate)\n        for i in range(num_stages):\n            block = nn.CellList(\n                [Block(dim=embed_dims[i], num_heads=num_heads[i], mlp_ratio=mlp_ratios[i], qkv_bias=qkv_bias,\n                       qk_scale=qk_scale, drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[cur + j],\n                       norm_layer=norm_layer, sr_ratio=sr_ratios[i])\n                 for j in range(depths[i])\n                 ])\n\n            b_list.append(block)\n            cur += depths[0]\n\n        self.patch_embed1 = PatchEmbed(img_size=img_size,\n                                       patch_size=patch_size,\n                                       in_chans=in_chans,\n                                       embed_dim=embed_dims[0])\n        num_patches = self.patch_embed1.num_patches\n        self.pos_embed1 = mindspore.Parameter(ops.zeros((1, num_patches, embed_dims[0]), mindspore.float16))\n        self.pos_drop1 = Dropout(p=drop_rate)\n\n        self.patch_embed2 = PatchEmbed(img_size=img_size // (2 ** (1 + 1)),\n                                       patch_size=2,\n                                       in_chans=embed_dims[1 - 1],\n                                       embed_dim=embed_dims[1])\n        num_patches = self.patch_embed2.num_patches\n        self.pos_embed2 = mindspore.Parameter(ops.zeros((1, num_patches, embed_dims[1]), mindspore.float16))\n        self.pos_drop2 = Dropout(p=drop_rate)\n\n        self.patch_embed3 = PatchEmbed(img_size=img_size // (2 ** (2 + 1)),\n                                       patch_size=2,\n                                       in_chans=embed_dims[2 - 1],\n                                       embed_dim=embed_dims[2])\n        num_patches = self.patch_embed3.num_patches\n        self.pos_embed3 = mindspore.Parameter(ops.zeros((1, num_patches, embed_dims[2]), mindspore.float16))\n        self.pos_drop3 = Dropout(p=drop_rate)\n\n        self.patch_embed4 = PatchEmbed(img_size // (2 ** (3 + 1)),\n                                       patch_size=2,\n                                       in_chans=embed_dims[3 - 1],\n                                       embed_dim=embed_dims[3])\n        num_patches = self.patch_embed4.num_patches + 1\n        self.pos_embed4 = mindspore.Parameter(ops.zeros((1, num_patches, embed_dims[3]), mindspore.float16))\n        self.pos_drop4 = Dropout(p=drop_rate)\n        self.Blocks = nn.CellList(b_list)\n\n        self.norm = norm_layer([embed_dims[3]])\n\n        # cls_token\n        self.cls_token = mindspore.Parameter(ops.zeros((1, 1, embed_dims[3]), mindspore.float32))\n\n        # classification head\n        self.head = nn.Dense(embed_dims[3], num_classes) if num_classes &gt; 0 else Identity()\n        self.reshape = ops.reshape\n        self.transpose = ops.transpose\n        self.tile = ops.Tile()\n        self.Concat = ops.Concat(axis=1)\n        self._initialize_weights()\n\n    def _initialize_weights(self):\n        for _, cell in self.cells_and_names():\n            if isinstance(cell, nn.Dense):\n                cell.weight.set_data(weight_init.initializer(weight_init.TruncatedNormal(sigma=0.02),\n                                                             cell.weight.shape, cell.weight.dtype))\n                if isinstance(cell, nn.Dense) and cell.bias is not None:\n                    cell.bias.set_data(weight_init.initializer(weight_init.Zero(), cell.bias.shape, cell.bias.dtype))\n            elif isinstance(cell, nn.LayerNorm):\n                cell.gamma.set_data(weight_init.initializer(weight_init.One(), cell.gamma.shape, cell.gamma.dtype))\n                cell.beta.set_data(weight_init.initializer(weight_init.Zero(), cell.beta.shape, cell.beta.dtype))\n            elif isinstance(cell, nn.Conv2d):\n                fan_out = cell.kernel_size[0] * cell.kernel_size[1] * cell.out_channels\n                fan_out //= cell.group\n                cell.weight.set_data(weight_init.initializer(weight_init.Normal(sigma=math.sqrt(2.0 / fan_out)),\n                                                             cell.weight.shape, cell.weight.dtype))\n                if cell.bias is not None:\n                    cell.bias.set_data(weight_init.initializer(weight_init.Zero(), cell.bias.shape, cell.bias.dtype))\n\n    def get_classifier(self):\n        return self.head\n\n    def reset_classifier(self, num_classes, global_pool=\"\"):\n        self.num_classes = num_classes\n        self.head = nn.Dense(self.embed_dim, num_classes) if num_classes &gt; 0 else Identity()\n\n    def _get_pos_embed(self, pos_embed, ph, pw, H, W):\n        if H * W == self.patch_embed1.num_patches:\n            return pos_embed\n        else:\n            pos_embed = self.transpose(self.reshape(pos_embed, (1, ph, pw, -1)), (0, 3, 1, 2))\n            resize_bilinear = ResizeBilinear((H, W))\n            pos_embed = resize_bilinear(pos_embed)\n\n            pos_embed = self.transpose(self.reshape(pos_embed, (1, -1, H * W)), (0, 2, 1))\n\n            return pos_embed\n\n    def forward_features(self, x):\n        B = x.shape[0]\n\n        x, (H, W) = self.patch_embed1(x)\n        pos_embed = self.pos_embed1\n        x = self.pos_drop1(x + pos_embed)\n        for blk in self.Blocks[0]:\n            x = blk(x, H, W)\n        x = self.transpose(self.reshape(x, (B, H, W, -1)), (0, 3, 1, 2))\n\n        x, (H, W) = self.patch_embed2(x)\n        ph, pw = self.patch_embed2.H, self.patch_embed2.W\n        pos_embed = self._get_pos_embed(self.pos_embed2, ph, pw, H, W)\n        x = self.pos_drop2(x + pos_embed)\n        for blk in self.Blocks[1]:\n            x = blk(x, H, W)\n        x = self.transpose(self.reshape(x, (B, H, W, -1)), (0, 3, 1, 2))\n\n        x, (H, W) = self.patch_embed3(x)\n        ph, pw = self.patch_embed3.H, self.patch_embed3.W\n        pos_embed = self._get_pos_embed(self.pos_embed3, ph, pw, H, W)\n        x = self.pos_drop3(x + pos_embed)\n        for blk in self.Blocks[2]:\n            x = blk(x, H, W)\n        x = self.transpose(self.reshape(x, (B, H, W, -1)), (0, 3, 1, 2))\n\n        x, (H, W) = self.patch_embed4(x)\n        cls_tokens = self.tile(self.cls_token, (B, 1, 1))\n\n        x = self.Concat((cls_tokens, x))\n        ph, pw = self.patch_embed4.H, self.patch_embed4.W\n        pos_embed_ = self._get_pos_embed(self.pos_embed4[:, 1:], ph, pw, H, W)\n        pos_embed = self.Concat((self.pos_embed4[:, 0:1], pos_embed_))\n        x = self.pos_drop4(x + pos_embed)\n        for blk in self.Blocks[3]:\n            x = blk(x, H, W)\n\n        x = self.norm(x)\n\n        return x[:, 0]\n\n    def forward_head(self, x: Tensor) -&gt; Tensor:\n        return self.head(x)\n\n    def construct(self, x):\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n\n        return x\n</code></pre>"},{"location":"reference/models/#mindcv.models.pvt.pvt_large","title":"<code>mindcv.models.pvt.pvt_large(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get PVT large model Refer to the base class \"models.PVT\" for more details.</p> Source code in <code>mindcv\\models\\pvt.py</code> <pre><code>@register_model\ndef pvt_large(\n    pretrained: bool = False, num_classes: int = 1000, in_channels: int = 3, **kwargs\n) -&gt; PyramidVisionTransformer:\n    \"\"\"Get PVT large model\n    Refer to the base class \"models.PVT\" for more details.\n    \"\"\"\n    default_cfg = default_cfgs['pvt_large']\n    model = PyramidVisionTransformer(in_chans=in_channels, num_classes=num_classes,\n                                     patch_size=4, embed_dims=[64, 128, 320, 512], num_heads=[1, 2, 5, 8],\n                                     mlp_ratios=[8, 8, 4, 4], qkv_bias=True,\n                                     norm_layer=partial(nn.LayerNorm, epsilon=1e-6), depths=[3, 8, 27, 3],\n                                     sr_ratios=[8, 4, 2, 1], **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre>"},{"location":"reference/models/#mindcv.models.pvt.pvt_medium","title":"<code>mindcv.models.pvt.pvt_medium(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get PVT medium model Refer to the base class \"models.PVT\" for more details.</p> Source code in <code>mindcv\\models\\pvt.py</code> <pre><code>@register_model\ndef pvt_medium(\n    pretrained: bool = False, num_classes: int = 1000, in_channels: int = 3, **kwargs\n) -&gt; PyramidVisionTransformer:\n    \"\"\"Get PVT medium model\n    Refer to the base class \"models.PVT\" for more details.\n    \"\"\"\n    default_cfg = default_cfgs['pvt_medium']\n    model = PyramidVisionTransformer(in_chans=in_channels, num_classes=num_classes,\n                                     patch_size=4, embed_dims=[64, 128, 320, 512], num_heads=[1, 2, 5, 8],\n                                     mlp_ratios=[8, 8, 4, 4], qkv_bias=True,\n                                     norm_layer=partial(nn.LayerNorm, epsilon=1e-6), depths=[3, 4, 18, 3],\n                                     sr_ratios=[8, 4, 2, 1], **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre>"},{"location":"reference/models/#mindcv.models.pvt.pvt_small","title":"<code>mindcv.models.pvt.pvt_small(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get PVT small model Refer to the base class \"models.PVT\" for more details.</p> Source code in <code>mindcv\\models\\pvt.py</code> <pre><code>@register_model\ndef pvt_small(\n    pretrained: bool = False, num_classes: int = 1000, in_channels: int = 3, **kwargs\n) -&gt; PyramidVisionTransformer:\n    \"\"\"Get PVT small model\n    Refer to the base class \"models.PVT\" for more details.\n    \"\"\"\n    default_cfg = default_cfgs['pvt_small']\n    model = PyramidVisionTransformer(in_chans=in_channels, num_classes=num_classes,\n                                     patch_size=4, embed_dims=[64, 128, 320, 512], num_heads=[1, 2, 5, 8],\n                                     mlp_ratios=[8, 8, 4, 4], qkv_bias=True,\n                                     norm_layer=partial(nn.LayerNorm, epsilon=1e-6), depths=[3, 4, 6, 3],\n                                     sr_ratios=[8, 4, 2, 1], **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre>"},{"location":"reference/models/#mindcv.models.pvt.pvt_tiny","title":"<code>mindcv.models.pvt.pvt_tiny(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get PVT tiny model Refer to the base class \"models.PVT\" for more details.</p> Source code in <code>mindcv\\models\\pvt.py</code> <pre><code>@register_model\ndef pvt_tiny(\n    pretrained: bool = False, num_classes: int = 1000, in_channels: int = 3, **kwargs\n) -&gt; PyramidVisionTransformer:\n    \"\"\"Get PVT tiny model\n    Refer to the base class \"models.PVT\" for more details.\n    \"\"\"\n    default_cfg = default_cfgs['pvt_tiny']\n    model = PyramidVisionTransformer(in_chans=in_channels, num_classes=num_classes,\n                                     patch_size=4, embed_dims=[64, 128, 320, 512], num_heads=[1, 2, 5, 8],\n                                     mlp_ratios=[8, 8, 4, 4], qkv_bias=True,\n                                     norm_layer=partial(nn.LayerNorm, epsilon=1e-6), depths=[2, 2, 2, 2],\n                                     sr_ratios=[8, 4, 2, 1], **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre>"},{"location":"reference/models/#pvtv2","title":"pvtv2","text":""},{"location":"reference/models/#mindcv.models.pvtv2","title":"<code>mindcv.models.pvtv2</code>","text":"<p>MindSpore implementation of <code>PVTv2</code>. Refer to PVTv2: PVTv2: Improved Baselines with Pyramid Vision Transformer</p>"},{"location":"reference/models/#mindcv.models.pvtv2.Attention","title":"<code>mindcv.models.pvtv2.Attention</code>","text":"<p>               Bases: <code>Cell</code></p> <p>Linear Spatial Reduction Attention</p> Source code in <code>mindcv\\models\\pvtv2.py</code> <pre><code>class Attention(nn.Cell):\n    \"\"\"Linear Spatial Reduction Attention\"\"\"\n\n    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0., sr_ratio=1,\n                 linear=False):\n        super().__init__()\n        assert dim % num_heads == 0, f\"dim {dim} should be divided by num_heads {num_heads}.\"\n\n        self.dim = dim\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        self.scale = qk_scale or head_dim**-0.5\n\n        self.q = nn.Dense(dim, dim, has_bias=qkv_bias)\n        self.kv = nn.Dense(dim, dim * 2, has_bias=qkv_bias)\n        self.attn_drop = Dropout(p=attn_drop)\n        self.proj = nn.Dense(dim, dim)\n        self.proj_drop = Dropout(p=proj_drop)\n        self.qk_batmatmul = ops.BatchMatMul(transpose_b=True)\n        self.batmatmul = ops.BatchMatMul()\n        self.softmax = nn.Softmax(axis=-1)\n\n        self.linear = linear\n        self.sr_ratio = sr_ratio\n        if not linear:\n            if sr_ratio &gt; 1:\n                self.sr = nn.Conv2d(dim, dim, kernel_size=sr_ratio, stride=sr_ratio, has_bias=True)\n                self.norm = nn.LayerNorm([dim])\n\n        else:\n            self.pool = nn.AdaptiveAvgPool2d(7)\n            self.sr = nn.Conv2d(dim, dim, kernel_size=1, stride=1, has_bias=True)\n            self.norm = nn.LayerNorm([dim])\n            self.act = nn.GELU()\n\n    def construct(self, x, H, W):\n        B, N, C = x.shape\n        q = self.q(x)\n        q = ops.reshape(q, (B, N, self.num_heads, C // self.num_heads))\n        q = ops.transpose(q, (0, 2, 1, 3))\n\n        if not self.linear:\n            if self.sr_ratio &gt; 1:\n                x_ = ops.reshape(ops.transpose(x, (0, 2, 1)), (B, C, H, W))\n\n                x_ = self.sr(x_)\n                x_ = ops.transpose(ops.reshape(x_, (B, C, -1)), (0, 2, 1))\n                x_ = self.norm(x_)\n\n                kv = self.kv(x_)\n                kv = ops.transpose(ops.reshape(kv, (B, -1, 2, self.num_heads, C // self.num_heads)), (2, 0, 3, 1, 4))\n            else:\n                kv = self.kv(x)\n                kv = ops.transpose(ops.reshape(kv, (B, -1, 2, self.num_heads, C // self.num_heads)), (2, 0, 3, 1, 4))\n\n        else:\n            x_ = ops.reshape(ops.transpose(x, (0, 2, 1)), (B, C, H, W))\n            x_ = self.sr(self.pool(x_))\n            x_ = ops.reshape(ops.transpose(x_, (0, 2, 1)), (B, C, -1))\n            x_ = self.norm(x_)\n            x_ = self.act(x_)\n            kv = ops.transpose(ops.reshape(self.kv(x_), (B, -1, 2, self.num_heads, C // self.num_heads)),\n                               (2, 0, 3, 1, 4))\n        k, v = kv[0], kv[1]\n\n        attn = self.qk_batmatmul(q, k) * self.scale\n        attn = self.softmax(attn)\n        attn = self.attn_drop(attn)\n\n        x = self.batmatmul(attn, v)\n        x = ops.reshape(ops.transpose(x, (0, 2, 1, 3)), (B, N, C))\n        x = self.proj(x)\n        x = self.proj_drop(x)\n\n        return x\n</code></pre>"},{"location":"reference/models/#mindcv.models.pvtv2.Block","title":"<code>mindcv.models.pvtv2.Block</code>","text":"<p>               Bases: <code>Cell</code></p> <p>Block with Linear Spatial Reduction Attention and Convolutional Feed-Forward</p> Source code in <code>mindcv\\models\\pvtv2.py</code> <pre><code>class Block(nn.Cell):\n    \"\"\"Block with Linear Spatial Reduction Attention and Convolutional Feed-Forward\"\"\"\n\n    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm, sr_ratio=1, linear=False, block_id=0):\n        super().__init__()\n        self.norm1 = norm_layer([dim])\n\n        self.attn = Attention(\n            dim,\n            num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale,\n            attn_drop=attn_drop, proj_drop=drop, sr_ratio=sr_ratio, linear=linear)\n\n        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n        self.drop_path = DropPath(drop_path) if drop_path &gt; 0.0 else Identity()\n\n        self.norm2 = norm_layer([dim])\n\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop, linear=linear)\n\n    def construct(self, x, H, W):\n        x = x + self.drop_path(self.attn(self.norm1(x), H, W))\n        x = x + self.drop_path(self.mlp(self.norm2(x), H, W))\n\n        return x\n</code></pre>"},{"location":"reference/models/#mindcv.models.pvtv2.DWConv","title":"<code>mindcv.models.pvtv2.DWConv</code>","text":"<p>               Bases: <code>Cell</code></p> <p>Depthwise separable convolution</p> Source code in <code>mindcv\\models\\pvtv2.py</code> <pre><code>class DWConv(nn.Cell):\n    \"\"\"Depthwise separable convolution\"\"\"\n\n    def __init__(self, dim=768):\n        super(DWConv, self).__init__()\n        self.dwconv = nn.Conv2d(dim, dim, 3, 1, has_bias=True, group=dim)\n\n    def construct(self, x, H, W):\n        B, N, C = x.shape\n        x = ops.transpose(x, (0, 2, 1)).view((B, C, H, W))\n        x = self.dwconv(x)\n        x = ops.transpose(x.view((B, C, H * W)), (0, 2, 1))\n\n        return x\n</code></pre>"},{"location":"reference/models/#mindcv.models.pvtv2.Mlp","title":"<code>mindcv.models.pvtv2.Mlp</code>","text":"<p>               Bases: <code>Cell</code></p> <p>MLP with depthwise separable convolution</p> Source code in <code>mindcv\\models\\pvtv2.py</code> <pre><code>class Mlp(nn.Cell):\n    \"\"\"MLP with depthwise separable convolution\"\"\"\n\n    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.0, linear=False):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Dense(in_features, hidden_features)\n        self.dwconv = DWConv(hidden_features)\n        self.act = act_layer()\n        self.fc2 = nn.Dense(hidden_features, out_features)\n        self.drop = Dropout(p=drop)\n        self.linear = linear\n        if self.linear:\n            self.relu = nn.ReLU()\n\n    def construct(self, x, H, W):\n        x = self.fc1(x)\n        if self.linear:\n            x = self.relu(x)\n        x = self.dwconv(x, H, W)\n        x = self.act(x)\n        x = self.drop(x)\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x\n</code></pre>"},{"location":"reference/models/#mindcv.models.pvtv2.OverlapPatchEmbed","title":"<code>mindcv.models.pvtv2.OverlapPatchEmbed</code>","text":"<p>               Bases: <code>Cell</code></p> <p>Overlapping Patch Embedding</p> Source code in <code>mindcv\\models\\pvtv2.py</code> <pre><code>class OverlapPatchEmbed(nn.Cell):\n    \"\"\"Overlapping Patch Embedding\"\"\"\n\n    def __init__(self, img_size=224, patch_size=7, stride=4, in_chans=3, embed_dim=768):\n        super().__init__()\n\n        img_size = (img_size, img_size)\n        patch_size = (patch_size, patch_size)\n\n        assert max(patch_size) &gt; stride, \"Set larger patch_size than stride\"\n\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.H, self.W = img_size[0] // stride, img_size[1] // stride\n        self.num_patches = self.H * self.W\n        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=stride, has_bias=True)\n        self.norm = nn.LayerNorm([embed_dim])\n\n    def construct(self, x):\n        x = self.proj(x)\n        B, C, H, W = x.shape\n        x = ops.transpose(ops.reshape(x, (B, C, H * W)), (0, 2, 1))\n        x = self.norm(x)\n\n        return x, H, W\n</code></pre>"},{"location":"reference/models/#mindcv.models.pvtv2.PyramidVisionTransformerV2","title":"<code>mindcv.models.pvtv2.PyramidVisionTransformerV2</code>","text":"<p>               Bases: <code>Cell</code></p> <p>Pyramid Vision Transformer V2 model class, based on <code>\"PVTv2: Improved Baselines with Pyramid Vision Transformer\" &lt;https://arxiv.org/abs/2106.13797&gt;</code>_</p> PARAMETER DESCRIPTION <code>img_size(int)</code> <p>size of a input image.</p> <p> </p> <code>patch_size</code> <p>size of a single image patch.</p> <p> TYPE: <code>int) </code> DEFAULT: <code>16</code> </p> <code>in_chans</code> <p>number the channels of the input. Default: 3.</p> <p> TYPE: <code>int) </code> DEFAULT: <code>3</code> </p> <code>num_classes</code> <p>number of classification classes. Default: 1000.</p> <p> TYPE: <code>int) </code> DEFAULT: <code>1000</code> </p> <code>embed_dims</code> <p>how many hidden dim in each PatchEmbed.</p> <p> TYPE: <code>list) </code> DEFAULT: <code>[64, 128, 256, 512]</code> </p> <code>num_heads</code> <p>number of attention head in each stage.</p> <p> TYPE: <code>list) </code> DEFAULT: <code>[1, 2, 4, 8]</code> </p> <code>mlp_ratios</code> <p>ratios of MLP hidden dims in each stage.</p> <p> TYPE: <code>list</code> DEFAULT: <code>[4, 4, 4, 4]</code> </p> <code>qkv_bias(bool)</code> <p>use bias in attention.</p> <p> </p> <code>qk_scale(float)</code> <p>Scale multiplied by qk in attention(if not none), otherwise head_dim ** -0.5.</p> <p> </p> <code>drop_rate(float)</code> <p>The drop rate for each block. Default: 0.0.</p> <p> </p> <code>attn_drop_rate(float)</code> <p>The drop rate for attention. Default: 0.0.</p> <p> </p> <code>drop_path_rate(float)</code> <p>The drop rate for drop path. Default: 0.0.</p> <p> </p> <code>norm_layer(nn.Cell)</code> <p>Norm layer that will be used in blocks. Default: nn.LayerNorm.</p> <p> </p> <code>depths</code> <p>number of Blocks.</p> <p> TYPE: <code>list) </code> DEFAULT: <code>[3, 4, 6, 3]</code> </p> <code>sr_ratios(list)</code> <p>stride and kernel size of each attention.</p> <p> </p> <code>num_stages(int)</code> <p>number of stage. Default: 4.</p> <p> </p> <code>linear(bool)</code> <p>use linear SRA.</p> <p> </p> Source code in <code>mindcv\\models\\pvtv2.py</code> <pre><code>class PyramidVisionTransformerV2(nn.Cell):\n    r\"\"\"Pyramid Vision Transformer V2 model class, based on\n    `\"PVTv2: Improved Baselines with Pyramid Vision Transformer\" &lt;https://arxiv.org/abs/2106.13797&gt;`_\n\n    Args:\n        img_size(int) : size of a input image.\n        patch_size (int) : size of a single image patch.\n        in_chans (int) : number the channels of the input. Default: 3.\n        num_classes (int) : number of classification classes. Default: 1000.\n        embed_dims (list) : how many hidden dim in each PatchEmbed.\n        num_heads (list) : number of attention head in each stage.\n        mlp_ratios (list): ratios of MLP hidden dims in each stage.\n        qkv_bias(bool) : use bias in attention.\n        qk_scale(float) : Scale multiplied by qk in attention(if not none), otherwise head_dim ** -0.5.\n        drop_rate(float) : The drop rate for each block. Default: 0.0.\n        attn_drop_rate(float) : The drop rate for attention. Default: 0.0.\n        drop_path_rate(float) : The drop rate for drop path. Default: 0.0.\n        norm_layer(nn.Cell) : Norm layer that will be used in blocks. Default: nn.LayerNorm.\n        depths (list) : number of Blocks.\n        sr_ratios(list) : stride and kernel size of each attention.\n        num_stages(int) : number of stage. Default: 4.\n        linear(bool) :  use linear SRA.\n    \"\"\"\n\n    def __init__(self, img_size=224, patch_size=16, in_chans=3, num_classes=1000, embed_dims=[64, 128, 256, 512],\n                 num_heads=[1, 2, 4, 8], mlp_ratios=[4, 4, 4, 4], qkv_bias=False, qk_scale=None, drop_rate=0.,\n                 attn_drop_rate=0., drop_path_rate=0., norm_layer=nn.LayerNorm,\n                 depths=[3, 4, 6, 3], sr_ratios=[8, 4, 2, 1], num_stages=4, linear=False):\n        super().__init__()\n        self.num_classes = num_classes\n        self.depths = depths\n        self.num_stages = num_stages\n\n        dpr = [x.item() for x in np.linspace(0, drop_path_rate, sum(depths))]  # stochastic depth decay rule\n        cur = 0\n\n        patch_embed_list = []\n        block_list = []\n        norm_list = []\n\n        for i in range(num_stages):\n            patch_embed = OverlapPatchEmbed(img_size=img_size if i == 0 else img_size // (2 ** (i + 1)),\n                                            patch_size=7 if i == 0 else 3,\n                                            stride=4 if i == 0 else 2,\n                                            in_chans=in_chans if i == 0 else embed_dims[i - 1],\n                                            embed_dim=embed_dims[i])\n\n            block = nn.CellList([Block(\n                dim=embed_dims[i], num_heads=num_heads[i], mlp_ratio=mlp_ratios[i], qkv_bias=qkv_bias,\n                qk_scale=qk_scale,\n                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[cur + j], norm_layer=norm_layer,\n                sr_ratio=sr_ratios[i], linear=linear, block_id=j)\n                for j in range(depths[i])])\n\n            norm = norm_layer([embed_dims[i]])\n\n            cur += depths[i]\n\n            patch_embed_list.append(patch_embed)\n            block_list.append(block)\n            norm_list.append(norm)\n        self.patch_embed_list = nn.CellList(patch_embed_list)\n        self.block_list = nn.CellList(block_list)\n        self.norm_list = nn.CellList(norm_list)\n        # classification head\n        self.head = nn.Dense(embed_dims[3], num_classes) if num_classes &gt; 0 else Identity()\n        self._initialize_weights()\n\n    def freeze_patch_emb(self):\n        self.patch_embed_list[0].requires_grad = False\n\n    def _initialize_weights(self):\n        for _, cell in self.cells_and_names():\n            if isinstance(cell, nn.Dense):\n                cell.weight.set_data(weight_init.initializer(weight_init.TruncatedNormal(sigma=0.02),\n                                                             cell.weight.shape, cell.weight.dtype))\n                if isinstance(cell, nn.Dense) and cell.bias is not None:\n                    cell.bias.set_data(weight_init.initializer(weight_init.Zero(), cell.bias.shape, cell.bias.dtype))\n            elif isinstance(cell, nn.LayerNorm):\n                cell.gamma.set_data(weight_init.initializer(weight_init.One(), cell.gamma.shape, cell.gamma.dtype))\n                cell.beta.set_data(weight_init.initializer(weight_init.Zero(), cell.beta.shape, cell.beta.dtype))\n            elif isinstance(cell, nn.Conv2d):\n                fan_out = cell.kernel_size[0] * cell.kernel_size[1] * cell.out_channels\n                fan_out //= cell.group\n                cell.weight.set_data(weight_init.initializer(weight_init.Normal(sigma=math.sqrt(2.0 / fan_out)),\n                                                             cell.weight.shape, cell.weight.dtype))\n                if cell.bias is not None:\n                    cell.bias.set_data(weight_init.initializer(weight_init.Zero(), cell.bias.shape, cell.bias.dtype))\n\n    def get_classifier(self):\n        return self.head\n\n    def reset_classifier(self, num_classes, global_pool=\"\"):\n        self.num_classes = num_classes\n        self.head = nn.Dense(self.embed_dim, num_classes) if num_classes &gt; 0 else Identity()\n\n    def forward_features(self, x):\n        B = x.shape[0]\n\n        for i in range(self.num_stages):\n            patch_embed = self.patch_embed_list[i]\n            block = self.block_list[i]\n            norm = self.norm_list[i]\n            x, H, W = patch_embed(x)\n            for blk in block:\n                x = blk(x, H, W)\n            x = norm(x)\n            if i != self.num_stages - 1:\n                x = ops.transpose(ops.reshape(x, (B, H, W, -1)), (0, 3, 1, 2))\n\n        return x.mean(axis=1)\n\n    def forward_head(self, x: Tensor) -&gt; Tensor:\n        return self.head(x)\n\n    def construct(self, x):\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n\n        return x\n</code></pre>"},{"location":"reference/models/#mindcv.models.pvtv2.pvt_v2_b0","title":"<code>mindcv.models.pvtv2.pvt_v2_b0(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get PVTV2-b0 model Refer to the base class \"models.PVTv2\" for more details.</p> Source code in <code>mindcv\\models\\pvtv2.py</code> <pre><code>@register_model\ndef pvt_v2_b0(\n    pretrained: bool = False, num_classes: int = 1000, in_channels: int = 3, **kwargs\n) -&gt; PyramidVisionTransformerV2:\n    \"\"\"Get PVTV2-b0 model\n    Refer to the base class \"models.PVTv2\" for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"pvt_v2_b0\"]\n    model = PyramidVisionTransformerV2(\n        in_chans=in_channels, num_classes=num_classes,\n        patch_size=4, embed_dims=[32, 64, 160, 256], num_heads=[1, 2, 5, 8], mlp_ratios=[8, 8, 4, 4], qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, epsilon=1e-6), depths=[2, 2, 2, 2], sr_ratios=[8, 4, 2, 1], **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre>"},{"location":"reference/models/#mindcv.models.pvtv2.pvt_v2_b1","title":"<code>mindcv.models.pvtv2.pvt_v2_b1(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get PVTV2-b1 model Refer to the base class \"models.PVTv2\" for more details.</p> Source code in <code>mindcv\\models\\pvtv2.py</code> <pre><code>@register_model\ndef pvt_v2_b1(\n    pretrained: bool = False, num_classes: int = 1000, in_channels: int = 3, **kwargs\n) -&gt; PyramidVisionTransformerV2:\n    \"\"\"Get PVTV2-b1 model\n    Refer to the base class \"models.PVTv2\" for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"pvt_v2_b1\"]\n    model = PyramidVisionTransformerV2(\n        in_chans=in_channels, num_classes=num_classes,\n        patch_size=4, embed_dims=[64, 128, 320, 512], num_heads=[1, 2, 5, 8], mlp_ratios=[8, 8, 4, 4], qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, epsilon=1e-6), depths=[2, 2, 2, 2], sr_ratios=[8, 4, 2, 1], **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre>"},{"location":"reference/models/#mindcv.models.pvtv2.pvt_v2_b2","title":"<code>mindcv.models.pvtv2.pvt_v2_b2(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get PVTV2-b2 model Refer to the base class \"models.PVTv2\" for more details.</p> Source code in <code>mindcv\\models\\pvtv2.py</code> <pre><code>@register_model\ndef pvt_v2_b2(\n    pretrained: bool = False, num_classes: int = 1000, in_channels: int = 3, **kwargs\n) -&gt; PyramidVisionTransformerV2:\n    \"\"\"Get PVTV2-b2 model\n    Refer to the base class \"models.PVTv2\" for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"pvt_v2_b2\"]\n    model = PyramidVisionTransformerV2(\n        in_chans=in_channels, num_classes=num_classes,\n        patch_size=4, embed_dims=[64, 128, 320, 512], num_heads=[1, 2, 5, 8], mlp_ratios=[8, 8, 4, 4], qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, epsilon=1e-6), depths=[3, 4, 6, 3], sr_ratios=[8, 4, 2, 1], **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre>"},{"location":"reference/models/#mindcv.models.pvtv2.pvt_v2_b3","title":"<code>mindcv.models.pvtv2.pvt_v2_b3(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get PVTV2-b3 model Refer to the base class \"models.PVTv2\" for more details.</p> Source code in <code>mindcv\\models\\pvtv2.py</code> <pre><code>@register_model\ndef pvt_v2_b3(\n    pretrained: bool = False, num_classes: int = 1000, in_channels: int = 3, **kwargs\n) -&gt; PyramidVisionTransformerV2:\n    \"\"\"Get PVTV2-b3 model\n    Refer to the base class \"models.PVTv2\" for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"pvt_v2_b3\"]\n    model = PyramidVisionTransformerV2(\n        in_chans=in_channels, num_classes=num_classes,\n        patch_size=4, embed_dims=[64, 128, 320, 512], num_heads=[1, 2, 5, 8], mlp_ratios=[8, 8, 4, 4], qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, epsilon=1e-6), depths=[3, 4, 18, 3], sr_ratios=[8, 4, 2, 1], **kwargs)\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre>"},{"location":"reference/models/#mindcv.models.pvtv2.pvt_v2_b4","title":"<code>mindcv.models.pvtv2.pvt_v2_b4(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get PVTV2-b4 model Refer to the base class \"models.PVTv2\" for more details.</p> Source code in <code>mindcv\\models\\pvtv2.py</code> <pre><code>@register_model\ndef pvt_v2_b4(\n    pretrained: bool = False, num_classes: int = 1000, in_channels: int = 3, **kwargs\n) -&gt; PyramidVisionTransformerV2:\n    \"\"\"Get PVTV2-b4 model\n    Refer to the base class \"models.PVTv2\" for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"pvt_v2_b4\"]\n    model = PyramidVisionTransformerV2(\n        in_chans=in_channels, num_classes=num_classes,\n        patch_size=4, embed_dims=[64, 128, 320, 512], num_heads=[1, 2, 5, 8], mlp_ratios=[8, 8, 4, 4], qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, epsilon=1e-6), depths=[3, 8, 27, 3], sr_ratios=[8, 4, 2, 1], **kwargs)\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre>"},{"location":"reference/models/#mindcv.models.pvtv2.pvt_v2_b5","title":"<code>mindcv.models.pvtv2.pvt_v2_b5(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get PVTV2-b5 model Refer to the base class \"models.PVTv2\" for more details.</p> Source code in <code>mindcv\\models\\pvtv2.py</code> <pre><code>@register_model\ndef pvt_v2_b5(\n    pretrained: bool = False, num_classes: int = 1000, in_channels: int = 3, **kwargs\n) -&gt; PyramidVisionTransformerV2:\n    \"\"\"Get PVTV2-b5 model\n    Refer to the base class \"models.PVTv2\" for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"pvt_v2_b5\"]\n    model = PyramidVisionTransformerV2(\n        in_chans=in_channels, num_classes=num_classes,\n        patch_size=4, embed_dims=[64, 128, 320, 512], num_heads=[1, 2, 5, 8], mlp_ratios=[4, 4, 4, 4], qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, epsilon=1e-6), depths=[3, 6, 40, 3], sr_ratios=[8, 4, 2, 1], **kwargs)\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre>"},{"location":"reference/models/#regnet","title":"regnet","text":""},{"location":"reference/models/#mindcv.models.regnet","title":"<code>mindcv.models.regnet</code>","text":"<p>MindSpore implementation of <code>RegNet</code>. Refer to: Designing Network Design Spaces</p>"},{"location":"reference/models/#mindcv.models.regnet.AnyHead","title":"<code>mindcv.models.regnet.AnyHead</code>","text":"<p>               Bases: <code>Cell</code></p> <p>AnyNet head: optional conv, AvgPool, 1x1.</p> Source code in <code>mindcv\\models\\regnet.py</code> <pre><code>class AnyHead(nn.Cell):\n    \"\"\"AnyNet head: optional conv, AvgPool, 1x1.\"\"\"\n\n    def __init__(self, w_in, head_width, num_classes):\n        super(AnyHead, self).__init__()\n        self.head_width = head_width\n        if head_width &gt; 0:\n            self.conv = conv2d(w_in, head_width, 1)\n            self.bn = norm2d(head_width)\n            self.af = activation()\n            w_in = head_width\n        self.avg_pool = gap2d()\n        self.fc = linear(w_in, num_classes, bias=True)\n\n    def construct(self, x):\n        x = self.af(self.bn(self.conv(x))) if self.head_width &gt; 0 else x\n        x = self.avg_pool(x)\n        x = self.fc(x)\n        return x\n</code></pre>"},{"location":"reference/models/#mindcv.models.regnet.AnyNet","title":"<code>mindcv.models.regnet.AnyNet</code>","text":"<p>               Bases: <code>Cell</code></p> <p>AnyNet model.</p> Source code in <code>mindcv\\models\\regnet.py</code> <pre><code>class AnyNet(nn.Cell):\n    \"\"\"AnyNet model.\"\"\"\n\n    @staticmethod\n    def anynet_get_params(depths, stem_type, stem_w, block_type, widths, strides, bot_muls, group_ws, head_w,\n                          num_classes, se_r):\n        nones = [None for _ in depths]\n        return {\n            \"stem_type\": stem_type,\n            \"stem_w\": stem_w,\n            \"block_type\": block_type,\n            \"depths\": depths,\n            \"widths\": widths,\n            \"strides\": strides,\n            \"bot_muls\": bot_muls if bot_muls else nones,\n            \"group_ws\": group_ws if group_ws else nones,\n            \"head_w\": head_w,\n            \"se_r\": se_r,\n            \"num_classes\": num_classes,\n        }\n\n    def __init__(self, depths, stem_type, stem_w, block_type, widths, strides, bot_muls, group_ws, head_w, num_classes,\n                 se_r, in_channels):\n        super(AnyNet, self).__init__()\n        p = AnyNet.anynet_get_params(depths, stem_type, stem_w, block_type, widths, strides, bot_muls, group_ws, head_w,\n                                     num_classes, se_r)\n        stem_fun = get_stem_fun(p[\"stem_type\"])\n        block_fun = get_block_fun(p[\"block_type\"])\n        self.stem = stem_fun(in_channels, p[\"stem_w\"])\n        prev_w = p[\"stem_w\"]\n        keys = [\"depths\", \"widths\", \"strides\", \"bot_muls\", \"group_ws\"]\n        self.stages = nn.CellList()\n        for i, (d, w, s, b, g) in enumerate(zip(*[p[k] for k in keys])):\n            params = {\"bot_mul\": b, \"group_w\": g, \"se_r\": p[\"se_r\"]}\n            stage = AnyStage(prev_w, w, s, d, block_fun, params)\n            self.stages.append(stage)\n            prev_w = w\n        self.head = AnyHead(prev_w, p[\"head_w\"], p[\"num_classes\"])\n        self._initialize_weights()\n\n    def _initialize_weights(self) -&gt; None:\n        \"\"\"Initialize weights for cells.\"\"\"\n        for _, cell in self.cells_and_names():\n            if isinstance(cell, nn.Conv2d):\n                fan_out = cell.kernel_size[0] * cell.kernel_size[1] * cell.out_channels\n                cell.weight.set_data(\n                    init.initializer(init.Normal(sigma=math.sqrt(2.0 / fan_out), mean=0.0),\n                                     cell.weight.shape, cell.weight.dtype))\n            elif isinstance(cell, nn.BatchNorm2d):\n                cell.gamma.set_data(init.initializer(\"ones\", cell.gamma.shape, cell.gamma.dtype))\n                cell.beta.set_data(init.initializer(\"zeros\", cell.beta.shape, cell.beta.dtype))\n            elif isinstance(cell, nn.Dense):\n                cell.weight.set_data(\n                    init.initializer(init.Normal(sigma=0.01, mean=0.0), cell.weight.shape, cell.weight.dtype))\n                if cell.bias is not None:\n                    cell.bias.set_data(init.initializer(\"zeros\", cell.bias.shape, cell.bias.dtype))\n\n    def forward_features(self, x):\n        x = self.stem(x)\n        for module in self.stages:\n            x = module(x)\n        return x\n\n    def forward_head(self, x):\n        x = self.head(x)\n        return x\n\n    def construct(self, x):\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n</code></pre>"},{"location":"reference/models/#mindcv.models.regnet.AnyStage","title":"<code>mindcv.models.regnet.AnyStage</code>","text":"<p>               Bases: <code>Cell</code></p> <p>AnyNet stage (sequence of blocks w/ the same output shape).</p> Source code in <code>mindcv\\models\\regnet.py</code> <pre><code>class AnyStage(nn.Cell):\n    \"\"\"AnyNet stage (sequence of blocks w/ the same output shape).\"\"\"\n\n    def __init__(self, w_in, w_out, stride, d, block_fun, params):\n        super(AnyStage, self).__init__()\n        self.blocks = nn.CellList()\n        for _ in range(d):\n            block = block_fun(w_in, w_out, stride, params)\n            self.blocks.append(block)\n            stride, w_in = 1, w_out\n\n    def construct(self, x):\n        for block in self.blocks:\n            x = block(x)\n        return x\n</code></pre>"},{"location":"reference/models/#mindcv.models.regnet.BasicTransform","title":"<code>mindcv.models.regnet.BasicTransform</code>","text":"<p>               Bases: <code>Cell</code></p> <p>Basic transformation: [3x3 conv, BN, Relu] x2.</p> Source code in <code>mindcv\\models\\regnet.py</code> <pre><code>class BasicTransform(nn.Cell):\n    \"\"\"Basic transformation: [3x3 conv, BN, Relu] x2.\"\"\"\n\n    def __init__(self, w_in, w_out, stride, _params):\n        super(BasicTransform, self).__init__()\n        self.a = conv2d(w_in, w_out, 3, stride=stride)\n        self.a_bn = norm2d(w_out)\n        self.a_af = activation()\n        self.b = conv2d(w_out, w_out, 3)\n        self.b_bn = norm2d(w_out)\n        self.b_bn.final_bn = True\n\n    def construct(self, x):\n        x = self.a(x)\n        x = self.a_bn(x)\n        x = self.a_af(x)\n        x = self.b(x)\n        x = self.b_bn(x)\n        return x\n</code></pre>"},{"location":"reference/models/#mindcv.models.regnet.BottleneckTransform","title":"<code>mindcv.models.regnet.BottleneckTransform</code>","text":"<p>               Bases: <code>Cell</code></p> <p>Bottleneck transformation: 1x1, 3x3 [+SE], 1x1.</p> Source code in <code>mindcv\\models\\regnet.py</code> <pre><code>class BottleneckTransform(nn.Cell):\n    \"\"\"Bottleneck transformation: 1x1, 3x3 [+SE], 1x1.\"\"\"\n\n    def __init__(self, w_in, w_out, stride, params):\n        super(BottleneckTransform, self).__init__()\n        w_b = int(round(w_out * params[\"bot_mul\"]))\n        w_se = int(round(w_in * params[\"se_r\"]))\n        groups = w_b // params[\"group_w\"]\n        self.a = conv2d(w_in, w_b, 1)\n        self.a_bn = norm2d(w_b)\n        self.a_af = activation()\n        self.b = conv2d(w_b, w_b, 3, stride=stride, groups=groups)\n        self.b_bn = norm2d(w_b)\n        self.b_af = activation()\n        self.se = SqueezeExcite(in_channels=w_b, rd_channels=w_se) if w_se else None\n        self.c = conv2d(w_b, w_out, 1)\n        self.c_bn = norm2d(w_out)\n        self.c_bn.final_bn = True\n\n    def construct(self, x):\n        x = self.a(x)\n        x = self.a_bn(x)\n        x = self.a_af(x)\n        x = self.b(x)\n        x = self.b_bn(x)\n        x = self.b_af(x)\n        x = self.se(x) if self.se is not None else x\n        x = self.c(x)\n        x = self.c_bn(x)\n        return x\n</code></pre>"},{"location":"reference/models/#mindcv.models.regnet.RegNet","title":"<code>mindcv.models.regnet.RegNet</code>","text":"<p>               Bases: <code>AnyNet</code></p> <p>RegNet model class, based on <code>\"Designing Network Design Spaces\" &lt;https://arxiv.org/abs/2003.13678&gt;</code>_</p> Source code in <code>mindcv\\models\\regnet.py</code> <pre><code>class RegNet(AnyNet):\n    r\"\"\"RegNet model class, based on\n    `\"Designing Network Design Spaces\" &lt;https://arxiv.org/abs/2003.13678&gt;`_\n    \"\"\"\n\n    @staticmethod\n    def regnet_get_params(w_a, w_0, w_m, d, stride, bot_mul, group_w, stem_type, stem_w, block_type, head_w,\n                          num_classes, se_r):\n        \"\"\"Get AnyNet parameters that correspond to the RegNet.\"\"\"\n        ws, ds, ss, bs, gs = generate_regnet_full(w_a, w_0, w_m, d, stride, bot_mul, group_w)\n        return {\n            \"stem_type\": stem_type,\n            \"stem_w\": stem_w,\n            \"block_type\": block_type,\n            \"depths\": ds,\n            \"widths\": ws,\n            \"strides\": ss,\n            \"bot_muls\": bs,\n            \"group_ws\": gs,\n            \"head_w\": head_w,\n            \"se_r\": se_r,\n            \"num_classes\": num_classes,\n        }\n\n    def __init__(self, w_a, w_0, w_m, d, group_w, stride=2, bot_mul=1.0, stem_type=\"simple_stem_in\", stem_w=32,\n                 block_type=\"res_bottleneck_block\", head_w=0, num_classes=1000, se_r=0.0, in_channels=3):\n        params = RegNet.regnet_get_params(w_a, w_0, w_m, d, stride, bot_mul, group_w, stem_type, stem_w, block_type,\n                                          head_w, num_classes, se_r)\n        super(RegNet, self).__init__(params[\"depths\"], params[\"stem_type\"], params[\"stem_w\"], params[\"block_type\"],\n                                     params[\"widths\"], params[\"strides\"], params[\"bot_muls\"], params[\"group_ws\"],\n                                     params[\"head_w\"], params[\"num_classes\"], params[\"se_r\"], in_channels)\n</code></pre>"},{"location":"reference/models/#mindcv.models.regnet.RegNet.regnet_get_params","title":"<code>mindcv.models.regnet.RegNet.regnet_get_params(w_a, w_0, w_m, d, stride, bot_mul, group_w, stem_type, stem_w, block_type, head_w, num_classes, se_r)</code>  <code>staticmethod</code>","text":"<p>Get AnyNet parameters that correspond to the RegNet.</p> Source code in <code>mindcv\\models\\regnet.py</code> <pre><code>@staticmethod\ndef regnet_get_params(w_a, w_0, w_m, d, stride, bot_mul, group_w, stem_type, stem_w, block_type, head_w,\n                      num_classes, se_r):\n    \"\"\"Get AnyNet parameters that correspond to the RegNet.\"\"\"\n    ws, ds, ss, bs, gs = generate_regnet_full(w_a, w_0, w_m, d, stride, bot_mul, group_w)\n    return {\n        \"stem_type\": stem_type,\n        \"stem_w\": stem_w,\n        \"block_type\": block_type,\n        \"depths\": ds,\n        \"widths\": ws,\n        \"strides\": ss,\n        \"bot_muls\": bs,\n        \"group_ws\": gs,\n        \"head_w\": head_w,\n        \"se_r\": se_r,\n        \"num_classes\": num_classes,\n    }\n</code></pre>"},{"location":"reference/models/#mindcv.models.regnet.ResBasicBlock","title":"<code>mindcv.models.regnet.ResBasicBlock</code>","text":"<p>               Bases: <code>Cell</code></p> <p>Residual basic block: x + f(x), f = basic transform.</p> Source code in <code>mindcv\\models\\regnet.py</code> <pre><code>class ResBasicBlock(nn.Cell):\n    \"\"\"Residual basic block: x + f(x), f = basic transform.\"\"\"\n\n    def __init__(self, w_in, w_out, stride, params):\n        super(ResBasicBlock, self).__init__()\n        self.proj, self.bn = None, None\n        if (w_in != w_out) or (stride != 1):\n            self.proj = conv2d(w_in, w_out, 1, stride=stride)\n            self.bn = norm2d(w_out)\n        self.f = BasicTransform(w_in, w_out, stride, params)\n        self.af = activation()\n\n    def construct(self, x):\n        x_p = self.bn(self.proj(x)) if self.proj is not None else x\n        return self.af(x_p + self.f(x))\n</code></pre>"},{"location":"reference/models/#mindcv.models.regnet.ResBottleneckBlock","title":"<code>mindcv.models.regnet.ResBottleneckBlock</code>","text":"<p>               Bases: <code>Cell</code></p> <p>Residual bottleneck block: x + f(x), f = bottleneck transform.</p> Source code in <code>mindcv\\models\\regnet.py</code> <pre><code>class ResBottleneckBlock(nn.Cell):\n    \"\"\"Residual bottleneck block: x + f(x), f = bottleneck transform.\"\"\"\n\n    def __init__(self, w_in, w_out, stride, params):\n        super(ResBottleneckBlock, self).__init__()\n        self.proj, self.bn = None, None\n        if (w_in != w_out) or (stride != 1):\n            self.proj = conv2d(w_in, w_out, 1, stride=stride)\n            self.bn = norm2d(w_out)\n        self.f = BottleneckTransform(w_in, w_out, stride, params)\n        self.af = activation()\n\n    def construct(self, x):\n        x_p = self.bn(self.proj(x)) if self.proj is not None else x\n        return self.af(x_p + self.f(x))\n</code></pre>"},{"location":"reference/models/#mindcv.models.regnet.ResBottleneckLinearBlock","title":"<code>mindcv.models.regnet.ResBottleneckLinearBlock</code>","text":"<p>               Bases: <code>Cell</code></p> <p>Residual linear bottleneck block: x + f(x), f = bottleneck transform.</p> Source code in <code>mindcv\\models\\regnet.py</code> <pre><code>class ResBottleneckLinearBlock(nn.Cell):\n    \"\"\"Residual linear bottleneck block: x + f(x), f = bottleneck transform.\"\"\"\n\n    def __init__(self, w_in, w_out, stride, params):\n        super(ResBottleneckLinearBlock, self).__init__()\n        self.has_skip = (w_in == w_out) and (stride == 1)\n        self.f = BottleneckTransform(w_in, w_out, stride, params)\n\n    def construct(self, x):\n        return x + self.f(x) if self.has_skip else self.f(x)\n</code></pre>"},{"location":"reference/models/#mindcv.models.regnet.ResStem","title":"<code>mindcv.models.regnet.ResStem</code>","text":"<p>               Bases: <code>Cell</code></p> <p>ResNet stem for ImageNet: 7x7, BN, AF, MaxPool.</p> Source code in <code>mindcv\\models\\regnet.py</code> <pre><code>class ResStem(nn.Cell):\n    \"\"\"ResNet stem for ImageNet: 7x7, BN, AF, MaxPool.\"\"\"\n\n    def __init__(self, w_in, w_out):\n        super(ResStem, self).__init__()\n        self.conv = conv2d(w_in, w_out, 7, stride=2)\n        self.bn = norm2d(w_out)\n        self.af = activation()\n        self.pool = pool2d(w_out, 3, stride=2)\n\n    def construct(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        x = self.af(x)\n        x = self.pool(x)\n        return x\n</code></pre>"},{"location":"reference/models/#mindcv.models.regnet.ResStemCifar","title":"<code>mindcv.models.regnet.ResStemCifar</code>","text":"<p>               Bases: <code>Cell</code></p> <p>ResNet stem for CIFAR: 3x3, BN, AF.</p> Source code in <code>mindcv\\models\\regnet.py</code> <pre><code>class ResStemCifar(nn.Cell):\n    \"\"\"ResNet stem for CIFAR: 3x3, BN, AF.\"\"\"\n\n    def __init__(self, w_in, w_out):\n        super(ResStemCifar, self).__init__()\n        self.conv = conv2d(w_in, w_out, 3)\n        self.bn = norm2d(w_out)\n        self.af = activation()\n\n    def construct(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        x = self.af(x)\n        return x\n</code></pre>"},{"location":"reference/models/#mindcv.models.regnet.SimpleStem","title":"<code>mindcv.models.regnet.SimpleStem</code>","text":"<p>               Bases: <code>Cell</code></p> <p>Simple stem for ImageNet: 3x3, BN, AF.</p> Source code in <code>mindcv\\models\\regnet.py</code> <pre><code>class SimpleStem(nn.Cell):\n    \"\"\"Simple stem for ImageNet: 3x3, BN, AF.\"\"\"\n\n    def __init__(self, w_in, w_out):\n        super(SimpleStem, self).__init__()\n        self.conv = conv2d(w_in, w_out, 3, stride=2)\n        self.bn = norm2d(w_out)\n        self.af = activation()\n\n    def construct(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        x = self.af(x)\n        return x\n</code></pre>"},{"location":"reference/models/#mindcv.models.regnet.VanillaBlock","title":"<code>mindcv.models.regnet.VanillaBlock</code>","text":"<p>               Bases: <code>Cell</code></p> <p>Vanilla block: [3x3 conv, BN, Relu] x2.</p> Source code in <code>mindcv\\models\\regnet.py</code> <pre><code>class VanillaBlock(nn.Cell):\n    \"\"\"Vanilla block: [3x3 conv, BN, Relu] x2.\"\"\"\n\n    def __init__(self, w_in, w_out, stride, _params):\n        super(VanillaBlock, self).__init__()\n        self.a = conv2d(w_in, w_out, 3, stride=stride)\n        self.a_bn = norm2d(w_out)\n        self.a_af = activation()\n        self.b = conv2d(w_out, w_out, 3)\n        self.b_bn = norm2d(w_out)\n        self.b_af = activation()\n\n    def construct(self, x):\n        x = self.a(x)\n        x = self.a_bn(x)\n        x = self.a_af(x)\n        x = self.b(x)\n        x = self.b_bn(x)\n        x = self.b_af(x)\n        return x\n</code></pre>"},{"location":"reference/models/#mindcv.models.regnet.activation","title":"<code>mindcv.models.regnet.activation()</code>","text":"<p>Helper for building an activation layer.</p> Source code in <code>mindcv\\models\\regnet.py</code> <pre><code>def activation():\n    \"\"\"Helper for building an activation layer.\"\"\"\n    return nn.ReLU()\n</code></pre>"},{"location":"reference/models/#mindcv.models.regnet.adjust_block_compatibility","title":"<code>mindcv.models.regnet.adjust_block_compatibility(ws, bs, gs)</code>","text":"<p>Adjusts the compatibility of widths, bottlenecks, and groups.</p> Source code in <code>mindcv\\models\\regnet.py</code> <pre><code>def adjust_block_compatibility(ws, bs, gs):\n    \"\"\"Adjusts the compatibility of widths, bottlenecks, and groups.\"\"\"\n    assert len(ws) == len(bs) == len(gs)\n    assert all(w &gt; 0 and b &gt; 0 and g &gt; 0 for w, b, g in zip(ws, bs, gs))\n    assert all(b &lt; 1 or b % 1 == 0 for b in bs)\n    vs = [int(max(1, w * b)) for w, b in zip(ws, bs)]\n    gs = [int(min(g, v)) for g, v in zip(gs, vs)]\n    ms = [np.lcm(g, int(b)) if b &gt; 1 else g for g, b in zip(gs, bs)]\n    vs = [max(m, int(round(v / m) * m)) for v, m in zip(vs, ms)]\n    ws = [int(v / b) for v, b in zip(vs, bs)]\n    assert all(w * b % g == 0 for w, b, g in zip(ws, bs, gs))\n    return ws, bs, gs\n</code></pre>"},{"location":"reference/models/#mindcv.models.regnet.conv2d","title":"<code>mindcv.models.regnet.conv2d(w_in, w_out, k, *, stride=1, groups=1, bias=False)</code>","text":"<p>Helper for building a conv2d layer.</p> Source code in <code>mindcv\\models\\regnet.py</code> <pre><code>def conv2d(w_in, w_out, k, *, stride=1, groups=1, bias=False):\n    \"\"\"Helper for building a conv2d layer.\"\"\"\n    assert k % 2 == 1, \"Only odd size kernels supported to avoid padding issues.\"\n    s, p, g, b = stride, (k - 1) // 2, groups, bias\n    return nn.Conv2d(w_in, w_out, k, stride=s, pad_mode=\"pad\", padding=p, group=g, has_bias=b)\n</code></pre>"},{"location":"reference/models/#mindcv.models.regnet.gap2d","title":"<code>mindcv.models.regnet.gap2d(keep_dims=False)</code>","text":"<p>Helper for building a gap2d layer.</p> Source code in <code>mindcv\\models\\regnet.py</code> <pre><code>def gap2d(keep_dims=False):\n    \"\"\"Helper for building a gap2d layer.\"\"\"\n    return GlobalAvgPooling(keep_dims)\n</code></pre>"},{"location":"reference/models/#mindcv.models.regnet.generate_regnet","title":"<code>mindcv.models.regnet.generate_regnet(w_a, w_0, w_m, d, q=8)</code>","text":"<p>Generates per stage widths and depths from RegNet parameters.</p> Source code in <code>mindcv\\models\\regnet.py</code> <pre><code>def generate_regnet(w_a, w_0, w_m, d, q=8):\n    \"\"\"Generates per stage widths and depths from RegNet parameters.\"\"\"\n    assert w_a &gt;= 0 and w_0 &gt; 0 and w_m &gt; 1 and w_0 % q == 0\n    # Generate continuous per-block ws\n    ws_cont = np.arange(d) * w_a + w_0\n    # Generate quantized per-block ws\n    ks = np.round(np.log(ws_cont / w_0) / np.log(w_m))\n    ws_all = w_0 * np.power(w_m, ks)\n    ws_all = np.round(np.divide(ws_all, q)).astype(int) * q\n    # Generate per stage ws and ds (assumes ws_all are sorted)\n    ws, ds = np.unique(ws_all, return_counts=True)\n    # Compute number of actual stages and total possible stages\n    num_stages, total_stages = len(ws), ks.max() + 1\n    # Convert numpy arrays to lists and return\n    ws, ds, ws_all, ws_cont = (x.tolist() for x in (ws, ds, ws_all, ws_cont))\n    return ws, ds, num_stages, total_stages, ws_all, ws_cont\n</code></pre>"},{"location":"reference/models/#mindcv.models.regnet.generate_regnet_full","title":"<code>mindcv.models.regnet.generate_regnet_full(w_a, w_0, w_m, d, stride, bot_mul, group_w)</code>","text":"<p>Generates per stage ws, ds, gs, bs, and ss from RegNet cfg.</p> Source code in <code>mindcv\\models\\regnet.py</code> <pre><code>def generate_regnet_full(w_a, w_0, w_m, d, stride, bot_mul, group_w):\n    \"\"\"Generates per stage ws, ds, gs, bs, and ss from RegNet cfg.\"\"\"\n    ws, ds = generate_regnet(w_a, w_0, w_m, d)[0:2]\n    ss = [stride for _ in ws]\n    bs = [bot_mul for _ in ws]\n    gs = [group_w for _ in ws]\n    ws, bs, gs = adjust_block_compatibility(ws, bs, gs)\n    return ws, ds, ss, bs, gs\n</code></pre>"},{"location":"reference/models/#mindcv.models.regnet.get_block_fun","title":"<code>mindcv.models.regnet.get_block_fun(block_type)</code>","text":"<p>Retrieves the block function by name.</p> Source code in <code>mindcv\\models\\regnet.py</code> <pre><code>def get_block_fun(block_type):\n    \"\"\"Retrieves the block function by name.\"\"\"\n    block_funs = {\n        \"vanilla_block\": VanillaBlock,\n        \"res_basic_block\": ResBasicBlock,\n        \"res_bottleneck_block\": ResBottleneckBlock,\n        \"res_bottleneck_linear_block\": ResBottleneckLinearBlock,\n    }\n    err_str = \"Block type '{}' not supported\"\n    assert block_type in block_funs.keys(), err_str.format(block_type)\n    return block_funs[block_type]\n</code></pre>"},{"location":"reference/models/#mindcv.models.regnet.get_stem_fun","title":"<code>mindcv.models.regnet.get_stem_fun(stem_type)</code>","text":"<p>Retrieves the stem function by name.</p> Source code in <code>mindcv\\models\\regnet.py</code> <pre><code>def get_stem_fun(stem_type):\n    \"\"\"Retrieves the stem function by name.\"\"\"\n    stem_funs = {\n        \"res_stem_cifar\": ResStemCifar,\n        \"res_stem_in\": ResStem,\n        \"simple_stem_in\": SimpleStem,\n    }\n    err_str = \"Stem type '{}' not supported\"\n    assert stem_type in stem_funs.keys(), err_str.format(stem_type)\n    return stem_funs[stem_type]\n</code></pre>"},{"location":"reference/models/#mindcv.models.regnet.linear","title":"<code>mindcv.models.regnet.linear(w_in, w_out, *, bias=False)</code>","text":"<p>Helper for building a linear layer.</p> Source code in <code>mindcv\\models\\regnet.py</code> <pre><code>def linear(w_in, w_out, *, bias=False):\n    \"\"\"Helper for building a linear layer.\"\"\"\n    return nn.Dense(w_in, w_out, has_bias=bias)\n</code></pre>"},{"location":"reference/models/#mindcv.models.regnet.norm2d","title":"<code>mindcv.models.regnet.norm2d(w_in, eps=1e-05, mom=0.9)</code>","text":"<p>Helper for building a norm2d layer.</p> Source code in <code>mindcv\\models\\regnet.py</code> <pre><code>def norm2d(w_in, eps=1e-5, mom=0.9):\n    \"\"\"Helper for building a norm2d layer.\"\"\"\n    return nn.BatchNorm2d(num_features=w_in, eps=eps, momentum=mom)\n</code></pre>"},{"location":"reference/models/#mindcv.models.regnet.pool2d","title":"<code>mindcv.models.regnet.pool2d(_w_in, k, *, stride=1)</code>","text":"<p>Helper for building a pool2d layer.</p> Source code in <code>mindcv\\models\\regnet.py</code> <pre><code>def pool2d(_w_in, k, *, stride=1):\n    \"\"\"Helper for building a pool2d layer.\"\"\"\n    assert k % 2 == 1, \"Only odd size kernels supported to avoid padding issues.\"\n    padding = (k - 1) // 2\n    pad2d = nn.Pad(((0, 0), (0, 0), (padding, padding), (padding, padding)), mode=\"CONSTANT\")\n    max_pool = nn.MaxPool2d(kernel_size=k, stride=stride, pad_mode=\"valid\")\n    return nn.SequentialCell([pad2d, max_pool])\n</code></pre>"},{"location":"reference/models/#repmlp","title":"repmlp","text":""},{"location":"reference/models/#mindcv.models.repmlp","title":"<code>mindcv.models.repmlp</code>","text":"<p>MindSpore implementation of <code>RepMLPNet</code>. Refer to RepMLPNet: Hierarchical Vision MLP with Re-parameterized Locality.</p>"},{"location":"reference/models/#mindcv.models.repmlp.FFNBlock","title":"<code>mindcv.models.repmlp.FFNBlock</code>","text":"<p>               Bases: <code>Cell</code></p> <p>Common FFN layer</p> Source code in <code>mindcv\\models\\repmlp.py</code> <pre><code>class FFNBlock(nn.Cell):\n    \"\"\"Common FFN layer\"\"\"\n\n    def __init__(self, in_channels, hidden_channels=None, out_channels=None, act_layer=nn.GELU):\n        super().__init__()\n        out_features = out_channels or in_channels\n        hidden_features = hidden_channels or in_channels\n        self.ffn_fc1 = conv_bn(in_channels, hidden_features, 1, 1, 0, has_bias=False)\n        self.ffn_fc2 = conv_bn(hidden_features, out_features, 1, 1, 0, has_bias=False)\n        self.act = act_layer()\n\n    def construct(self, inputs):\n        x = self.ffn_fc1(inputs)\n        x = self.act(x)\n        x = self.ffn_fc2(x)\n        return x\n</code></pre>"},{"location":"reference/models/#mindcv.models.repmlp.GlobalPerceptron","title":"<code>mindcv.models.repmlp.GlobalPerceptron</code>","text":"<p>               Bases: <code>Cell</code></p> <p>GlobalPerceptron Layers provides global information(One of the three components of RepMLPBlock)</p> Source code in <code>mindcv\\models\\repmlp.py</code> <pre><code>class GlobalPerceptron(nn.Cell):\n    \"\"\"GlobalPerceptron Layers provides global information(One of the three components of RepMLPBlock)\"\"\"\n\n    def __init__(self, input_channels, internal_neurons):\n        super(GlobalPerceptron, self).__init__()\n        self.fc1 = nn.Conv2d(in_channels=input_channels, out_channels=internal_neurons, kernel_size=(1, 1), stride=1,\n                             has_bias=True)\n        self.fc2 = nn.Conv2d(in_channels=internal_neurons, out_channels=input_channels, kernel_size=(1, 1), stride=1,\n                             has_bias=True)\n\n        self.relu = nn.ReLU()\n        self.sigmoid = nn.Sigmoid()\n        self.input_channels = input_channels\n        self.shape = ops.Shape()\n\n    def construct(self, x):\n        shape = self.shape(x)\n        pool = nn.AvgPool2d(kernel_size=(shape[2], shape[3]), stride=1)\n        x = pool(x)\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        x = self.sigmoid(x)\n        x = x.view(-1, self.input_channels, 1, 1)\n        return x\n</code></pre>"},{"location":"reference/models/#mindcv.models.repmlp.RepMLPBlock","title":"<code>mindcv.models.repmlp.RepMLPBlock</code>","text":"<p>               Bases: <code>Cell</code></p> <p>Basic RepMLPBlock Layer(compose of Global Perceptron, Channel Perceptron and Local Perceptron)</p> Source code in <code>mindcv\\models\\repmlp.py</code> <pre><code>class RepMLPBlock(nn.Cell):\n    \"\"\"Basic RepMLPBlock Layer(compose of Global Perceptron, Channel Perceptron and Local Perceptron)\"\"\"\n\n    def __init__(self, in_channels, out_channels,\n                 h, w,\n                 reparam_conv_k=None,\n                 globalperceptron_reduce=4,\n                 num_sharesets=1,\n                 deploy=False):\n        super().__init__()\n\n        self.C = in_channels  # noqa: E741\n        self.O = out_channels  # noqa: E741\n        self.S = num_sharesets  # noqa: E741\n\n        self.h, self.w = h, w\n\n        self.deploy = deploy\n        self.transpose = ops.Transpose()\n        self.shape = ops.Shape()\n        self.reshape = ops.Reshape()\n\n        assert in_channels == out_channels\n        self.gp = GlobalPerceptron(input_channels=in_channels, internal_neurons=in_channels // globalperceptron_reduce)\n\n        self.fc3 = nn.Conv2d(in_channels=self.h * self.w * num_sharesets, out_channels=self.h * self.w * num_sharesets,\n                             kernel_size=(1, 1), stride=1, padding=0, has_bias=deploy, group=num_sharesets)\n        if deploy:\n            self.fc3_bn = ops.Identity()\n        else:\n            self.fc3_bn = nn.BatchNorm2d(num_sharesets).set_train()\n\n        self.reparam_conv_k = reparam_conv_k\n        self.conv_branch_k = []\n        if not deploy and reparam_conv_k is not None:\n            for k in reparam_conv_k:\n                conv_branch = conv_bn(num_sharesets, num_sharesets, kernel_size=k, stride=1, padding=k // 2,\n                                      group=num_sharesets, has_bias=False)\n                self.__setattr__(\"repconv{}\".format(k), conv_branch)\n                self.conv_branch_k.append(conv_branch)\n                # print(conv_branch)\n\n    def partition(self, x, h_parts, w_parts):\n        x = x.reshape(-1, self.C, h_parts, self.h, w_parts, self.w)\n        input_perm = (0, 2, 4, 1, 3, 5)\n        x = self.transpose(x, input_perm)\n        return x\n\n    def partition_affine(self, x, h_parts, w_parts):\n        fc_inputs = x.reshape(-1, self.S * self.h * self.w, 1, 1)\n        out = self.fc3(fc_inputs)\n        out = out.reshape(-1, self.S, self.h, self.w)\n        out = self.fc3_bn(out)\n        out = out.reshape(-1, h_parts, w_parts, self.S, self.h, self.w)\n        return out\n\n    def construct(self, inputs):\n        # Global Perceptron\n        global_vec = self.gp(inputs)\n\n        origin_shape = self.shape(inputs)\n\n        h_parts = origin_shape[2] // self.h\n        w_parts = origin_shape[3] // self.w\n\n        partitions = self.partition(inputs, h_parts, w_parts)\n\n        #   Channel Perceptron\n        fc3_out = self.partition_affine(partitions, h_parts, w_parts)\n\n        #   Local Perceptron\n        if self.reparam_conv_k is not None and not self.deploy:\n            conv_inputs = self.reshape(partitions, (-1, self.S, self.h, self.w))\n            conv_out = 0\n            for k in self.conv_branch_k:\n                conv_out += k(conv_inputs)\n            conv_out = self.reshape(conv_out, (-1, h_parts, w_parts, self.S, self.h, self.w))\n            fc3_out += conv_out\n\n        input_perm = (0, 3, 1, 4, 2, 5)\n        fc3_out = self.transpose(fc3_out, input_perm)  # N, O, h_parts, out_h, w_parts, out_w\n        out = fc3_out.reshape(*origin_shape)\n        out = out * global_vec\n        return out\n\n    def get_equivalent_fc3(self):\n        fc_weight, fc_bias = fuse_bn(self.fc3, self.fc3_bn)\n        if self.reparam_conv_k is not None:\n            largest_k = max(self.reparam_conv_k)\n            largest_branch = self.__getattr__(\"repconv{}\".format(largest_k))\n            total_kernel, total_bias = fuse_bn(largest_branch.conv, largest_branch.bn)\n            for k in self.reparam_conv_k:\n                if k != largest_k:\n                    k_branch = self.__getattr__(\"repconv{}\".format(k))\n                    kernel, bias = fuse_bn(k_branch.conv, k_branch.bn)\n                    total_kernel += nn.Pad(kernel, [(largest_k - k) // 2] * 4)\n                    total_bias += bias\n            rep_weight, rep_bias = self._convert_conv_to_fc(total_kernel, total_bias)\n            final_fc3_weight = rep_weight.reshape_as(fc_weight) + fc_weight\n            final_fc3_bias = rep_bias + fc_bias\n        else:\n            final_fc3_weight = fc_weight\n            final_fc3_bias = fc_bias\n        return final_fc3_weight, final_fc3_bias\n\n    def local_inject(self):\n        self.deploy = True\n        #   Locality Injection\n        fc3_weight, fc3_bias = self.get_equivalent_fc3()\n        #   Remove Local Perceptron\n        if self.reparam_conv_k is not None:\n            for k in self.reparam_conv_k:\n                self.__delattr__(\"repconv{}\".format(k))\n        self.__delattr__(\"fc3\")\n        self.__delattr__(\"fc3_bn\")\n        self.fc3 = nn.Conv2d(self.S * self.h * self.w, self.S * self.h * self.w, 1, 1, 0, has_bias=True, group=self.S)\n        self.fc3_bn = ops.Identity()\n        self.fc3.weight.data = fc3_weight\n        self.fc3.bias.data = fc3_bias\n\n    def _convert_conv_to_fc(self, conv_kernel, conv_bias):\n        I = ops.eye(self.h * self.w).repeat(1, self.S).reshape(self.h * self.w, self.S, self.h, self.w)  # noqa: E741\n        fc_k = ops.Conv2D(I, conv_kernel, pad=(conv_kernel.size(2) // 2, conv_kernel.size(3) // 2), group=self.S)\n        fc_k = fc_k.reshape(self.h * self.w, self.S * self.h * self.w).t()\n        fc_bias = conv_bias.repeat_interleave(self.h * self.w)\n        return fc_k, fc_bias\n</code></pre>"},{"location":"reference/models/#mindcv.models.repmlp.RepMLPNet","title":"<code>mindcv.models.repmlp.RepMLPNet</code>","text":"<p>               Bases: <code>Cell</code></p> <p>RepMLPNet model class, based on <code>\"RepMLPNet: Hierarchical Vision MLP with Re-parameterized Locality\" &lt;https://arxiv.org/pdf/2112.11081v2.pdf&gt;</code>_</p> PARAMETER DESCRIPTION <code>in_channels</code> <p>number of input channels. Default: 3.</p> <p> DEFAULT: <code>3</code> </p> <code>num_classes</code> <p>number of classification classes. Default: 1000.</p> <p> </p> <code>patch_size</code> <p>size of a single image patch. Default: (4, 4)</p> <p> DEFAULT: <code>(4, 4)</code> </p> <code>num_blocks</code> <p>number of blocks per stage. Default: (2,2,6,2)</p> <p> DEFAULT: <code>(2, 2, 6, 2)</code> </p> <code>channels</code> <p>number of in_channels(channels[stage_idx]) and out_channels(channels[stage_idx + 1]) per stage. Default: (192,384,768,1536)</p> <p> DEFAULT: <code>(192, 384, 768, 1536)</code> </p> <code>hs</code> <p>height of picture per stage. Default: (64,32,16,8)</p> <p> DEFAULT: <code>(64, 32, 16, 8)</code> </p> <code>ws</code> <p>width of picture per stage. Default: (64,32,16,8)</p> <p> DEFAULT: <code>(64, 32, 16, 8)</code> </p> <code>sharesets_nums</code> <p>number of share sets per stage. Default: (4,8,16,32)</p> <p> DEFAULT: <code>(4, 8, 16, 32)</code> </p> <code>reparam_conv_k</code> <p>convolution kernel size in local Perceptron. Default: (3,)</p> <p> DEFAULT: <code>(3)</code> </p> <code>globalperceptron_reduce</code> <p>Intermediate convolution output size (in_channal = inchannal, out_channel = in_channel/globalperceptron_reduce) in globalperceptron. Default: 4</p> <p> DEFAULT: <code>4</code> </p> <code>use_checkpoint</code> <p>whether to use checkpoint</p> <p> DEFAULT: <code>False</code> </p> <code>deploy</code> <p>whether to use bias</p> <p> DEFAULT: <code>False</code> </p> Source code in <code>mindcv\\models\\repmlp.py</code> <pre><code>class RepMLPNet(nn.Cell):\n    r\"\"\"RepMLPNet model class, based on\n    `\"RepMLPNet: Hierarchical Vision MLP with Re-parameterized Locality\" &lt;https://arxiv.org/pdf/2112.11081v2.pdf&gt;`_\n\n    Args:\n        in_channels: number of input channels. Default: 3.\n        num_classes: number of classification classes. Default: 1000.\n        patch_size: size of a single image patch. Default: (4, 4)\n        num_blocks: number of blocks per stage. Default: (2,2,6,2)\n        channels: number of in_channels(channels[stage_idx]) and out_channels(channels[stage_idx + 1]) per stage.\n            Default: (192,384,768,1536)\n        hs: height of picture per stage. Default: (64,32,16,8)\n        ws: width of picture per stage. Default: (64,32,16,8)\n        sharesets_nums: number of share sets per stage. Default: (4,8,16,32)\n        reparam_conv_k: convolution kernel size in local Perceptron. Default: (3,)\n        globalperceptron_reduce: Intermediate convolution output size\n            (in_channal = inchannal, out_channel = in_channel/globalperceptron_reduce) in globalperceptron. Default: 4\n        use_checkpoint: whether to use checkpoint\n        deploy: whether to use bias\n    \"\"\"\n\n    def __init__(self,\n                 in_channels=3, num_class=1000,\n                 patch_size=(4, 4),\n                 num_blocks=(2, 2, 6, 2), channels=(192, 384, 768, 1536),\n                 hs=(64, 32, 16, 8), ws=(64, 32, 16, 8),\n                 sharesets_nums=(4, 8, 16, 32),\n                 reparam_conv_k=(3,),\n                 globalperceptron_reduce=4, use_checkpoint=False,\n                 deploy=False):\n        super().__init__()\n        num_stages = len(num_blocks)\n        assert num_stages == len(channels)\n        assert num_stages == len(hs)\n        assert num_stages == len(ws)\n        assert num_stages == len(sharesets_nums)\n\n        self.conv_embedding = conv_bn_relu(in_channels, channels[0], kernel_size=patch_size, stride=patch_size,\n                                           padding=0, has_bias=False)\n        self.conv2d = nn.Conv2d(in_channels, channels[0], kernel_size=patch_size, stride=patch_size, padding=0)\n\n        stages = []\n        embeds = []\n        for stage_idx in range(num_stages):\n            stage_blocks = [RepMLPNetUnit(channels=channels[stage_idx], h=hs[stage_idx], w=ws[stage_idx],\n                                          reparam_conv_k=reparam_conv_k,\n                                          globalperceptron_reduce=globalperceptron_reduce, ffn_expand=4,\n                                          num_sharesets=sharesets_nums[stage_idx],\n                                          deploy=deploy) for _ in range(num_blocks[stage_idx])]\n            stages.append(nn.CellList(stage_blocks))\n            if stage_idx &lt; num_stages - 1:\n                embeds.append(\n                    conv_bn_relu(in_channels=channels[stage_idx], out_channels=channels[stage_idx + 1], kernel_size=2,\n                                 stride=2, padding=0))\n        self.stages = nn.CellList(stages)\n        self.embeds = nn.CellList(embeds)\n        self.head_norm = nn.BatchNorm2d(channels[-1]).set_train()\n        self.head = nn.Dense(channels[-1], num_class)\n\n        self.use_checkpoint = use_checkpoint\n        self.shape = ops.Shape()\n        self.reshape = ops.Reshape()\n        self._initialize_weights()\n\n    def _initialize_weights(self):\n        \"\"\"Initialize weights for cells.\"\"\"\n        for name, cell in self.cells_and_names():\n            if isinstance(cell, nn.Conv2d):\n                k = cell.group / (cell.in_channels * cell.kernel_size[0] * cell.kernel_size[1])\n                k = k ** 0.5\n                cell.weight.set_data(init.initializer(init.Uniform(k), cell.weight.shape, cell.weight.dtype))\n                if cell.bias is not None:\n                    cell.bias.set_data(init.initializer(init.Uniform(k), cell.bias.shape, cell.bias.dtype))\n            elif isinstance(cell, nn.Dense):\n                k = 1 / cell.in_channels\n                k = k ** 0.5\n                cell.weight.set_data(init.initializer(init.Uniform(k), cell.weight.shape, cell.weight.dtype))\n                if cell.bias is not None:\n                    cell.bias.set_data(init.initializer(init.Uniform(k), cell.bias.shape, cell.bias.dtype))\n\n    def forward_features(self, x: Tensor) -&gt; Tensor:\n        x = self.conv_embedding(x)\n\n        for i, stage in enumerate(self.stages):\n            for block in stage:\n                x = block(x)\n\n            if i &lt; len(self.stages) - 1:\n                embed = self.embeds[i]\n                x = embed(x)\n        x = self.head_norm(x)\n        shape = self.shape(x)\n        pool = nn.AvgPool2d(kernel_size=(shape[2], shape[3]))\n        x = pool(x)\n        return x.view(shape[0], -1)\n\n    def forward_head(self, x: Tensor) -&gt; Tensor:\n        return self.head(x)\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        x = self.forward_features(x)\n        return self.forward_head(x)\n</code></pre>"},{"location":"reference/models/#mindcv.models.repmlp.RepMLPNetUnit","title":"<code>mindcv.models.repmlp.RepMLPNetUnit</code>","text":"<p>               Bases: <code>Cell</code></p> <p>Basic unit of RepMLPNet</p> Source code in <code>mindcv\\models\\repmlp.py</code> <pre><code>class RepMLPNetUnit(nn.Cell):\n    \"\"\"Basic unit of RepMLPNet\"\"\"\n\n    def __init__(self, channels, h, w, reparam_conv_k, globalperceptron_reduce, ffn_expand=4,\n                 num_sharesets=1, deploy=False):\n        super().__init__()\n        self.repmlp_block = RepMLPBlock(in_channels=channels, out_channels=channels, h=h, w=w,\n                                        reparam_conv_k=reparam_conv_k, globalperceptron_reduce=globalperceptron_reduce,\n                                        num_sharesets=num_sharesets, deploy=deploy)\n        self.ffn_block = FFNBlock(channels, channels * ffn_expand)\n        self.prebn1 = nn.BatchNorm2d(channels).set_train()\n        self.prebn2 = nn.BatchNorm2d(channels).set_train()\n\n    def construct(self, x):\n        y = x + self.repmlp_block(self.prebn1(x))\n        # print(y)\n        z = y + self.ffn_block(self.prebn2(y))\n        return z\n</code></pre>"},{"location":"reference/models/#mindcv.models.repmlp.repmlp_b224","title":"<code>mindcv.models.repmlp.repmlp_b224(pretrained=False, image_size=224, num_classes=1000, in_channels=3, deploy=False, **kwargs)</code>","text":"<p>Get repmlp_b224 model. Refer to the base class <code>models.RepMLPNet</code> for more details.</p> Source code in <code>mindcv\\models\\repmlp.py</code> <pre><code>@register_model\ndef repmlp_b224(pretrained: bool = False, image_size: int = 224, num_classes: int = 1000, in_channels=3,\n                deploy=False, **kwargs):\n    \"\"\"Get repmlp_b224 model.\n    Refer to the base class `models.RepMLPNet` for more details.\"\"\"\n    default_cfg = default_cfgs[\"repmlp_b224\"]\n    model = RepMLPNet(in_channels=in_channels, num_class=num_classes, channels=(96, 192, 384, 768), hs=(56, 28, 14, 7),\n                      ws=(56, 28, 14, 7),\n                      num_blocks=(2, 2, 12, 2), reparam_conv_k=(1, 3), sharesets_nums=(1, 4, 32, 128),\n                      deploy=deploy)\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre>"},{"location":"reference/models/#mindcv.models.repmlp.repmlp_b256","title":"<code>mindcv.models.repmlp.repmlp_b256(pretrained=False, image_size=256, num_classes=1000, in_channels=3, deploy=False, **kwargs)</code>","text":"<p>Get repmlp_b256 model. Refer to the base class <code>models.RepMLPNet</code> for more details.</p> Source code in <code>mindcv\\models\\repmlp.py</code> <pre><code>@register_model\ndef repmlp_b256(pretrained: bool = False, image_size: int = 256, num_classes: int = 1000, in_channels=3,\n                deploy=False, **kwargs):\n    \"\"\"Get repmlp_b256 model.\n    Refer to the base class `models.RepMLPNet` for more details.\"\"\"\n    default_cfg = default_cfgs[\"repmlp_b256\"]\n    model = RepMLPNet(in_channels=in_channels, num_class=num_classes, channels=(96, 192, 384, 768), hs=(64, 32, 16, 8),\n                      ws=(64, 32, 16, 8),\n                      num_blocks=(2, 2, 12, 2), reparam_conv_k=(1, 3), sharesets_nums=(1, 4, 32, 128),\n                      deploy=deploy)\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre>"},{"location":"reference/models/#mindcv.models.repmlp.repmlp_d256","title":"<code>mindcv.models.repmlp.repmlp_d256(pretrained=False, image_size=256, num_classes=1000, in_channels=3, deploy=False, **kwargs)</code>","text":"<p>Get repmlp_d256 model. Refer to the base class <code>models.RepMLPNet</code> for more details.</p> Source code in <code>mindcv\\models\\repmlp.py</code> <pre><code>@register_model\ndef repmlp_d256(pretrained: bool = False, image_size: int = 256, num_classes: int = 1000, in_channels=3,\n                deploy=False, **kwargs):\n    \"\"\"Get repmlp_d256 model.\n    Refer to the base class `models.RepMLPNet` for more details.\"\"\"\n    default_cfg = default_cfgs[\"repmlp_d256\"]\n    model = RepMLPNet(in_channels=in_channels, num_class=num_classes, channels=(80, 160, 320, 640), hs=(64, 32, 16, 8),\n                      ws=(64, 32, 16, 8),\n                      num_blocks=(2, 2, 18, 2), reparam_conv_k=(1, 3), sharesets_nums=(1, 4, 16, 128),\n                      deploy=deploy)\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre>"},{"location":"reference/models/#mindcv.models.repmlp.repmlp_l256","title":"<code>mindcv.models.repmlp.repmlp_l256(pretrained=False, image_size=256, num_classes=1000, in_channels=3, deploy=False, **kwargs)</code>","text":"<p>Get repmlp_l256 model. Refer to the base class <code>models.RepMLPNet</code> for more details.</p> Source code in <code>mindcv\\models\\repmlp.py</code> <pre><code>@register_model\ndef repmlp_l256(pretrained: bool = False, image_size: int = 256, num_classes: int = 1000, in_channels=3,\n                deploy=False, **kwargs):\n    \"\"\"Get repmlp_l256 model.\n    Refer to the base class `models.RepMLPNet` for more details.\"\"\"\n    default_cfg = default_cfgs[\"repmlp_l256\"]\n    model = RepMLPNet(in_channels=in_channels, num_class=num_classes, channels=(96, 192, 384, 768), hs=(64, 32, 16, 8),\n                      ws=(64, 32, 16, 8),\n                      num_blocks=(2, 2, 18, 2), reparam_conv_k=(1, 3), sharesets_nums=(1, 4, 32, 256),\n                      deploy=deploy)\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre>"},{"location":"reference/models/#mindcv.models.repmlp.repmlp_t224","title":"<code>mindcv.models.repmlp.repmlp_t224(pretrained=False, image_size=224, num_classes=1000, in_channels=3, deploy=False, **kwargs)</code>","text":"<p>Get repmlp_t224 model. Refer to the base class <code>models.RepMLPNet</code> for more details.</p> Source code in <code>mindcv\\models\\repmlp.py</code> <pre><code>@register_model\ndef repmlp_t224(pretrained: bool = False, image_size: int = 224, num_classes: int = 1000, in_channels=3,\n                deploy=False, **kwargs):\n    \"\"\"Get repmlp_t224 model. Refer to the base class `models.RepMLPNet` for more details.\"\"\"\n    default_cfg = default_cfgs[\"repmlp_t224\"]\n    model = RepMLPNet(in_channels=in_channels, num_class=num_classes, channels=(64, 128, 256, 512), hs=(56, 28, 14, 7),\n                      ws=(56, 28, 14, 7),\n                      num_blocks=(2, 2, 6, 2), reparam_conv_k=(1, 3), sharesets_nums=(1, 4, 16, 128),\n                      deploy=deploy)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre>"},{"location":"reference/models/#mindcv.models.repmlp.repmlp_t256","title":"<code>mindcv.models.repmlp.repmlp_t256(pretrained=False, image_size=256, num_classes=1000, in_channels=3, deploy=False, **kwargs)</code>","text":"<p>Get repmlp_t256 model. Refer to the base class <code>models.RepMLPNet</code> for more details.</p> Source code in <code>mindcv\\models\\repmlp.py</code> <pre><code>@register_model\ndef repmlp_t256(pretrained: bool = False, image_size: int = 256, num_classes: int = 1000, in_channels=3,\n                deploy=False, **kwargs):\n    \"\"\"Get repmlp_t256 model.\n    Refer to the base class `models.RepMLPNet` for more details.\"\"\"\n    default_cfg = default_cfgs[\"repmlp_t256\"]\n    model = RepMLPNet(in_channels=in_channels, num_class=num_classes, channels=(64, 128, 256, 512), hs=(64, 32, 16, 8),\n                      ws=(64, 32, 16, 8),\n                      num_blocks=(2, 2, 6, 2), reparam_conv_k=(1, 3), sharesets_nums=(1, 4, 16, 128),\n                      deploy=deploy)\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre>"},{"location":"reference/models/#repvgg","title":"repvgg","text":""},{"location":"reference/models/#mindcv.models.repvgg","title":"<code>mindcv.models.repvgg</code>","text":"<p>MindSpore implementation of <code>RepVGG</code>. Refer to RepVGG: Making VGG_style ConvNets Great Again</p>"},{"location":"reference/models/#mindcv.models.repvgg.RepVGG","title":"<code>mindcv.models.repvgg.RepVGG</code>","text":"<p>               Bases: <code>Cell</code></p> <p>RepVGG model class, based on <code>\"RepVGGBlock: An all-MLP Architecture for Vision\" &lt;https://arxiv.org/pdf/2101.03697&gt;</code>_</p> PARAMETER DESCRIPTION <code>num_blocks</code> <p>number of RepVGGBlocks</p> <p> TYPE: <code>list) </code> </p> <code>num_classes</code> <p>number of classification classes. Default: 1000.</p> <p> TYPE: <code>int) </code> DEFAULT: <code>1000</code> </p> <code>in_channels</code> <p>number the channels of the input. Default: 3.</p> <p> TYPE: <code>in_channels) </code> DEFAULT: <code>3</code> </p> <code>width_multiplier</code> <p>the numbers of MLP Architecture.</p> <p> TYPE: <code>list) </code> DEFAULT: <code>None</code> </p> <code>override_group_map</code> <p>the numbers of MLP Architecture.</p> <p> TYPE: <code>dict) </code> DEFAULT: <code>None</code> </p> <code>deploy</code> <p>use rbr_reparam block or not. Default: False</p> <p> TYPE: <code>bool) </code> DEFAULT: <code>False</code> </p> <code>use_se</code> <p>use se_block or not. Default: False</p> <p> TYPE: <code>bool) </code> DEFAULT: <code>False</code> </p> Source code in <code>mindcv\\models\\repvgg.py</code> <pre><code>class RepVGG(nn.Cell):\n    r\"\"\"RepVGG model class, based on\n    `\"RepVGGBlock: An all-MLP Architecture for Vision\" &lt;https://arxiv.org/pdf/2101.03697&gt;`_\n\n    Args:\n        num_blocks (list) : number of RepVGGBlocks\n        num_classes (int) : number of classification classes. Default: 1000.\n        in_channels (in_channels) : number the channels of the input. Default: 3.\n        width_multiplier (list) : the numbers of MLP Architecture.\n        override_group_map (dict) : the numbers of MLP Architecture.\n        deploy (bool) : use rbr_reparam block or not. Default: False\n        use_se (bool) : use se_block or not. Default: False\n    \"\"\"\n\n    def __init__(self, num_blocks, num_classes=1000, in_channels=3, width_multiplier=None, override_group_map=None,\n                 deploy=False, use_se=False):\n        super().__init__()\n\n        assert len(width_multiplier) == 4\n\n        self.deploy = deploy\n        self.override_group_map = override_group_map or {}\n        self.use_se = use_se\n\n        assert 0 not in self.override_group_map\n\n        self.in_planes = min(64, int(64 * width_multiplier[0]))\n\n        self.stage0 = RepVGGBlock(in_channels=in_channels, out_channels=self.in_planes, kernel_size=3, stride=2,\n                                  padding=1,\n                                  deploy=self.deploy, use_se=self.use_se)\n        self.feature_info = [dict(chs=self.in_planes, reduction=2, name=\"stage0\")]\n        self.cur_layer_idx = 1\n        self.stage1 = self._make_stage(\n            int(64 * width_multiplier[0]), num_blocks[0], stride=2)\n        self.feature_info.append(dict(chs=int(64 * width_multiplier[0]), reduction=4, name=\"stage1\"))\n        self.stage2 = self._make_stage(\n            int(128 * width_multiplier[1]), num_blocks[1], stride=2)\n        self.feature_info.append(dict(chs=int(128 * width_multiplier[1]), reduction=8, name=\"stage2\"))\n        self.stage3 = self._make_stage(\n            int(256 * width_multiplier[2]), num_blocks[2], stride=2)\n        self.feature_info.append(dict(chs=int(256 * width_multiplier[2]), reduction=16, name=\"stage3\"))\n        self.stage4 = self._make_stage(\n            int(512 * width_multiplier[3]), num_blocks[3], stride=2)\n        self.feature_info.append(dict(chs=int(512 * width_multiplier[3]), reduction=32, name=\"stage4\"))\n        self.gap = GlobalAvgPooling()\n        self.linear = nn.Dense(int(512 * width_multiplier[3]), num_classes)\n        self._initialize_weights()\n\n    def _make_stage(self, planes, num_blocks, stride):\n        strides = [stride] + [1] * (num_blocks - 1)\n        blocks = []\n        for s in strides:\n            cur_group = self.override_group_map.get(self.cur_layer_idx, 1)\n            blocks.append(RepVGGBlock(in_channels=self.in_planes, out_channels=planes, kernel_size=3,\n                                      stride=s, padding=1, group=cur_group, deploy=self.deploy,\n                                      use_se=self.use_se))\n            self.in_planes = planes\n            self.cur_layer_idx += 1\n\n        return nn.SequentialCell(blocks)\n\n    def _initialize_weights(self) -&gt; None:\n        \"\"\"Initialize weights for cells.\"\"\"\n        for _, cell in self.cells_and_names():\n            if isinstance(cell, nn.Conv2d):\n                cell.weight.set_data(\n                    init.initializer(init.HeNormal(mode='fan_out', nonlinearity='relu'),\n                                     cell.weight.shape, cell.weight.dtype))\n                if cell.bias is not None:\n                    cell.bias.set_data(\n                        init.initializer('zeros', cell.bias.shape, cell.bias.dtype))\n            elif isinstance(cell, nn.BatchNorm2d):\n                cell.gamma.set_data(init.initializer('ones', cell.gamma.shape, cell.gamma.dtype))\n                cell.beta.set_data(init.initializer('zeros', cell.beta.shape, cell.beta.dtype))\n            elif isinstance(cell, nn.Dense):\n                cell.weight.set_data(\n                    init.initializer(init.HeUniform(mode='fan_in', nonlinearity='sigmoid'),\n                                     cell.weight.shape, cell.weight.dtype))\n                if cell.bias is not None:\n                    cell.bias.set_data(init.initializer('zeros', cell.bias.shape, cell.bias.dtype))\n\n    def construct(self, x):\n        x = self.stage0(x)\n        x = self.stage1(x)\n        x = self.stage2(x)\n        x = self.stage3(x)\n        x = self.stage4(x)\n        x = self.gap(x)\n        x = self.linear(x)\n        return x\n</code></pre>"},{"location":"reference/models/#mindcv.models.repvgg.RepVGGBlock","title":"<code>mindcv.models.repvgg.RepVGGBlock</code>","text":"<p>               Bases: <code>Cell</code></p> <p>Basic Block of RepVGG</p> Source code in <code>mindcv\\models\\repvgg.py</code> <pre><code>class RepVGGBlock(nn.Cell):\n    \"\"\"Basic Block of RepVGG\"\"\"\n\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int,\n                 stride: int = 1, padding: int = 0, dilation: int = 1,\n                 group: int = 1, padding_mode: str = \"zeros\",\n                 deploy: bool = False, use_se: bool = False) -&gt; None:\n        super().__init__()\n        self.deploy = deploy\n        self.group = group\n        self.in_channels = in_channels\n\n        assert kernel_size == 3\n        assert padding == 1\n\n        padding_11 = padding - kernel_size // 2\n\n        self.nonlinearity = nn.ReLU()\n\n        if use_se:\n            self.se = SqueezeExcite(\n                in_channels=out_channels, rd_channels=out_channels // 16)\n        else:\n            self.se = Identity()\n\n        if deploy:\n            self.rbr_reparam = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size,\n                                         stride=stride, padding=padding, dilation=dilation, group=group, has_bias=True,\n                                         pad_mode=padding_mode)\n        else:\n            self.rbr_reparam = None\n            self.rbr_identity = nn.BatchNorm2d(\n                num_features=in_channels) if out_channels == in_channels and stride == 1 else None\n\n            self.rbr_dense = conv_bn(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size,\n                                     stride=stride, padding=padding, group=group)\n            self.rbr_1x1 = conv_bn(in_channels=in_channels, out_channels=out_channels, kernel_size=1, stride=stride,\n                                   padding=padding_11, group=group)\n\n    def construct(self, inputs: Tensor) -&gt; Tensor:\n        if self.rbr_reparam is not None:\n            return self.nonlinearity(self.se(self.rbr_reparam(inputs)))\n\n        if self.rbr_identity is None:\n            id_out = 0\n        else:\n            id_out = self.rbr_identity(inputs)\n\n        return self.nonlinearity(self.se(self.rbr_dense(inputs) + self.rbr_1x1(inputs) + id_out))\n\n    def get_custom_l2(self):\n        \"\"\"This may improve the accuracy and facilitates quantization in some cases.\"\"\"\n        k3 = self.rbr_dense.conv.weight\n        k1 = self.rbr_1x1.conv.weight\n\n        t3 = self.rbr_dense.bn.weight / (\n            ops.sqrt((self.rbr_dense.bn.moving_variance + self.rbr_dense.bn.eps)))\n        t3 = ops.reshape(t3, (-1, 1, 1, 1))\n\n        t1 = (self.rbr_1x1.bn.weight /\n              ((self.rbr_1x1.bn.moving_variance + self.rbr_1x1.bn.eps).sqrt()))\n        t1 = ops.reshape(t1, (-1, 1, 1, 1))\n\n        l2_loss_circle = ops.reduce_sum(k3 ** 2) - ops.reduce_sum(k3[:, :, 1:2, 1:2] ** 2)\n        eq_kernel = k3[:, :, 1:2, 1:2] * t3 + k1 * t1\n        l2_loss_eq_kernel = ops.reduce_sum(eq_kernel ** 2 / (t3 ** 2 + t1 ** 2))\n        return l2_loss_eq_kernel + l2_loss_circle\n\n    #   This func derives the equivalent kernel and bias in a DIFFERENTIABLE way.\n    #   You can get the equivalent kernel and bias at any time and do whatever you want,\n    #   for example, apply some penalties or constraints during training, just like you do to the other models.\n    #   May be useful for quantization or pruning.\n    def get_equivalent_kernel_bias(self):\n        kernel3x3, bias3x3 = self._fuse_bn_tensor(self.rbr_dense)\n        kernel1x1, bias1x1 = self._fuse_bn_tensor(self.rbr_1x1)\n        kernelid, biasid = self._fuse_bn_tensor(self.rbr_identity)\n        return kernel3x3 + self._pad_1x1_to_3x3_tensor(kernel1x1) + kernelid, bias3x3 + bias1x1 + biasid\n\n    def _pad_1x1_to_3x3_tensor(self, kernel1x1):\n        if kernel1x1 is None:\n            return 0\n        return ops.pad(kernel1x1, ((1, 1), (1, 1)))\n\n    def _fuse_bn_tensor(self, branch):\n        if branch is None:\n            return 0, 0\n        if isinstance(branch, nn.SequentialCell):\n            kernel = branch.conv.weight\n            moving_mean = branch.bn.moving_mean\n            moving_variance = branch.bn.moving_variance\n            gamma = branch.bn.gamma\n            beta = branch.bn.beta\n            eps = branch.bn.eps\n        else:\n            assert isinstance(branch, (nn.BatchNorm2d, nn.SyncBatchNorm))\n            if not hasattr(self, \"id_tensor\"):\n                input_dim = self.in_channels // self.group\n                kernel_value = np.zeros((self.in_channels, input_dim, 3, 3), dtype=np.float32)\n                for i in range(self.in_channels):\n                    kernel_value[i, i % input_dim, 1, 1] = 1\n                self.id_tensor = Tensor(kernel_value, dtype=branch.weight.dtype)\n            kernel = self.id_tensor\n            moving_mean = branch.moving_mean\n            moving_variance = branch.moving_variance\n            gamma = branch.gamma\n            beta = branch.beta\n            eps = branch.eps\n        std = ops.sqrt(moving_variance + eps)\n        t = ops.reshape(gamma / std, (-1, 1, 1, 1))\n        return kernel * t, beta - moving_mean * gamma / std\n\n    def switch_to_deploy(self):\n        \"\"\"Model_convert\"\"\"\n        if self.rbr_reparam is not None:\n            return\n        kernel, bias = self.get_equivalent_kernel_bias()\n        self.rbr_reparam = nn.Conv2d(in_channels=self.rbr_dense.conv.in_channels,\n                                     out_channels=self.rbr_dense.conv.out_channels,\n                                     kernel_size=self.rbr_dense.conv.kernel_size, stride=self.rbr_dense.conv.stride,\n                                     padding=self.rbr_dense.conv.padding, dilation=self.rbr_dense.conv.dilation,\n                                     group=self.rbr_dense.conv.group, has_bias=True, pad_mode=\"pad\")\n        self.rbr_reparam.weight.data = kernel\n        self.rbr_reparam.bias.data = bias\n        for para in self.parameters():\n            para.detach_()\n        self.__delattr__(\"rbr_dense\")\n        self.__delattr__(\"rbr_1x1\")\n        if hasattr(self, \"rbr_identity\"):\n            self.__delattr__(\"rbr_identity\")\n        if hasattr(self, \"id_tensor\"):\n            self.__delattr__(\"id_tensor\")\n        self.deploy = True\n</code></pre>"},{"location":"reference/models/#mindcv.models.repvgg.RepVGGBlock.get_custom_l2","title":"<code>mindcv.models.repvgg.RepVGGBlock.get_custom_l2()</code>","text":"<p>This may improve the accuracy and facilitates quantization in some cases.</p> Source code in <code>mindcv\\models\\repvgg.py</code> <pre><code>def get_custom_l2(self):\n    \"\"\"This may improve the accuracy and facilitates quantization in some cases.\"\"\"\n    k3 = self.rbr_dense.conv.weight\n    k1 = self.rbr_1x1.conv.weight\n\n    t3 = self.rbr_dense.bn.weight / (\n        ops.sqrt((self.rbr_dense.bn.moving_variance + self.rbr_dense.bn.eps)))\n    t3 = ops.reshape(t3, (-1, 1, 1, 1))\n\n    t1 = (self.rbr_1x1.bn.weight /\n          ((self.rbr_1x1.bn.moving_variance + self.rbr_1x1.bn.eps).sqrt()))\n    t1 = ops.reshape(t1, (-1, 1, 1, 1))\n\n    l2_loss_circle = ops.reduce_sum(k3 ** 2) - ops.reduce_sum(k3[:, :, 1:2, 1:2] ** 2)\n    eq_kernel = k3[:, :, 1:2, 1:2] * t3 + k1 * t1\n    l2_loss_eq_kernel = ops.reduce_sum(eq_kernel ** 2 / (t3 ** 2 + t1 ** 2))\n    return l2_loss_eq_kernel + l2_loss_circle\n</code></pre>"},{"location":"reference/models/#mindcv.models.repvgg.RepVGGBlock.switch_to_deploy","title":"<code>mindcv.models.repvgg.RepVGGBlock.switch_to_deploy()</code>","text":"<p>Model_convert</p> Source code in <code>mindcv\\models\\repvgg.py</code> <pre><code>def switch_to_deploy(self):\n    \"\"\"Model_convert\"\"\"\n    if self.rbr_reparam is not None:\n        return\n    kernel, bias = self.get_equivalent_kernel_bias()\n    self.rbr_reparam = nn.Conv2d(in_channels=self.rbr_dense.conv.in_channels,\n                                 out_channels=self.rbr_dense.conv.out_channels,\n                                 kernel_size=self.rbr_dense.conv.kernel_size, stride=self.rbr_dense.conv.stride,\n                                 padding=self.rbr_dense.conv.padding, dilation=self.rbr_dense.conv.dilation,\n                                 group=self.rbr_dense.conv.group, has_bias=True, pad_mode=\"pad\")\n    self.rbr_reparam.weight.data = kernel\n    self.rbr_reparam.bias.data = bias\n    for para in self.parameters():\n        para.detach_()\n    self.__delattr__(\"rbr_dense\")\n    self.__delattr__(\"rbr_1x1\")\n    if hasattr(self, \"rbr_identity\"):\n        self.__delattr__(\"rbr_identity\")\n    if hasattr(self, \"id_tensor\"):\n        self.__delattr__(\"id_tensor\")\n    self.deploy = True\n</code></pre>"},{"location":"reference/models/#mindcv.models.repvgg.repvgg_a0","title":"<code>mindcv.models.repvgg.repvgg_a0(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get RepVGG model with num_blocks=[2, 4, 14, 1], width_multiplier=[0.75, 0.75, 0.75, 2.5]. Refer to the base class <code>models.RepVGG</code> for more details.</p> Source code in <code>mindcv\\models\\repvgg.py</code> <pre><code>@register_model\ndef repvgg_a0(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; RepVGG:\n    \"\"\"Get RepVGG model with num_blocks=[2, 4, 14, 1], width_multiplier=[0.75, 0.75, 0.75, 2.5].\n    Refer to the base class `models.RepVGG` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"repvgg_a0\"]\n    model_args = dict(num_blocks=[2, 4, 14, 1], num_classes=num_classes, in_channels=in_channels,\n                      width_multiplier=[0.75, 0.75, 0.75, 2.5], override_group_map=None, deploy=False, **kwargs)\n    return _create_repvgg(pretrained, **dict(default_cfg=default_cfg, **model_args))\n</code></pre>"},{"location":"reference/models/#mindcv.models.repvgg.repvgg_a1","title":"<code>mindcv.models.repvgg.repvgg_a1(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get RepVGG model with num_blocks=[2, 4, 14, 1], width_multiplier=[1.0, 1.0, 1.0, 2.5]. Refer to the base class <code>models.RepVGG</code> for more details.</p> Source code in <code>mindcv\\models\\repvgg.py</code> <pre><code>@register_model\ndef repvgg_a1(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; RepVGG:\n    \"\"\"Get RepVGG model with num_blocks=[2, 4, 14, 1], width_multiplier=[1.0, 1.0, 1.0, 2.5].\n     Refer to the base class `models.RepVGG` for more details.\n     \"\"\"\n    default_cfg = default_cfgs[\"repvgg_a1\"]\n    model_args = dict(num_blocks=[2, 4, 14, 1], num_classes=num_classes, in_channels=in_channels,\n                      width_multiplier=[1.0, 1.0, 1.0, 2.5], override_group_map=None, deploy=False, **kwargs)\n    return _create_repvgg(pretrained, **dict(default_cfg=default_cfg, **model_args))\n</code></pre>"},{"location":"reference/models/#mindcv.models.repvgg.repvgg_a2","title":"<code>mindcv.models.repvgg.repvgg_a2(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get RepVGG model with num_blocks=[2, 4, 14, 1], width_multiplier=[1.5, 1.5, 1.5, 2.75]. Refer to the base class <code>models.RepVGG</code> for more details.</p> Source code in <code>mindcv\\models\\repvgg.py</code> <pre><code>@register_model\ndef repvgg_a2(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; RepVGG:\n    \"\"\"Get RepVGG model with num_blocks=[2, 4, 14, 1], width_multiplier=[1.5, 1.5, 1.5, 2.75].\n     Refer to the base class `models.RepVGG` for more details.\n     \"\"\"\n    default_cfg = default_cfgs[\"repvgg_a2\"]\n    model_args = dict(num_blocks=[2, 4, 14, 1], num_classes=num_classes, in_channels=in_channels,\n                      width_multiplier=[1.5, 1.5, 1.5, 2.75], override_group_map=None, deploy=False, **kwargs)\n    return _create_repvgg(pretrained, **dict(default_cfg=default_cfg, **model_args))\n</code></pre>"},{"location":"reference/models/#mindcv.models.repvgg.repvgg_b0","title":"<code>mindcv.models.repvgg.repvgg_b0(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get RepVGG model with num_blocks=[4, 6, 16, 1], width_multiplier=[1.0, 1.0, 1.0, 2.5]. Refer to the base class <code>models.RepVGG</code> for more details.</p> Source code in <code>mindcv\\models\\repvgg.py</code> <pre><code>@register_model\ndef repvgg_b0(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; RepVGG:\n    \"\"\"Get RepVGG model with num_blocks=[4, 6, 16, 1], width_multiplier=[1.0, 1.0, 1.0, 2.5].\n     Refer to the base class `models.RepVGG` for more details.\n     \"\"\"\n    default_cfg = default_cfgs['repvgg_b0']\n    model_args = dict(num_blocks=[4, 6, 16, 1], num_classes=num_classes, in_channels=in_channels,\n                      width_multiplier=[1.0, 1.0, 1.0, 2.5], override_group_map=None, deploy=False, **kwargs)\n    return _create_repvgg(pretrained, **dict(default_cfg=default_cfg, **model_args))\n</code></pre>"},{"location":"reference/models/#mindcv.models.repvgg.repvgg_b1","title":"<code>mindcv.models.repvgg.repvgg_b1(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get RepVGG model with num_blocks=[4, 6, 16, 1], width_multiplier=[2.0, 2.0, 2.0, 4.0]. Refer to the base class <code>models.RepVGG</code> for more details.</p> Source code in <code>mindcv\\models\\repvgg.py</code> <pre><code>@register_model\ndef repvgg_b1(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; RepVGG:\n    \"\"\"Get RepVGG model with num_blocks=[4, 6, 16, 1], width_multiplier=[2.0, 2.0, 2.0, 4.0].\n     Refer to the base class `models.RepVGG` for more details.\n     \"\"\"\n    default_cfg = default_cfgs['repvgg_b1']\n    model_args = dict(num_blocks=[4, 6, 16, 1], num_classes=num_classes, in_channels=in_channels,\n                      width_multiplier=[2.0, 2.0, 2.0, 4.0], override_group_map=None, deploy=False, **kwargs)\n    return _create_repvgg(pretrained, **dict(default_cfg=default_cfg, **model_args))\n</code></pre>"},{"location":"reference/models/#mindcv.models.repvgg.repvgg_b1g2","title":"<code>mindcv.models.repvgg.repvgg_b1g2(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get RepVGG model with num_blocks=[4, 6, 16, 1], width_multiplier=[2.0, 2.0, 2.0, 4.0]. Refer to the base class <code>models.RepVGG</code> for more details.</p> Source code in <code>mindcv\\models\\repvgg.py</code> <pre><code>@register_model\ndef repvgg_b1g2(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; RepVGG:\n    \"\"\"Get RepVGG model with num_blocks=[4, 6, 16, 1], width_multiplier=[2.0, 2.0, 2.0, 4.0].\n    Refer to the base class `models.RepVGG` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"repvgg_b1g2\"]\n    model_args = dict(num_blocks=[4, 6, 16, 1], num_classes=num_classes, in_channels=in_channels,\n                      width_multiplier=[2.0, 2.0, 2.0, 4.0], override_group_map=g2_map, deploy=False, **kwargs)\n    return _create_repvgg(pretrained, **dict(default_cfg=default_cfg, **model_args))\n</code></pre>"},{"location":"reference/models/#mindcv.models.repvgg.repvgg_b1g4","title":"<code>mindcv.models.repvgg.repvgg_b1g4(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get RepVGG model with num_blocks=[4, 6, 16, 1], width_multiplier=[2.0, 2.0, 2.0, 4.0]. Refer to the base class <code>models.RepVGG</code> for more details.</p> Source code in <code>mindcv\\models\\repvgg.py</code> <pre><code>@register_model\ndef repvgg_b1g4(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; RepVGG:\n    \"\"\"Get RepVGG model with num_blocks=[4, 6, 16, 1], width_multiplier=[2.0, 2.0, 2.0, 4.0].\n    Refer to the base class `models.RepVGG` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"repvgg_b1g4\"]\n    model_args = dict(num_blocks=[4, 6, 16, 1], num_classes=num_classes, in_channels=in_channels,\n                      width_multiplier=[2.0, 2.0, 2.0, 4.0], override_group_map=g4_map, deploy=False, **kwargs)\n    return _create_repvgg(pretrained, **dict(default_cfg=default_cfg, **model_args))\n</code></pre>"},{"location":"reference/models/#mindcv.models.repvgg.repvgg_b2","title":"<code>mindcv.models.repvgg.repvgg_b2(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get RepVGG model with num_blocks=[4, 6, 16, 1], width_multiplier=[2.5, 2.5, 2.5, 5.0]. Refer to the base class <code>models.RepVGG</code> for more details.</p> Source code in <code>mindcv\\models\\repvgg.py</code> <pre><code>@register_model\ndef repvgg_b2(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; RepVGG:\n    \"\"\"Get RepVGG model with num_blocks=[4, 6, 16, 1], width_multiplier=[2.5, 2.5, 2.5, 5.0].\n     Refer to the base class `models.RepVGG` for more details.\n     \"\"\"\n    default_cfg = default_cfgs['repvgg_b2']\n    model_args = dict(num_blocks=[4, 6, 16, 1], num_classes=num_classes, in_channels=in_channels,\n                      width_multiplier=[2.5, 2.5, 2.5, 5.0], override_group_map=None, deploy=False, **kwargs)\n    return _create_repvgg(pretrained, **dict(default_cfg=default_cfg, **model_args))\n</code></pre>"},{"location":"reference/models/#mindcv.models.repvgg.repvgg_b2g4","title":"<code>mindcv.models.repvgg.repvgg_b2g4(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get RepVGG model with num_blocks=[4, 6, 16, 1], width_multiplier=[2.5, 2.5, 2.5, 5.0]. Refer to the base class <code>models.RepVGG</code> for more details.</p> Source code in <code>mindcv\\models\\repvgg.py</code> <pre><code>@register_model\ndef repvgg_b2g4(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; RepVGG:\n    \"\"\"Get RepVGG model with num_blocks=[4, 6, 16, 1], width_multiplier=[2.5, 2.5, 2.5, 5.0].\n    Refer to the base class `models.RepVGG` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"repvgg_b2g4\"]\n    model_args = dict(num_blocks=[4, 6, 16, 1], num_classes=num_classes, in_channels=in_channels,\n                      width_multiplier=[2.5, 2.5, 2.5, 5.0], override_group_map=g4_map, deploy=False, **kwargs)\n    return _create_repvgg(pretrained, **dict(default_cfg=default_cfg, **model_args))\n</code></pre>"},{"location":"reference/models/#mindcv.models.repvgg.repvgg_b3","title":"<code>mindcv.models.repvgg.repvgg_b3(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get RepVGG model with num_blocks=[4, 6, 16, 1], width_multiplier=[3.0, 3.0, 3.0, 5.0]. Refer to the base class <code>models.RepVGG</code> for more details.</p> Source code in <code>mindcv\\models\\repvgg.py</code> <pre><code>@register_model\ndef repvgg_b3(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; RepVGG:\n    \"\"\"Get RepVGG model with num_blocks=[4, 6, 16, 1], width_multiplier=[3.0, 3.0, 3.0, 5.0].\n     Refer to the base class `models.RepVGG` for more details.\n     \"\"\"\n    default_cfg = default_cfgs['repvgg_b3']\n    model_args = dict(num_blocks=[4, 6, 16, 1], num_classes=num_classes, in_channels=in_channels,\n                      width_multiplier=[3.0, 3.0, 3.0, 5.0], override_group_map=None, deploy=False, **kwargs)\n    return _create_repvgg(pretrained, **dict(default_cfg=default_cfg, **model_args))\n</code></pre>"},{"location":"reference/models/#mindcv.models.repvgg.repvgg_model_convert","title":"<code>mindcv.models.repvgg.repvgg_model_convert(model, save_path=None, do_copy=True)</code>","text":"<p>repvgg_model_convert</p> Source code in <code>mindcv\\models\\repvgg.py</code> <pre><code>def repvgg_model_convert(model: nn.Cell, save_path=None, do_copy=True):\n    \"\"\"repvgg_model_convert\"\"\"\n    if do_copy:\n        model = copy.deepcopy(model)\n    for module in model.modules():\n        if hasattr(module, \"switch_to_deploy\"):\n            module.switch_to_deploy()\n    if save_path is not None:\n        save_checkpoint(model.parameters_and_names(), save_path)\n    return model\n</code></pre>"},{"location":"reference/models/#res2net","title":"res2net","text":""},{"location":"reference/models/#mindcv.models.res2net","title":"<code>mindcv.models.res2net</code>","text":"<p>MindSpore implementation of <code>Res2Net</code>. Refer to Res2Net: A New Multi-scale Backbone Architecture.</p>"},{"location":"reference/models/#mindcv.models.res2net.Res2Net","title":"<code>mindcv.models.res2net.Res2Net</code>","text":"<p>               Bases: <code>Cell</code></p> <p>Res2Net model class, based on <code>\"Res2Net: A New Multi-scale Backbone Architecture\" &lt;https://arxiv.org/abs/1904.01169&gt;</code>_</p> PARAMETER DESCRIPTION <code>block</code> <p>block of resnet.</p> <p> TYPE: <code>Type[Cell]</code> </p> <code>layer_nums</code> <p>number of layers of each stage.</p> <p> TYPE: <code>List[int]</code> </p> <code>version</code> <p>variety of Res2Net, 'res2net' or 'res2net_v1b'. Default: 'res2net'.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'res2net'</code> </p> <code>num_classes</code> <p>number of classification classes. Default: 1000.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1000</code> </p> <code>in_channels</code> <p>number the channels of the input. Default: 3.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>groups</code> <p>number of groups for group conv in blocks. Default: 1.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>base_width</code> <p>base width of pre group hidden channel in blocks. Default: 26.</p> <p> TYPE: <code>int</code> DEFAULT: <code>26</code> </p> <code>scale</code> <p>scale factor of Bottle2neck. Default: 4.</p> <p> DEFAULT: <code>4</code> </p> <code>norm</code> <p>normalization layer in blocks. Default: None.</p> <p> TYPE: <code>Optional[Cell]</code> DEFAULT: <code>None</code> </p> Source code in <code>mindcv\\models\\res2net.py</code> <pre><code>class Res2Net(nn.Cell):\n    r\"\"\"Res2Net model class, based on\n    `\"Res2Net: A New Multi-scale Backbone Architecture\" &lt;https://arxiv.org/abs/1904.01169&gt;`_\n\n    Args:\n        block: block of resnet.\n        layer_nums: number of layers of each stage.\n        version: variety of Res2Net, 'res2net' or 'res2net_v1b'. Default: 'res2net'.\n        num_classes: number of classification classes. Default: 1000.\n        in_channels: number the channels of the input. Default: 3.\n        groups: number of groups for group conv in blocks. Default: 1.\n        base_width: base width of pre group hidden channel in blocks. Default: 26.\n        scale: scale factor of Bottle2neck. Default: 4.\n        norm: normalization layer in blocks. Default: None.\n    \"\"\"\n\n    def __init__(\n        self,\n        block: Type[nn.Cell],\n        layer_nums: List[int],\n        version: str = \"res2net\",\n        num_classes: int = 1000,\n        in_channels: int = 3,\n        groups: int = 1,\n        base_width: int = 26,\n        scale=4,\n        norm: Optional[nn.Cell] = None,\n    ) -&gt; None:\n        super().__init__()\n        assert version in [\"res2net\", \"res2net_v1b\"]\n        self.version = version\n\n        if norm is None:\n            norm = nn.BatchNorm2d\n        self.norm = norm\n\n        self.num_classes = num_classes\n        self.input_channels = 64\n        self.groups = groups\n        self.base_width = base_width\n        self.scale = scale\n        if self.version == \"res2net\":\n            self.conv1 = nn.Conv2d(in_channels, self.input_channels, kernel_size=7,\n                                   stride=2, padding=3, pad_mode=\"pad\")\n        elif self.version == \"res2net_v1b\":\n            self.conv1 = nn.SequentialCell([\n                nn.Conv2d(in_channels, self.input_channels // 2, kernel_size=3,\n                          stride=2, padding=1, pad_mode=\"pad\"),\n                norm(self.input_channels // 2),\n                nn.ReLU(),\n                nn.Conv2d(self.input_channels // 2, self.input_channels // 2, kernel_size=3,\n                          stride=1, padding=1, pad_mode=\"pad\"),\n                norm(self.input_channels // 2),\n                nn.ReLU(),\n                nn.Conv2d(self.input_channels // 2, self.input_channels, kernel_size=3,\n                          stride=1, padding=1, pad_mode=\"pad\"),\n            ])\n\n        self.bn1 = norm(self.input_channels)\n        self.relu = nn.ReLU()\n        self.max_pool = nn.SequentialCell([\n            nn.Pad(paddings=((0, 0), (0, 0), (1, 1), (1, 1)), mode=\"CONSTANT\"),\n            nn.MaxPool2d(kernel_size=3, stride=2)\n        ])\n        self.layer1 = self._make_layer(block, 64, layer_nums[0])\n        self.layer2 = self._make_layer(block, 128, layer_nums[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layer_nums[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, layer_nums[3], stride=2)\n\n        self.pool = GlobalAvgPooling()\n        self.num_features = 512 * block.expansion\n        self.classifier = nn.Dense(self.num_features, num_classes)\n        self._initialize_weights()\n\n    def _initialize_weights(self) -&gt; None:\n        \"\"\"Initialize weights for cells.\"\"\"\n        for _, cell in self.cells_and_names():\n            if isinstance(cell, nn.Conv2d):\n                cell.weight.set_data(\n                    init.initializer(init.HeNormal(math.sqrt(5), mode=\"fan_out\", nonlinearity=\"relu\"),\n                                     cell.weight.shape, cell.weight.dtype))\n                if cell.bias is not None:\n                    cell.bias.set_data(\n                        init.initializer(init.HeUniform(math.sqrt(5), mode=\"fan_in\", nonlinearity=\"leaky_relu\"),\n                                         cell.bias.shape, cell.bias.dtype))\n            elif isinstance(cell, nn.BatchNorm2d):\n                cell.gamma.set_data(init.initializer(\"ones\", cell.gamma.shape, cell.gamma.dtype))\n                cell.beta.set_data(init.initializer(\"zeros\", cell.beta.shape, cell.beta.dtype))\n            elif isinstance(cell, nn.Dense):\n                cell.weight.set_data(\n                    init.initializer(init.HeUniform(math.sqrt(5), mode=\"fan_in\", nonlinearity=\"leaky_relu\"),\n                                     cell.weight.shape, cell.weight.dtype))\n                if cell.bias is not None:\n                    cell.bias.set_data(init.initializer(\"zeros\", cell.bias.shape, cell.bias.dtype))\n\n    def _make_layer(\n        self,\n        block: Type[nn.Cell],\n        channels: int,\n        block_nums: int,\n        stride: int = 1,\n    ) -&gt; nn.SequentialCell:\n        down_sample = None\n\n        if stride != 1 or self.input_channels != channels * block.expansion:\n            if stride == 1 or self.version == \"res2net\":\n                down_sample = nn.SequentialCell([\n                    nn.Conv2d(self.input_channels, channels * block.expansion, kernel_size=1, stride=stride),\n                    self.norm(channels * block.expansion)\n                ])\n            else:\n                down_sample = nn.SequentialCell([\n                    nn.AvgPool2d(kernel_size=stride, stride=stride, pad_mode=\"same\"),\n                    nn.Conv2d(self.input_channels, channels * block.expansion, kernel_size=1, stride=1),\n                    self.norm(channels * block.expansion)\n                ])\n\n        layers = []\n        layers.append(\n            block(\n                self.input_channels,\n                channels,\n                stride=stride,\n                down_sample=down_sample,\n                groups=self.groups,\n                base_width=self.base_width,\n                scale=self.scale,\n                stype=\"stage\",\n                norm=self.norm,\n            )\n        )\n        self.input_channels = channels * block.expansion\n\n        for _ in range(1, block_nums):\n            layers.append(\n                block(\n                    self.input_channels,\n                    channels,\n                    groups=self.groups,\n                    base_width=self.base_width,\n                    scale=self.scale,\n                    norm=self.norm,\n                )\n            )\n\n        return nn.SequentialCell(layers)\n\n    def forward_features(self, x: Tensor) -&gt; Tensor:\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.max_pool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        return x\n\n    def forward_head(self, x: Tensor) -&gt; Tensor:\n        x = self.pool(x)\n        x = self.classifier(x)\n        return x\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n</code></pre>"},{"location":"reference/models/#mindcv.models.res2net.res2net101","title":"<code>mindcv.models.res2net.res2net101(pretrained=False, num_classes=1001, in_channels=3, **kwargs)</code>","text":"<p>Get 101 layers Res2Net model. Refer to the base class <code>models.Res2Net</code> for more details.</p> Source code in <code>mindcv\\models\\res2net.py</code> <pre><code>@register_model\ndef res2net101(pretrained: bool = False, num_classes: int = 1001, in_channels=3, **kwargs):\n    \"\"\"Get 101 layers Res2Net model.\n    Refer to the base class `models.Res2Net` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"res2net101\"]\n    model = Res2Net(Bottle2neck, [3, 4, 23, 3], num_classes=num_classes, in_channels=in_channels, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre>"},{"location":"reference/models/#mindcv.models.res2net.res2net152","title":"<code>mindcv.models.res2net.res2net152(pretrained=False, num_classes=1001, in_channels=3, **kwargs)</code>","text":"<p>Get 152 layers Res2Net model. Refer to the base class <code>models.Res2Net</code> for more details.</p> Source code in <code>mindcv\\models\\res2net.py</code> <pre><code>@register_model\ndef res2net152(pretrained: bool = False, num_classes: int = 1001, in_channels=3, **kwargs):\n    \"\"\"Get 152 layers Res2Net model.\n    Refer to the base class `models.Res2Net` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"res2net152\"]\n    model = Res2Net(Bottle2neck, [3, 8, 36, 3], num_classes=num_classes, in_channels=in_channels, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre>"},{"location":"reference/models/#mindcv.models.res2net.res2net50","title":"<code>mindcv.models.res2net.res2net50(pretrained=False, num_classes=1001, in_channels=3, **kwargs)</code>","text":"<p>Get 50 layers Res2Net model. Refer to the base class <code>models.Res2Net</code> for more details.</p> Source code in <code>mindcv\\models\\res2net.py</code> <pre><code>@register_model\ndef res2net50(pretrained: bool = False, num_classes: int = 1001, in_channels=3, **kwargs):\n    \"\"\"Get 50 layers Res2Net model.\n    Refer to the base class `models.Res2Net` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"res2net50\"]\n    model = Res2Net(Bottle2neck, [3, 4, 6, 3], num_classes=num_classes, in_channels=in_channels, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre>"},{"location":"reference/models/#resnest","title":"resnest","text":""},{"location":"reference/models/#mindcv.models.resnest","title":"<code>mindcv.models.resnest</code>","text":"<p>MindSpore implementation of <code>ResNeSt</code>. Refer to ResNeSt: Split-Attention Networks.</p>"},{"location":"reference/models/#mindcv.models.resnest.Bottleneck","title":"<code>mindcv.models.resnest.Bottleneck</code>","text":"<p>               Bases: <code>Cell</code></p> <p>ResNeSt Bottleneck</p> Source code in <code>mindcv\\models\\resnest.py</code> <pre><code>class Bottleneck(nn.Cell):\n    \"\"\"ResNeSt Bottleneck\"\"\"\n\n    expansion = 4\n\n    def __init__(\n        self,\n        inplanes: int,\n        planes: int,\n        stride=1,\n        downsample: Optional[nn.SequentialCell] = None,\n        radix: int = 1,\n        cardinality: int = 1,\n        bottleneck_width: int = 64,\n        avd: bool = False,\n        avd_first: bool = False,\n        dilation: int = 1,\n        is_first: bool = False,\n        norm_layer: Optional[nn.Cell] = None,\n    ) -&gt; None:\n        super(Bottleneck, self).__init__()\n        group_width = int(planes * (bottleneck_width / 64.0)) * cardinality\n        self.conv1 = nn.Conv2d(inplanes, group_width, kernel_size=1, has_bias=False)\n        self.bn1 = norm_layer(group_width)\n        self.radix = radix\n        self.avd = avd and (stride &gt; 1 or is_first)\n        self.avd_first = avd_first\n\n        if self.avd:\n            self.avd_layer = nn.AvgPool2d(3, stride, pad_mode=\"same\")\n            stride = 1\n\n        if radix &gt;= 1:\n            self.conv2 = SplitAttn(group_width, group_width, kernel_size=3, stride=stride,\n                                   padding=dilation, dilation=dilation, group=cardinality,\n                                   bias=False, radix=radix, norm_layer=norm_layer)\n        else:\n            self.conv2 = nn.Conv2d(group_width, group_width, kernel_size=3, stride=stride,\n                                   pad_mode=\"pad\", padding=dilation, dilation=dilation,\n                                   group=cardinality, has_bias=False)\n            self.bn2 = norm_layer(group_width)\n\n        self.conv3 = nn.Conv2d(group_width, planes * 4, kernel_size=1, has_bias=False)\n        self.bn3 = norm_layer(planes * 4)\n\n        self.relu = nn.ReLU()\n        self.downsample = downsample\n        self.dilation = dilation\n        self.stride = stride\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        if self.avd and self.avd_first:\n            out = self.avd_layer(out)\n\n        out = self.conv2(out)\n        if self.radix == 0:\n            out = self.bn2(out)\n            out = self.relu(out)\n\n        if self.avd and not self.avd_first:\n            out = self.avd_layer(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n</code></pre>"},{"location":"reference/models/#mindcv.models.resnest.ResNeSt","title":"<code>mindcv.models.resnest.ResNeSt</code>","text":"<p>               Bases: <code>Cell</code></p> <p>ResNeSt model class, based on <code>\"ResNeSt: Split-Attention Networks\" &lt;https://arxiv.org/abs/2004.08955&gt;</code>_</p> PARAMETER DESCRIPTION <code>block</code> <p>Class for the residual block. Option is Bottleneck.</p> <p> TYPE: <code>Type[Bottleneck]</code> </p> <code>layers</code> <p>Numbers of layers in each block.</p> <p> TYPE: <code>List[int]</code> </p> <code>radix</code> <p>Number of groups for Split-Attention conv. Default: 1.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>group</code> <p>Number of groups for the conv in each bottleneck block. Default: 1.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>bottleneck_width</code> <p>bottleneck channels factor. Default: 64.</p> <p> TYPE: <code>int</code> DEFAULT: <code>64</code> </p> <code>num_classes</code> <p>Number of classification classes. Default: 1000.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1000</code> </p> <code>dilated</code> <p>Applying dilation strategy to pretrained ResNeSt yielding a stride-8 model,      typically used in Semantic Segmentation. Default: False.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>dilation</code> <p>Number of dilation in the conv. Default: 1.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>deep_stem</code> <p>three 3x3 convolution layers of widths stem_width, stem_width, stem_width * 2.        Default: False.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>stem_width</code> <p>number of channels in stem convolutions. Default: 64.</p> <p> TYPE: <code>int</code> DEFAULT: <code>64</code> </p> <code>avg_down</code> <p>use avg pooling for projection skip connection between stages/downsample.       Default: False.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>avd</code> <p>use avg pooling before or after split-attention conv. Default: False.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>avd_first</code> <p>use avg pooling before or after split-attention conv. Default: False.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>drop_rate</code> <p>Drop probability for the Dropout layer. Default: 0.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>norm_layer</code> <p>Normalization layer used in backbone network. Default: nn.BatchNorm2d.</p> <p> TYPE: <code>Cell</code> DEFAULT: <code>BatchNorm2d</code> </p> Source code in <code>mindcv\\models\\resnest.py</code> <pre><code>class ResNeSt(nn.Cell):\n    r\"\"\"ResNeSt model class, based on\n    `\"ResNeSt: Split-Attention Networks\" &lt;https://arxiv.org/abs/2004.08955&gt;`_\n\n    Args:\n        block: Class for the residual block. Option is Bottleneck.\n        layers: Numbers of layers in each block.\n        radix: Number of groups for Split-Attention conv. Default: 1.\n        group: Number of groups for the conv in each bottleneck block. Default: 1.\n        bottleneck_width: bottleneck channels factor. Default: 64.\n        num_classes: Number of classification classes. Default: 1000.\n        dilated: Applying dilation strategy to pretrained ResNeSt yielding a stride-8 model,\n                 typically used in Semantic Segmentation. Default: False.\n        dilation: Number of dilation in the conv. Default: 1.\n        deep_stem: three 3x3 convolution layers of widths stem_width, stem_width, stem_width * 2.\n                   Default: False.\n        stem_width: number of channels in stem convolutions. Default: 64.\n        avg_down: use avg pooling for projection skip connection between stages/downsample.\n                  Default: False.\n        avd: use avg pooling before or after split-attention conv. Default: False.\n        avd_first: use avg pooling before or after split-attention conv. Default: False.\n        drop_rate: Drop probability for the Dropout layer. Default: 0.\n        norm_layer: Normalization layer used in backbone network. Default: nn.BatchNorm2d.\n    \"\"\"\n\n    def __init__(\n        self,\n        block: Type[Bottleneck],\n        layers: List[int],\n        radix: int = 1,\n        group: int = 1,\n        bottleneck_width: int = 64,\n        num_classes: int = 1000,\n        dilated: bool = False,\n        dilation: int = 1,\n        deep_stem: bool = False,\n        stem_width: int = 64,\n        avg_down: bool = False,\n        avd: bool = False,\n        avd_first: bool = False,\n        drop_rate: float = 0.0,\n        norm_layer: nn.Cell = nn.BatchNorm2d,\n    ) -&gt; None:\n        super(ResNeSt, self).__init__()\n        self.cardinality = group\n        self.bottleneck_width = bottleneck_width\n        # ResNet-D params\n        self.inplanes = stem_width * 2 if deep_stem else 64\n        self.avg_down = avg_down\n        # ResNeSt params\n        self.radix = radix\n        self.avd = avd\n        self.avd_first = avd_first\n\n        if deep_stem:\n            self.conv1 = nn.SequentialCell([\n                nn.Conv2d(3, stem_width, kernel_size=3, stride=2, pad_mode=\"pad\",\n                          padding=1, has_bias=False),\n                norm_layer(stem_width),\n                nn.ReLU(),\n                nn.Conv2d(stem_width, stem_width, kernel_size=3, stride=1, pad_mode=\"pad\",\n                          padding=1, has_bias=False),\n                norm_layer(stem_width),\n                nn.ReLU(),\n                nn.Conv2d(stem_width, stem_width * 2, kernel_size=3, stride=1, pad_mode=\"pad\",\n                          padding=1, has_bias=False),\n            ])\n        else:\n            self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, pad_mode=\"pad\", padding=3,\n                                   has_bias=False)\n\n        self.bn1 = norm_layer(self.inplanes)\n        self.relu = nn.ReLU()\n        self.feature_info = [dict(chs=self.inplanes, reduction=2, name=\"relu\")]\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, pad_mode=\"same\")\n\n        self.layer1 = self._make_layer(block, 64, layers[0], norm_layer=norm_layer, is_first=False)\n        self.feature_info.append(dict(chs=block.expansion * 64, reduction=4, name='layer1'))\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2, norm_layer=norm_layer)\n        self.feature_info.append(dict(chs=block.expansion * 128, reduction=8, name='layer2'))\n\n        if dilated or dilation == 4:\n            self.layer3 = self._make_layer(block, 256, layers[2], stride=1, dilation=2, norm_layer=norm_layer)\n            self.feature_info.append(dict(chs=block.expansion * 256, reduction=8, name='layer3'))\n            self.layer4 = self._make_layer(block, 512, layers[3], stride=1, dilation=4, norm_layer=norm_layer)\n            self.feature_info.append(dict(chs=block.expansion * 512, reduction=8, name='layer4'))\n        elif dilation == 2:\n            self.layer3 = self._make_layer(block, 256, layers[2], stride=2, dilation=1, norm_layer=norm_layer)\n            self.feature_info.append(dict(chs=block.expansion * 256, reduction=16, name='layer3'))\n            self.layer4 = self._make_layer(block, 512, layers[3], stride=1, dilation=2, norm_layer=norm_layer)\n            self.feature_info.append(dict(chs=block.expansion * 512, reduction=16, name='layer4'))\n        else:\n            self.layer3 = self._make_layer(block, 256, layers[2], stride=2, norm_layer=norm_layer)\n            self.feature_info.append(dict(chs=block.expansion * 256, reduction=16, name='layer3'))\n            self.layer4 = self._make_layer(block, 512, layers[3], stride=2, norm_layer=norm_layer)\n            self.feature_info.append(dict(chs=block.expansion * 512, reduction=32, name='layer4'))\n\n        self.avgpool = GlobalAvgPooling()\n        self.drop = Dropout(p=drop_rate) if drop_rate &gt; 0.0 else None\n        self.fc = nn.Dense(512 * block.expansion, num_classes)\n\n        self._initialize_weights()\n\n    def _initialize_weights(self) -&gt; None:\n        \"\"\"Initialize weights for cells.\"\"\"\n        for _, cell in self.cells_and_names():\n            if isinstance(cell, nn.Conv2d):\n                cell.weight.set_data(\n                    init.initializer(\n                        init.HeNormal(mode=\"fan_out\", nonlinearity=\"relu\"), cell.weight.shape, cell.weight.dtype\n                    )\n                )\n                if cell.bias is not None:\n                    cell.bias.set_data(init.initializer(\"zeros\", cell.bias.shape, cell.bias.dtype))\n            elif isinstance(cell, nn.BatchNorm2d):\n                cell.gamma.set_data(init.initializer(\"ones\", cell.gamma.shape, cell.gamma.dtype))\n                cell.beta.set_data(init.initializer(\"zeros\", cell.beta.shape, cell.beta.dtype))\n            elif isinstance(cell, nn.Dense):\n                cell.weight.set_data(\n                    init.initializer(\n                        init.HeUniform(mode=\"fan_in\", nonlinearity=\"sigmoid\"), cell.weight.shape, cell.weight.dtype\n                    )\n                )\n                if cell.bias is not None:\n                    cell.bias.set_data(init.initializer(\"zeros\", cell.bias.shape, cell.bias.dtype))\n\n    def _make_layer(\n        self,\n        block: Type[Bottleneck],\n        planes: int,\n        blocks: int,\n        stride: int = 1,\n        dilation: int = 1,\n        norm_layer: Optional[nn.Cell] = None,\n        is_first: bool = True,\n    ) -&gt; nn.SequentialCell:\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            down_layers = []\n            if self.avg_down:\n                if dilation == 1:\n                    down_layers.append(nn.AvgPool2d(kernel_size=stride, stride=stride, pad_mode=\"valid\"))\n                else:\n                    down_layers.append(nn.AvgPool2d(kernel_size=1, stride=1, pad_mode=\"valid\"))\n\n                down_layers.append(nn.Conv2d(self.inplanes, planes * block.expansion, kernel_size=1,\n                                             stride=1, has_bias=False))\n            else:\n                down_layers.append(\n                    nn.Conv2d(self.inplanes, planes * block.expansion, kernel_size=1, stride=stride,\n                              has_bias=False))\n            down_layers.append(norm_layer(planes * block.expansion))\n            downsample = nn.SequentialCell(down_layers)\n\n        layers = []\n        if dilation == 1 or dilation == 2:\n            layers.append(\n                block(\n                    self.inplanes,\n                    planes,\n                    stride,\n                    downsample=downsample,\n                    radix=self.radix,\n                    cardinality=self.cardinality,\n                    bottleneck_width=self.bottleneck_width,\n                    avd=self.avd,\n                    avd_first=self.avd_first,\n                    dilation=1,\n                    is_first=is_first,\n                    norm_layer=norm_layer,\n                )\n            )\n        elif dilation == 4:\n            layers.append(\n                block(\n                    self.inplanes,\n                    planes,\n                    stride,\n                    downsample=downsample,\n                    radix=self.radix,\n                    cardinality=self.cardinality,\n                    bottleneck_width=self.bottleneck_width,\n                    avd=self.avd,\n                    avd_first=self.avd_first,\n                    dilation=2,\n                    is_first=is_first,\n                    norm_layer=norm_layer,\n                )\n            )\n        else:\n            raise ValueError(f\"Unsupported model type {dilation}\")\n\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(\n                block(\n                    self.inplanes,\n                    planes,\n                    radix=self.radix,\n                    cardinality=self.cardinality,\n                    bottleneck_width=self.bottleneck_width,\n                    avd=self.avd,\n                    avd_first=self.avd_first,\n                    dilation=dilation,\n                    norm_layer=norm_layer,\n                )\n            )\n\n        return nn.SequentialCell(layers)\n\n    def forward_features(self, x: Tensor) -&gt; Tensor:\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        return x\n\n    def forward_head(self, x: Tensor) -&gt; Tensor:\n        x = self.avgpool(x)\n        if self.drop:\n            x = self.drop(x)\n        x = self.fc(x)\n        return x\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n</code></pre>"},{"location":"reference/models/#mindcv.models.resnest.SplitAttn","title":"<code>mindcv.models.resnest.SplitAttn</code>","text":"<p>               Bases: <code>Cell</code></p> <p>Split-Attention Conv2d</p> Source code in <code>mindcv\\models\\resnest.py</code> <pre><code>class SplitAttn(nn.Cell):\n    \"\"\"Split-Attention Conv2d\"\"\"\n\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        kernel_size: int = 3,\n        stride: int = 1,\n        padding: int = 0,\n        dilation: int = 1,\n        group: int = 1,\n        bias: bool = False,\n        radix: int = 2,\n        rd_ratio: float = 0.25,\n        rd_channels: Optional[int] = None,\n        rd_divisor: int = 8,\n        act_layer: nn.Cell = nn.ReLU,\n        norm_layer: Optional[nn.Cell] = None,\n    ) -&gt; None:\n        super(SplitAttn, self).__init__()\n        out_channels = out_channels or in_channels\n        self.radix = radix\n        mid_chs = out_channels * radix\n\n        if rd_channels is None:\n            attn_chs = make_divisible(in_channels * radix * rd_ratio, min_value=32, divisor=rd_divisor)\n        else:\n            attn_chs = rd_channels * radix\n\n        padding = kernel_size // 2 if padding is None else padding\n\n        self.conv = nn.Conv2d(in_channels, mid_chs, kernel_size=kernel_size, stride=stride,\n                              pad_mode=\"pad\", padding=padding, dilation=dilation,\n                              group=group * radix, has_bias=bias)\n        self.bn0 = norm_layer(mid_chs) if norm_layer else Identity()\n        self.act0 = act_layer()\n        self.fc1 = nn.Conv2d(out_channels, attn_chs, 1, group=group, has_bias=True)\n        self.bn1 = norm_layer(attn_chs) if norm_layer else nn.Identity()\n        self.act1 = act_layer()\n        self.fc2 = nn.Conv2d(attn_chs, mid_chs, 1, group=group, has_bias=True)\n        self.rsoftmax = RadixSoftmax(radix, group)\n        self.pool = GlobalAvgPooling(keep_dims=True)\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        x = self.conv(x)\n        x = self.bn0(x)\n        x = self.act0(x)\n\n        B, RC, H, W = x.shape\n        if self.radix &gt; 1:\n            x = ops.reshape(x, (B, self.radix, RC // self.radix, H, W))\n            x_gap = x.sum(axis=1)\n        else:\n            x_gap = x\n        x_gap = self.pool(x_gap)\n        x_gap = self.fc1(x_gap)\n        x_gap = self.bn1(x_gap)\n        x_gap = self.act1(x_gap)\n        x_attn = self.fc2(x_gap)\n\n        x_attn = self.rsoftmax(x_attn)\n        x_attn = ops.reshape(x_attn, (B, -1, 1, 1))\n        if self.radix &gt; 1:\n            out = x * ops.reshape(x_attn, (B, self.radix, RC // self.radix, 1, 1))\n            out = out.sum(axis=1)\n        else:\n            out = x * x_attn\n\n        return out\n</code></pre>"},{"location":"reference/models/#resnet","title":"resnet","text":""},{"location":"reference/models/#mindcv.models.resnet","title":"<code>mindcv.models.resnet</code>","text":"<p>MindSpore implementation of <code>ResNet</code>. Refer to Deep Residual Learning for Image Recognition.</p>"},{"location":"reference/models/#mindcv.models.resnet.BasicBlock","title":"<code>mindcv.models.resnet.BasicBlock</code>","text":"<p>               Bases: <code>Cell</code></p> <p>define the basic block of resnet</p> Source code in <code>mindcv\\models\\resnet.py</code> <pre><code>class BasicBlock(nn.Cell):\n    \"\"\"define the basic block of resnet\"\"\"\n    expansion: int = 1\n\n    def __init__(\n        self,\n        in_channels: int,\n        channels: int,\n        stride: int = 1,\n        groups: int = 1,\n        base_width: int = 64,\n        norm: Optional[nn.Cell] = None,\n        down_sample: Optional[nn.Cell] = None,\n    ) -&gt; None:\n        super().__init__()\n        if norm is None:\n            norm = nn.BatchNorm2d\n        assert groups == 1, \"BasicBlock only supports groups=1\"\n        assert base_width == 64, \"BasicBlock only supports base_width=64\"\n\n        self.conv1 = nn.Conv2d(in_channels, channels, kernel_size=3,\n                               stride=stride, padding=1, pad_mode=\"pad\")\n        self.bn1 = norm(channels)\n        self.relu = nn.ReLU()\n        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3,\n                               stride=1, padding=1, pad_mode=\"pad\")\n        self.bn2 = norm(channels)\n        self.down_sample = down_sample\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.down_sample is not None:\n            identity = self.down_sample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n</code></pre>"},{"location":"reference/models/#mindcv.models.resnet.Bottleneck","title":"<code>mindcv.models.resnet.Bottleneck</code>","text":"<p>               Bases: <code>Cell</code></p> <p>Bottleneck here places the stride for downsampling at 3x3 convolution(self.conv2) as torchvision does, while original implementation places the stride at the first 1x1 convolution(self.conv1)</p> Source code in <code>mindcv\\models\\resnet.py</code> <pre><code>class Bottleneck(nn.Cell):\n    \"\"\"\n    Bottleneck here places the stride for downsampling at 3x3 convolution(self.conv2) as torchvision does,\n    while original implementation places the stride at the first 1x1 convolution(self.conv1)\n    \"\"\"\n    expansion: int = 4\n\n    def __init__(\n        self,\n        in_channels: int,\n        channels: int,\n        stride: int = 1,\n        groups: int = 1,\n        base_width: int = 64,\n        norm: Optional[nn.Cell] = None,\n        down_sample: Optional[nn.Cell] = None,\n    ) -&gt; None:\n        super().__init__()\n        if norm is None:\n            norm = nn.BatchNorm2d\n\n        width = int(channels * (base_width / 64.0)) * groups\n\n        self.conv1 = nn.Conv2d(in_channels, width, kernel_size=1, stride=1)\n        self.bn1 = norm(width)\n        self.conv2 = nn.Conv2d(width, width, kernel_size=3, stride=stride,\n                               padding=1, pad_mode=\"pad\", group=groups)\n        self.bn2 = norm(width)\n        self.conv3 = nn.Conv2d(width, channels * self.expansion,\n                               kernel_size=1, stride=1)\n        self.bn3 = norm(channels * self.expansion)\n        self.relu = nn.ReLU()\n        self.down_sample = down_sample\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.down_sample is not None:\n            identity = self.down_sample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n</code></pre>"},{"location":"reference/models/#mindcv.models.resnet.ResNet","title":"<code>mindcv.models.resnet.ResNet</code>","text":"<p>               Bases: <code>Cell</code></p> <p>ResNet model class, based on <code>\"Deep Residual Learning for Image Recognition\" &lt;https://arxiv.org/abs/1512.03385&gt;</code>_</p> PARAMETER DESCRIPTION <code>block</code> <p>block of resnet.</p> <p> TYPE: <code>Type[Union[BasicBlock, Bottleneck]]</code> </p> <code>layers</code> <p>number of layers of each stage.</p> <p> TYPE: <code>List[int]</code> </p> <code>num_classes</code> <p>number of classification classes. Default: 1000.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1000</code> </p> <code>in_channels</code> <p>number the channels of the input. Default: 3.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>groups</code> <p>number of groups for group conv in blocks. Default: 1.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>base_width</code> <p>base width of pre group hidden channel in blocks. Default: 64.</p> <p> TYPE: <code>int</code> DEFAULT: <code>64</code> </p> <code>norm</code> <p>normalization layer in blocks. Default: None.</p> <p> TYPE: <code>Optional[Cell]</code> DEFAULT: <code>None</code> </p> Source code in <code>mindcv\\models\\resnet.py</code> <pre><code>class ResNet(nn.Cell):\n    r\"\"\"ResNet model class, based on\n    `\"Deep Residual Learning for Image Recognition\" &lt;https://arxiv.org/abs/1512.03385&gt;`_\n\n    Args:\n        block: block of resnet.\n        layers: number of layers of each stage.\n        num_classes: number of classification classes. Default: 1000.\n        in_channels: number the channels of the input. Default: 3.\n        groups: number of groups for group conv in blocks. Default: 1.\n        base_width: base width of pre group hidden channel in blocks. Default: 64.\n        norm: normalization layer in blocks. Default: None.\n    \"\"\"\n\n    def __init__(\n        self,\n        block: Type[Union[BasicBlock, Bottleneck]],\n        layers: List[int],\n        num_classes: int = 1000,\n        in_channels: int = 3,\n        groups: int = 1,\n        base_width: int = 64,\n        norm: Optional[nn.Cell] = None,\n    ) -&gt; None:\n        super().__init__()\n        if norm is None:\n            norm = nn.BatchNorm2d\n\n        self.norm: nn.Cell = norm  # add type hints to make pylint happy\n        self.input_channels = 64\n        self.groups = groups\n        self.base_with = base_width\n\n        self.conv1 = nn.Conv2d(in_channels, self.input_channels, kernel_size=7,\n                               stride=2, pad_mode=\"pad\", padding=3)\n        self.bn1 = norm(self.input_channels)\n        self.relu = nn.ReLU()\n        self.feature_info = [dict(chs=self.input_channels, reduction=2, name=\"relu\")]\n        self.max_pool = nn.MaxPool2d(kernel_size=3, stride=2, pad_mode=\"same\")\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.feature_info.append(dict(chs=block.expansion * 64, reduction=4, name=\"layer1\"))\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.feature_info.append(dict(chs=block.expansion * 128, reduction=8, name=\"layer2\"))\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n        self.feature_info.append(dict(chs=block.expansion * 256, reduction=16, name=\"layer3\"))\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n        self.feature_info.append(dict(chs=block.expansion * 512, reduction=32, name=\"layer4\"))\n\n        self.pool = GlobalAvgPooling()\n        self.num_features = 512 * block.expansion\n        self.classifier = nn.Dense(self.num_features, num_classes)\n\n        self._initialize_weights()\n\n    def _initialize_weights(self) -&gt; None:\n        \"\"\"Initialize weights for cells.\"\"\"\n        for _, cell in self.cells_and_names():\n            if isinstance(cell, nn.Conv2d):\n                cell.weight.set_data(\n                    init.initializer(init.HeNormal(mode='fan_out', nonlinearity='relu'),\n                                     cell.weight.shape, cell.weight.dtype))\n                if cell.bias is not None:\n                    cell.bias.set_data(\n                        init.initializer('zeros', cell.bias.shape, cell.bias.dtype))\n            elif isinstance(cell, nn.BatchNorm2d):\n                cell.gamma.set_data(init.initializer('ones', cell.gamma.shape, cell.gamma.dtype))\n                cell.beta.set_data(init.initializer('zeros', cell.beta.shape, cell.beta.dtype))\n            elif isinstance(cell, nn.Dense):\n                cell.weight.set_data(\n                    init.initializer(init.HeUniform(mode='fan_in', nonlinearity='sigmoid'),\n                                     cell.weight.shape, cell.weight.dtype))\n                if cell.bias is not None:\n                    cell.bias.set_data(init.initializer('zeros', cell.bias.shape, cell.bias.dtype))\n\n    def _make_layer(\n        self,\n        block: Type[Union[BasicBlock, Bottleneck]],\n        channels: int,\n        block_nums: int,\n        stride: int = 1,\n    ) -&gt; nn.SequentialCell:\n        \"\"\"build model depending on cfgs\"\"\"\n        down_sample = None\n\n        if stride != 1 or self.input_channels != channels * block.expansion:\n            down_sample = nn.SequentialCell([\n                nn.Conv2d(self.input_channels, channels * block.expansion, kernel_size=1, stride=stride),\n                self.norm(channels * block.expansion)\n            ])\n\n        layers = []\n        layers.append(\n            block(\n                self.input_channels,\n                channels,\n                stride=stride,\n                down_sample=down_sample,\n                groups=self.groups,\n                base_width=self.base_with,\n                norm=self.norm,\n            )\n        )\n        self.input_channels = channels * block.expansion\n\n        for _ in range(1, block_nums):\n            layers.append(\n                block(\n                    self.input_channels,\n                    channels,\n                    groups=self.groups,\n                    base_width=self.base_with,\n                    norm=self.norm\n                )\n            )\n\n        return nn.SequentialCell(layers)\n\n    def forward_features(self, x: Tensor) -&gt; Tensor:\n        \"\"\"Network forward feature extraction.\"\"\"\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.max_pool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        return x\n\n    def forward_head(self, x: Tensor) -&gt; Tensor:\n        x = self.pool(x)\n        x = self.classifier(x)\n        return x\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n</code></pre>"},{"location":"reference/models/#mindcv.models.resnet.ResNet.forward_features","title":"<code>mindcv.models.resnet.ResNet.forward_features(x)</code>","text":"<p>Network forward feature extraction.</p> Source code in <code>mindcv\\models\\resnet.py</code> <pre><code>def forward_features(self, x: Tensor) -&gt; Tensor:\n    \"\"\"Network forward feature extraction.\"\"\"\n    x = self.conv1(x)\n    x = self.bn1(x)\n    x = self.relu(x)\n    x = self.max_pool(x)\n\n    x = self.layer1(x)\n    x = self.layer2(x)\n    x = self.layer3(x)\n    x = self.layer4(x)\n    return x\n</code></pre>"},{"location":"reference/models/#mindcv.models.resnet.resnet101","title":"<code>mindcv.models.resnet.resnet101(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get 101 layers ResNet model. Refer to the base class <code>models.ResNet</code> for more details.</p> Source code in <code>mindcv\\models\\resnet.py</code> <pre><code>@register_model\ndef resnet101(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs):\n    \"\"\"Get 101 layers ResNet model.\n    Refer to the base class `models.ResNet` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"resnet101\"]\n    model_args = dict(block=Bottleneck, layers=[3, 4, 23, 3], num_classes=num_classes, in_channels=in_channels,\n                      **kwargs)\n    return _create_resnet(pretrained, **dict(default_cfg=default_cfg, **model_args))\n</code></pre>"},{"location":"reference/models/#mindcv.models.resnet.resnet152","title":"<code>mindcv.models.resnet.resnet152(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get 152 layers ResNet model. Refer to the base class <code>models.ResNet</code> for more details.</p> Source code in <code>mindcv\\models\\resnet.py</code> <pre><code>@register_model\ndef resnet152(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs):\n    \"\"\"Get 152 layers ResNet model.\n    Refer to the base class `models.ResNet` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"resnet152\"]\n    model_args = dict(block=Bottleneck, layers=[3, 8, 36, 3], num_classes=num_classes, in_channels=in_channels,\n                      **kwargs)\n    return _create_resnet(pretrained, **dict(default_cfg=default_cfg, **model_args))\n</code></pre>"},{"location":"reference/models/#mindcv.models.resnet.resnet18","title":"<code>mindcv.models.resnet.resnet18(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get 18 layers ResNet model. Refer to the base class <code>models.ResNet</code> for more details.</p> Source code in <code>mindcv\\models\\resnet.py</code> <pre><code>@register_model\ndef resnet18(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs):\n    \"\"\"Get 18 layers ResNet model.\n    Refer to the base class `models.ResNet` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"resnet18\"]\n    model_args = dict(block=BasicBlock, layers=[2, 2, 2, 2], num_classes=num_classes, in_channels=in_channels,\n                      **kwargs)\n    return _create_resnet(pretrained, **dict(default_cfg=default_cfg, **model_args))\n</code></pre>"},{"location":"reference/models/#mindcv.models.resnet.resnet34","title":"<code>mindcv.models.resnet.resnet34(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get 34 layers ResNet model. Refer to the base class <code>models.ResNet</code> for more details.</p> Source code in <code>mindcv\\models\\resnet.py</code> <pre><code>@register_model\ndef resnet34(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs):\n    \"\"\"Get 34 layers ResNet model.\n    Refer to the base class `models.ResNet` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"resnet34\"]\n    model_args = dict(block=BasicBlock, layers=[3, 4, 6, 3], num_classes=num_classes, in_channels=in_channels,\n                      **kwargs)\n    return _create_resnet(pretrained, **dict(default_cfg=default_cfg, **model_args))\n</code></pre>"},{"location":"reference/models/#mindcv.models.resnet.resnet50","title":"<code>mindcv.models.resnet.resnet50(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get 50 layers ResNet model. Refer to the base class <code>models.ResNet</code> for more details.</p> Source code in <code>mindcv\\models\\resnet.py</code> <pre><code>@register_model\ndef resnet50(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs):\n    \"\"\"Get 50 layers ResNet model.\n    Refer to the base class `models.ResNet` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"resnet50\"]\n    model_args = dict(block=Bottleneck, layers=[3, 4, 6, 3], num_classes=num_classes, in_channels=in_channels,\n                      **kwargs)\n    return _create_resnet(pretrained, **dict(default_cfg=default_cfg, **model_args))\n</code></pre>"},{"location":"reference/models/#mindcv.models.resnet.resnext101_32x4d","title":"<code>mindcv.models.resnet.resnext101_32x4d(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get 101 layers ResNeXt model with 32 groups of GPConv. Refer to the base class <code>models.ResNet</code> for more details.</p> Source code in <code>mindcv\\models\\resnet.py</code> <pre><code>@register_model\ndef resnext101_32x4d(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs):\n    \"\"\"Get 101 layers ResNeXt model with 32 groups of GPConv.\n    Refer to the base class `models.ResNet` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"resnext101_32x4d\"]\n    model_args = dict(block=Bottleneck, layers=[3, 4, 23, 3], groups=32, base_width=4, num_classes=num_classes,\n                      in_channels=in_channels, **kwargs)\n    return _create_resnet(pretrained, **dict(default_cfg=default_cfg, **model_args))\n</code></pre>"},{"location":"reference/models/#mindcv.models.resnet.resnext101_64x4d","title":"<code>mindcv.models.resnet.resnext101_64x4d(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get 101 layers ResNeXt model with 64 groups of GPConv. Refer to the base class <code>models.ResNet</code> for more details.</p> Source code in <code>mindcv\\models\\resnet.py</code> <pre><code>@register_model\ndef resnext101_64x4d(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs):\n    \"\"\"Get 101 layers ResNeXt model with 64 groups of GPConv.\n    Refer to the base class `models.ResNet` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"resnext101_64x4d\"]\n    model_args = dict(block=Bottleneck, layers=[3, 4, 23, 3], groups=64, base_width=4, num_classes=num_classes,\n                      in_channels=in_channels, **kwargs)\n    return _create_resnet(pretrained, **dict(default_cfg=default_cfg, **model_args))\n</code></pre>"},{"location":"reference/models/#mindcv.models.resnet.resnext50_32x4d","title":"<code>mindcv.models.resnet.resnext50_32x4d(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get 50 layers ResNeXt model with 32 groups of GPConv. Refer to the base class <code>models.ResNet</code> for more details.</p> Source code in <code>mindcv\\models\\resnet.py</code> <pre><code>@register_model\ndef resnext50_32x4d(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs):\n    \"\"\"Get 50 layers ResNeXt model with 32 groups of GPConv.\n    Refer to the base class `models.ResNet` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"resnext50_32x4d\"]\n    model_args = dict(block=Bottleneck, layers=[3, 4, 6, 3], groups=32, base_width=4, num_classes=num_classes,\n                      in_channels=in_channels, **kwargs)\n    return _create_resnet(pretrained, **dict(default_cfg=default_cfg, **model_args))\n</code></pre>"},{"location":"reference/models/#resnetv2","title":"resnetv2","text":""},{"location":"reference/models/#mindcv.models.resnetv2","title":"<code>mindcv.models.resnetv2</code>","text":"<p>MindSpore implementation of <code>ResNetV2</code>. Refer to Identity Mappings in Deep Residual Networks.</p>"},{"location":"reference/models/#mindcv.models.resnetv2.resnetv2_101","title":"<code>mindcv.models.resnetv2.resnetv2_101(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get 101 layers ResNetV2 model. Refer to the base class <code>models.ResNet</code> for more details.</p> Source code in <code>mindcv\\models\\resnetv2.py</code> <pre><code>@register_model\ndef resnetv2_101(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs):\n    \"\"\"Get 101 layers ResNetV2 model.\n    Refer to the base class `models.ResNet` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"resnetv2_101\"]\n    model = ResNet(PreActBottleneck, [3, 4, 23, 3], num_classes=num_classes, in_channels=in_channels, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre>"},{"location":"reference/models/#mindcv.models.resnetv2.resnetv2_50","title":"<code>mindcv.models.resnetv2.resnetv2_50(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get 50 layers ResNetV2 model. Refer to the base class <code>models.ResNet</code> for more details.</p> Source code in <code>mindcv\\models\\resnetv2.py</code> <pre><code>@register_model\ndef resnetv2_50(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs):\n    \"\"\"Get 50 layers ResNetV2 model.\n    Refer to the base class `models.ResNet` for more details.\n    \"\"\"\n    default_cfg = default_cfgs['resnetv2_50']\n    model = ResNet(PreActBottleneck, [3, 4, 6, 3], num_classes=num_classes, in_channels=in_channels, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre>"},{"location":"reference/models/#rexnet","title":"rexnet","text":""},{"location":"reference/models/#mindcv.models.rexnet","title":"<code>mindcv.models.rexnet</code>","text":"<p>MindSpore implementation of <code>ReXNet</code>. Refer to ReXNet: Rethinking Channel Dimensions for Efficient Model Design.</p>"},{"location":"reference/models/#mindcv.models.rexnet.LinearBottleneck","title":"<code>mindcv.models.rexnet.LinearBottleneck</code>","text":"<p>               Bases: <code>Cell</code></p> <p>LinearBottleneck</p> Source code in <code>mindcv\\models\\rexnet.py</code> <pre><code>class LinearBottleneck(nn.Cell):\n    \"\"\"LinearBottleneck\"\"\"\n\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        exp_ratio,\n        stride,\n        use_se=True,\n        se_ratio=1 / 12,\n        ch_div=1,\n        act_layer=nn.SiLU,\n        dw_act_layer=nn.ReLU6,\n        drop_path=None,\n        **kwargs,\n    ):\n        super(LinearBottleneck, self).__init__(**kwargs)\n        self.use_shortcut = stride == 1 and in_channels &lt;= out_channels\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n\n        if exp_ratio != 1:\n            dw_channels = in_channels * exp_ratio\n            self.conv_exp = Conv2dNormActivation(in_channels, dw_channels, 1, activation=act_layer)\n        else:\n            dw_channels = in_channels\n            self.conv_exp = None\n\n        self.conv_dw = Conv2dNormActivation(dw_channels, dw_channels, 3, stride, padding=1,\n                                            groups=dw_channels, activation=None)\n\n        if use_se:\n            self.se = SqueezeExcite(dw_channels,\n                                    rd_channels=make_divisible(int(dw_channels * se_ratio), ch_div),\n                                    norm=nn.BatchNorm2d)\n        else:\n            self.se = None\n        self.act_dw = dw_act_layer()\n\n        self.conv_pwl = Conv2dNormActivation(dw_channels, out_channels, 1, padding=0, activation=None)\n        self.drop_path = drop_path\n\n    def construct(self, x):\n        shortcut = x\n        if self.conv_exp is not None:\n            x = self.conv_exp(x)\n        x = self.conv_dw(x)\n        if self.se is not None:\n            x = self.se(x)\n        x = self.act_dw(x)\n        x = self.conv_pwl(x)\n        if self.use_shortcut:\n            if self.drop_path is not None:\n                x = self.drop_path(x)\n            x[:, 0:self.in_channels] += shortcut\n        return x\n</code></pre>"},{"location":"reference/models/#mindcv.models.rexnet.ReXNetV1","title":"<code>mindcv.models.rexnet.ReXNetV1</code>","text":"<p>               Bases: <code>Cell</code></p> <p>ReXNet model class, based on <code>\"Rethinking Channel Dimensions for Efficient Model Design\" &lt;https://arxiv.org/abs/2007.00992&gt;</code>_</p> PARAMETER DESCRIPTION <code>in_channels</code> <p>number of the input channels. Default: 3.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>fi_channels</code> <p>number of the final channels. Default: 180.</p> <p> TYPE: <code>int</code> DEFAULT: <code>180</code> </p> <code>initial_channels</code> <p>initialize inplanes. Default: 16.</p> <p> TYPE: <code>int</code> DEFAULT: <code>16</code> </p> <code>width_mult</code> <p>The ratio of the channel. Default: 1.0.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>depth_mult</code> <p>The ratio of num_layers. Default: 1.0.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>num_classes</code> <p>number of classification classes. Default: 1000.</p> <p> TYPE: <code>int) </code> DEFAULT: <code>1000</code> </p> <code>use_se</code> <p>use SENet in LinearBottleneck. Default: True.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>se_ratio</code> <p>(float): SENet reduction ratio. Default 1/12.</p> <p> DEFAULT: <code>1 / 12</code> </p> <code>drop_rate</code> <p>dropout ratio. Default: 0.2.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.2</code> </p> <code>ch_div</code> <p>divisible by ch_div. Default: 1.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>act_layer</code> <p>activation function in ConvNormAct. Default: nn.SiLU.</p> <p> TYPE: <code>Cell</code> DEFAULT: <code>SiLU</code> </p> <code>dw_act_layer</code> <p>activation function after dw_conv. Default: nn.ReLU6.</p> <p> TYPE: <code>Cell</code> DEFAULT: <code>ReLU6</code> </p> <code>cls_useconv</code> <p>use conv in classification. Default: False.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Source code in <code>mindcv\\models\\rexnet.py</code> <pre><code>class ReXNetV1(nn.Cell):\n    r\"\"\"ReXNet model class, based on\n    `\"Rethinking Channel Dimensions for Efficient Model Design\" &lt;https://arxiv.org/abs/2007.00992&gt;`_\n\n    Args:\n        in_channels (int): number of the input channels. Default: 3.\n        fi_channels (int): number of the final channels. Default: 180.\n        initial_channels (int): initialize inplanes. Default: 16.\n        width_mult (float): The ratio of the channel. Default: 1.0.\n        depth_mult (float): The ratio of num_layers. Default: 1.0.\n        num_classes (int) : number of classification classes. Default: 1000.\n        use_se (bool): use SENet in LinearBottleneck. Default: True.\n        se_ratio: (float): SENet reduction ratio. Default 1/12.\n        drop_rate (float): dropout ratio. Default: 0.2.\n        ch_div (int): divisible by ch_div. Default: 1.\n        act_layer (nn.Cell): activation function in ConvNormAct. Default: nn.SiLU.\n        dw_act_layer (nn.Cell): activation function after dw_conv. Default: nn.ReLU6.\n        cls_useconv (bool): use conv in classification. Default: False.\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels=3,\n        fi_channels=180,\n        initial_channels=16,\n        width_mult=1.0,\n        depth_mult=1.0,\n        num_classes=1000,\n        use_se=True,\n        se_ratio=1 / 12,\n        drop_rate=0.2,\n        drop_path_rate=0.0,\n        ch_div=1,\n        act_layer=nn.SiLU,\n        dw_act_layer=nn.ReLU6,\n        cls_useconv=False,\n    ):\n        super(ReXNetV1, self).__init__()\n\n        layers = [1, 2, 2, 3, 3, 5]\n        strides = [1, 2, 2, 2, 1, 2]\n        use_ses = [False, False, True, True, True, True]\n\n        layers = [ceil(element * depth_mult) for element in layers]\n        strides = sum([[element] + [1] * (layers[idx] - 1)\n                       for idx, element in enumerate(strides)], [])\n        if use_se:\n            use_ses = sum([[element] * layers[idx] for idx, element in enumerate(use_ses)], [])\n        else:\n            use_ses = [False] * sum(layers[:])\n        exp_ratios = [1] * layers[0] + [6] * sum(layers[1:])\n\n        self.depth = sum(layers[:]) * 3\n        stem_channel = 32 / width_mult if width_mult &lt; 1.0 else 32\n        inplanes = initial_channels / width_mult if width_mult &lt; 1.0 else initial_channels\n\n        features = []\n        in_channels_group = []\n        out_channels_group = []\n\n        for i in range(self.depth // 3):\n            if i == 0:\n                in_channels_group.append(int(round(stem_channel * width_mult)))\n                out_channels_group.append(int(round(inplanes * width_mult)))\n            else:\n                in_channels_group.append(int(round(inplanes * width_mult)))\n                inplanes += fi_channels / (self.depth // 3 * 1.0)\n                out_channels_group.append(int(round(inplanes * width_mult)))\n\n        stem_chs = make_divisible(round(stem_channel * width_mult), divisor=ch_div)\n        self.stem = Conv2dNormActivation(in_channels, stem_chs, stride=2, padding=1, activation=act_layer)\n\n        feat_chs = [stem_chs]\n        self.feature_info = []\n        curr_stride = 2\n        features = []\n        num_blocks = len(in_channels_group)\n        for block_idx, (in_c, out_c, exp_ratio, stride, use_se) in enumerate(\n            zip(in_channels_group, out_channels_group, exp_ratios, strides, use_ses)\n        ):\n            if stride &gt; 1:\n                fname = \"stem\" if block_idx == 0 else f\"features.{block_idx - 1}\"\n                self.feature_info += [dict(chs=feat_chs[-1], reduction=curr_stride, name=fname)]\n            block_dpr = drop_path_rate * block_idx / (num_blocks - 1)  # stochastic depth linear decay rule\n            drop_path = DropPath(block_dpr) if block_dpr &gt; 0. else None\n            features.append(LinearBottleneck(in_channels=in_c,\n                                             out_channels=out_c,\n                                             exp_ratio=exp_ratio,\n                                             stride=stride,\n                                             use_se=use_se,\n                                             se_ratio=se_ratio,\n                                             act_layer=act_layer,\n                                             dw_act_layer=dw_act_layer,\n                                             drop_path=drop_path))\n            curr_stride *= stride\n            feat_chs.append(out_c)\n\n        pen_channels = make_divisible(int(1280 * width_mult), divisor=ch_div)\n        self.feature_info += [dict(chs=feat_chs[-1], reduction=curr_stride, name=f'features.{len(features) - 1}')]\n        self.flatten_sequential = True\n        features.append(Conv2dNormActivation(out_channels_group[-1],\n                                             pen_channels,\n                                             kernel_size=1,\n                                             activation=act_layer))\n\n        features.append(GlobalAvgPooling(keep_dims=True))\n        self.useconv = cls_useconv\n        self.features = nn.SequentialCell(*features)\n        if self.useconv:\n            self.cls = nn.SequentialCell(\n                Dropout(p=drop_rate),\n                nn.Conv2d(pen_channels, num_classes, 1, has_bias=True))\n        else:\n            self.cls = nn.SequentialCell(\n                Dropout(p=drop_rate),\n                nn.Dense(pen_channels, num_classes))\n        self._initialize_weights()\n\n    def _initialize_weights(self) -&gt; None:\n        \"\"\"Initialize weights for cells.\"\"\"\n        for _, cell in self.cells_and_names():\n            if isinstance(cell, (nn.Conv2d, nn.Dense)):\n                cell.weight.set_data(\n                    init.initializer(init.HeUniform(math.sqrt(5), mode=\"fan_in\", nonlinearity=\"relu\"),\n                                     cell.weight.shape, cell.weight.dtype))\n                if cell.bias is not None:\n                    cell.bias.set_data(\n                        init.initializer(init.HeUniform(math.sqrt(5), mode=\"fan_in\", nonlinearity=\"leaky_relu\"),\n                                         [1, cell.bias.shape[0]], cell.bias.dtype).reshape((-1)))\n\n    def forward_features(self, x):\n        x = self.stem(x)\n        x = self.features(x)\n        return x\n\n    def forward_head(self, x):\n        if not self.useconv:\n            x = x.reshape((x.shape[0], -1))\n            x = self.cls(x)\n        else:\n            x = self.cls(x).reshape((x.shape[0], -1))\n        return x\n\n    def construct(self, x):\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n</code></pre>"},{"location":"reference/models/#mindcv.models.rexnet.rexnet_09","title":"<code>mindcv.models.rexnet.rexnet_09(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get ReXNet model with width multiplier of 0.9. Refer to the base class <code>models.ReXNetV1</code> for more details.</p> Source code in <code>mindcv\\models\\rexnet.py</code> <pre><code>@register_model\ndef rexnet_09(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; ReXNetV1:\n    \"\"\"Get ReXNet model with width multiplier of 0.9.\n    Refer to the base class `models.ReXNetV1` for more details.\n    \"\"\"\n    return _rexnet(\"rexnet_09\", 0.9, in_channels, num_classes, pretrained, **kwargs)\n</code></pre>"},{"location":"reference/models/#mindcv.models.rexnet.rexnet_10","title":"<code>mindcv.models.rexnet.rexnet_10(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get ReXNet model with width multiplier of 1.0. Refer to the base class <code>models.ReXNetV1</code> for more details.</p> Source code in <code>mindcv\\models\\rexnet.py</code> <pre><code>@register_model\ndef rexnet_10(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; ReXNetV1:\n    \"\"\"Get ReXNet model with width multiplier of 1.0.\n    Refer to the base class `models.ReXNetV1` for more details.\n    \"\"\"\n    return _rexnet(\"rexnet_10\", 1.0, in_channels, num_classes, pretrained, **kwargs)\n</code></pre>"},{"location":"reference/models/#mindcv.models.rexnet.rexnet_13","title":"<code>mindcv.models.rexnet.rexnet_13(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get ReXNet model with width multiplier of 1.3. Refer to the base class <code>models.ReXNetV1</code> for more details.</p> Source code in <code>mindcv\\models\\rexnet.py</code> <pre><code>@register_model\ndef rexnet_13(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; ReXNetV1:\n    \"\"\"Get ReXNet model with width multiplier of 1.3.\n    Refer to the base class `models.ReXNetV1` for more details.\n    \"\"\"\n    return _rexnet(\"rexnet_13\", 1.3, in_channels, num_classes, pretrained, **kwargs)\n</code></pre>"},{"location":"reference/models/#mindcv.models.rexnet.rexnet_15","title":"<code>mindcv.models.rexnet.rexnet_15(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get ReXNet model with width multiplier of 1.5. Refer to the base class <code>models.ReXNetV1</code> for more details.</p> Source code in <code>mindcv\\models\\rexnet.py</code> <pre><code>@register_model\ndef rexnet_15(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; ReXNetV1:\n    \"\"\"Get ReXNet model with width multiplier of 1.5.\n    Refer to the base class `models.ReXNetV1` for more details.\n    \"\"\"\n    return _rexnet(\"rexnet_15\", 1.5, in_channels, num_classes, pretrained, **kwargs)\n</code></pre>"},{"location":"reference/models/#mindcv.models.rexnet.rexnet_20","title":"<code>mindcv.models.rexnet.rexnet_20(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get ReXNet model with width multiplier of 2.0. Refer to the base class <code>models.ReXNetV1</code> for more details.</p> Source code in <code>mindcv\\models\\rexnet.py</code> <pre><code>@register_model\ndef rexnet_20(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; ReXNetV1:\n    \"\"\"Get ReXNet model with width multiplier of 2.0.\n    Refer to the base class `models.ReXNetV1` for more details.\n    \"\"\"\n    return _rexnet(\"rexnet_20\", 2.0, in_channels, num_classes, pretrained, **kwargs)\n</code></pre>"},{"location":"reference/models/#senet","title":"senet","text":""},{"location":"reference/models/#mindcv.models.senet","title":"<code>mindcv.models.senet</code>","text":"<p>MindSpore implementation of <code>SENet</code>. Refer to Squeeze-and-Excitation Networks.</p>"},{"location":"reference/models/#mindcv.models.senet.Bottleneck","title":"<code>mindcv.models.senet.Bottleneck</code>","text":"<p>               Bases: <code>Cell</code></p> <p>Define the base block class for [SEnet, SEResNet, SEResNext] bottlenecks that implements <code>construct</code> method.</p> Source code in <code>mindcv\\models\\senet.py</code> <pre><code>class Bottleneck(nn.Cell):\n    \"\"\"\n    Define the base block class for [SEnet, SEResNet, SEResNext] bottlenecks\n    that implements `construct` method.\n    \"\"\"\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        shortcut = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            shortcut = self.downsample(x)\n\n        out = self.se_module(out) + shortcut\n        out = self.relu(out)\n\n        return out\n</code></pre>"},{"location":"reference/models/#mindcv.models.senet.SEBottleneck","title":"<code>mindcv.models.senet.SEBottleneck</code>","text":"<p>               Bases: <code>Bottleneck</code></p> <p>Define the Bottleneck for SENet154.</p> Source code in <code>mindcv\\models\\senet.py</code> <pre><code>class SEBottleneck(Bottleneck):\n    \"\"\"\n    Define the Bottleneck for SENet154.\n    \"\"\"\n\n    expansion: int = 4\n\n    def __init__(\n        self,\n        in_channels: int,\n        channels: int,\n        group: int,\n        reduction: int,\n        stride: int = 1,\n        downsample: Optional[nn.SequentialCell] = None,\n    ) -&gt; None:\n        super(SEBottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels, channels * 2, kernel_size=1, pad_mode=\"pad\",\n                               padding=0, has_bias=False)\n        self.bn1 = nn.BatchNorm2d(channels * 2)\n        self.conv2 = nn.Conv2d(channels * 2, channels * 4, kernel_size=3, stride=stride,\n                               pad_mode=\"pad\", padding=1, group=group, has_bias=False)\n        self.bn2 = nn.BatchNorm2d(channels * 4)\n        self.conv3 = nn.Conv2d(channels * 4, channels * 4, kernel_size=1, pad_mode=\"pad\",\n                               padding=0, has_bias=False)\n        self.bn3 = nn.BatchNorm2d(channels * 4)\n        self.relu = nn.ReLU()\n        self.se_module = SqueezeExciteV2(channels * 4, rd_ratio=1.0 / reduction)\n        self.downsample = downsample\n        self.stride = stride\n</code></pre>"},{"location":"reference/models/#mindcv.models.senet.SENet","title":"<code>mindcv.models.senet.SENet</code>","text":"<p>               Bases: <code>Cell</code></p> <p>SENet model class, based on <code>\"Squeeze-and-Excitation Networks\" &lt;https://arxiv.org/abs/1709.01507&gt;</code>_</p> PARAMETER DESCRIPTION <code>block</code> <p>block class of SENet.</p> <p> TYPE: <code>Type[Union[SEBottleneck, SEResNetBottleneck, SEResNetBlock, SEResNeXtBottleneck]]</code> </p> <code>layers</code> <p>Number of residual blocks for 4 layers.</p> <p> TYPE: <code>List[int]</code> </p> <code>group</code> <p>Number of groups for the conv in each bottleneck block.</p> <p> TYPE: <code>int</code> </p> <code>reduction</code> <p>Reduction ratio for Squeeze-and-Excitation modules.</p> <p> TYPE: <code>int</code> </p> <code>drop_rate</code> <p>Drop probability for the Dropout layer. Default: 0.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>in_channels</code> <p>number the channels of the input. Default: 3.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>inplanes</code> <p>Number of input channels for layer1. Default: 64.</p> <p> TYPE: <code>int</code> DEFAULT: <code>64</code> </p> <code>input3x3</code> <p>If <code>True</code>, use three 3x3 convolutions in layer0. Default: False.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>downsample_kernel_size</code> <p>Kernel size for downsampling convolutions. Default: 1.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>downsample_padding</code> <p>Padding for downsampling convolutions. Default: 0.</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>num_classes</code> <p>number of classification classes. Default: 1000.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1000</code> </p> Source code in <code>mindcv\\models\\senet.py</code> <pre><code>class SENet(nn.Cell):\n    r\"\"\"SENet model class, based on\n    `\"Squeeze-and-Excitation Networks\" &lt;https://arxiv.org/abs/1709.01507&gt;`_\n\n    Args:\n        block: block class of SENet.\n        layers: Number of residual blocks for 4 layers.\n        group: Number of groups for the conv in each bottleneck block.\n        reduction: Reduction ratio for Squeeze-and-Excitation modules.\n        drop_rate: Drop probability for the Dropout layer. Default: 0.\n        in_channels: number the channels of the input. Default: 3.\n        inplanes:  Number of input channels for layer1. Default: 64.\n        input3x3: If `True`, use three 3x3 convolutions in layer0. Default: False.\n        downsample_kernel_size: Kernel size for downsampling convolutions. Default: 1.\n        downsample_padding: Padding for downsampling convolutions. Default: 0.\n        num_classes (int): number of classification classes. Default: 1000.\n    \"\"\"\n\n    def __init__(\n        self,\n        block: Type[Union[SEBottleneck, SEResNetBottleneck, SEResNetBlock, SEResNeXtBottleneck]],\n        layers: List[int],\n        group: int,\n        reduction: int,\n        drop_rate: float = 0.0,\n        in_channels: int = 3,\n        inplanes: int = 64,\n        input3x3: bool = False,\n        downsample_kernel_size: int = 1,\n        downsample_padding: int = 0,\n        num_classes: int = 1000,\n    ) -&gt; None:\n        super(SENet, self).__init__()\n        self.inplanes = inplanes\n        self.num_classes = num_classes\n        self.drop_rate = drop_rate\n        if input3x3:\n            self.layer0 = nn.SequentialCell([\n                nn.Conv2d(in_channels, 64, 3, stride=2, pad_mode=\"pad\", padding=1, has_bias=False),\n                nn.BatchNorm2d(64),\n                nn.ReLU(),\n                nn.Conv2d(64, 64, 3, stride=1, pad_mode=\"pad\", padding=1, has_bias=False),\n                nn.BatchNorm2d(64),\n                nn.ReLU(),\n                nn.Conv2d(64, inplanes, 3, stride=1, pad_mode=\"pad\", padding=1, has_bias=False),\n                nn.BatchNorm2d(inplanes),\n                nn.ReLU()\n            ])\n        else:\n            self.layer0 = nn.SequentialCell([\n                nn.Conv2d(in_channels, inplanes, kernel_size=7, stride=2, pad_mode=\"pad\",\n                          padding=3, has_bias=False),\n                nn.BatchNorm2d(inplanes),\n                nn.ReLU()\n            ])\n        self.pool0 = nn.MaxPool2d(3, stride=2, pad_mode=\"same\")\n\n        self.layer1 = self._make_layer(block, planes=64, blocks=layers[0], group=group,\n                                       reduction=reduction, downsample_kernel_size=1,\n                                       downsample_padding=0)\n\n        self.layer2 = self._make_layer(block, planes=128, blocks=layers[1], stride=2,\n                                       group=group, reduction=reduction,\n                                       downsample_kernel_size=downsample_kernel_size,\n                                       downsample_padding=downsample_padding)\n\n        self.layer3 = self._make_layer(block, planes=256, blocks=layers[2], stride=2,\n                                       group=group, reduction=reduction,\n                                       downsample_kernel_size=downsample_kernel_size,\n                                       downsample_padding=downsample_padding)\n\n        self.layer4 = self._make_layer(block, planes=512, blocks=layers[3], stride=2,\n                                       group=group, reduction=reduction,\n                                       downsample_kernel_size=downsample_kernel_size,\n                                       downsample_padding=downsample_padding)\n\n        self.num_features = 512 * block.expansion\n\n        self.pool = GlobalAvgPooling()\n        if self.drop_rate &gt; 0.:\n            self.dropout = Dropout(p=self.drop_rate)\n        self.classifier = nn.Dense(self.num_features, self.num_classes)\n\n        self._initialize_weights()\n\n    def _make_layer(\n        self,\n        block: Type[Union[SEBottleneck, SEResNetBottleneck, SEResNetBlock, SEResNeXtBottleneck]],\n        planes: int,\n        blocks: int,\n        group: int,\n        reduction: int,\n        stride: int = 1,\n        downsample_kernel_size: int = 1,\n        downsample_padding: int = 0,\n    ) -&gt; nn.SequentialCell:\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.SequentialCell([\n                nn.Conv2d(self.inplanes, planes * block.expansion, kernel_size=downsample_kernel_size,\n                          stride=stride, pad_mode=\"pad\", padding=downsample_padding, has_bias=False),\n                nn.BatchNorm2d(planes * block.expansion)\n            ])\n\n        layers = [block(self.inplanes, planes, group, reduction, stride, downsample)]\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes, group, reduction))\n\n        return nn.SequentialCell(layers)\n\n    def _initialize_weights(self) -&gt; None:\n        \"\"\"Initialize weights for cells.\"\"\"\n        for _, cell in self.cells_and_names():\n            if isinstance(cell, nn.Conv2d):\n                cell.weight.set_data(\n                    init.initializer(init.HeNormal(mode=\"fan_out\", nonlinearity=\"relu\"),\n                                     cell.weight.shape, cell.weight.dtype))\n                if cell.bias is not None:\n                    cell.bias.set_data(\n                        init.initializer(\"zeros\", cell.bias.shape, cell.bias.dtype))\n            elif isinstance(cell, nn.BatchNorm2d):\n                cell.gamma.set_data(init.initializer(\"ones\", cell.gamma.shape, cell.gamma.dtype))\n                cell.beta.set_data(init.initializer(\"zeros\", cell.beta.shape, cell.beta.dtype))\n            elif isinstance(cell, nn.Dense):\n                cell.weight.set_data(\n                    init.initializer(init.HeUniform(mode=\"fan_in\", nonlinearity=\"sigmoid\"),\n                                     cell.weight.shape, cell.weight.dtype))\n                if cell.bias is not None:\n                    cell.bias.set_data(init.initializer(\"zeros\", cell.bias.shape, cell.bias.dtype))\n\n    def forward_features(self, x: Tensor) -&gt; Tensor:\n        x = self.layer0(x)\n        x = self.pool0(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        return x\n\n    def forward_head(self, x: Tensor) -&gt; Tensor:\n        x = self.pool(x)\n        if self.drop_rate &gt; 0.0:\n            x = self.dropout(x)\n        x = self.classifier(x)\n        return x\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n</code></pre>"},{"location":"reference/models/#mindcv.models.senet.SEResNeXtBottleneck","title":"<code>mindcv.models.senet.SEResNeXtBottleneck</code>","text":"<p>               Bases: <code>Bottleneck</code></p> <p>Define the ResNeXt bottleneck type C with a Squeeze-and-Excitation module.</p> Source code in <code>mindcv\\models\\senet.py</code> <pre><code>class SEResNeXtBottleneck(Bottleneck):\n    \"\"\"\n    Define the ResNeXt bottleneck type C with a Squeeze-and-Excitation module.\n    \"\"\"\n\n    expansion: int = 4\n\n    def __init__(\n        self,\n        in_channels: int,\n        channels: int,\n        group: int,\n        reduction: int,\n        stride: int = 1,\n        downsample: Optional[nn.SequentialCell] = None,\n        base_width: int = 4,\n    ) -&gt; None:\n        super(SEResNeXtBottleneck, self).__init__()\n        width = math.floor(channels * (base_width / 64)) * group\n        self.conv1 = nn.Conv2d(in_channels, width, kernel_size=1, stride=1, pad_mode=\"pad\",\n                               padding=0, has_bias=False)\n        self.bn1 = nn.BatchNorm2d(width)\n        self.conv2 = nn.Conv2d(width, width, kernel_size=3, stride=stride, pad_mode=\"pad\",\n                               padding=1, group=group, has_bias=False)\n        self.bn2 = nn.BatchNorm2d(width)\n        self.conv3 = nn.Conv2d(width, channels * 4, kernel_size=1, pad_mode=\"pad\", padding=0,\n                               has_bias=False)\n        self.bn3 = nn.BatchNorm2d(channels * 4)\n        self.relu = nn.ReLU()\n        self.se_module = SqueezeExciteV2(channels * 4, rd_ratio=1.0 / reduction)\n        self.downsample = downsample\n        self.stride = stride\n</code></pre>"},{"location":"reference/models/#mindcv.models.senet.SEResNetBlock","title":"<code>mindcv.models.senet.SEResNetBlock</code>","text":"<p>               Bases: <code>Cell</code></p> <p>Define the basic block of resnet with a Squeeze-and-Excitation module.</p> Source code in <code>mindcv\\models\\senet.py</code> <pre><code>class SEResNetBlock(nn.Cell):\n    \"\"\"\n    Define the basic block of resnet with a Squeeze-and-Excitation module.\n    \"\"\"\n\n    expansion = 1\n\n    def __init__(\n        self,\n        in_channels: int,\n        channels: int,\n        group: int,\n        reduction: int,\n        stride: int = 1,\n        downsample: Optional[nn.SequentialCell] = None,\n    ) -&gt; None:\n        super(SEResNetBlock, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels, channels, kernel_size=3, stride=stride, pad_mode=\"pad\",\n                               padding=1, has_bias=False)\n        self.bn1 = nn.BatchNorm2d(channels)\n        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, pad_mode=\"pad\", padding=1,\n                               group=group, has_bias=False)\n        self.bn2 = nn.BatchNorm2d(channels)\n        self.relu = nn.ReLU()\n        self.se_module = SqueezeExciteV2(channels, rd_ratio=1.0 / reduction)\n        self.downsample = downsample\n        self.stride = stride\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        shortcut = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            shortcut = self.downsample(x)\n\n        out = self.se_module(out) + shortcut\n        out = self.relu(out)\n\n        return out\n</code></pre>"},{"location":"reference/models/#mindcv.models.senet.SEResNetBottleneck","title":"<code>mindcv.models.senet.SEResNetBottleneck</code>","text":"<p>               Bases: <code>Bottleneck</code></p> <p>Define the ResNet bottleneck with a Squeeze-and-Excitation module, and the latter is used in the torchvision implementation of ResNet.</p> Source code in <code>mindcv\\models\\senet.py</code> <pre><code>class SEResNetBottleneck(Bottleneck):\n    \"\"\"\n    Define the ResNet bottleneck with a Squeeze-and-Excitation module,\n    and the latter is used in the torchvision implementation of ResNet.\n    \"\"\"\n\n    expansion: int = 4\n\n    def __init__(\n        self,\n        in_channels: int,\n        channels: int,\n        group: int,\n        reduction: int,\n        stride: int = 1,\n        downsample: Optional[nn.SequentialCell] = None,\n    ) -&gt; None:\n        super(SEResNetBottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels, channels, kernel_size=1, pad_mode=\"pad\",\n                               padding=0, has_bias=False)\n        self.bn1 = nn.BatchNorm2d(channels)\n        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, stride=stride, pad_mode=\"pad\",\n                               padding=1, group=group, has_bias=False)\n        self.bn2 = nn.BatchNorm2d(channels)\n        self.conv3 = nn.Conv2d(channels, channels * 4, kernel_size=1, pad_mode=\"pad\", padding=0,\n                               has_bias=False)\n        self.bn3 = nn.BatchNorm2d(channels * 4)\n        self.relu = nn.ReLU()\n        self.se_module = SqueezeExciteV2(channels * 4, rd_ratio=1.0 / reduction)\n        self.downsample = downsample\n        self.stride = stride\n</code></pre>"},{"location":"reference/models/#shufflenetv1","title":"shufflenetv1","text":""},{"location":"reference/models/#mindcv.models.shufflenetv1","title":"<code>mindcv.models.shufflenetv1</code>","text":"<p>MindSpore implementation of <code>ShuffleNetV1</code>. Refer to ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices</p>"},{"location":"reference/models/#mindcv.models.shufflenetv1.GroupConv","title":"<code>mindcv.models.shufflenetv1.GroupConv</code>","text":"<p>               Bases: <code>Cell</code></p> <p>Group convolution operation. Due to MindSpore doesn't support group conv in shufflenet, we need to define the group convolution manually, instead of using the origin nn.Conv2d by changing the parameter <code>group</code>.</p> PARAMETER DESCRIPTION <code>in_channels</code> <p>Input channels of feature map.</p> <p> TYPE: <code>int</code> </p> <code>out_channels</code> <p>Output channels of feature map.</p> <p> TYPE: <code>int</code> </p> <code>kernel_size</code> <p>Size of convolution kernel.</p> <p> TYPE: <code>int</code> </p> <code>stride</code> <p>Stride size for the group convolution layer.</p> <p> TYPE: <code>int</code> </p> <code>pad_mode</code> <p>Specifies padding mode.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'pad'</code> </p> <code>pad</code> <p>The number of padding on the height and width directions of the input.</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>groups</code> <p>Splits filter into groups, <code>in_channels</code> and <code>out_channels</code> must be divisible by <code>group</code>.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>has_bias</code> <p>Whether the Conv2d layer has a bias parameter.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Source code in <code>mindcv\\models\\shufflenetv1.py</code> <pre><code>class GroupConv(nn.Cell):\n    \"\"\"\n    Group convolution operation.\n    Due to MindSpore doesn't support group conv in shufflenet, we need to define the group convolution manually, instead\n    of using the origin nn.Conv2d by changing the parameter `group`.\n\n    Args:\n        in_channels (int): Input channels of feature map.\n        out_channels (int): Output channels of feature map.\n        kernel_size (int): Size of convolution kernel.\n        stride (int): Stride size for the group convolution layer.\n        pad_mode (str): Specifies padding mode.\n        pad (int): The number of padding on the height and width directions of the input.\n        groups (int): Splits filter into groups, `in_channels` and `out_channels` must be divisible by `group`.\n        has_bias (bool): Whether the Conv2d layer has a bias parameter.\n    \"\"\"\n\n    def __init__(self, in_channels, out_channels, kernel_size, stride, pad_mode=\"pad\", pad=0, groups=1, has_bias=False):\n        super(GroupConv, self).__init__()\n        assert in_channels % groups == 0 and out_channels % groups == 0\n        self.groups = groups\n        self.convs = nn.CellList()\n        self.split = Split(split_size_or_sections=in_channels // groups, output_num=self.groups, axis=1)\n        for _ in range(groups):\n            self.convs.append(\n                nn.Conv2d(\n                    in_channels // groups,\n                    out_channels // groups,\n                    kernel_size=kernel_size,\n                    stride=stride,\n                    has_bias=has_bias,\n                    padding=pad,\n                    pad_mode=pad_mode,\n                )\n            )\n\n    def construct(self, x):\n        features = self.split(x)\n        outputs = ()\n        for i in range(self.groups):\n            outputs = outputs + (self.convs[i](features[i]),)\n        out = ops.concat(outputs, axis=1)\n        return out\n</code></pre>"},{"location":"reference/models/#mindcv.models.shufflenetv1.ShuffleNetV1","title":"<code>mindcv.models.shufflenetv1.ShuffleNetV1</code>","text":"<p>               Bases: <code>Cell</code></p> <p>ShuffleNetV1 model class, based on <code>\"ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices\" &lt;https://arxiv.org/abs/1707.01083&gt;</code>_  # noqa: E501</p> PARAMETER DESCRIPTION <code>num_classes</code> <p>number of classification classes. Default: 1000.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1000</code> </p> <code>in_channels</code> <p>number of input channels. Default: 3.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>model_size</code> <p>scale factor which controls the number of channels. Default: '2.0x'.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'2.0x'</code> </p> <code>group</code> <p>number of group for group convolution. Default: 3.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> Source code in <code>mindcv\\models\\shufflenetv1.py</code> <pre><code>class ShuffleNetV1(nn.Cell):\n    r\"\"\"ShuffleNetV1 model class, based on\n    `\"ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices\" &lt;https://arxiv.org/abs/1707.01083&gt;`_  # noqa: E501\n\n    Args:\n        num_classes: number of classification classes. Default: 1000.\n        in_channels: number of input channels. Default: 3.\n        model_size: scale factor which controls the number of channels. Default: '2.0x'.\n        group: number of group for group convolution. Default: 3.\n    \"\"\"\n\n    def __init__(\n        self,\n        num_classes: int = 1000,\n        in_channels: int = 3,\n        model_size: str = \"2.0x\",\n        group: int = 3,\n    ):\n        super().__init__()\n        self.stage_repeats = [4, 8, 4]\n        self.model_size = model_size\n        if group == 3:\n            if model_size == \"0.5x\":\n                self.stage_out_channels = [-1, 12, 120, 240, 480]\n            elif model_size == \"1.0x\":\n                self.stage_out_channels = [-1, 24, 240, 480, 960]\n            elif model_size == \"1.5x\":\n                self.stage_out_channels = [-1, 24, 360, 720, 1440]\n            elif model_size == \"2.0x\":\n                self.stage_out_channels = [-1, 48, 480, 960, 1920]\n            else:\n                raise NotImplementedError\n        elif group == 8:\n            if model_size == \"0.5x\":\n                self.stage_out_channels = [-1, 16, 192, 384, 768]\n            elif model_size == \"1.0x\":\n                self.stage_out_channels = [-1, 24, 384, 768, 1536]\n            elif model_size == \"1.5x\":\n                self.stage_out_channels = [-1, 24, 576, 1152, 2304]\n            elif model_size == \"2.0x\":\n                self.stage_out_channels = [-1, 48, 768, 1536, 3072]\n            else:\n                raise NotImplementedError\n\n        # building first layer\n        input_channel = self.stage_out_channels[1]\n        self.first_conv = nn.SequentialCell(\n            nn.Conv2d(in_channels, input_channel, kernel_size=3, stride=2, pad_mode=\"pad\", padding=1),\n            nn.BatchNorm2d(input_channel),\n            nn.ReLU(),\n        )\n        self.max_pool = nn.MaxPool2d(kernel_size=3, stride=2, pad_mode=\"same\")\n\n        features = []\n        for idxstage, numrepeat in enumerate(self.stage_repeats):\n            output_channel = self.stage_out_channels[idxstage + 2]\n            for i in range(numrepeat):\n                stride = 2 if i == 0 else 1\n                first_group = idxstage == 0 and i == 0\n                features.append(ShuffleV1Block(input_channel, output_channel,\n                                               group=group, first_group=first_group,\n                                               mid_channels=output_channel // 4, stride=stride))\n                input_channel = output_channel\n\n        self.features = nn.SequentialCell(features)\n        self.global_pool = GlobalAvgPooling()\n        self.classifier = nn.Dense(self.stage_out_channels[-1], num_classes, has_bias=False)\n        self._initialize_weights()\n\n    def _initialize_weights(self):\n        \"\"\"Initialize weights for cells.\"\"\"\n        for name, cell in self.cells_and_names():\n            if isinstance(cell, nn.Conv2d):\n                if \"first\" in name:\n                    cell.weight.set_data(\n                        init.initializer(init.Normal(0.01, 0), cell.weight.shape, cell.weight.dtype))\n                else:\n                    cell.weight.set_data(\n                        init.initializer(init.Normal(1.0 / cell.weight.shape[1], 0), cell.weight.shape,\n                                         cell.weight.dtype))\n                if cell.bias is not None:\n                    cell.bias.set_data(\n                        init.initializer(\"zeros\", cell.bias.shape, cell.bias.dtype))\n            elif isinstance(cell, nn.Dense):\n                cell.weight.set_data(\n                    init.initializer(init.Normal(0.01, 0), cell.weight.shape, cell.weight.dtype))\n                if cell.bias is not None:\n                    cell.bias.set_data(\n                        init.initializer(\"zeros\", cell.bias.shape, cell.bias.dtype))\n\n    def forward_features(self, x: Tensor) -&gt; Tensor:\n        x = self.first_conv(x)\n        x = self.max_pool(x)\n        x = self.features(x)\n        return x\n\n    def forward_head(self, x: Tensor) -&gt; Tensor:\n        x = self.global_pool(x)\n        x = self.classifier(x)\n        return x\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n</code></pre>"},{"location":"reference/models/#mindcv.models.shufflenetv1.ShuffleV1Block","title":"<code>mindcv.models.shufflenetv1.ShuffleV1Block</code>","text":"<p>               Bases: <code>Cell</code></p> <p>Basic block of ShuffleNetV1. 1x1 GC -&gt; CS -&gt; 3x3 DWC -&gt; 1x1 GC</p> Source code in <code>mindcv\\models\\shufflenetv1.py</code> <pre><code>class ShuffleV1Block(nn.Cell):\n    \"\"\"Basic block of ShuffleNetV1. 1x1 GC -&gt; CS -&gt; 3x3 DWC -&gt; 1x1 GC\"\"\"\n\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        mid_channels: int,\n        stride: int,\n        group: int,\n        first_group: bool,\n    ) -&gt; None:\n        super().__init__()\n        assert stride in [1, 2]\n        self.stride = stride\n        self.group = group\n\n        if stride == 2:\n            out_channels = out_channels - in_channels\n\n        branch_main_1 = [\n            # pw\n            GroupConv(\n                in_channels=in_channels,\n                out_channels=mid_channels,\n                kernel_size=1,\n                stride=1,\n                pad_mode=\"pad\",\n                pad=0,\n                groups=1 if first_group else group,\n            ),\n            nn.BatchNorm2d(mid_channels),\n            nn.ReLU(),\n        ]\n\n        branch_main_2 = [\n            # dw\n            nn.Conv2d(mid_channels, mid_channels, kernel_size=3, stride=stride, pad_mode=\"pad\", padding=1,\n                      group=mid_channels),\n            nn.BatchNorm2d(mid_channels),\n            # pw-linear\n            GroupConv(\n                in_channels=mid_channels,\n                out_channels=out_channels,\n                kernel_size=1,\n                stride=1,\n                pad_mode=\"pad\",\n                pad=0,\n                groups=group,\n            ),\n            nn.BatchNorm2d(out_channels),\n        ]\n        self.branch_main_1 = nn.SequentialCell(branch_main_1)\n        self.branch_main_2 = nn.SequentialCell(branch_main_2)\n        if stride == 2:\n            self.branch_proj = nn.AvgPool2d(kernel_size=3, stride=2, pad_mode=\"same\")\n\n        self.relu = nn.ReLU()\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        identify = x\n        x = self.branch_main_1(x)\n        if self.group &gt; 1:\n            x = self.channel_shuffle(x)\n        x = self.branch_main_2(x)\n        if self.stride == 1:\n            out = self.relu(identify + x)\n        else:\n            out = self.relu(ops.concat((self.branch_proj(identify), x), axis=1))\n\n        return out\n\n    def channel_shuffle(self, x: Tensor) -&gt; Tensor:\n        batch_size, num_channels, height, width = x.shape\n\n        group_channels = num_channels // self.group\n        x = ops.reshape(x, (batch_size, group_channels, self.group, height, width))\n        x = ops.transpose(x, (0, 2, 1, 3, 4))\n        x = ops.reshape(x, (batch_size, num_channels, height, width))\n        return x\n</code></pre>"},{"location":"reference/models/#mindcv.models.shufflenetv1.shufflenet_v1_g3_05","title":"<code>mindcv.models.shufflenetv1.shufflenet_v1_g3_05(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get ShuffleNetV1 model with width scaled by 0.5 and 3 groups of GPConv. Refer to the base class <code>models.ShuffleNetV1</code> for more details.</p> Source code in <code>mindcv\\models\\shufflenetv1.py</code> <pre><code>@register_model\ndef shufflenet_v1_g3_05(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; ShuffleNetV1:\n    \"\"\"Get ShuffleNetV1 model with width scaled by 0.5 and 3 groups of GPConv.\n    Refer to the base class `models.ShuffleNetV1` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"shufflenet_v1_g3_05\"]\n    model = ShuffleNetV1(group=3, model_size=\"0.5x\", num_classes=num_classes, in_channels=in_channels, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre>"},{"location":"reference/models/#mindcv.models.shufflenetv1.shufflenet_v1_g3_10","title":"<code>mindcv.models.shufflenetv1.shufflenet_v1_g3_10(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get ShuffleNetV1 model with width scaled by 1.0 and 3 groups of GPConv. Refer to the base class <code>models.ShuffleNetV1</code> for more details.</p> Source code in <code>mindcv\\models\\shufflenetv1.py</code> <pre><code>@register_model\ndef shufflenet_v1_g3_10(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; ShuffleNetV1:\n    \"\"\"Get ShuffleNetV1 model with width scaled by 1.0 and 3 groups of GPConv.\n    Refer to the base class `models.ShuffleNetV1` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"shufflenet_v1_g3_10\"]\n    model = ShuffleNetV1(group=3, model_size=\"1.0x\", num_classes=num_classes, in_channels=in_channels, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre>"},{"location":"reference/models/#mindcv.models.shufflenetv1.shufflenet_v1_g3_15","title":"<code>mindcv.models.shufflenetv1.shufflenet_v1_g3_15(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get ShuffleNetV1 model with width scaled by 1.5 and 3 groups of GPConv. Refer to the base class <code>models.ShuffleNetV1</code> for more details.</p> Source code in <code>mindcv\\models\\shufflenetv1.py</code> <pre><code>@register_model\ndef shufflenet_v1_g3_15(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; ShuffleNetV1:\n    \"\"\"Get ShuffleNetV1 model with width scaled by 1.5 and 3 groups of GPConv.\n    Refer to the base class `models.ShuffleNetV1` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"shufflenet_v1_g3_15\"]\n    model = ShuffleNetV1(group=3, model_size=\"1.5x\", num_classes=num_classes, in_channels=in_channels, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre>"},{"location":"reference/models/#mindcv.models.shufflenetv1.shufflenet_v1_g3_20","title":"<code>mindcv.models.shufflenetv1.shufflenet_v1_g3_20(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get ShuffleNetV1 model with width scaled by 2.0 and 3 groups of GPConv. Refer to the base class <code>models.ShuffleNetV1</code> for more details.</p> Source code in <code>mindcv\\models\\shufflenetv1.py</code> <pre><code>@register_model\ndef shufflenet_v1_g3_20(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; ShuffleNetV1:\n    \"\"\"Get ShuffleNetV1 model with width scaled by 2.0 and 3 groups of GPConv.\n    Refer to the base class `models.ShuffleNetV1` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"shufflenet_v1_g3_20\"]\n    model = ShuffleNetV1(group=3, model_size=\"2.0x\", num_classes=num_classes, in_channels=in_channels, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre>"},{"location":"reference/models/#mindcv.models.shufflenetv1.shufflenet_v1_g8_05","title":"<code>mindcv.models.shufflenetv1.shufflenet_v1_g8_05(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get ShuffleNetV1 model with width scaled by 0.5 and 8 groups of GPConv. Refer to the base class <code>models.ShuffleNetV1</code> for more details.</p> Source code in <code>mindcv\\models\\shufflenetv1.py</code> <pre><code>@register_model\ndef shufflenet_v1_g8_05(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; ShuffleNetV1:\n    \"\"\"Get ShuffleNetV1 model with width scaled by 0.5 and 8 groups of GPConv.\n    Refer to the base class `models.ShuffleNetV1` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"shufflenet_v1_g8_05\"]\n    model = ShuffleNetV1(group=8, model_size=\"0.5x\", num_classes=num_classes, in_channels=in_channels, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre>"},{"location":"reference/models/#mindcv.models.shufflenetv1.shufflenet_v1_g8_10","title":"<code>mindcv.models.shufflenetv1.shufflenet_v1_g8_10(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get ShuffleNetV1 model with width scaled by 1.0 and 8 groups of GPConv. Refer to the base class <code>models.ShuffleNetV1</code> for more details.</p> Source code in <code>mindcv\\models\\shufflenetv1.py</code> <pre><code>@register_model\ndef shufflenet_v1_g8_10(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; ShuffleNetV1:\n    \"\"\"Get ShuffleNetV1 model with width scaled by 1.0 and 8 groups of GPConv.\n    Refer to the base class `models.ShuffleNetV1` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"shufflenet_v1_g8_10\"]\n    model = ShuffleNetV1(group=8, model_size=\"1.0x\", num_classes=num_classes, in_channels=in_channels, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre>"},{"location":"reference/models/#mindcv.models.shufflenetv1.shufflenet_v1_g8_15","title":"<code>mindcv.models.shufflenetv1.shufflenet_v1_g8_15(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get ShuffleNetV1 model with width scaled by 1.5 and 8 groups of GPConv. Refer to the base class <code>models.ShuffleNetV1</code> for more details.</p> Source code in <code>mindcv\\models\\shufflenetv1.py</code> <pre><code>@register_model\ndef shufflenet_v1_g8_15(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; ShuffleNetV1:\n    \"\"\"Get ShuffleNetV1 model with width scaled by 1.5 and 8 groups of GPConv.\n    Refer to the base class `models.ShuffleNetV1` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"shufflenet_v1_g8_15\"]\n    model = ShuffleNetV1(group=8, model_size=\"1.5x\", num_classes=num_classes, in_channels=in_channels, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre>"},{"location":"reference/models/#mindcv.models.shufflenetv1.shufflenet_v1_g8_20","title":"<code>mindcv.models.shufflenetv1.shufflenet_v1_g8_20(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get ShuffleNetV1 model with width scaled by 2.0 and 8 groups of GPConv. Refer to the base class <code>models.ShuffleNetV1</code> for more details.</p> Source code in <code>mindcv\\models\\shufflenetv1.py</code> <pre><code>@register_model\ndef shufflenet_v1_g8_20(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; ShuffleNetV1:\n    \"\"\"Get ShuffleNetV1 model with width scaled by 2.0 and 8 groups of GPConv.\n    Refer to the base class `models.ShuffleNetV1` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"shufflenet_v1_g8_20\"]\n    model = ShuffleNetV1(group=8, model_size=\"2.0x\", num_classes=num_classes, in_channels=in_channels, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre>"},{"location":"reference/models/#shufflenetv2","title":"shufflenetv2","text":""},{"location":"reference/models/#mindcv.models.shufflenetv2","title":"<code>mindcv.models.shufflenetv2</code>","text":"<p>MindSpore implementation of <code>ShuffleNetV2</code>. Refer to ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design</p>"},{"location":"reference/models/#mindcv.models.shufflenetv2.ShuffleNetV2","title":"<code>mindcv.models.shufflenetv2.ShuffleNetV2</code>","text":"<p>               Bases: <code>Cell</code></p> <p>ShuffleNetV2 model class, based on <code>\"ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design\" &lt;https://arxiv.org/abs/1807.11164&gt;</code>_</p> PARAMETER DESCRIPTION <code>num_classes</code> <p>number of classification classes. Default: 1000.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1000</code> </p> <code>in_channels</code> <p>number of input channels. Default: 3.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>model_size</code> <p>scale factor which controls the number of channels. Default: '1.5x'.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'1.5x'</code> </p> Source code in <code>mindcv\\models\\shufflenetv2.py</code> <pre><code>class ShuffleNetV2(nn.Cell):\n    r\"\"\"ShuffleNetV2 model class, based on\n    `\"ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design\" &lt;https://arxiv.org/abs/1807.11164&gt;`_\n\n    Args:\n        num_classes: number of classification classes. Default: 1000.\n        in_channels: number of input channels. Default: 3.\n        model_size: scale factor which controls the number of channels. Default: '1.5x'.\n    \"\"\"\n\n    def __init__(\n        self,\n        num_classes: int = 1000,\n        in_channels: int = 3,\n        model_size: str = \"1.5x\",\n    ):\n        super().__init__()\n\n        self.stage_repeats = [4, 8, 4]\n        self.model_size = model_size\n        if model_size == \"0.5x\":\n            self.stage_out_channels = [-1, 24, 48, 96, 192, 1024]\n        elif model_size == \"1.0x\":\n            self.stage_out_channels = [-1, 24, 116, 232, 464, 1024]\n        elif model_size == \"1.5x\":\n            self.stage_out_channels = [-1, 24, 176, 352, 704, 1024]\n        elif model_size == \"2.0x\":\n            self.stage_out_channels = [-1, 24, 244, 488, 976, 2048]\n        else:\n            raise NotImplementedError\n\n        # building first layer\n        input_channel = self.stage_out_channels[1]\n        self.first_conv = nn.SequentialCell([\n            nn.Conv2d(in_channels, input_channel, kernel_size=3, stride=2,\n                      pad_mode=\"pad\", padding=1),\n            nn.BatchNorm2d(input_channel),\n            nn.ReLU(),\n        ])\n        self.max_pool = nn.MaxPool2d(kernel_size=3, stride=2, pad_mode=\"same\")\n\n        self.features = []\n        for idxstage, numrepeat in enumerate(self.stage_repeats):\n            output_channel = self.stage_out_channels[idxstage + 2]\n            for i in range(numrepeat):\n                if i == 0:\n                    self.features.append(ShuffleV2Block(input_channel, output_channel,\n                                                        mid_channels=output_channel // 2, kernel_size=3, stride=2))\n                else:\n                    self.features.append(ShuffleV2Block(input_channel // 2, output_channel,\n                                                        mid_channels=output_channel // 2, kernel_size=3, stride=1))\n                input_channel = output_channel\n\n        self.features = nn.SequentialCell(self.features)\n\n        self.conv_last = nn.SequentialCell([\n            nn.Conv2d(input_channel, self.stage_out_channels[-1], kernel_size=1, stride=1),\n            nn.BatchNorm2d(self.stage_out_channels[-1]),\n            nn.ReLU()\n        ])\n        self.pool = GlobalAvgPooling()\n        self.classifier = nn.Dense(self.stage_out_channels[-1], num_classes, has_bias=False)\n        self._initialize_weights()\n\n    def _initialize_weights(self):\n        \"\"\"Initialize weights for cells.\"\"\"\n        for name, cell in self.cells_and_names():\n            if isinstance(cell, nn.Conv2d):\n                if \"first\" in name:\n                    cell.weight.set_data(\n                        init.initializer(init.Normal(0.01, 0), cell.weight.shape, cell.weight.dtype))\n                else:\n                    cell.weight.set_data(\n                        init.initializer(init.Normal(1.0 / cell.weight.shape[1], 0), cell.weight.shape,\n                                         cell.weight.dtype))\n                if cell.bias is not None:\n                    cell.bias.set_data(\n                        init.initializer(\"zeros\", cell.bias.shape, cell.bias.dtype))\n            elif isinstance(cell, nn.Dense):\n                cell.weight.set_data(\n                    init.initializer(init.Normal(0.01, 0), cell.weight.shape, cell.weight.dtype))\n                if cell.bias is not None:\n                    cell.bias.set_data(\n                        init.initializer(\"zeros\", cell.bias.shape, cell.bias.dtype))\n\n    def forward_features(self, x: Tensor) -&gt; Tensor:\n        x = self.first_conv(x)\n        x = self.max_pool(x)\n        x = self.features(x)\n        return x\n\n    def forward_head(self, x: Tensor) -&gt; Tensor:\n        x = self.conv_last(x)\n        x = self.pool(x)\n        x = self.classifier(x)\n        return x\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n</code></pre>"},{"location":"reference/models/#mindcv.models.shufflenetv2.ShuffleV2Block","title":"<code>mindcv.models.shufflenetv2.ShuffleV2Block</code>","text":"<p>               Bases: <code>Cell</code></p> <p>define the basic block of ShuffleV2</p> Source code in <code>mindcv\\models\\shufflenetv2.py</code> <pre><code>class ShuffleV2Block(nn.Cell):\n    \"\"\"define the basic block of ShuffleV2\"\"\"\n\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        mid_channels: int,\n        kernel_size: int,\n        stride: int,\n    ) -&gt; None:\n        super().__init__()\n        assert stride in [1, 2]\n        self.stride = stride\n        pad = kernel_size // 2\n        out_channels = out_channels - in_channels\n        branch_main = [\n            # pw\n            nn.Conv2d(in_channels, mid_channels, kernel_size=1, stride=1),\n            nn.BatchNorm2d(mid_channels),\n            nn.ReLU(),\n            # dw\n            nn.Conv2d(mid_channels, mid_channels, kernel_size=kernel_size, stride=stride,\n                      pad_mode=\"pad\", padding=pad, group=mid_channels),\n            nn.BatchNorm2d(mid_channels),\n            # pw-linear\n            nn.Conv2d(mid_channels, out_channels, kernel_size=1, stride=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(),\n        ]\n        self.branch_main = nn.SequentialCell(branch_main)\n\n        if stride == 2:\n            branch_proj = [\n                # dw\n                nn.Conv2d(in_channels, in_channels, kernel_size=kernel_size, stride=stride,\n                          pad_mode=\"pad\", padding=pad, group=in_channels),\n                nn.BatchNorm2d(in_channels),\n                # pw-linear\n                nn.Conv2d(in_channels, in_channels, kernel_size=1, stride=1),\n                nn.BatchNorm2d(in_channels),\n                nn.ReLU(),\n            ]\n            self.branch_proj = nn.SequentialCell(branch_proj)\n        else:\n            self.branch_proj = None\n\n    def construct(self, old_x: Tensor) -&gt; Tensor:\n        if self.stride == 1:\n            x_proj, x = self.channel_shuffle(old_x)\n            return ops.concat((x_proj, self.branch_main(x)), axis=1)\n\n        if self.stride == 2:\n            x_proj = old_x\n            x = old_x\n            return ops.concat((self.branch_proj(x_proj), self.branch_main(x)), axis=1)\n        return None\n\n    @staticmethod\n    def channel_shuffle(x: Tensor) -&gt; Tuple[Tensor, Tensor]:\n        batch_size, num_channels, height, width = x.shape\n        x = ops.reshape(x, (batch_size * num_channels // 2, 2, height * width,))\n        x = ops.transpose(x, (1, 0, 2,))\n        x = ops.reshape(x, (2, -1, num_channels // 2, height, width,))\n        return x[0], x[1]\n</code></pre>"},{"location":"reference/models/#mindcv.models.shufflenetv2.shufflenet_v2_x0_5","title":"<code>mindcv.models.shufflenetv2.shufflenet_v2_x0_5(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get ShuffleNetV2 model with width scaled by 0.5. Refer to the base class <code>models.ShuffleNetV2</code> for more details.</p> Source code in <code>mindcv\\models\\shufflenetv2.py</code> <pre><code>@register_model\ndef shufflenet_v2_x0_5(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; ShuffleNetV2:\n    \"\"\"Get ShuffleNetV2 model with width scaled by 0.5.\n    Refer to the base class `models.ShuffleNetV2` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"shufflenet_v2_x0_5\"]\n    model = ShuffleNetV2(model_size=\"0.5x\", num_classes=num_classes, in_channels=in_channels, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre>"},{"location":"reference/models/#mindcv.models.shufflenetv2.shufflenet_v2_x1_0","title":"<code>mindcv.models.shufflenetv2.shufflenet_v2_x1_0(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get ShuffleNetV2 model with width scaled by 1.0. Refer to the base class <code>models.ShuffleNetV2</code> for more details.</p> Source code in <code>mindcv\\models\\shufflenetv2.py</code> <pre><code>@register_model\ndef shufflenet_v2_x1_0(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; ShuffleNetV2:\n    \"\"\"Get ShuffleNetV2 model with width scaled by 1.0.\n    Refer to the base class `models.ShuffleNetV2` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"shufflenet_v2_x1_0\"]\n    model = ShuffleNetV2(model_size=\"1.0x\", num_classes=num_classes, in_channels=in_channels, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre>"},{"location":"reference/models/#mindcv.models.shufflenetv2.shufflenet_v2_x1_5","title":"<code>mindcv.models.shufflenetv2.shufflenet_v2_x1_5(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get ShuffleNetV2 model with width scaled by 1.5. Refer to the base class <code>models.ShuffleNetV2</code> for more details.</p> Source code in <code>mindcv\\models\\shufflenetv2.py</code> <pre><code>@register_model\ndef shufflenet_v2_x1_5(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; ShuffleNetV2:\n    \"\"\"Get ShuffleNetV2 model with width scaled by 1.5.\n    Refer to the base class `models.ShuffleNetV2` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"shufflenet_v2_x1_5\"]\n    model = ShuffleNetV2(model_size=\"1.5x\", num_classes=num_classes, in_channels=in_channels, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre>"},{"location":"reference/models/#mindcv.models.shufflenetv2.shufflenet_v2_x2_0","title":"<code>mindcv.models.shufflenetv2.shufflenet_v2_x2_0(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get ShuffleNetV2 model with width scaled by 2.0. Refer to the base class <code>models.ShuffleNetV2</code> for more details.</p> Source code in <code>mindcv\\models\\shufflenetv2.py</code> <pre><code>@register_model\ndef shufflenet_v2_x2_0(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; ShuffleNetV2:\n    \"\"\"Get ShuffleNetV2 model with width scaled by 2.0.\n    Refer to the base class `models.ShuffleNetV2` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"shufflenet_v2_x2_0\"]\n    model = ShuffleNetV2(model_size=\"2.0x\", num_classes=num_classes, in_channels=in_channels, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre>"},{"location":"reference/models/#sknet","title":"sknet","text":""},{"location":"reference/models/#mindcv.models.sknet","title":"<code>mindcv.models.sknet</code>","text":"<p>MindSpore implementation of <code>SKNet</code>. Refer to Selective Kernel Networks.</p>"},{"location":"reference/models/#mindcv.models.sknet.SKNet","title":"<code>mindcv.models.sknet.SKNet</code>","text":"<p>               Bases: <code>ResNet</code></p> <p>SKNet model class, based on <code>\"Selective Kernel Networks\" &lt;https://arxiv.org/abs/1903.06586&gt;</code>_</p> PARAMETER DESCRIPTION <code>block</code> <p>block of sknet.</p> <p> TYPE: <code>Type[Cell]</code> </p> <code>layers</code> <p>number of layers of each stage.</p> <p> TYPE: <code>List[int]</code> </p> <code>num_classes</code> <p>number of classification classes. Default: 1000.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1000</code> </p> <code>in_channels</code> <p>number the channels of the input. Default: 3.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>groups</code> <p>number of groups for group conv in blocks. Default: 1.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>base_width</code> <p>base width of pre group hidden channel in blocks. Default: 64.</p> <p> TYPE: <code>int</code> DEFAULT: <code>64</code> </p> <code>norm</code> <p>normalization layer in blocks. Default: None.</p> <p> TYPE: <code>Optional[Cell]</code> DEFAULT: <code>None</code> </p> <code>sk_kwargs</code> <p>kwargs of selective kernel. Default: None.</p> <p> TYPE: <code>Optional[Dict]</code> DEFAULT: <code>None</code> </p> Source code in <code>mindcv\\models\\sknet.py</code> <pre><code>class SKNet(ResNet):\n    r\"\"\"SKNet model class, based on\n    `\"Selective Kernel Networks\" &lt;https://arxiv.org/abs/1903.06586&gt;`_\n\n    Args:\n        block: block of sknet.\n        layers: number of layers of each stage.\n        num_classes: number of classification classes. Default: 1000.\n        in_channels: number the channels of the input. Default: 3.\n        groups: number of groups for group conv in blocks. Default: 1.\n        base_width: base width of pre group hidden channel in blocks. Default: 64.\n        norm: normalization layer in blocks. Default: None.\n        sk_kwargs: kwargs of selective kernel. Default: None.\n    \"\"\"\n\n    def __init__(\n        self,\n        block: Type[nn.Cell],\n        layers: List[int],\n        num_classes: int = 1000,\n        in_channels: int = 3,\n        groups: int = 1,\n        base_width: int = 64,\n        norm: Optional[nn.Cell] = None,\n        sk_kwargs: Optional[Dict] = None,\n    ) -&gt; None:\n        self.sk_kwargs: Optional[Dict] = sk_kwargs  # make pylint happy\n        super().__init__(block, layers, num_classes, in_channels, groups, base_width, norm)\n\n    def _make_layer(\n        self,\n        block: Type[Union[SelectiveKernelBasic, SelectiveKernelBottleneck]],\n        channels: int,\n        block_nums: int,\n        stride: int = 1,\n    ) -&gt; nn.SequentialCell:\n        down_sample = None\n\n        if stride != 1 or self.input_channels != channels * block.expansion:\n            down_sample = nn.SequentialCell([\n                nn.Conv2d(self.input_channels, channels * block.expansion, kernel_size=1, stride=stride),\n                self.norm(channels * block.expansion)\n            ])\n\n        layers = []\n        layers.append(\n            block(\n                self.input_channels,\n                channels,\n                stride=stride,\n                down_sample=down_sample,\n                groups=self.groups,\n                base_width=self.base_with,\n                norm=self.norm,\n                sk_kwargs=self.sk_kwargs,\n            )\n        )\n        self.input_channels = channels * block.expansion\n\n        for _ in range(1, block_nums):\n            layers.append(\n                block(\n                    self.input_channels,\n                    channels,\n                    groups=self.groups,\n                    base_width=self.base_with,\n                    norm=self.norm,\n                    sk_kwargs=self.sk_kwargs,\n                )\n            )\n\n        return nn.SequentialCell(layers)\n</code></pre>"},{"location":"reference/models/#mindcv.models.sknet.SelectiveKernelBasic","title":"<code>mindcv.models.sknet.SelectiveKernelBasic</code>","text":"<p>               Bases: <code>Cell</code></p> <p>build basic block of sknet</p> Source code in <code>mindcv\\models\\sknet.py</code> <pre><code>class SelectiveKernelBasic(nn.Cell):\n    \"\"\"build basic block of sknet\"\"\"\n\n    expansion = 1\n\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        stride: int = 1,\n        groups: int = 1,\n        down_sample: Optional[nn.Cell] = None,\n        base_width: int = 64,\n        norm: Optional[nn.Cell] = None,\n        sk_kwargs: Optional[Dict] = None,\n    ):\n        super().__init__()\n        if norm is None:\n            norm = nn.BatchNorm2d\n\n        if sk_kwargs is None:\n            sk_kwargs = {}\n\n        assert groups == 1, \"BasicBlock only supports cardinality of 1\"\n        assert base_width == 64, \"BasicBlock doest not support changing base width\"\n\n        self.conv1 = SelectiveKernel(\n            in_channels, out_channels, stride=stride, **sk_kwargs)\n        self.conv2 = nn.SequentialCell([\n            nn.Conv2d(out_channels, out_channels * self.expansion, kernel_size=3, padding=1, pad_mode=\"pad\"),\n            norm(out_channels * self.expansion)\n        ])\n\n        self.relu = nn.ReLU()\n        self.down_sample = down_sample\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        identity = x\n\n        out = self.conv1(x)\n        out = self.conv2(out)\n\n        if self.down_sample is not None:\n            identity = self.down_sample(x)\n        out += identity\n        out = self.relu(out)\n        return out\n</code></pre>"},{"location":"reference/models/#mindcv.models.sknet.SelectiveKernelBottleneck","title":"<code>mindcv.models.sknet.SelectiveKernelBottleneck</code>","text":"<p>               Bases: <code>Cell</code></p> <p>build the bottleneck of the sknet</p> Source code in <code>mindcv\\models\\sknet.py</code> <pre><code>class SelectiveKernelBottleneck(nn.Cell):\n    \"\"\"build the bottleneck of the sknet\"\"\"\n\n    expansion = 4\n\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        stride: int = 1,\n        down_sample: Optional[nn.Cell] = None,\n        groups: int = 1,\n        base_width: int = 64,\n        norm: Optional[nn.Cell] = None,\n        sk_kwargs: Optional[Dict] = None,\n    ):\n        super().__init__()\n        if norm is None:\n            norm = nn.BatchNorm2d\n\n        if sk_kwargs is None:\n            sk_kwargs = {}\n\n        width = int(out_channels * (base_width / 64.0)) * groups\n        self.conv1 = nn.SequentialCell([\n            nn.Conv2d(in_channels, width, kernel_size=1),\n            norm(width)\n        ])\n        self.conv2 = SelectiveKernel(\n            width, width, stride=stride, groups=groups, **sk_kwargs)\n        self.conv3 = nn.SequentialCell([\n            nn.Conv2d(width, out_channels * self.expansion, kernel_size=1),\n            norm(out_channels * self.expansion)\n        ])\n\n        self.relu = nn.ReLU()\n        self.down_sample = down_sample\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        identity = x\n\n        out = self.conv1(x)\n        out = self.conv2(out)\n        out = self.conv3(out)\n\n        if self.down_sample:\n            identity = self.down_sample(x)\n        out += identity\n        out = self.relu(out)\n        return out\n</code></pre>"},{"location":"reference/models/#mindcv.models.sknet.skresnet18","title":"<code>mindcv.models.sknet.skresnet18(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get 18 layers SKNet model. Refer to the base class <code>models.SKNet</code> for more details.</p> Source code in <code>mindcv\\models\\sknet.py</code> <pre><code>@register_model\ndef skresnet18(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; ResNet:\n    \"\"\"Get 18 layers SKNet model.\n    Refer to the base class `models.SKNet` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"skresnet18\"]\n    sk_kwargs = dict(rd_ratio=1 / 8, rd_divisor=16, split_input=True)\n    model = SKNet(SelectiveKernelBasic, [2, 2, 2, 2], num_classes=num_classes, in_channels=in_channels,\n                  sk_kwargs=sk_kwargs, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre>"},{"location":"reference/models/#mindcv.models.sknet.skresnet34","title":"<code>mindcv.models.sknet.skresnet34(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get 34 layers SKNet model. Refer to the base class <code>models.SKNet</code> for more details.</p> Source code in <code>mindcv\\models\\sknet.py</code> <pre><code>@register_model\ndef skresnet34(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; ResNet:\n    \"\"\"Get 34 layers SKNet model.\n    Refer to the base class `models.SKNet` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"skresnet34\"]\n    sk_kwargs = dict(rd_ratio=1 / 8, rd_divisor=16, split_input=True)\n    model = SKNet(SelectiveKernelBasic, [3, 4, 6, 3], num_classes=num_classes, in_channels=in_channels,\n                  sk_kwargs=sk_kwargs, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre>"},{"location":"reference/models/#mindcv.models.sknet.skresnet50","title":"<code>mindcv.models.sknet.skresnet50(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get 50 layers SKNet model. Refer to the base class <code>models.SKNet</code> for more details.</p> Source code in <code>mindcv\\models\\sknet.py</code> <pre><code>@register_model\ndef skresnet50(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; ResNet:\n    \"\"\"Get 50 layers SKNet model.\n    Refer to the base class `models.SKNet` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"skresnet50\"]\n    sk_kwargs = dict(split_input=True)\n    model = SKNet(SelectiveKernelBottleneck, [3, 4, 6, 3], num_classes=num_classes, in_channels=in_channels,\n                  sk_kwargs=sk_kwargs, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre>"},{"location":"reference/models/#mindcv.models.sknet.skresnext50_32x4d","title":"<code>mindcv.models.sknet.skresnext50_32x4d(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get 50 layers SKNeXt model with 32 groups of GPConv. Refer to the base class <code>models.SKNet</code> for more details.</p> Source code in <code>mindcv\\models\\sknet.py</code> <pre><code>@register_model\ndef skresnext50_32x4d(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; ResNet:\n    \"\"\"Get 50 layers SKNeXt model with 32 groups of GPConv.\n    Refer to the base class `models.SKNet` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"skresnext50_32x4d\"]\n    sk_kwargs = dict(rd_ratio=1 / 16, rd_divisor=32, split_input=False)\n    model = SKNet(SelectiveKernelBottleneck, [3, 4, 6, 3], num_classes=num_classes, in_channels=in_channels,\n                  sk_kwargs=sk_kwargs, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre>"},{"location":"reference/models/#squeezenet","title":"squeezenet","text":""},{"location":"reference/models/#mindcv.models.squeezenet","title":"<code>mindcv.models.squeezenet</code>","text":"<p>MindSpore implementation of <code>SqueezeNet</code>. Refer to SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and &lt;0.5MB model size.</p>"},{"location":"reference/models/#mindcv.models.squeezenet.Fire","title":"<code>mindcv.models.squeezenet.Fire</code>","text":"<p>               Bases: <code>Cell</code></p> <p>define the basic block of squeezenet</p> Source code in <code>mindcv\\models\\squeezenet.py</code> <pre><code>class Fire(nn.Cell):\n    \"\"\"define the basic block of squeezenet\"\"\"\n\n    def __init__(\n        self,\n        in_channels: int,\n        squeeze_channels: int,\n        expand1x1_channels: int,\n        expand3x3_channels: int,\n    ) -&gt; None:\n        super().__init__()\n        self.squeeze = nn.Conv2d(in_channels, squeeze_channels, kernel_size=1, has_bias=True)\n        self.squeeze_activation = nn.ReLU()\n        self.expand1x1 = nn.Conv2d(squeeze_channels, expand1x1_channels, kernel_size=1, has_bias=True)\n        self.expand1x1_activation = nn.ReLU()\n        self.expand3x3 = nn.Conv2d(squeeze_channels, expand3x3_channels, kernel_size=3, pad_mode=\"same\", has_bias=True)\n        self.expand3x3_activation = nn.ReLU()\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        x = self.squeeze_activation(self.squeeze(x))\n        return ops.concat((self.expand1x1_activation(self.expand1x1(x)),\n                           self.expand3x3_activation(self.expand3x3(x))), axis=1)\n</code></pre>"},{"location":"reference/models/#mindcv.models.squeezenet.SqueezeNet","title":"<code>mindcv.models.squeezenet.SqueezeNet</code>","text":"<p>               Bases: <code>Cell</code></p> <p>SqueezeNet model class, based on <code>\"SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and &lt;0.5MB model size\" &lt;https://arxiv.org/abs/1602.07360&gt;</code>_  # noqa: E501</p> <p>.. note::     Important: In contrast to the other models the inception_v3 expects tensors with a size of     N x 3 x 227 x 227, so ensure your images are sized accordingly.</p> PARAMETER DESCRIPTION <code>version</code> <p>version of the architecture, '1_0' or '1_1'. Default: '1_0'.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'1_0'</code> </p> <code>num_classes</code> <p>number of classification classes. Default: 1000.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1000</code> </p> <code>drop_rate</code> <p>dropout rate of the classifier. Default: 0.5.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.5</code> </p> <code>in_channels</code> <p>number the channels of the input. Default: 3.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> Source code in <code>mindcv\\models\\squeezenet.py</code> <pre><code>class SqueezeNet(nn.Cell):\n    r\"\"\"SqueezeNet model class, based on\n    `\"SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and &lt;0.5MB model size\" &lt;https://arxiv.org/abs/1602.07360&gt;`_  # noqa: E501\n\n    .. note::\n        **Important**: In contrast to the other models the inception_v3 expects tensors with a size of\n        N x 3 x 227 x 227, so ensure your images are sized accordingly.\n\n    Args:\n        version: version of the architecture, '1_0' or '1_1'. Default: '1_0'.\n        num_classes: number of classification classes. Default: 1000.\n        drop_rate: dropout rate of the classifier. Default: 0.5.\n        in_channels: number the channels of the input. Default: 3.\n    \"\"\"\n\n    def __init__(\n        self,\n        version: str = \"1_0\",\n        num_classes: int = 1000,\n        drop_rate: float = 0.5,\n        in_channels: int = 3,\n    ) -&gt; None:\n        super().__init__()\n        if version == \"1_0\":\n            self.features = nn.SequentialCell([\n                nn.Conv2d(in_channels, 96, kernel_size=7, stride=2, pad_mode=\"valid\", has_bias=True),\n                nn.ReLU(),\n                nn.MaxPool2d(kernel_size=3, stride=2),\n                Fire(96, 16, 64, 64),\n                Fire(128, 16, 64, 64),\n                Fire(128, 32, 128, 128),\n                nn.MaxPool2d(kernel_size=3, stride=2),\n                Fire(256, 32, 128, 128),\n                Fire(256, 48, 192, 192),\n                Fire(384, 48, 192, 192),\n                Fire(384, 64, 256, 256),\n                nn.MaxPool2d(kernel_size=3, stride=2),\n                Fire(512, 64, 256, 256),\n            ])\n        elif version == \"1_1\":\n            self.features = nn.SequentialCell([\n                nn.Conv2d(in_channels, 64, kernel_size=3, stride=2, padding=1, pad_mode=\"pad\", has_bias=True),\n                nn.ReLU(),\n                nn.MaxPool2d(kernel_size=3, stride=2),\n                Fire(64, 16, 64, 64),\n                Fire(128, 16, 64, 64),\n                nn.MaxPool2d(kernel_size=3, stride=2),\n                Fire(128, 32, 128, 128),\n                Fire(256, 32, 128, 128),\n                nn.MaxPool2d(kernel_size=3, stride=2),\n                Fire(256, 48, 192, 192),\n                Fire(384, 48, 192, 192),\n                Fire(384, 64, 256, 256),\n                Fire(512, 64, 256, 256),\n            ])\n        else:\n            raise ValueError(f\"Unsupported SqueezeNet version {version}: 1_0 or 1_1 expected\")\n\n        self.final_conv = nn.Conv2d(512, num_classes, kernel_size=1, has_bias=True)\n        self.classifier = nn.SequentialCell([\n            Dropout(p=drop_rate),\n            self.final_conv,\n            nn.ReLU(),\n            GlobalAvgPooling()\n        ])\n        self._initialize_weights()\n\n    def _initialize_weights(self):\n        \"\"\"Initialize weights for cells.\"\"\"\n        for _, cell in self.cells_and_names():\n            if isinstance(cell, nn.Conv2d):\n                if cell is self.final_conv:\n                    cell.weight.set_data(init.initializer(init.Normal(), cell.weight.shape, cell.weight.dtype))\n                else:\n                    cell.weight.set_data(init.initializer(init.HeUniform(), cell.weight.shape, cell.weight.dtype))\n                if cell.bias is not None:\n                    cell.bias.set_data(init.initializer(\"zeros\", cell.bias.shape, cell.bias.dtype))\n\n    def forward_features(self, x: Tensor) -&gt; Tensor:\n        x = self.features(x)\n        return x\n\n    def forward_head(self, x: Tensor) -&gt; Tensor:\n        x = self.classifier(x)\n        return x\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n</code></pre>"},{"location":"reference/models/#mindcv.models.squeezenet.squeezenet1_0","title":"<code>mindcv.models.squeezenet.squeezenet1_0(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get SqueezeNet model of version 1.0. Refer to the base class <code>models.SqueezeNet</code> for more details.</p> Source code in <code>mindcv\\models\\squeezenet.py</code> <pre><code>@register_model\ndef squeezenet1_0(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; SqueezeNet:\n    \"\"\"Get SqueezeNet model of version 1.0.\n    Refer to the base class `models.SqueezeNet` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"squeezenet1_0\"]\n    model = SqueezeNet(version=\"1_0\", num_classes=num_classes, in_channels=in_channels, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre>"},{"location":"reference/models/#mindcv.models.squeezenet.squeezenet1_1","title":"<code>mindcv.models.squeezenet.squeezenet1_1(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get SqueezeNet model of version 1.1. Refer to the base class <code>models.SqueezeNet</code> for more details.</p> Source code in <code>mindcv\\models\\squeezenet.py</code> <pre><code>@register_model\ndef squeezenet1_1(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; SqueezeNet:\n    \"\"\"Get SqueezeNet model of version 1.1.\n    Refer to the base class `models.SqueezeNet` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"squeezenet1_1\"]\n    model = SqueezeNet(version=\"1_1\", num_classes=num_classes, in_channels=in_channels, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre>"},{"location":"reference/models/#swintransformer","title":"swintransformer","text":""},{"location":"reference/models/#mindcv.models.swintransformer","title":"<code>mindcv.models.swintransformer</code>","text":"<p>Define SwinTransformer model</p>"},{"location":"reference/models/#mindcv.models.swintransformer.BasicLayer","title":"<code>mindcv.models.swintransformer.BasicLayer</code>","text":"<p>               Bases: <code>Cell</code></p> <p>A basic Swin Transformer layer for one stage.</p> PARAMETER DESCRIPTION <code>dim</code> <p>Number of input channels.</p> <p> TYPE: <code>int</code> </p> <code>input_resolution</code> <p>Input resolution.</p> <p> TYPE: <code>tuple[int]</code> </p> <code>depth</code> <p>Number of blocks.</p> <p> TYPE: <code>int</code> </p> <code>num_heads</code> <p>Number of attention heads.</p> <p> TYPE: <code>int</code> </p> <code>window_size</code> <p>Local window size.</p> <p> TYPE: <code>int</code> </p> <code>mlp_ratio</code> <p>Ratio of mlp hidden dim to embedding dim.</p> <p> TYPE: <code>float</code> DEFAULT: <code>4.0</code> </p> <code>qkv_bias</code> <p>If True, add a learnable bias to query, key, value. Default: True</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>qk_scale</code> <p>Override default qk scale of head_dim ** -0.5 if set.</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> <code>drop</code> <p>Dropout rate. Default: 0.0</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>attn_drop</code> <p>Attention dropout rate. Default: 0.0</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>drop_path</code> <p>Stochastic depth rate. Default: 0.0</p> <p> TYPE: <code>float | tuple[float]</code> DEFAULT: <code>0.0</code> </p> <code>norm_layer</code> <p>Normalization layer. Default: nn.LayerNorm</p> <p> TYPE: <code>Cell</code> DEFAULT: <code>LayerNorm</code> </p> <code>downsample</code> <p>Downsample layer at the end of the layer. Default: None</p> <p> TYPE: <code>Cell | None</code> DEFAULT: <code>None</code> </p> Source code in <code>mindcv\\models\\swintransformer.py</code> <pre><code>class BasicLayer(nn.Cell):\n    \"\"\"A basic Swin Transformer layer for one stage.\n\n    Args:\n        dim (int): Number of input channels.\n        input_resolution (tuple[int]): Input resolution.\n        depth (int): Number of blocks.\n        num_heads (int): Number of attention heads.\n        window_size (int): Local window size.\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n        drop (float, optional): Dropout rate. Default: 0.0\n        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n        norm_layer (nn.Cell, optional): Normalization layer. Default: nn.LayerNorm\n        downsample (nn.Cell | None, optional): Downsample layer at the end of the layer. Default: None\n    \"\"\"\n\n    def __init__(\n        self,\n        dim: int,\n        input_resolution: Tuple[int],\n        depth: int,\n        num_heads: int,\n        window_size: int,\n        mlp_ratio: float = 4.0,\n        qkv_bias: bool = True,\n        qk_scale: Optional[float] = None,\n        drop: float = 0.0,\n        attn_drop: float = 0.0,\n        drop_path: Optional[float] = 0.0,\n        norm_layer: Optional[nn.Cell] = nn.LayerNorm,\n        downsample: Optional[nn.Cell] = None,\n    ) -&gt; None:\n        super().__init__()\n        self.dim = dim\n        self.input_resolution = input_resolution\n        self.depth = depth\n\n        # build blocks\n        self.blocks = nn.CellList([\n            SwinTransformerBlock(dim=dim, input_resolution=input_resolution,\n                                 num_heads=num_heads, window_size=window_size,\n                                 shift_size=0 if (i % 2 == 0) else window_size // 2,  # TODO: \u8fd9\u91ccwindow_size//2\u7684\u65f6\u5019\u7279\u522b\u6162\n                                 mlp_ratio=mlp_ratio,\n                                 qkv_bias=qkv_bias, qk_scale=qk_scale,\n                                 drop=drop, attn_drop=attn_drop,\n                                 drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,\n                                 norm_layer=norm_layer)\n            for i in range(depth)])\n\n        # patch merging layer\n        if downsample is not None:\n            self.downsample = downsample(input_resolution, dim=dim, norm_layer=norm_layer)\n        else:\n            self.downsample = None\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        for blk in self.blocks:\n            x = blk(x)\n        if self.downsample is not None:\n            x = self.downsample(x)\n        return x\n\n    def extra_repr(self) -&gt; str:\n        return f\"dim={self.dim}, input_resolution={self.input_resolution}, depth={self.depth}\"\n</code></pre>"},{"location":"reference/models/#mindcv.models.swintransformer.PatchEmbed","title":"<code>mindcv.models.swintransformer.PatchEmbed</code>","text":"<p>               Bases: <code>Cell</code></p> <p>Image to Patch Embedding</p> PARAMETER DESCRIPTION <code>image_size</code> <p>Image size.  Default: 224.</p> <p> TYPE: <code>int</code> DEFAULT: <code>224</code> </p> <code>patch_size</code> <p>Patch token size. Default: 4.</p> <p> TYPE: <code>int</code> DEFAULT: <code>4</code> </p> <code>in_chans</code> <p>Number of input image channels. Default: 3.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>embed_dim</code> <p>Number of linear projection output channels. Default: 96.</p> <p> TYPE: <code>int</code> DEFAULT: <code>96</code> </p> <code>norm_layer</code> <p>Normalization layer. Default: None</p> <p> TYPE: <code>Cell</code> DEFAULT: <code>None</code> </p> Source code in <code>mindcv\\models\\swintransformer.py</code> <pre><code>class PatchEmbed(nn.Cell):\n    \"\"\"Image to Patch Embedding\n\n    Args:\n        image_size (int): Image size.  Default: 224.\n        patch_size (int): Patch token size. Default: 4.\n        in_chans (int): Number of input image channels. Default: 3.\n        embed_dim (int): Number of linear projection output channels. Default: 96.\n        norm_layer (nn.Cell, optional): Normalization layer. Default: None\n    \"\"\"\n\n    def __init__(\n        self,\n        image_size: int = 224,\n        patch_size: int = 4,\n        in_chans: int = 3,\n        embed_dim: int = 96,\n        norm_layer: Optional[nn.Cell] = None,\n    ) -&gt; None:\n        super().__init__()\n        image_size = to_2tuple(image_size)\n        patch_size = to_2tuple(patch_size)\n        patches_resolution = [image_size[0] // patch_size[0], image_size[1] // patch_size[1]]\n        self.image_size = image_size\n        self.patch_size = patch_size\n        self.patches_resolution = patches_resolution\n        self.num_patches = patches_resolution[0] * patches_resolution[1]\n\n        self.in_chans = in_chans\n        self.embed_dim = embed_dim\n\n        self.proj = nn.Conv2d(in_channels=in_chans, out_channels=embed_dim, kernel_size=patch_size, stride=patch_size,\n                              pad_mode=\"pad\", has_bias=True, weight_init=\"TruncatedNormal\")\n\n        if norm_layer is not None:\n            if isinstance(embed_dim, int):\n                embed_dim = (embed_dim,)\n            self.norm = norm_layer(embed_dim, epsilon=1e-5)\n        else:\n            self.norm = None\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        b = x.shape[0]\n        # FIXME look at relaxing size constraints\n        x = ops.reshape(self.proj(x), (b, self.embed_dim, -1))  # b Ph*Pw c\n        x = ops.transpose(x, (0, 2, 1))\n\n        if self.norm is not None:\n            x = self.norm(x)\n        return x\n</code></pre>"},{"location":"reference/models/#mindcv.models.swintransformer.PatchMerging","title":"<code>mindcv.models.swintransformer.PatchMerging</code>","text":"<p>               Bases: <code>Cell</code></p> <p>Patch Merging Layer.</p> PARAMETER DESCRIPTION <code>input_resolution</code> <p>Resolution of input feature.</p> <p> TYPE: <code>tuple[int]</code> </p> <code>dim</code> <p>Number of input channels.</p> <p> TYPE: <code>int</code> </p> <code>norm_layer</code> <p>Normalization layer.  Default: nn.LayerNorm</p> <p> TYPE: <code>Module</code> DEFAULT: <code>LayerNorm</code> </p> Source code in <code>mindcv\\models\\swintransformer.py</code> <pre><code>class PatchMerging(nn.Cell):\n    \"\"\"Patch Merging Layer.\n\n    Args:\n        input_resolution (tuple[int]): Resolution of input feature.\n        dim (int): Number of input channels.\n        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n    \"\"\"\n\n    def __init__(\n        self,\n        input_resolution: Tuple[int],\n        dim: int,\n        norm_layer: Optional[nn.Cell] = nn.LayerNorm,\n    ) -&gt; None:\n        super().__init__()\n        self.input_resolution = input_resolution\n        self.dim = dim[0] if isinstance(dim, tuple) and len(dim) == 1 else dim\n        # Default False\n        self.reduction = nn.Dense(in_channels=4 * dim, out_channels=2 * dim, has_bias=False)\n        self.norm = norm_layer([dim * 4, ])\n        self.H, self.W = self.input_resolution\n        self.H_2, self.W_2 = self.H // 2, self.W // 2\n        self.H2W2 = int(self.H * self.W // 4)\n        self.dim_mul_4 = int(dim * 4)\n        self.H2W2 = int(self.H * self.W // 4)\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        \"\"\"\n        x: B, H*W, C\n        \"\"\"\n        b = x.shape[0]\n        x = ops.reshape(x, (b, self.H_2, 2, self.W_2, 2, self.dim))\n        x = ops.transpose(x, (0, 1, 3, 4, 2, 5))\n        x = ops.reshape(x, (b, self.H2W2, self.dim_mul_4))\n        x = self.norm(x)\n        x = self.reduction(x)\n\n        return x\n\n    def extra_repr(self) -&gt; str:\n        return f\"input_resolution={self.input_resolution}, dim={self.dim}\"\n</code></pre>"},{"location":"reference/models/#mindcv.models.swintransformer.PatchMerging.construct","title":"<code>mindcv.models.swintransformer.PatchMerging.construct(x)</code>","text":"Source code in <code>mindcv\\models\\swintransformer.py</code> <pre><code>def construct(self, x: Tensor) -&gt; Tensor:\n    \"\"\"\n    x: B, H*W, C\n    \"\"\"\n    b = x.shape[0]\n    x = ops.reshape(x, (b, self.H_2, 2, self.W_2, 2, self.dim))\n    x = ops.transpose(x, (0, 1, 3, 4, 2, 5))\n    x = ops.reshape(x, (b, self.H2W2, self.dim_mul_4))\n    x = self.norm(x)\n    x = self.reduction(x)\n\n    return x\n</code></pre>"},{"location":"reference/models/#mindcv.models.swintransformer.SwinTransformer","title":"<code>mindcv.models.swintransformer.SwinTransformer</code>","text":"<p>               Bases: <code>Cell</code></p> <p>SwinTransformer model class, based on <code>\"Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\" &lt;https://arxiv.org/pdf/2103.14030&gt;</code>_</p> PARAMETER DESCRIPTION <code>image_size</code> <p>Input image size. Default 224</p> <p> TYPE: <code>int | tuple(int</code> DEFAULT: <code>224</code> </p> <code>patch_size</code> <p>Patch size. Default: 4</p> <p> TYPE: <code>int | tuple(int</code> DEFAULT: <code>4</code> </p> <code>in_chans</code> <p>Number of input image channels. Default: 3</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>num_classes</code> <p>Number of classes for classification head. Default: 1000</p> <p> TYPE: <code>int</code> DEFAULT: <code>1000</code> </p> <code>embed_dim</code> <p>Patch embedding dimension. Default: 96</p> <p> TYPE: <code>int</code> DEFAULT: <code>96</code> </p> <code>depths</code> <p>Depth of each Swin Transformer layer.</p> <p> TYPE: <code>tuple(int</code> DEFAULT: <code>None</code> </p> <code>num_heads</code> <p>Number of attention heads in different layers.</p> <p> TYPE: <code>tuple(int</code> DEFAULT: <code>None</code> </p> <code>window_size</code> <p>Window size. Default: 7</p> <p> TYPE: <code>int</code> DEFAULT: <code>7</code> </p> <code>mlp_ratio</code> <p>Ratio of mlp hidden dim to embedding dim. Default: 4</p> <p> TYPE: <code>float</code> DEFAULT: <code>4.0</code> </p> <code>qkv_bias</code> <p>If True, add a learnable bias to query, key, value. Default: True</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>qk_scale</code> <p>Override default qk scale of head_dim ** -0.5 if set. Default: None</p> <p> TYPE: <code>float</code> DEFAULT: <code>None</code> </p> <code>drop_rate</code> <p>Dropout rate. Default: 0</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>attn_drop_rate</code> <p>Attention dropout rate. Default: 0</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>drop_path_rate</code> <p>Stochastic depth rate. Default: 0.1</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.1</code> </p> <code>norm_layer</code> <p>Normalization layer. Default: nn.LayerNorm.</p> <p> TYPE: <code>Cell</code> DEFAULT: <code>LayerNorm</code> </p> <code>ape</code> <p>If True, add absolute position embedding to the patch embedding. Default: False</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>patch_norm</code> <p>If True, add normalization after patch embedding. Default: True</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> Source code in <code>mindcv\\models\\swintransformer.py</code> <pre><code>class SwinTransformer(nn.Cell):\n    r\"\"\"SwinTransformer model class, based on\n    `\"Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\" &lt;https://arxiv.org/pdf/2103.14030&gt;`_\n\n    Args:\n        image_size (int | tuple(int)): Input image size. Default 224\n        patch_size (int | tuple(int)): Patch size. Default: 4\n        in_chans (int): Number of input image channels. Default: 3\n        num_classes (int): Number of classes for classification head. Default: 1000\n        embed_dim (int): Patch embedding dimension. Default: 96\n        depths (tuple(int)): Depth of each Swin Transformer layer.\n        num_heads (tuple(int)): Number of attention heads in different layers.\n        window_size (int): Window size. Default: 7\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4\n        qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: True\n        qk_scale (float): Override default qk scale of head_dim ** -0.5 if set. Default: None\n        drop_rate (float): Dropout rate. Default: 0\n        attn_drop_rate (float): Attention dropout rate. Default: 0\n        drop_path_rate (float): Stochastic depth rate. Default: 0.1\n        norm_layer (nn.Cell): Normalization layer. Default: nn.LayerNorm.\n        ape (bool): If True, add absolute position embedding to the patch embedding. Default: False\n        patch_norm (bool): If True, add normalization after patch embedding. Default: True\n    \"\"\"\n\n    def __init__(\n        self,\n        image_size: int = 224,\n        patch_size: int = 4,\n        in_chans: int = 3,\n        num_classes: int = 1000,\n        embed_dim: int = 96,\n        depths: Optional[List[int]] = None,\n        num_heads: Optional[List[int]] = None,\n        window_size: int = 7,\n        mlp_ratio: float = 4.0,\n        qkv_bias: bool = True,\n        qk_scale: Optional[int] = None,\n        drop_rate: float = 0.0,\n        attn_drop_rate: float = 0.0,\n        drop_path_rate: float = 0.1,\n        norm_layer: Optional[nn.Cell] = nn.LayerNorm,\n        ape: bool = False,\n        patch_norm: bool = True,\n    ) -&gt; None:\n        super().__init__()\n\n        self.num_classes = num_classes\n        self.num_layers = len(depths)\n        self.embed_dim = embed_dim\n        self.ape = ape\n        self.patch_norm = patch_norm\n        self.num_features = int(embed_dim * 2 ** (self.num_layers - 1))\n        self.mlp_ratio = mlp_ratio\n\n        # split image into non-overlapping patches\n        self.patch_embed = PatchEmbed(\n            image_size=image_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim,\n            norm_layer=norm_layer if self.patch_norm else None)\n        num_patches = self.patch_embed.num_patches\n        patches_resolution = self.patch_embed.patches_resolution\n        self.patches_resolution = patches_resolution\n\n        # absolute position embedding\n        if self.ape:\n            self.absolute_pos_embed = Parameter(Tensor(np.zeros(1, num_patches, embed_dim), dtype=mstype.float32))\n\n        self.pos_drop = Dropout(p=drop_rate)\n\n        # stochastic depth\n        dpr = [x for x in np.linspace(0, drop_path_rate, sum(depths))]  # stochastic depth decay rule\n\n        # build layers\n        self.layers = nn.CellList()\n        for i_layer in range(self.num_layers):\n            layer = BasicLayer(dim=int(embed_dim * 2 ** i_layer),\n                               input_resolution=(patches_resolution[0] // (2 ** i_layer),\n                                                 patches_resolution[1] // (2 ** i_layer)),\n                               depth=depths[i_layer],\n                               num_heads=num_heads[i_layer],\n                               window_size=window_size,\n                               mlp_ratio=self.mlp_ratio,\n                               qkv_bias=qkv_bias, qk_scale=qk_scale,\n                               drop=drop_rate, attn_drop=attn_drop_rate,\n                               drop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])],\n                               norm_layer=norm_layer,\n                               downsample=PatchMerging if (i_layer &lt; self.num_layers - 1) else None)\n            self.layers.append(layer)\n\n        self.norm = norm_layer([self.num_features, ], epsilon=1e-5)\n        self.classifier = nn.Dense(in_channels=self.num_features,\n                                   out_channels=num_classes, has_bias=True) if num_classes &gt; 0 else Identity()\n        self._initialize_weights()\n\n    def _initialize_weights(self) -&gt; None:\n        \"\"\"Initialize weights for cells.\"\"\"\n        for _, cell in self.cells_and_names():\n            if isinstance(cell, nn.Dense):\n                cell.weight.set_data(init.initializer(init.TruncatedNormal(sigma=0.02),\n                                                      cell.weight.shape, cell.weight.dtype))\n                if isinstance(cell, nn.Dense) and cell.bias is not None:\n                    cell.bias.set_data(init.initializer(init.Zero(), cell.bias.shape, cell.bias.dtype))\n            elif isinstance(cell, nn.LayerNorm):\n                cell.gamma.set_data(init.initializer(init.One(), cell.gamma.shape, cell.gamma.dtype))\n                cell.beta.set_data(init.initializer(init.Zero(), cell.beta.shape, cell.beta.dtype))\n\n    def no_weight_decay(self) -&gt; None:\n        return {\"absolute_pos_embed\"}\n\n    def no_weight_decay_keywords(self) -&gt; None:\n        return {\"relative_position_bias_table\"}\n\n    def forward_head(self, x: Tensor) -&gt; Tensor:\n        x = self.classifier(x)\n        return x\n\n    def forward_features(self, x: Tensor) -&gt; Tensor:\n        x = self.patch_embed(x)\n        if self.ape:\n            x = x + self.absolute_pos_embed\n        x = self.pos_drop(x)\n        for layer in self.layers:\n            x = layer(x)\n        x = self.norm(x)  # B L C\n        x = ops.mean(ops.transpose(x, (0, 2, 1)), 2)  # B C 1\n        return x\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n</code></pre>"},{"location":"reference/models/#mindcv.models.swintransformer.SwinTransformerBlock","title":"<code>mindcv.models.swintransformer.SwinTransformerBlock</code>","text":"<p>               Bases: <code>Cell</code></p> <p>Swin Transformer Block.</p> PARAMETER DESCRIPTION <code>dim</code> <p>Number of input channels.</p> <p> TYPE: <code>int</code> </p> <code>input_resolution</code> <p>Input resolution.</p> <p> TYPE: <code>tuple[int]</code> </p> <code>num_heads</code> <p>Number of attention heads.</p> <p> TYPE: <code>int</code> </p> <code>window_size</code> <p>Window size.</p> <p> TYPE: <code>int</code> DEFAULT: <code>7</code> </p> <code>shift_size</code> <p>Shift size for SW-MSA.</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>mlp_ratio</code> <p>Ratio of mlp hidden dim to embedding dim.</p> <p> TYPE: <code>float</code> DEFAULT: <code>4.0</code> </p> <code>qkv_bias</code> <p>If True, add a learnable bias to query, key, value. Default: True</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>qk_scale</code> <p>Override default qk scale of head_dim ** -0.5 if set.</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> <code>drop</code> <p>Dropout rate. Default: 0.0</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>attn_drop</code> <p>Attention dropout rate. Default: 0.0</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>drop_path</code> <p>Stochastic depth rate. Default: 0.0</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>act_layer</code> <p>Activation layer. Default: nn.GELU</p> <p> TYPE: <code>Cell</code> DEFAULT: <code>GELU</code> </p> <code>norm_layer</code> <p>Normalization layer.  Default: nn.LayerNorm</p> <p> TYPE: <code>Cell</code> DEFAULT: <code>LayerNorm</code> </p> Source code in <code>mindcv\\models\\swintransformer.py</code> <pre><code>class SwinTransformerBlock(nn.Cell):\n    \"\"\"Swin Transformer Block.\n\n    Args:\n        dim (int): Number of input channels.\n        input_resolution (tuple[int]): Input resolution.\n        num_heads (int): Number of attention heads.\n        window_size (int): Window size.\n        shift_size (int): Shift size for SW-MSA.\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n        drop (float, optional): Dropout rate. Default: 0.0\n        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n        drop_path (float, optional): Stochastic depth rate. Default: 0.0\n        act_layer (nn.Cell, optional): Activation layer. Default: nn.GELU\n        norm_layer (nn.Cell, optional): Normalization layer.  Default: nn.LayerNorm\n    \"\"\"\n\n    def __init__(\n        self,\n        dim: int,\n        input_resolution: Tuple[int],\n        num_heads: int,\n        window_size: int = 7,\n        shift_size: int = 0,\n        mlp_ratio: float = 4.0,\n        qkv_bias: bool = True,\n        qk_scale: Optional[float] = None,\n        drop: float = 0.0,\n        attn_drop: float = 0.0,\n        drop_path: float = 0.0,\n        act_layer: Optional[nn.Cell] = nn.GELU,\n        norm_layer: Optional[nn.Cell] = nn.LayerNorm,\n    ) -&gt; None:\n        super(SwinTransformerBlock, self).__init__()\n        self.dim = dim\n        self.input_resolution = input_resolution\n        self.num_heads = num_heads\n        self.window_size = window_size\n        self.shift_size = shift_size\n        self.mlp_ratio = mlp_ratio\n        if min(self.input_resolution) &lt;= self.window_size:\n            # if window size is larger than input resolution, we don't partition windows\n            self.shift_size = 0\n            self.window_size = min(self.input_resolution)\n\n        if isinstance(dim, int):\n            dim = (dim,)\n\n        self.norm1 = norm_layer(dim, epsilon=1e-5)\n        self.attn = WindowAttention(\n            dim, window_size=to_2tuple(self.window_size), num_heads=num_heads,\n            qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n\n        self.drop_path = DropPath(drop_path) if drop_path &gt; 0.0 else Identity()\n        self.norm2 = norm_layer(dim, epsilon=1e-5)\n        mlp_hidden_dim = int((dim[0] if isinstance(dim, tuple) else dim) * mlp_ratio)\n        self.mlp = Mlp(in_features=dim[0] if isinstance(dim, tuple) else dim, hidden_features=mlp_hidden_dim,\n                       act_layer=act_layer, drop=drop)\n        if self.shift_size &gt; 0:\n            # calculate attention mask for SW-MSA\n            h_, w_ = self.input_resolution\n            img_mask = np.zeros((1, h_, w_, 1))  # 1 H W 1\n            h_slices = (slice(0, -self.window_size),\n                        slice(-self.window_size, -self.shift_size),\n                        slice(-self.shift_size, None))\n            w_slices = (slice(0, -self.window_size),\n                        slice(-self.window_size, -self.shift_size),\n                        slice(-self.shift_size, None))\n            cnt = 0\n            for h in h_slices:\n                for w in w_slices:\n                    img_mask[:, h, w, :] = cnt\n                    cnt += 1\n            # img_mask: [1, 56, 56, 1] window_size: 7\n            mask_windows = window_partition(img_mask, self.window_size)  # nW, window_size, window_size, 1\n            mask_windows = mask_windows.reshape(-1, self.window_size * self.window_size)\n            attn_mask = mask_windows[:, np.newaxis] - mask_windows[:, :, np.newaxis]\n            # [64, 49, 49] ==&gt; [1, 64, 1, 49, 49]\n            attn_mask = np.expand_dims(attn_mask, axis=1)\n            attn_mask = np.expand_dims(attn_mask, axis=0)\n            attn_mask = Tensor(np.where(attn_mask == 0, 0.0, -100.0), dtype=mstype.float32)\n            self.attn_mask = Parameter(attn_mask, requires_grad=False)\n            self.roll_pos = Roll(self.shift_size)\n            self.roll_neg = Roll(-self.shift_size)\n        else:\n            self.attn_mask = None\n\n        self.window_partition = WindowPartition(self.window_size)\n        self.window_reverse = WindowReverse()\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        h, w = self.input_resolution\n        b, _, c = x.shape\n\n        shortcut = x\n        x = self.norm1(x)\n        x = ops.reshape(x, (b, h, w, c,))\n\n        # cyclic shift\n        if self.shift_size &gt; 0:\n            shifted_x = self.roll_neg(x)\n            # shifted_x = numpy.roll(x, (-self.shift_size, -self.shift_size), (1, 2))\n        else:\n            shifted_x = x\n\n        # partition windows\n        x_windows = self.window_partition(shifted_x)  # nW*B, window_size, window_size, C\n        x_windows = ops.reshape(x_windows,\n                                (-1, self.window_size * self.window_size, c,))  # nW*B, window_size*window_size, C\n\n        # W-MSA/SW-MSA\n        attn_windows = self.attn(x_windows, mask=self.attn_mask)  # nW*B, window_size*window_size, C\n\n        # merge windows\n        attn_windows = ops.reshape(attn_windows, (-1, self.window_size, self.window_size, c,))\n        shifted_x = self.window_reverse(attn_windows, self.window_size, h, w)  # B H' W' C\n\n        # reverse cyclic shift\n        if self.shift_size &gt; 0:\n            x = self.roll_pos(shifted_x)\n        else:\n            x = shifted_x\n\n        x = ops.reshape(x, (b, h * w, c,))\n\n        # FFN\n        x = shortcut + self.drop_path(x)\n\n        x = x + self.drop_path(self.mlp(self.norm2(x)))\n\n        return x\n\n    def extra_repr(self) -&gt; str:\n        return f\"dim={self.dim}, input_resolution={self.input_resolution}, num_heads={self.num_heads}, \" \\\n               f\"window_size={self.window_size}, shift_size={self.shift_size}, mlp_ratio={self.mlp_ratio}\"\n</code></pre>"},{"location":"reference/models/#mindcv.models.swintransformer.WindowAttention","title":"<code>mindcv.models.swintransformer.WindowAttention</code>","text":"<p>               Bases: <code>Cell</code></p> <p>Window based multi-head self attention (W-MSA) Cell with relative position bias. It supports both of shifted and non-shifted window.</p> PARAMETER DESCRIPTION <code>dim</code> <p>Number of input channels.</p> <p> TYPE: <code>int</code> </p> <code>window_size</code> <p>The height and width of the window.</p> <p> TYPE: <code>tuple[int]</code> </p> <code>num_heads</code> <p>Number of attention heads.</p> <p> TYPE: <code>int</code> </p> <code>qkv_bias</code> <p>If True, add a learnable bias to query, key, value. Default: True</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>qZk_scale</code> <p>Override default qk scale of head_dim ** -0.5 if set</p> <p> TYPE: <code>float | None</code> </p> <code>attn_drop</code> <p>Dropout ratio of attention weight. Default: 0.0</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>proj_drop</code> <p>Dropout ratio of output. Default: 0.0</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> Source code in <code>mindcv\\models\\swintransformer.py</code> <pre><code>class WindowAttention(nn.Cell):\n    r\"\"\"Window based multi-head self attention (W-MSA) Cell with relative position bias.\n    It supports both of shifted and non-shifted window.\n\n    Args:\n        dim (int): Number of input channels.\n        window_size (tuple[int]): The height and width of the window.\n        num_heads (int): Number of attention heads.\n        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True\n        qZk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set\n        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0\n        proj_drop (float, optional): Dropout ratio of output. Default: 0.0\n    \"\"\"\n\n    def __init__(\n        self,\n        dim: int,\n        window_size: int,\n        num_heads: int,\n        qkv_bias: bool = True,\n        qk_scale: Optional[float] = None,\n        attn_drop: float = 0.0,\n        proj_drop: float = 0.0,\n    ) -&gt; None:\n        super().__init__()\n        if isinstance(dim, tuple) and len(dim) == 1:\n            dim = dim[0]\n        self.dim = dim\n        self.window_size = window_size  # Wh, Ww\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        self.scale = Tensor(qk_scale or head_dim**-0.5, mstype.float32)\n        self.relative_bias = RelativeBias(self.window_size, num_heads)\n\n        # get pair-wise relative position index for each token inside the window\n        self.q = nn.Dense(in_channels=dim, out_channels=dim, has_bias=qkv_bias)\n        self.k = nn.Dense(in_channels=dim, out_channels=dim, has_bias=qkv_bias)\n        self.v = nn.Dense(in_channels=dim, out_channels=dim, has_bias=qkv_bias)\n\n        self.attn_drop = Dropout(p=attn_drop)\n        self.proj = nn.Dense(in_channels=dim, out_channels=dim, has_bias=True)\n        self.proj_drop = Dropout(p=proj_drop)\n        self.softmax = nn.Softmax(axis=-1)\n        self.batch_matmul = ops.BatchMatMul()\n\n    def construct(self, x: Tensor, mask: Optional[Tensor] = None) -&gt; Tensor:\n        \"\"\"\n        Args:\n            x: input features with shape of (num_windows*B, N, C)\n            mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None\n        \"\"\"\n        b_, n, c = x.shape\n        q = ops.reshape(self.q(x), (b_, n, self.num_heads, c // self.num_heads)) * self.scale\n        q = ops.transpose(q, (0, 2, 1, 3))\n        k = ops.reshape(self.k(x), (b_, n, self.num_heads, c // self.num_heads))\n        k = ops.transpose(k, (0, 2, 3, 1))\n        v = ops.reshape(self.v(x), (b_, n, self.num_heads, c // self.num_heads))\n        v = ops.transpose(v, (0, 2, 1, 3))\n\n        attn = self.batch_matmul(q, k)\n        attn = attn + self.relative_bias()\n\n        if mask is not None:\n            nw = mask.shape[1]\n            attn = ops.reshape(attn, (b_ // nw, nw, self.num_heads, n, n,)) + mask\n            attn = ops.reshape(attn, (-1, self.num_heads, n, n,))\n            attn = self.softmax(attn)\n        else:\n            attn = self.softmax(attn)\n        attn = self.attn_drop(attn)\n        x = ops.reshape(ops.transpose(self.batch_matmul(attn, v), (0, 2, 1, 3)), (b_, n, c))\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n\n    def extra_repr(self) -&gt; str:\n        return f\"dim={self.dim}, window_size={self.window_size}, num_heads={self.num_heads}\"\n</code></pre>"},{"location":"reference/models/#mindcv.models.swintransformer.WindowAttention.construct","title":"<code>mindcv.models.swintransformer.WindowAttention.construct(x, mask=None)</code>","text":"PARAMETER DESCRIPTION <code>x</code> <p>input features with shape of (num_windows*B, N, C)</p> <p> TYPE: <code>Tensor</code> </p> <code>mask</code> <p>(0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None</p> <p> TYPE: <code>Optional[Tensor]</code> DEFAULT: <code>None</code> </p> Source code in <code>mindcv\\models\\swintransformer.py</code> <pre><code>def construct(self, x: Tensor, mask: Optional[Tensor] = None) -&gt; Tensor:\n    \"\"\"\n    Args:\n        x: input features with shape of (num_windows*B, N, C)\n        mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None\n    \"\"\"\n    b_, n, c = x.shape\n    q = ops.reshape(self.q(x), (b_, n, self.num_heads, c // self.num_heads)) * self.scale\n    q = ops.transpose(q, (0, 2, 1, 3))\n    k = ops.reshape(self.k(x), (b_, n, self.num_heads, c // self.num_heads))\n    k = ops.transpose(k, (0, 2, 3, 1))\n    v = ops.reshape(self.v(x), (b_, n, self.num_heads, c // self.num_heads))\n    v = ops.transpose(v, (0, 2, 1, 3))\n\n    attn = self.batch_matmul(q, k)\n    attn = attn + self.relative_bias()\n\n    if mask is not None:\n        nw = mask.shape[1]\n        attn = ops.reshape(attn, (b_ // nw, nw, self.num_heads, n, n,)) + mask\n        attn = ops.reshape(attn, (-1, self.num_heads, n, n,))\n        attn = self.softmax(attn)\n    else:\n        attn = self.softmax(attn)\n    attn = self.attn_drop(attn)\n    x = ops.reshape(ops.transpose(self.batch_matmul(attn, v), (0, 2, 1, 3)), (b_, n, c))\n    x = self.proj(x)\n    x = self.proj_drop(x)\n    return x\n</code></pre>"},{"location":"reference/models/#mindcv.models.swintransformer.WindowPartition","title":"<code>mindcv.models.swintransformer.WindowPartition</code>","text":"<p>               Bases: <code>Cell</code></p> Source code in <code>mindcv\\models\\swintransformer.py</code> <pre><code>class WindowPartition(nn.Cell):\n    def __init__(\n        self,\n        window_size: int,\n    ) -&gt; None:\n        super(WindowPartition, self).__init__()\n\n        self.window_size = window_size\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        \"\"\"\n        Args:\n            x: (b, h, w, c)\n            window_size (int): window size\n\n        Returns:\n            windows: Tensor(num_windows*b, window_size, window_size, c)\n        \"\"\"\n        b, h, w, c = x.shape\n        x = ops.reshape(x, (b, h // self.window_size, self.window_size, w // self.window_size, self.window_size, c))\n        x = ops.transpose(x, (0, 1, 3, 2, 4, 5))\n        x = ops.reshape(x, (b * h * w // (self.window_size**2), self.window_size, self.window_size, c))\n\n        return x\n</code></pre>"},{"location":"reference/models/#mindcv.models.swintransformer.WindowPartition.construct","title":"<code>mindcv.models.swintransformer.WindowPartition.construct(x)</code>","text":"PARAMETER DESCRIPTION <code>x</code> <p>(b, h, w, c)</p> <p> TYPE: <code>Tensor</code> </p> <code>window_size</code> <p>window size</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>windows</code> <p>Tensor(num_windows*b, window_size, window_size, c)</p> <p> TYPE: <code>Tensor</code> </p> Source code in <code>mindcv\\models\\swintransformer.py</code> <pre><code>def construct(self, x: Tensor) -&gt; Tensor:\n    \"\"\"\n    Args:\n        x: (b, h, w, c)\n        window_size (int): window size\n\n    Returns:\n        windows: Tensor(num_windows*b, window_size, window_size, c)\n    \"\"\"\n    b, h, w, c = x.shape\n    x = ops.reshape(x, (b, h // self.window_size, self.window_size, w // self.window_size, self.window_size, c))\n    x = ops.transpose(x, (0, 1, 3, 2, 4, 5))\n    x = ops.reshape(x, (b * h * w // (self.window_size**2), self.window_size, self.window_size, c))\n\n    return x\n</code></pre>"},{"location":"reference/models/#mindcv.models.swintransformer.WindowReverse","title":"<code>mindcv.models.swintransformer.WindowReverse</code>","text":"<p>               Bases: <code>Cell</code></p> Source code in <code>mindcv\\models\\swintransformer.py</code> <pre><code>class WindowReverse(nn.Cell):\n    def construct(\n        self,\n        windows: Tensor,\n        window_size: int,\n        h: int,\n        w: int,\n    ) -&gt; Tensor:\n        \"\"\"\n        Args:\n            windows: (num_windows*B, window_size, window_size, C)\n            window_size (int): Window size\n            h (int): Height of image\n            w (int): Width of image\n\n        Returns:\n            x: (B, H, W, C)\n        \"\"\"\n        b = windows.shape[0] // (h * w // window_size // window_size)\n        x = ops.reshape(windows, (b, h // window_size, w // window_size, window_size, window_size, -1))\n        x = ops.transpose(x, (0, 1, 3, 2, 4, 5))\n        x = ops.reshape(x, (b, h, w, -1))\n        return x\n</code></pre>"},{"location":"reference/models/#mindcv.models.swintransformer.WindowReverse.construct","title":"<code>mindcv.models.swintransformer.WindowReverse.construct(windows, window_size, h, w)</code>","text":"PARAMETER DESCRIPTION <code>windows</code> <p>(num_windows*B, window_size, window_size, C)</p> <p> TYPE: <code>Tensor</code> </p> <code>window_size</code> <p>Window size</p> <p> TYPE: <code>int</code> </p> <code>h</code> <p>Height of image</p> <p> TYPE: <code>int</code> </p> <code>w</code> <p>Width of image</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>x</code> <p>(B, H, W, C)</p> <p> TYPE: <code>Tensor</code> </p> Source code in <code>mindcv\\models\\swintransformer.py</code> <pre><code>def construct(\n    self,\n    windows: Tensor,\n    window_size: int,\n    h: int,\n    w: int,\n) -&gt; Tensor:\n    \"\"\"\n    Args:\n        windows: (num_windows*B, window_size, window_size, C)\n        window_size (int): Window size\n        h (int): Height of image\n        w (int): Width of image\n\n    Returns:\n        x: (B, H, W, C)\n    \"\"\"\n    b = windows.shape[0] // (h * w // window_size // window_size)\n    x = ops.reshape(windows, (b, h // window_size, w // window_size, window_size, window_size, -1))\n    x = ops.transpose(x, (0, 1, 3, 2, 4, 5))\n    x = ops.reshape(x, (b, h, w, -1))\n    return x\n</code></pre>"},{"location":"reference/models/#mindcv.models.swintransformer.swin_tiny","title":"<code>mindcv.models.swintransformer.swin_tiny(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get SwinTransformer tiny model. Refer to the base class 'models.SwinTransformer' for more details.</p> Source code in <code>mindcv\\models\\swintransformer.py</code> <pre><code>@register_model\ndef swin_tiny(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; SwinTransformer:\n    \"\"\"Get SwinTransformer tiny model.\n    Refer to the base class 'models.SwinTransformer' for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"swin_tiny\"]\n    model = SwinTransformer(image_size=224, patch_size=4, in_chans=in_channels, num_classes=num_classes,\n                            embed_dim=96, depths=[2, 2, 6, 2], num_heads=[3, 6, 12, 24], window_size=7,\n                            mlp_ratio=4., qkv_bias=True, qk_scale=None,\n                            drop_rate=0., attn_drop_rate=0., drop_path_rate=0.2,\n                            norm_layer=nn.LayerNorm, ape=False, patch_norm=True, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre>"},{"location":"reference/models/#mindcv.models.swintransformer.window_partition","title":"<code>mindcv.models.swintransformer.window_partition(x, window_size)</code>","text":"PARAMETER DESCRIPTION <code>x</code> <p>(B, H, W, C)</p> <p> </p> <code>window_size</code> <p>window size</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>windows</code> <p>numpy(num_windows*B, window_size, window_size, C)</p> Source code in <code>mindcv\\models\\swintransformer.py</code> <pre><code>def window_partition(x, window_size: int):\n    \"\"\"\n    Args:\n        x: (B, H, W, C)\n        window_size (int): window size\n\n    Returns:\n        windows: numpy(num_windows*B, window_size, window_size, C)\n    \"\"\"\n    b, h, w, c = x.shape\n    x = np.reshape(x, (b, h // window_size, window_size, w // window_size, window_size, c))\n    windows = x.transpose(0, 1, 3, 2, 4, 5).reshape(-1, window_size, window_size, c)\n    return windows\n</code></pre>"},{"location":"reference/models/#swintransformerv2","title":"swintransformerv2","text":""},{"location":"reference/models/#mindcv.models.swintransformerv2","title":"<code>mindcv.models.swintransformerv2</code>","text":"<p>MindSpore implementation of <code>SwinTransformer V2</code>. Refer to Swin Transformer V2: Scaling Up Capacity and Resolution.</p>"},{"location":"reference/models/#mindcv.models.swintransformerv2.SwinTransformerV2","title":"<code>mindcv.models.swintransformerv2.SwinTransformerV2</code>","text":"<p>               Bases: <code>Cell</code></p> <p>SwinTransformerV2 model class, based on <code>\"Swin Transformer V2: Scaling Up Capacity and Resolution\" &lt;https://arxiv.org/abs/2111.09883&gt;</code>_</p> PARAMETER DESCRIPTION <code>image_size</code> <p>Input image size. Default: 256.</p> <p> TYPE: <code>int</code> DEFAULT: <code>256</code> </p> <code>patch_size</code> <p>Patch size. Default: 4.</p> <p> TYPE: <code>int</code> DEFAULT: <code>4</code> </p> <code>in_channels</code> <p>Number the channels of the input. Default: 3.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>num_classes</code> <p>Number of classification classes. Default: 1000.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1000</code> </p> <code>embed_dim</code> <p>Patch embedding dimension. Default: 96.</p> <p> TYPE: <code>int</code> DEFAULT: <code>96</code> </p> <code>depths</code> <p>Depth of each Swin Transformer layer. Default: [2, 2, 6, 2].</p> <p> TYPE: <code>List[int]</code> DEFAULT: <code>[2, 2, 6, 2]</code> </p> <code>num_heads</code> <p>Number of attention heads in different layers. Default: [3, 6, 12, 24].</p> <p> TYPE: <code>List[int]</code> DEFAULT: <code>[3, 6, 12, 24]</code> </p> <code>window_size</code> <p>Window size. Default: 7.</p> <p> TYPE: <code>int</code> DEFAULT: <code>7</code> </p> <code>mlp_ratio</code> <p>Ratio of mlp hidden dim to embedding dim. Default: 4.</p> <p> TYPE: <code>float</code> DEFAULT: <code>4.0</code> </p> <code>qkv_bias</code> <p>If True, add a bias for query, key, value. Default: True.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>drop_rate</code> <p>Drop probability for the Dropout layer. Default: 0.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>attn_drop_rate</code> <p>Attention drop probability for the Dropout layer. Default: 0.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>drop_path_rate</code> <p>Stochastic depth rate. Default: 0.1.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.1</code> </p> <code>norm_layer</code> <p>Normalization layer. Default: nn.LayerNorm.</p> <p> TYPE: <code>Cell</code> DEFAULT: <code>LayerNorm</code> </p> <code>patch_norm</code> <p>If True, add normalization after patch embedding. Default: True.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>pretrained_window_sizes</code> <p>Pretrained window sizes of each layer. Default: [0, 0, 0, 0].</p> <p> TYPE: <code>List[int]</code> DEFAULT: <code>[0, 0, 0, 0]</code> </p> Source code in <code>mindcv\\models\\swintransformerv2.py</code> <pre><code>class SwinTransformerV2(nn.Cell):\n    r\"\"\"SwinTransformerV2 model class, based on\n    `\"Swin Transformer V2: Scaling Up Capacity and Resolution\" &lt;https://arxiv.org/abs/2111.09883&gt;`_\n\n    Args:\n        image_size: Input image size. Default: 256.\n        patch_size: Patch size. Default: 4.\n        in_channels: Number the channels of the input. Default: 3.\n        num_classes: Number of classification classes. Default: 1000.\n        embed_dim: Patch embedding dimension. Default: 96.\n        depths: Depth of each Swin Transformer layer. Default: [2, 2, 6, 2].\n        num_heads: Number of attention heads in different layers. Default: [3, 6, 12, 24].\n        window_size: Window size. Default: 7.\n        mlp_ratio: Ratio of mlp hidden dim to embedding dim. Default: 4.\n        qkv_bias: If True, add a bias for query, key, value. Default: True.\n        drop_rate: Drop probability for the Dropout layer. Default: 0.\n        attn_drop_rate: Attention drop probability for the Dropout layer. Default: 0.\n        drop_path_rate: Stochastic depth rate. Default: 0.1.\n        norm_layer: Normalization layer. Default: nn.LayerNorm.\n        patch_norm: If True, add normalization after patch embedding. Default: True.\n        pretrained_window_sizes: Pretrained window sizes of each layer. Default: [0, 0, 0, 0].\n    \"\"\"\n\n    def __init__(\n        self,\n        image_size: int = 256,\n        patch_size: int = 4,\n        in_channels: int = 3,\n        num_classes: int = 1000,\n        embed_dim: int = 96,\n        depths: List[int] = [2, 2, 6, 2],\n        num_heads: List[int] = [3, 6, 12, 24],\n        window_size: int = 7,\n        mlp_ratio: float = 4.0,\n        qkv_bias: bool = True,\n        drop_rate: float = 0.0,\n        attn_drop_rate: float = 0.0,\n        drop_path_rate: float = 0.1,\n        norm_layer: nn.Cell = nn.LayerNorm,\n        patch_norm: bool = True,\n        pretrained_window_sizes: List[int] = [0, 0, 0, 0],\n    ) -&gt; None:\n        super().__init__()\n        self.num_classes = num_classes\n        self.num_layers = len(depths)\n        self.embed_dim = embed_dim\n        self.in_channels = in_channels\n        self.patch_size = patch_size\n        self.patch_norm = patch_norm\n        self.num_features = int(embed_dim * 2 ** (self.num_layers - 1))\n        self.mlp_ratio = mlp_ratio\n\n        # split image into non-overlapping patches\n        self.patch_embed = PatchEmbed(\n            image_size=image_size, patch_size=patch_size, in_chans=in_channels, embed_dim=embed_dim,\n            norm_layer=norm_layer if self.patch_norm else None)\n        num_patches = self.patch_embed.num_patches\n        self.num_patches = num_patches\n        patches_resolution = self.patch_embed.patches_resolution\n        self.patches_resolution = patches_resolution\n\n        self.pos_drop = Dropout(p=drop_rate)\n\n        # stochastic depth\n        dpr = [x for x in np.linspace(0, drop_path_rate, sum(depths))]  # stochastic depth decay rule\n\n        # build layers\n        self.layers = nn.CellList()\n        self.final_seq = num_patches  # downsample seq_length\n        for i_layer in range(self.num_layers):\n            layer = BasicLayer(\n                dim=int(embed_dim * 2**i_layer),\n                input_resolution=(patches_resolution[0] // (2**i_layer),\n                                  patches_resolution[1] // (2**i_layer)),\n                depth=depths[i_layer],\n                num_heads=num_heads[i_layer],\n                window_size=window_size,\n                mlp_ratio=self.mlp_ratio,\n                qkv_bias=qkv_bias,\n                drop=drop_rate, attn_drop=attn_drop_rate,\n                drop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])],\n                norm_layer=norm_layer,\n                downsample=PatchMerging if (i_layer &lt; self.num_layers - 1) else None,\n                pretrained_window_size=pretrained_window_sizes[i_layer]\n            )\n            # downsample seq_length\n            if i_layer &lt; self.num_layers - 1:\n                self.final_seq = self.final_seq // 4\n            self.layers.append(layer)\n        self.head = nn.Dense(self.num_features, self.num_classes)\n\n        self.norm = norm_layer([self.num_features, ], epsilon=1e-6)\n        self.avgpool = ops.ReduceMean(keep_dims=False)\n\n        self._initialize_weights()\n\n    def _initialize_weights(self):\n        for _, cell in self.cells_and_names():\n            if isinstance(cell, nn.Conv2d):\n                cell.weight.set_data(init.initializer(init.HeUniform(), cell.weight.shape, cell.weight.dtype))\n                if cell.bias is not None:\n                    cell.bias.set_data(init.initializer(\"zeros\", cell.bias.shape, cell.bias.dtype))\n            elif isinstance(cell, nn.LayerNorm):\n                cell.gamma.set_data(init.initializer(\"ones\", cell.gamma.shape, cell.gamma.dtype))\n                cell.beta.set_data(init.initializer(\"zeros\", cell.beta.shape, cell.beta.dtype))\n            elif isinstance(cell, nn.Dense):\n                cell.weight.set_data(\n                    init.initializer(init.TruncatedNormal(sigma=0.02), cell.weight.shape, cell.weight.dtype)\n                )\n                if cell.bias is not None:\n                    cell.bias.set_data(init.initializer(\"zeros\", cell.bias.shape, cell.bias.dtype))\n\n    def forward_features(self, x: Tensor) -&gt; Tensor:\n        x = self.patch_embed(x)\n        x = self.pos_drop(x)\n        for layer in self.layers:\n            x = layer(x)\n        x = self.norm(x)  # B L C\n        x = self.avgpool(ops.transpose(x, (0, 2, 1)), 2)  # B C 1\n        return x\n\n    def forward_head(self, x: Tensor) -&gt; Tensor:\n        x = self.head(x)\n        return x\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n</code></pre>"},{"location":"reference/models/#vgg","title":"vgg","text":""},{"location":"reference/models/#mindcv.models.vgg","title":"<code>mindcv.models.vgg</code>","text":"<p>MindSpore implementation of <code>VGGNet</code>. Refer to SqueezeNet: Very Deep Convolutional Networks for Large-Scale Image Recognition.</p>"},{"location":"reference/models/#mindcv.models.vgg.VGG","title":"<code>mindcv.models.vgg.VGG</code>","text":"<p>               Bases: <code>Cell</code></p> <p>VGGNet model class, based on <code>\"Very Deep Convolutional Networks for Large-Scale Image Recognition\" &lt;https://arxiv.org/abs/1409.1556&gt;</code>_</p> PARAMETER DESCRIPTION <code>model_name</code> <p>name of the architecture. 'vgg11', 'vgg13', 'vgg16' or 'vgg19'.</p> <p> TYPE: <code>str</code> </p> <code>batch_norm</code> <p>use batch normalization or not. Default: False.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>num_classes</code> <p>number of classification classes. Default: 1000.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1000</code> </p> <code>in_channels</code> <p>number the channels of the input. Default: 3.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>drop_rate</code> <p>dropout rate of the classifier. Default: 0.5.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.5</code> </p> Source code in <code>mindcv\\models\\vgg.py</code> <pre><code>class VGG(nn.Cell):\n    r\"\"\"VGGNet model class, based on\n    `\"Very Deep Convolutional Networks for Large-Scale Image Recognition\" &lt;https://arxiv.org/abs/1409.1556&gt;`_\n\n    Args:\n        model_name: name of the architecture. 'vgg11', 'vgg13', 'vgg16' or 'vgg19'.\n        batch_norm: use batch normalization or not. Default: False.\n        num_classes: number of classification classes. Default: 1000.\n        in_channels: number the channels of the input. Default: 3.\n        drop_rate: dropout rate of the classifier. Default: 0.5.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_name: str,\n        batch_norm: bool = False,\n        num_classes: int = 1000,\n        in_channels: int = 3,\n        drop_rate: float = 0.5,\n    ) -&gt; None:\n        super().__init__()\n        cfg = cfgs[model_name]\n        self.features = _make_layers(cfg, batch_norm=batch_norm, in_channels=in_channels)\n        self.flatten = nn.Flatten()\n        self.classifier = nn.SequentialCell([\n            nn.Dense(512 * 7 * 7, 4096),\n            nn.ReLU(),\n            Dropout(p=drop_rate),\n            nn.Dense(4096, 4096),\n            nn.ReLU(),\n            Dropout(p=drop_rate),\n            nn.Dense(4096, num_classes),\n        ])\n        self._initialize_weights()\n\n    def _initialize_weights(self) -&gt; None:\n        \"\"\"Initialize weights for cells.\"\"\"\n        for _, cell in self.cells_and_names():\n            if isinstance(cell, nn.Conv2d):\n                cell.weight.set_data(\n                    init.initializer(init.HeNormal(math.sqrt(5), mode=\"fan_out\", nonlinearity=\"relu\"),\n                                     cell.weight.shape, cell.weight.dtype))\n                if cell.bias is not None:\n                    cell.bias.set_data(\n                        init.initializer(\"zeros\", cell.bias.shape, cell.bias.dtype))\n            elif isinstance(cell, nn.Dense):\n                cell.weight.set_data(\n                    init.initializer(init.Normal(0.01), cell.weight.shape, cell.weight.dtype))\n                if cell.bias is not None:\n                    cell.bias.set_data(init.initializer(\"zeros\", cell.bias.shape, cell.bias.dtype))\n\n    def forward_features(self, x: Tensor) -&gt; Tensor:\n        x = self.features(x)\n        return x\n\n    def forward_head(self, x: Tensor) -&gt; Tensor:\n        x = self.flatten(x)\n        x = self.classifier(x)\n        return x\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n</code></pre>"},{"location":"reference/models/#mindcv.models.vgg.vgg11","title":"<code>mindcv.models.vgg.vgg11(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get 11 layers VGG model. Refer to the base class <code>models.VGG</code> for more details.</p> Source code in <code>mindcv\\models\\vgg.py</code> <pre><code>@register_model\ndef vgg11(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; VGG:\n    \"\"\"Get 11 layers VGG model.\n    Refer to the base class `models.VGG` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"vgg11\"]\n    model = VGG(model_name=\"vgg11\", num_classes=num_classes, in_channels=in_channels, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre>"},{"location":"reference/models/#mindcv.models.vgg.vgg13","title":"<code>mindcv.models.vgg.vgg13(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get 13 layers VGG model. Refer to the base class <code>models.VGG</code> for more details.</p> Source code in <code>mindcv\\models\\vgg.py</code> <pre><code>@register_model\ndef vgg13(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; VGG:\n    \"\"\"Get 13 layers VGG model.\n    Refer to the base class `models.VGG` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"vgg13\"]\n    model = VGG(model_name=\"vgg13\", num_classes=num_classes, in_channels=in_channels, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre>"},{"location":"reference/models/#mindcv.models.vgg.vgg16","title":"<code>mindcv.models.vgg.vgg16(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get 16 layers VGG model. Refer to the base class <code>models.VGG</code> for more details.</p> Source code in <code>mindcv\\models\\vgg.py</code> <pre><code>@register_model\ndef vgg16(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; VGG:\n    \"\"\"Get 16 layers VGG model.\n    Refer to the base class `models.VGG` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"vgg16\"]\n    model = VGG(model_name=\"vgg16\", num_classes=num_classes, in_channels=in_channels, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre>"},{"location":"reference/models/#mindcv.models.vgg.vgg19","title":"<code>mindcv.models.vgg.vgg19(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get 19 layers VGG model. Refer to the base class <code>models.VGG</code> for more details.</p> Source code in <code>mindcv\\models\\vgg.py</code> <pre><code>@register_model\ndef vgg19(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; VGG:\n    \"\"\"Get 19 layers VGG model.\n    Refer to the base class `models.VGG` for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"vgg19\"]\n    model = VGG(model_name=\"vgg19\", num_classes=num_classes, in_channels=in_channels, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre>"},{"location":"reference/models/#visformer","title":"visformer","text":""},{"location":"reference/models/#mindcv.models.visformer","title":"<code>mindcv.models.visformer</code>","text":"<p>MindSpore implementation of <code>Visformer</code>. Refer to: Visformer: The Vision-friendly Transformer</p>"},{"location":"reference/models/#mindcv.models.visformer.Attention","title":"<code>mindcv.models.visformer.Attention</code>","text":"<p>               Bases: <code>Cell</code></p> <p>Attention layer</p> Source code in <code>mindcv\\models\\visformer.py</code> <pre><code>class Attention(nn.Cell):\n    \"\"\"Attention layer\"\"\"\n\n    def __init__(\n        self,\n        dim: int,\n        num_heads: int = 8,\n        head_dim_ratio: float = 1.0,\n        qkv_bias: bool = False,\n        qk_scale: float = None,\n        attn_drop: float = 0.0,\n        proj_drop: float = 0.0,\n    ) -&gt; None:\n        super(Attention, self).__init__()\n        self.dim = dim\n        self.num_heads = num_heads\n        head_dim = round(dim // num_heads * head_dim_ratio)\n        self.head_dim = head_dim\n\n        qk_scale_factor = qk_scale if qk_scale is not None else -0.25\n        self.scale = head_dim**qk_scale_factor\n\n        self.qkv = nn.Conv2d(dim, head_dim * num_heads * 3, 1, 1, pad_mode=\"pad\", padding=0, has_bias=qkv_bias)\n        self.attn_drop = Dropout(p=attn_drop)\n        self.proj = nn.Conv2d(self.head_dim * self.num_heads, dim, 1, 1, pad_mode=\"pad\", padding=0)\n        self.proj_drop = Dropout(p=proj_drop)\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        B, C, H, W = x.shape\n        x = self.qkv(x)\n        qkv = ops.reshape(x, (B, 3, self.num_heads, self.head_dim, H * W))\n        qkv = qkv.transpose((1, 0, 2, 4, 3))\n        q, k, v = qkv[0], qkv[1], qkv[2]\n        attn = ops.matmul(q * self.scale, k.transpose(0, 1, 3, 2) * self.scale)\n        attn = ops.Softmax(axis=-1)(attn)\n        attn = self.attn_drop(attn)\n        x = ops.matmul(attn, v)\n\n        x = x.transpose((0, 1, 3, 2)).reshape((B, -1, H, W))\n        x = self.proj(x)\n        x = self.proj_drop(x)\n\n        return x\n</code></pre>"},{"location":"reference/models/#mindcv.models.visformer.Block","title":"<code>mindcv.models.visformer.Block</code>","text":"<p>               Bases: <code>Cell</code></p> <p>visformer basic block</p> Source code in <code>mindcv\\models\\visformer.py</code> <pre><code>class Block(nn.Cell):\n    \"\"\"visformer basic block\"\"\"\n\n    def __init__(\n        self,\n        dim: int,\n        num_heads: int,\n        head_dim_ratio: float = 1.0,\n        mlp_ratio: float = 4.0,\n        qkv_bias: bool = False,\n        qk_scale: float = None,\n        drop: float = 0.0,\n        attn_drop: float = 0.0,\n        drop_path: float = 0.0,\n        act_layer: nn.Cell = nn.GELU,\n        group: int = 8,\n        attn_disabled: bool = False,\n        spatial_conv: bool = False,\n    ) -&gt; None:\n        super(Block, self).__init__()\n        self.attn_disabled = attn_disabled\n        self.spatial_conv = spatial_conv\n        self.drop_path = DropPath(drop_path) if drop_path &gt; 0.0 else Identity()\n        if not attn_disabled:\n            self.norm1 = nn.BatchNorm2d(dim)\n            self.attn = Attention(dim, num_heads=num_heads, head_dim_ratio=head_dim_ratio, qkv_bias=qkv_bias,\n                                  qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n\n        self.norm2 = nn.BatchNorm2d(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop,\n                       group=group, spatial_conv=spatial_conv)\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        if not self.attn_disabled:\n            x = x + self.drop_path(self.attn(self.norm1(x)))\n        x = x + self.drop_path(self.mlp(self.norm2(x)))\n        return x\n</code></pre>"},{"location":"reference/models/#mindcv.models.visformer.Mlp","title":"<code>mindcv.models.visformer.Mlp</code>","text":"<p>               Bases: <code>Cell</code></p> <p>MLP layer</p> Source code in <code>mindcv\\models\\visformer.py</code> <pre><code>class Mlp(nn.Cell):\n    \"\"\"MLP layer\"\"\"\n\n    def __init__(\n        self,\n        in_features: int,\n        hidden_features: int = None,\n        out_features: int = None,\n        act_layer: nn.Cell = nn.GELU,\n        drop: float = 0.0,\n        group: int = 8,\n        spatial_conv: bool = False,\n    ) -&gt; None:\n        super(Mlp, self).__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.in_features = in_features\n        self.out_features = out_features\n        self.spatial_conv = spatial_conv\n        if self.spatial_conv:\n            if group &lt; 2:\n                hidden_features = in_features * 5 // 6\n            else:\n                hidden_features = in_features * 2\n        self.hidden_features = hidden_features\n        self.group = group\n        self.drop = Dropout(p=drop)\n        self.conv1 = nn.Conv2d(in_features, hidden_features, 1, 1, pad_mode=\"pad\", padding=0)\n        self.act1 = act_layer()\n        if self.spatial_conv:\n            self.conv2 = nn.Conv2d(hidden_features, hidden_features, 3, 1, pad_mode=\"pad\", padding=1, group=self.group)\n            self.act2 = act_layer()\n        self.conv3 = nn.Conv2d(hidden_features, out_features, 1, 1, pad_mode=\"pad\", padding=0)\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        x = self.conv1(x)\n        x = self.act1(x)\n        x = self.drop(x)\n\n        if self.spatial_conv:\n            x = self.conv2(x)\n            x = self.act2(x)\n\n        x = self.conv3(x)\n        x = self.drop(x)\n        return x\n</code></pre>"},{"location":"reference/models/#mindcv.models.visformer.Visformer","title":"<code>mindcv.models.visformer.Visformer</code>","text":"<p>               Bases: <code>Cell</code></p> <p>Visformer model class, based on '\"Visformer: The Vision-friendly Transformer\" https://arxiv.org/pdf/2104.12533.pdf'</p> PARAMETER DESCRIPTION <code>image_size</code> <p>images input size. Default: 224.</p> <p> TYPE: <code>int) </code> </p> <code>number</code> <p>32.</p> <p> TYPE: <code>the channels of the input. Default</code> </p> <code>num_classes</code> <p>number of classification classes. Default: 1000.</p> <p> TYPE: <code>int) </code> DEFAULT: <code>1000</code> </p> <code>embed_dim</code> <p>embedding dimension in all head. Default: 384.</p> <p> TYPE: <code>int) </code> DEFAULT: <code>384</code> </p> <code>depth</code> <p>model block depth. Default: None.</p> <p> TYPE: <code>int) </code> DEFAULT: <code>None</code> </p> <code>num_heads</code> <p>number of heads. Default: None.</p> <p> TYPE: <code>int) </code> DEFAULT: <code>None</code> </p> <code>mlp_ratio</code> <p>ratio of hidden features in Mlp. Default: 4.</p> <p> TYPE: <code>float) </code> DEFAULT: <code>4.0</code> </p> <code>qkv_bias</code> <p>have bias in qkv layers or not. Default: False.</p> <p> TYPE: <code>bool) </code> DEFAULT: <code>False</code> </p> <code>qk_scale</code> <p>Override default qk scale of head_dim ** -0.5 if set.</p> <p> TYPE: <code>float) </code> DEFAULT: <code>None</code> </p> <code>drop_rate</code> <p>dropout rate. Default: 0.</p> <p> TYPE: <code>float) </code> DEFAULT: <code>0.0</code> </p> <code>attn_drop_rate</code> <p>attention layers dropout rate. Default: 0.</p> <p> TYPE: <code>float) </code> DEFAULT: <code>0.0</code> </p> <code>drop_path_rate</code> <p>drop path rate. Default: 0.1.</p> <p> TYPE: <code>float) </code> DEFAULT: <code>0.1</code> </p> <code>attn_stage</code> <p>block will have a attention layer if value = '1' else not. Default: '1111'.</p> <p> TYPE: <code>str) </code> DEFAULT: <code>'1111'</code> </p> <code>pos_embed</code> <p>position embedding. Default: True.</p> <p> TYPE: <code>bool) </code> DEFAULT: <code>True</code> </p> <code>spatial_conv</code> <p>block will have a spatial convolution layer if value = '1' else not. Default: '1111'.</p> <p> TYPE: <code>str) </code> DEFAULT: <code>'1111'</code> </p> <code>group</code> <p>convolution group. Default: 8.</p> <p> TYPE: <code>int) </code> DEFAULT: <code>8</code> </p> <code>pool</code> <p>if true will use global_pooling else not. Default: True.</p> <p> TYPE: <code>bool) </code> DEFAULT: <code>True</code> </p> <code>conv_init</code> <p>if true will init convolution weights else not. Default: False.</p> <p> DEFAULT: <code>False</code> </p> Source code in <code>mindcv\\models\\visformer.py</code> <pre><code>class Visformer(nn.Cell):\n    r\"\"\"Visformer model class, based on\n    '\"Visformer: The Vision-friendly Transformer\"\n    &lt;https://arxiv.org/pdf/2104.12533.pdf&gt;'\n\n    Args:\n        image_size (int) : images input size. Default: 224.\n        number the channels of the input. Default: 32.\n        num_classes (int) : number of classification classes. Default: 1000.\n        embed_dim (int) : embedding dimension in all head. Default: 384.\n        depth (int) : model block depth. Default: None.\n        num_heads (int) : number of heads. Default: None.\n        mlp_ratio (float) : ratio of hidden features in Mlp. Default: 4.\n        qkv_bias (bool) : have bias in qkv layers or not. Default: False.\n        qk_scale (float) : Override default qk scale of head_dim ** -0.5 if set.\n        drop_rate (float) : dropout rate. Default: 0.\n        attn_drop_rate (float) : attention layers dropout rate. Default: 0.\n        drop_path_rate (float) : drop path rate. Default: 0.1.\n        attn_stage (str) : block will have a attention layer if value = '1' else not. Default: '1111'.\n        pos_embed (bool) : position embedding. Default: True.\n        spatial_conv (str) : block will have a spatial convolution layer if value = '1' else not. Default: '1111'.\n        group (int) : convolution group. Default: 8.\n        pool (bool) : if true will use global_pooling else not. Default: True.\n        conv_init : if true will init convolution weights else not. Default: False.\n    \"\"\"\n\n    def __init__(\n        self,\n        img_size: int = 224,\n        init_channels: int = 32,\n        num_classes: int = 1000,\n        embed_dim: int = 384,\n        depth: List[int] = None,\n        num_heads: List[int] = None,\n        mlp_ratio: float = 4.0,\n        qkv_bias: bool = False,\n        qk_scale: float = None,\n        drop_rate: float = 0.0,\n        attn_drop_rate: float = 0.0,\n        drop_path_rate: float = 0.1,\n        attn_stage: str = \"1111\",\n        pos_embed: bool = True,\n        spatial_conv: str = \"1111\",\n        group: int = 8,\n        pool: bool = True,\n        conv_init: bool = False,\n    ) -&gt; None:\n        super(Visformer, self).__init__()\n        self.num_classes = num_classes\n        self.num_features = self.embed_dim = embed_dim\n        self.init_channels = init_channels\n        self.img_size = img_size\n        self.pool = pool\n        self.conv_init = conv_init\n        self.depth = depth\n        assert (isinstance(depth, list) or isinstance(depth, tuple)) and len(depth) == 4\n        if not (isinstance(num_heads, list) or isinstance(num_heads, tuple)):\n            num_heads = [num_heads] * 4\n\n        self.pos_embed = pos_embed\n        dpr = np.linspace(0, drop_path_rate, sum(depth)).tolist()\n\n        self.stem = nn.SequentialCell([\n            nn.Conv2d(3, self.init_channels, 7, 2, pad_mode=\"pad\", padding=3),\n            nn.BatchNorm2d(self.init_channels),\n            nn.ReLU()\n        ])\n        img_size //= 2\n\n        self.pos_drop = Dropout(p=drop_rate)\n        # stage0\n        if depth[0]:\n            self.patch_embed0 = PatchEmbed(img_size=img_size, patch_size=2, in_chans=self.init_channels,\n                                           embed_dim=embed_dim // 4)\n            img_size //= 2\n            if self.pos_embed:\n                self.pos_embed0 = mindspore.Parameter(\n                    ops.zeros((1, embed_dim // 4, img_size, img_size), mindspore.float32))\n            self.stage0 = nn.CellList([\n                Block(dim=embed_dim // 4, num_heads=num_heads[0], head_dim_ratio=0.25, mlp_ratio=mlp_ratio,\n                      qkv_bias=qkv_bias, qk_scale=qk_scale, drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i],\n                      group=group, attn_disabled=(attn_stage[0] == \"0\"), spatial_conv=(spatial_conv[0] == \"1\"))\n                for i in range(depth[0])\n            ])\n\n        # stage1\n        if depth[0]:\n            self.patch_embed1 = PatchEmbed(img_size=img_size, patch_size=2, in_chans=embed_dim // 4,\n                                           embed_dim=embed_dim // 2)\n            img_size //= 2\n        else:\n            self.patch_embed1 = PatchEmbed(img_size=img_size, patch_size=4, in_chans=self.init_channels,\n                                           embed_dim=embed_dim // 2)\n            img_size //= 4\n\n        if self.pos_embed:\n            self.pos_embed1 = mindspore.Parameter(ops.zeros((1, embed_dim // 2, img_size, img_size), mindspore.float32))\n\n        self.stage1 = nn.CellList([\n            Block(\n                dim=embed_dim // 2, num_heads=num_heads[1], head_dim_ratio=0.5, mlp_ratio=mlp_ratio,\n                qkv_bias=qkv_bias, qk_scale=qk_scale, drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i],\n                group=group, attn_disabled=(attn_stage[1] == \"0\"), spatial_conv=(spatial_conv[1] == \"1\")\n            )\n            for i in range(sum(depth[:1]), sum(depth[:2]))\n        ])\n\n        # stage2\n        self.patch_embed2 = PatchEmbed(img_size=img_size, patch_size=2, in_chans=embed_dim // 2, embed_dim=embed_dim)\n        img_size //= 2\n        if self.pos_embed:\n            self.pos_embed2 = mindspore.Parameter(ops.zeros((1, embed_dim, img_size, img_size), mindspore.float32))\n        self.stage2 = nn.CellList([\n            Block(\n                dim=embed_dim, num_heads=num_heads[2], head_dim_ratio=1.0, mlp_ratio=mlp_ratio,\n                qkv_bias=qkv_bias, qk_scale=qk_scale, drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i],\n                group=group, attn_disabled=(attn_stage[2] == \"0\"), spatial_conv=(spatial_conv[2] == \"1\")\n            )\n            for i in range(sum(depth[:2]), sum(depth[:3]))\n        ])\n\n        # stage3\n        self.patch_embed3 = PatchEmbed(img_size=img_size, patch_size=2, in_chans=embed_dim, embed_dim=embed_dim * 2)\n        img_size //= 2\n        if self.pos_embed:\n            self.pos_embed3 = mindspore.Parameter(ops.zeros((1, embed_dim * 2, img_size, img_size), mindspore.float32))\n        self.stage3 = nn.CellList([\n            Block(\n                dim=embed_dim * 2, num_heads=num_heads[3], head_dim_ratio=1.0, mlp_ratio=mlp_ratio,\n                qkv_bias=qkv_bias, qk_scale=qk_scale, drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i],\n                group=group, attn_disabled=(attn_stage[3] == \"0\"), spatial_conv=(spatial_conv[3] == \"1\")\n            )\n            for i in range(sum(depth[:3]), sum(depth[:4]))\n        ])\n\n        # head\n        if self.pool:\n            self.global_pooling = GlobalAvgPooling()\n\n        self.norm = nn.BatchNorm2d(embed_dim * 2)\n        self.head = nn.Dense(embed_dim * 2, num_classes)\n\n        # weight init\n        if self.pos_embed:\n            if depth[0]:\n                self.pos_embed0.set_data(initializer(TruncatedNormal(0.02),\n                                                     self.pos_embed0.shape, self.pos_embed0.dtype))\n            self.pos_embed1.set_data(initializer(TruncatedNormal(0.02),\n                                                 self.pos_embed1.shape, self.pos_embed1.dtype))\n            self.pos_embed2.set_data(initializer(TruncatedNormal(0.02),\n                                                 self.pos_embed2.shape, self.pos_embed2.dtype))\n            self.pos_embed3.set_data(initializer(TruncatedNormal(0.02),\n                                                 self.pos_embed3.shape, self.pos_embed3.dtype))\n        self._initialize_weights()\n\n    def _initialize_weights(self) -&gt; None:\n        for _, cell in self.cells_and_names():\n            if isinstance(cell, nn.Dense):\n                cell.weight.set_data(initializer(TruncatedNormal(0.02), cell.weight.shape, cell.weight.dtype))\n                if cell.bias is not None:\n                    cell.bias.set_data(initializer(Constant(0), cell.bias.shape, cell.bias.dtype))\n            elif isinstance(cell, nn.LayerNorm):\n                cell.beta.set_data(initializer(Constant(0), cell.beta.shape, cell.beta.dtype))\n                cell.gamma.set_data(initializer(Constant(1), cell.gamma.shape, cell.gamma.dtype))\n            elif isinstance(cell, nn.BatchNorm2d):\n                cell.beta.set_data(initializer(Constant(0), cell.beta.shape, cell.beta.dtype))\n                cell.gamma.set_data(initializer(Constant(1), cell.gamma.shape, cell.gamma.dtype))\n            elif isinstance(cell, nn.Conv2d):\n                if self.conv_init:\n                    cell.weight.set_data(initializer(HeNormal(mode=\"fan_out\", nonlinearity=\"relu\"), cell.weight.shape,\n                                                     cell.weight.dtype))\n                else:\n                    cell.weight.set_data(initializer(TruncatedNormal(0.02), cell.weight.shape, cell.weight.dtype))\n                if cell.bias is not None:\n                    cell.bias.set_data(initializer(Constant(0), cell.bias.shape, cell.bias.dtype))\n\n    def forward_features(self, x: Tensor) -&gt; Tensor:\n        x = self.stem(x)\n\n        # stage 0\n        if self.depth[0]:\n            x = self.patch_embed0(x)\n            if self.pos_embed:\n                x = x + self.pos_embed0\n                x = self.pos_drop(x)\n            for b in self.stage0:\n                x = b(x)\n\n        # stage 1\n        x = self.patch_embed1(x)\n        if self.pos_embed:\n            x = x + self.pos_embed1\n            x = self.pos_drop(x)\n        for b in self.stage1:\n            x = b(x)\n\n        # stage 2\n        x = self.patch_embed2(x)\n        if self.pos_embed:\n            x = x + self.pos_embed2\n            x = self.pos_drop(x)\n        for b in self.stage2:\n            x = b(x)\n\n        # stage 3\n        x = self.patch_embed3(x)\n        if self.pos_embed:\n            x = x + self.pos_embed3\n            x = self.pos_drop(x)\n        for b in self.stage3:\n            x = b(x)\n        x = self.norm(x)\n        return x\n\n    def forward_head(self, x: Tensor) -&gt; Tensor:\n        # head\n        if self.pool:\n            x = self.global_pooling(x)\n        else:\n            x = x[:, :, 0, 0]\n        x = self.head(x.view(x.shape[0], -1))\n        return x\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n</code></pre>"},{"location":"reference/models/#mindcv.models.visformer.visformer_small","title":"<code>mindcv.models.visformer.visformer_small(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get visformer small model. Refer to the base class 'models.visformer' for more details.</p> Source code in <code>mindcv\\models\\visformer.py</code> <pre><code>@register_model\ndef visformer_small(pretrained: bool = False, num_classes: int = 1000, in_channels: int = 3, **kwargs):\n    \"\"\"Get visformer small model.\n    Refer to the base class 'models.visformer' for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"visformer_small\"]\n    model = Visformer(img_size=224, init_channels=32, num_classes=num_classes, embed_dim=384,\n                      depth=[0, 7, 4, 4], num_heads=[6, 6, 6, 6], mlp_ratio=4., group=8,\n                      attn_stage=\"0011\", spatial_conv=\"1100\", conv_init=True, **kwargs)\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n    return model\n</code></pre>"},{"location":"reference/models/#mindcv.models.visformer.visformer_small_v2","title":"<code>mindcv.models.visformer.visformer_small_v2(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get visformer small2 model. Refer to the base class 'models.visformer' for more details.</p> Source code in <code>mindcv\\models\\visformer.py</code> <pre><code>@register_model\ndef visformer_small_v2(pretrained: bool = False, num_classes: int = 1000, in_channels: int = 3, **kwargs):\n    \"\"\"Get visformer small2 model.\n    Refer to the base class 'models.visformer' for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"visformer_small_v2\"]\n    model = Visformer(img_size=224, init_channels=32, num_classes=num_classes, embed_dim=256,\n                      depth=[1, 10, 14, 3], num_heads=[2, 4, 8, 16], mlp_ratio=4., qk_scale=-0.5,\n                      group=8, attn_stage=\"0011\", spatial_conv=\"1100\", conv_init=True, **kwargs)\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n    return model\n</code></pre>"},{"location":"reference/models/#mindcv.models.visformer.visformer_tiny","title":"<code>mindcv.models.visformer.visformer_tiny(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get visformer tiny model. Refer to the base class 'models.visformer' for more details.</p> Source code in <code>mindcv\\models\\visformer.py</code> <pre><code>@register_model\ndef visformer_tiny(pretrained: bool = False, num_classes: int = 1000, in_channels: int = 3, **kwargs):\n    \"\"\"Get visformer tiny model.\n    Refer to the base class 'models.visformer' for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"visformer_tiny\"]\n    model = Visformer(img_size=224, init_channels=16, num_classes=num_classes, embed_dim=192,\n                      depth=[0, 7, 4, 4], num_heads=[3, 3, 3, 3], mlp_ratio=4., group=8,\n                      attn_stage=\"0011\", spatial_conv=\"1100\", drop_path_rate=0.03, conv_init=True, **kwargs)\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre>"},{"location":"reference/models/#mindcv.models.visformer.visformer_tiny_v2","title":"<code>mindcv.models.visformer.visformer_tiny_v2(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get visformer tiny2 model. Refer to the base class 'models.visformer' for more details.</p> Source code in <code>mindcv\\models\\visformer.py</code> <pre><code>@register_model\ndef visformer_tiny_v2(pretrained: bool = False, num_classes: int = 1000, in_channels: int = 3, **kwargs):\n    \"\"\"Get visformer tiny2 model.\n    Refer to the base class 'models.visformer' for more details.\n    \"\"\"\n    default_cfg = default_cfgs[\"visformer_tiny_v2\"]\n    model = Visformer(img_size=224, init_channels=24, num_classes=num_classes, embed_dim=192,\n                      depth=[1, 4, 6, 3], num_heads=[1, 3, 6, 12], mlp_ratio=4., qk_scale=-0.5, group=8,\n                      attn_stage=\"0011\", spatial_conv=\"1100\", drop_path_rate=0.03, conv_init=True, **kwargs)\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n    return model\n</code></pre>"},{"location":"reference/models/#vit","title":"vit","text":""},{"location":"reference/models/#mindcv.models.vit","title":"<code>mindcv.models.vit</code>","text":"<p>ViT</p>"},{"location":"reference/models/#mindcv.models.vit.Attention","title":"<code>mindcv.models.vit.Attention</code>","text":"<p>               Bases: <code>Cell</code></p> <p>Attention layer implementation, Rearrange Input -&gt; B x N x hidden size.</p> PARAMETER DESCRIPTION <code>dim</code> <p>The dimension of input features.</p> <p> TYPE: <code>int</code> </p> <code>num_heads</code> <p>The number of attention heads. Default: 8.</p> <p> TYPE: <code>int</code> DEFAULT: <code>8</code> </p> <code>qkv_bias</code> <p>Specifies whether the linear layer uses a bias vector. Default: True.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>qk_norm</code> <p>Specifies whether to do normalization to q and k.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>attn_drop</code> <p>The drop rate of attention, greater than 0 and less equal than 1. Default: 0.0.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>proj_drop</code> <p>The drop rate of output, greater than 0 and less equal than 1. Default: 0.0.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> RETURNS DESCRIPTION <p>Tensor, output tensor.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; ops = Attention(768, 12)\n</code></pre> Source code in <code>mindcv\\models\\vit.py</code> <pre><code>class Attention(nn.Cell):\n    \"\"\"\n    Attention layer implementation, Rearrange Input -&gt; B x N x hidden size.\n\n    Args:\n        dim (int): The dimension of input features.\n        num_heads (int): The number of attention heads. Default: 8.\n        qkv_bias (bool): Specifies whether the linear layer uses a bias vector. Default: True.\n        qk_norm (bool): Specifies whether to do normalization to q and k.\n        attn_drop (float): The drop rate of attention, greater than 0 and less equal than 1. Default: 0.0.\n        proj_drop (float): The drop rate of output, greater than 0 and less equal than 1. Default: 0.0.\n\n    Returns:\n        Tensor, output tensor.\n\n    Examples:\n        &gt;&gt;&gt; ops = Attention(768, 12)\n    \"\"\"\n    def __init__(\n        self,\n        dim: int,\n        num_heads: int = 8,\n        qkv_bias: bool = True,\n        qk_norm: bool = False,\n        attn_drop: float = 0.0,\n        proj_drop: float = 0.0,\n        norm_layer: nn.Cell = nn.LayerNorm,\n    ):\n        super(Attention, self).__init__()\n        assert dim % num_heads == 0, 'dim should be divisible by num_heads'\n        self.num_heads = num_heads\n        self.head_dim = dim // num_heads\n        self.scale = Tensor(self.head_dim ** -0.5)\n\n        self.qkv = nn.Dense(dim, dim * 3, has_bias=qkv_bias)\n        self.q_norm = norm_layer((self.head_dim,)) if qk_norm else nn.Identity()\n        self.k_norm = norm_layer((self.head_dim,)) if qk_norm else nn.Identity()\n\n        self.attn_drop = Dropout(attn_drop)\n        self.proj = nn.Dense(dim, dim)\n        self.proj_drop = Dropout(proj_drop)\n\n        self.mul = ops.Mul()\n        self.reshape = ops.Reshape()\n        self.transpose = ops.Transpose()\n        self.unstack = ops.Unstack(axis=0)\n        self.attn_matmul_v = ops.BatchMatMul()\n        self.q_matmul_k = ops.BatchMatMul(transpose_b=True)\n\n    def construct(self, x):\n        b, n, c = x.shape\n        qkv = self.qkv(x)\n        qkv = self.reshape(qkv, (b, n, 3, self.num_heads, self.head_dim))\n        qkv = self.transpose(qkv, (2, 0, 3, 1, 4))\n        q, k, v = self.unstack(qkv)\n        q, k = self.q_norm(q), self.k_norm(k)\n\n        q = self.mul(q, self.scale**0.5)\n        k = self.mul(k, self.scale**0.5)\n        attn = self.q_matmul_k(q, k)\n\n        attn = ops.softmax(attn.astype(ms.float32), axis=-1).astype(attn.dtype)\n        attn = self.attn_drop(attn)\n\n        out = self.attn_matmul_v(attn, v)\n        out = self.transpose(out, (0, 2, 1, 3))\n        out = self.reshape(out, (b, n, c))\n        out = self.proj(out)\n        out = self.proj_drop(out)\n\n        return out\n</code></pre>"},{"location":"reference/models/#mindcv.models.vit.Block","title":"<code>mindcv.models.vit.Block</code>","text":"<p>               Bases: <code>Cell</code></p> <p>Transformer block implementation.</p> PARAMETER DESCRIPTION <code>dim</code> <p>The dimension of embedding.</p> <p> TYPE: <code>int</code> </p> <code>num_heads</code> <p>The number of attention heads.</p> <p> TYPE: <code>int</code> DEFAULT: <code>8</code> </p> <code>qkv_bias</code> <p>Specifies whether the linear layer uses a bias vector. Default: True.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>attn_drop</code> <p>The drop rate of attention, greater than 0 and less equal than 1. Default: 0.0.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>proj_drop</code> <p>The drop rate of dense layer output, greater than 0 and less equal than 1. Default: 0.0.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>mlp_ratio</code> <p>The ratio used to scale the input dimensions to obtain the dimensions of the hidden layer.</p> <p> TYPE: <code>float</code> DEFAULT: <code>4.0</code> </p> <code>drop_path</code> <p>The drop rate for drop path. Default: 0.0.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>act_layer</code> <p>Activation function which will be stacked on top of the normalization layer (if not None), otherwise on top of the conv layer. Default: nn.GELU.</p> <p> TYPE: <code>Cell</code> DEFAULT: <code>GELU</code> </p> <code>norm_layer</code> <p>Norm layer that will be stacked on top of the convolution layer. Default: nn.LayerNorm.</p> <p> TYPE: <code>Cell</code> DEFAULT: <code>LayerNorm</code> </p> RETURNS DESCRIPTION <p>Tensor, output tensor.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; ops = TransformerEncoder(768, 12, 12, 3072)\n</code></pre> Source code in <code>mindcv\\models\\vit.py</code> <pre><code>class Block(nn.Cell):\n    \"\"\"\n    Transformer block implementation.\n\n    Args:\n        dim (int): The dimension of embedding.\n        num_heads (int): The number of attention heads.\n        qkv_bias (bool): Specifies whether the linear layer uses a bias vector. Default: True.\n        attn_drop (float): The drop rate of attention, greater than 0 and less equal than 1. Default: 0.0.\n        proj_drop (float): The drop rate of dense layer output, greater than 0 and less equal than 1. Default: 0.0.\n        mlp_ratio (float): The ratio used to scale the input dimensions to obtain the dimensions of the hidden layer.\n        drop_path (float): The drop rate for drop path. Default: 0.0.\n        act_layer (nn.Cell): Activation function which will be stacked on top of the\n            normalization layer (if not None), otherwise on top of the conv layer. Default: nn.GELU.\n        norm_layer (nn.Cell): Norm layer that will be stacked on top of the convolution\n            layer. Default: nn.LayerNorm.\n\n    Returns:\n        Tensor, output tensor.\n\n    Examples:\n        &gt;&gt;&gt; ops = TransformerEncoder(768, 12, 12, 3072)\n    \"\"\"\n    def __init__(\n        self,\n        dim: int,\n        num_heads: int = 8,\n        mlp_ratio: float = 4.,\n        qkv_bias: bool = False,\n        qk_norm: bool = False,\n        proj_drop: float = 0.,\n        attn_drop: float = 0.,\n        init_values: Optional[float] = None,\n        drop_path: float = 0.,\n        act_layer: nn.Cell = nn.GELU,\n        norm_layer: nn.Cell = nn.LayerNorm,\n        mlp_layer: Callable = Mlp,\n    ):\n        super(Block, self).__init__()\n        self.norm1 = norm_layer((dim,))\n        self.attn = Attention(\n            dim=dim,\n            num_heads=num_heads,\n            qkv_bias=qkv_bias,\n            qk_norm=qk_norm,\n            attn_drop=attn_drop,\n            proj_drop=proj_drop,\n            norm_layer=norm_layer,\n        )\n        self.ls1 = LayerScale(dim=dim, init_values=init_values) if init_values else nn.Identity()\n        self.drop_path1 = DropPath(drop_path) if drop_path &gt; 0. else nn.Identity()\n\n        self.norm2 = norm_layer((dim,))\n        self.mlp = mlp_layer(\n            in_features=dim,\n            hidden_features=int(dim * mlp_ratio),\n            act_layer=act_layer,\n            drop=proj_drop\n        )\n        self.ls2 = LayerScale(dim=dim, init_values=init_values) if init_values else nn.Identity()\n        self.drop_path2 = DropPath(drop_path) if drop_path &gt; 0. else nn.Identity()\n\n    def construct(self, x):\n        x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x))))\n        x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n        return x\n</code></pre>"},{"location":"reference/models/#mindcv.models.vit.LayerScale","title":"<code>mindcv.models.vit.LayerScale</code>","text":"<p>               Bases: <code>Cell</code></p> <p>Layer scale, help ViT improve the training dynamic, allowing for the training of deeper high-capacity image transformers that benefit from depth</p> PARAMETER DESCRIPTION <code>dim</code> <p>The output dimension of attnetion layer or mlp layer.</p> <p> TYPE: <code>int</code> </p> <code>init_values</code> <p>The scale factor. Default: 1e-5.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1e-05</code> </p> RETURNS DESCRIPTION <p>Tensor, output tensor.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; ops = LayerScale(768, 0.01)\n</code></pre> Source code in <code>mindcv\\models\\vit.py</code> <pre><code>class LayerScale(nn.Cell):\n    \"\"\"\n    Layer scale, help ViT improve the training dynamic, allowing for the training\n    of deeper high-capacity image transformers that benefit from depth\n\n    Args:\n        dim (int): The output dimension of attnetion layer or mlp layer.\n        init_values (float): The scale factor. Default: 1e-5.\n\n    Returns:\n        Tensor, output tensor.\n\n    Examples:\n        &gt;&gt;&gt; ops = LayerScale(768, 0.01)\n    \"\"\"\n    def __init__(\n        self,\n        dim: int,\n        init_values: float = 1e-5\n    ):\n        super(LayerScale, self).__init__()\n        self.gamma = Parameter(initializer(init_values, dim))\n\n    def construct(self, x):\n        return self.gamma * x\n</code></pre>"},{"location":"reference/models/#mindcv.models.vit.VisionTransformer","title":"<code>mindcv.models.vit.VisionTransformer</code>","text":"<p>               Bases: <code>Cell</code></p> <p>ViT encoder, which returns the feature encoded by transformer encoder.</p> Source code in <code>mindcv\\models\\vit.py</code> <pre><code>class VisionTransformer(nn.Cell):\n    '''\n    ViT encoder, which returns the feature encoded by transformer encoder.\n    '''\n    def __init__(\n        self,\n        image_size: int = 224,\n        patch_size: int = 16,\n        in_channels: int = 3,\n        global_pool: str = 'token',\n        embed_dim: int = 768,\n        depth: int = 12,\n        num_heads: int = 12,\n        mlp_ratio: float = 4.,\n        qkv_bias: bool = True,\n        qk_norm: bool = False,\n        drop_rate: float = 0.,\n        pos_drop_rate: float = 0.,\n        patch_drop_rate: float = 0.,\n        proj_drop_rate: float = 0.,\n        attn_drop_rate: float = 0.,\n        drop_path_rate: float = 0.,\n        weight_init: bool = True,\n        init_values: Optional[float] = None,\n        no_embed_class: bool = False,\n        pre_norm: bool = False,\n        fc_norm: Optional[bool] = None,\n        dynamic_img_size: bool = False,\n        dynamic_img_pad: bool = False,\n        act_layer: nn.Cell = nn.GELU,\n        embed_layer: Callable = PatchEmbed,\n        norm_layer: nn.Cell = nn.LayerNorm,\n        mlp_layer: Callable = Mlp,\n        class_token: bool = True,\n        block_fn: Callable = Block,\n        num_classes: int = 1000,\n    ):\n        super(VisionTransformer, self).__init__()\n        assert global_pool in ('', 'avg', 'token')\n        assert class_token or global_pool != 'token'\n        use_fc_norm = global_pool == 'avg' if fc_norm is None else fc_norm\n\n        self.global_pool = global_pool\n        self.num_prefix_tokens = 1 if class_token else 0\n        self.no_embed_class = no_embed_class\n        self.dynamic_img_size = dynamic_img_size\n        self.dynamic_img_pad = dynamic_img_pad\n\n        embed_args = {}\n        if dynamic_img_size:\n            # flatten deferred until after pos embed\n            embed_args.update(dict(strict_img_size=False, output_fmt='NHWC'))\n        elif dynamic_img_pad:\n            embed_args.update(dict(output_fmt='NHWC'))\n\n        self.patch_embed = embed_layer(\n            image_size=image_size,\n            patch_size=patch_size,\n            in_chans=in_channels,\n            embed_dim=embed_dim,\n            bias=not pre_norm,  # disable bias if pre-norm is used (e.g. CLIP)\n            dynamic_img_pad=dynamic_img_pad,\n            **embed_args,\n        )\n        num_patches = self.patch_embed.num_patches\n\n        self.cls_token = Parameter(initializer(TruncatedNormal(0.02), (1, 1, embed_dim))) if class_token else None\n        embed_len = num_patches if no_embed_class else num_patches + self.num_prefix_tokens\n        self.pos_embed = Parameter(initializer(TruncatedNormal(0.02), (1, embed_len, embed_dim)))\n        self.pos_drop = Dropout(pos_drop_rate)\n        if patch_drop_rate &gt; 0:\n            self.patch_drop = PatchDropout(\n                patch_drop_rate,\n                num_prefix_tokens=self.num_prefix_tokens,\n            )\n        else:\n            self.patch_drop = nn.Identity()\n\n        self.norm_pre = norm_layer((embed_dim,)) if pre_norm else nn.Identity()\n        dpr = [x.item() for x in np.linspace(0, drop_path_rate, depth)]\n        self.blocks = nn.CellList([\n            block_fn(\n                dim=embed_dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_norm=qk_norm,\n                attn_drop=attn_drop_rate, proj_drop=proj_drop_rate,\n                mlp_ratio=mlp_ratio, drop_path=dpr[i], init_values=init_values,\n                act_layer=act_layer, norm_layer=norm_layer, mlp_layer=mlp_layer,\n            ) for i in range(depth)\n        ])\n\n        self.norm = norm_layer((embed_dim,)) if not use_fc_norm else nn.Identity()\n        self.fc_norm = norm_layer((embed_dim,)) if use_fc_norm else nn.Identity()\n        self.head_drop = Dropout(drop_rate)\n        self.head = nn.Dense(embed_dim, num_classes) if num_classes &gt; 0 else nn.Identity()\n\n        if weight_init:\n            self._init_weights()\n\n    def get_num_layers(self):\n        return len(self.blocks)\n\n    def _init_weights(self):\n        w = self.patch_embed.proj.weight\n        w_shape_flatted = (w.shape[0], functools.reduce(lambda x, y: x*y, w.shape[1:]))\n        w_value = initializer(XavierUniform(), w_shape_flatted, w.dtype)\n        w_value.init_data()\n        w.set_data(w_value.reshape(w.shape))\n        for _, cell in self.cells_and_names():\n            if isinstance(cell, nn.Dense):\n                cell.weight.set_data(\n                    initializer(XavierUniform(), cell.weight.shape, cell.weight.dtype)\n                )\n                if cell.bias is not None:\n                    cell.bias.set_data(\n                        initializer('zeros', cell.bias.shape, cell.bias.dtype)\n                    )\n            elif isinstance(cell, nn.LayerNorm):\n                cell.gamma.set_data(\n                    initializer('ones', cell.gamma.shape, cell.gamma.dtype)\n                )\n                cell.beta.set_data(\n                    initializer('zeros', cell.beta.shape, cell.beta.dtype)\n                )\n\n    def _pos_embed(self, x):\n        if self.dynamic_img_size or self.dynamic_img_pad:\n            # bhwc format\n            B, H, W, C = x.shape\n            pos_embed = resample_abs_pos_embed(\n                self.pos_embed,\n                (H, W),\n                num_prefix_tokens=0 if self.no_embed_class else self.num_prefix_tokens,\n            )\n            x = ops.reshape(x, (B, -1, C))\n        else:\n            pos_embed = self.pos_embed\n\n        if self.no_embed_class:\n            # deit-3, updated JAX (big vision)\n            # position embedding does not overlap with class token, add then concat\n            x = x + pos_embed\n            if self.cls_token is not None:\n                cls_tokens = ops.broadcast_to(self.cls_token, (x.shape[0], -1, -1))\n                cls_tokens = cls_tokens.astype(x.dtype)\n                x = ops.concat((cls_tokens, x), axis=1)\n        else:\n            # original timm, JAX, and deit vit impl\n            # pos_embed has entry for class token, concat then add\n            if self.cls_token is not None:\n                cls_tokens = ops.broadcast_to(self.cls_token, (x.shape[0], -1, -1))\n                cls_tokens = cls_tokens.astype(x.dtype)\n                x = ops.concat((cls_tokens, x), axis=1)\n            x = x + pos_embed\n\n        return self.pos_drop(x)\n\n    def forward_features(self, x):\n        x = self.patch_embed(x)\n        x = self._pos_embed(x)\n        x = self.patch_drop(x)\n        x = self.norm_pre(x)\n        for blk in self.blocks:\n            x = blk(x)\n        x = self.norm(x)\n        return x\n\n    def forward_head(self, x):\n        if self.global_pool:\n            x = x[:, self.num_prefix_tokens:].mean(axis=1) if self.global_pool == 'avg' else x[:, 0]\n        x = self.fc_norm(x)\n        x = self.head_drop(x)\n        x = self.head(x)\n        return x\n\n    def construct(self, x):\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n</code></pre>"},{"location":"reference/models/#volo","title":"volo","text":""},{"location":"reference/models/#mindcv.models.volo","title":"<code>mindcv.models.volo</code>","text":"<p>Vision OutLOoker (VOLO) implementation Modified from timm/models/vision_transformer.py</p>"},{"location":"reference/models/#mindcv.models.volo.Attention","title":"<code>mindcv.models.volo.Attention</code>","text":"<p>               Bases: <code>Cell</code></p> <p>Implementation of self-attention</p> Source code in <code>mindcv\\models\\volo.py</code> <pre><code>class Attention(nn.Cell):\n    \"Implementation of self-attention\"\n\n    def __init__(\n        self,\n        dim,\n        num_heads=8,\n        qkv_bias=False,\n        qk_scale=None,\n        attn_drop=0.0,\n        proj_drop=0.0,\n    ) -&gt; None:\n        super().__init__()\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        self.scale = qk_scale or head_dim**-0.5\n\n        self.qkv = nn.Dense(dim, dim * 3, has_bias=qkv_bias)\n        self.attn_drop = Dropout(p=attn_drop)\n        self.proj = nn.Dense(dim, dim)\n        self.proj_drop = Dropout(p=proj_drop)\n        self.softmax = nn.Softmax(axis=-1)\n        self.batch_mat_mul_transpose = ops.BatchMatMul(transpose_b=True)\n        self.batch_mat_mul = ops.BatchMatMul()\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        B, H, W, C = x.shape\n\n        qkv = self.qkv(x)\n        qkv = ops.reshape(qkv, (B, H * W, 3, self.num_heads, C // self.num_heads))\n        qkv = ops.transpose(qkv, (2, 0, 3, 1, 4))\n        q, k, v = qkv[0], qkv[1], qkv[2]\n\n        attn = self.batch_mat_mul_transpose(q, k) * self.scale\n        attn = self.softmax(attn)\n        attn = self.attn_drop(attn)\n\n        x = ops.transpose(self.batch_mat_mul(attn, v), (0, 2, 1, 3))\n        x = ops.reshape(x, (B, H, W, C))\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n</code></pre>"},{"location":"reference/models/#mindcv.models.volo.ClassAttention","title":"<code>mindcv.models.volo.ClassAttention</code>","text":"<p>               Bases: <code>Cell</code></p> <p>Class attention layer from CaiT, see details in CaiT Class attention is the post stage in our VOLO, which is optional.</p> Source code in <code>mindcv\\models\\volo.py</code> <pre><code>class ClassAttention(nn.Cell):\n    \"\"\"\n    Class attention layer from CaiT, see details in CaiT\n    Class attention is the post stage in our VOLO, which is optional.\n    \"\"\"\n    def __init__(\n        self,\n        dim,\n        num_heads=8,\n        head_dim=None,\n        qkv_bias=False,\n        qk_scale=None,\n        attn_drop=0.0,\n        proj_drop=0.0,\n    ) -&gt; None:\n        super().__init__()\n        self.num_heads = num_heads\n        if head_dim is not None:\n            self.head_dim = head_dim\n        else:\n            head_dim = dim // num_heads\n            self.head_dim = head_dim\n        self.scale = qk_scale or head_dim**-0.5\n\n        self.kv = nn.Dense(dim, self.head_dim * self.num_heads * 2, has_bias=qkv_bias)\n        self.q = nn.Dense(dim, self.head_dim * self.num_heads, has_bias=qkv_bias)\n        self.attn_drop = Dropout(p=attn_drop)\n        self.proj = nn.Dense(self.head_dim * self.num_heads, dim)\n        self.proj_drop = Dropout(p=proj_drop)\n        self.batch_mat_mul_transpose = ops.BatchMatMul(transpose_b=True)\n        self.batch_mat_mul = ops.BatchMatMul()\n        self.softmax = nn.Softmax(axis=-1)\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        B, N, C = x.shape\n\n        kv = self.kv(x)\n        kv = ops.reshape(kv, (B, N, 2, self.num_heads,\n                         self.head_dim))\n        kv = ops.transpose(kv, (2, 0, 3, 1, 4))\n        k, v = kv[0], kv[1]\n        q = self.q(x[:, :1, :])\n        q = ops.reshape(q, (B, self.num_heads, 1, self.head_dim))\n        attn = self.batch_mat_mul_transpose(q * self.scale, k)\n        attn = self.softmax(attn)\n        attn = self.attn_drop(attn)\n\n        cls_embed = ops.transpose(self.batch_mat_mul(attn, v), (0, 2, 1, 3))\n        cls_embed = ops.reshape(cls_embed, (B, 1, self.head_dim * self.num_heads))\n        cls_embed = self.proj(cls_embed)\n        cls_embed = self.proj_drop(cls_embed)\n        return cls_embed\n</code></pre>"},{"location":"reference/models/#mindcv.models.volo.ClassBlock","title":"<code>mindcv.models.volo.ClassBlock</code>","text":"<p>               Bases: <code>Cell</code></p> <p>Class attention block from CaiT, see details in CaiT We use two-layers class attention in our VOLO, which is optional.</p> Source code in <code>mindcv\\models\\volo.py</code> <pre><code>class ClassBlock(nn.Cell):\n    \"\"\"\n    Class attention block from CaiT, see details in CaiT\n    We use two-layers class attention in our VOLO, which is optional.\n    \"\"\"\n\n    def __init__(\n        self,\n        dim,\n        num_heads,\n        head_dim=None,\n        mlp_ratio=4.,\n        qkv_bias=False,\n        qk_scale=None,\n        drop=0.0,\n        attn_drop=0.0,\n        drop_path=0.0,\n        act_layer=nn.GELU,\n        norm_layer=nn.LayerNorm,\n    ) -&gt; None:\n        super().__init__()\n        self.norm1 = norm_layer([dim])\n        self.attn = ClassAttention(\n            dim, num_heads=num_heads, head_dim=head_dim, qkv_bias=qkv_bias,\n            qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n        # NOTE: drop path for stochastic depth\n        self.drop_path = DropPath(\n            drop_path) if drop_path &gt; 0.0 else Identity()\n        self.norm2 = norm_layer([dim])\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = Mlp(in_features=dim,\n                       hidden_features=mlp_hidden_dim,\n                       act_layer=act_layer,\n                       drop=drop)\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        cls_embed = x[:, :1]\n        cls_embed = cls_embed + self.drop_path(self.attn(self.norm1(x)))\n        cls_embed = cls_embed + self.drop_path(self.mlp(self.norm2(cls_embed)))\n        x = ops.concat([cls_embed, x[:, 1:]], 1)\n        return x\n</code></pre>"},{"location":"reference/models/#mindcv.models.volo.Downsample","title":"<code>mindcv.models.volo.Downsample</code>","text":"<p>               Bases: <code>Cell</code></p> <p>Image to Patch Embedding, downsampling between stage1 and stage2</p> Source code in <code>mindcv\\models\\volo.py</code> <pre><code>class Downsample(nn.Cell):\n    \"\"\"\n    Image to Patch Embedding, downsampling between stage1 and stage2\n    \"\"\"\n    def __init__(self, in_embed_dim, out_embed_dim, patch_size,) -&gt; None:\n        super().__init__()\n        self.proj = nn.Conv2d(in_embed_dim, out_embed_dim,\n                              kernel_size=patch_size, stride=patch_size, has_bias=True)\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        x = ops.transpose(x, (0, 3, 1, 2))\n        x = self.proj(x)  # B, C, H, W\n        x = ops.transpose(x, (0, 2, 3, 1))\n        return x\n</code></pre>"},{"location":"reference/models/#mindcv.models.volo.Fold","title":"<code>mindcv.models.volo.Fold</code>","text":"<p>               Bases: <code>Cell</code></p> Source code in <code>mindcv\\models\\volo.py</code> <pre><code>class Fold(nn.Cell):\n    def __init__(self, channels, output_size, kernel_size, dilation=1, padding=0, stride=1) -&gt; None:\n        \"\"\"Alternative implementation of fold layer via transposed convolution.\n        All parameters are same as `\"torch.nn.Fold\" &lt;https://pytorch.org/docs/stable/generated/torch.nn.Fold.html&gt;`_,\n        except for the additional `channels` parameter. We need `channels` to calculate the pre-allocated memory\n        size of the convolution kernel.\n        :param channels: same as the `C` in the document of `\"torch.nn.Fold\"\n                         &lt;https://pytorch.org/docs/stable/generated/torch.nn.Fold.html&gt;`_\n        :type channels: int\n        \"\"\"\n        super().__init__()\n\n        def int2tuple(a):\n            if isinstance(a, int):\n                return (a, a)\n            return a\n        self.output_size, self.kernel_size, self.dilation, self.padding, self.stride = map(\n                                    int2tuple, (output_size, kernel_size, dilation, padding, stride))\n        self.h = int((self.output_size[0] + 2 * self.padding[0] - self.dilation[0] * (self.kernel_size[0] - 1) - 1)\n                     / self.stride[0] + 1)\n        self.w = int((self.output_size[1] + 2 * self.padding[1] - self.dilation[1] * (self.kernel_size[1] - 1) - 1)\n                     / self.stride[1] + 1)\n        self.k = self.kernel_size[0] * self.kernel_size[1]\n        self.c = channels\n        self.ck = self.c * self.k\n        init_weight = np.zeros((self.ck, 1, self.kernel_size[0], self.kernel_size[1]))\n        for i in range(self.ck):\n            xy = i % self.k\n            x = xy // self.kernel_size[1]\n            y = xy % self.kernel_size[1]\n            init_weight[i, 0, x, y] = 1\n\n        self.weight = ms.Tensor(init_weight, ms.float16)\n        self.conv_transpose2d = ops.Conv2DTranspose(\n                                    self.ck, self.kernel_size, pad_mode=\"pad\",\n                                    pad=(self.padding[0], self.padding[0], self.padding[1], self.padding[1]),\n                                    stride=stride, dilation=dilation, group=self.c)\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        b, ck, hw = x.shape\n        # todo: assert is not allowed in construct, how to check the shape?\n        # assert ck == self.c * self.k\n        # assert l == self.h * self.w\n        # print(\"construct-b\", b, \"construct-ck\", ck, \"construct-l\", l)\n        # print(\"self.h\", self.h, \"self.w\", self.w)\n        x = ops.reshape(x, (b, ck, self.h, self.w))\n        out = self.conv_transpose2d(x, self.weight, (b, self.c, self.output_size[0], self.output_size[1]))\n\n        return out\n</code></pre>"},{"location":"reference/models/#mindcv.models.volo.Fold.__init__","title":"<code>mindcv.models.volo.Fold.__init__(channels, output_size, kernel_size, dilation=1, padding=0, stride=1)</code>","text":"<p>Alternative implementation of fold layer via transposed convolution. All parameters are same as <code>\"torch.nn.Fold\" &lt;https://pytorch.org/docs/stable/generated/torch.nn.Fold.html&gt;</code>, except for the additional <code>channels</code> parameter. We need <code>channels</code> to calculate the pre-allocated memory size of the convolution kernel. :param channels: same as the <code>C</code> in the document of <code>\"torch.nn.Fold\"                  &lt;https://pytorch.org/docs/stable/generated/torch.nn.Fold.html&gt;</code> :type channels: int</p> Source code in <code>mindcv\\models\\volo.py</code> <pre><code>def __init__(self, channels, output_size, kernel_size, dilation=1, padding=0, stride=1) -&gt; None:\n    \"\"\"Alternative implementation of fold layer via transposed convolution.\n    All parameters are same as `\"torch.nn.Fold\" &lt;https://pytorch.org/docs/stable/generated/torch.nn.Fold.html&gt;`_,\n    except for the additional `channels` parameter. We need `channels` to calculate the pre-allocated memory\n    size of the convolution kernel.\n    :param channels: same as the `C` in the document of `\"torch.nn.Fold\"\n                     &lt;https://pytorch.org/docs/stable/generated/torch.nn.Fold.html&gt;`_\n    :type channels: int\n    \"\"\"\n    super().__init__()\n\n    def int2tuple(a):\n        if isinstance(a, int):\n            return (a, a)\n        return a\n    self.output_size, self.kernel_size, self.dilation, self.padding, self.stride = map(\n                                int2tuple, (output_size, kernel_size, dilation, padding, stride))\n    self.h = int((self.output_size[0] + 2 * self.padding[0] - self.dilation[0] * (self.kernel_size[0] - 1) - 1)\n                 / self.stride[0] + 1)\n    self.w = int((self.output_size[1] + 2 * self.padding[1] - self.dilation[1] * (self.kernel_size[1] - 1) - 1)\n                 / self.stride[1] + 1)\n    self.k = self.kernel_size[0] * self.kernel_size[1]\n    self.c = channels\n    self.ck = self.c * self.k\n    init_weight = np.zeros((self.ck, 1, self.kernel_size[0], self.kernel_size[1]))\n    for i in range(self.ck):\n        xy = i % self.k\n        x = xy // self.kernel_size[1]\n        y = xy % self.kernel_size[1]\n        init_weight[i, 0, x, y] = 1\n\n    self.weight = ms.Tensor(init_weight, ms.float16)\n    self.conv_transpose2d = ops.Conv2DTranspose(\n                                self.ck, self.kernel_size, pad_mode=\"pad\",\n                                pad=(self.padding[0], self.padding[0], self.padding[1], self.padding[1]),\n                                stride=stride, dilation=dilation, group=self.c)\n</code></pre>"},{"location":"reference/models/#mindcv.models.volo.Mlp","title":"<code>mindcv.models.volo.Mlp</code>","text":"<p>               Bases: <code>Cell</code></p> <p>Implementation of MLP</p> Source code in <code>mindcv\\models\\volo.py</code> <pre><code>class Mlp(nn.Cell):\n    \"Implementation of MLP\"\n\n    def __init__(\n        self,\n        in_features,\n        hidden_features=None,\n        out_features=None,\n        act_layer=nn.GELU,\n        drop=0.0,\n    ) -&gt; None:\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Dense(in_features, hidden_features)\n        self.act = act_layer()\n        self.fc2 = nn.Dense(hidden_features, out_features)\n        self.drop = Dropout(p=drop)\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.drop(x)\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x\n</code></pre>"},{"location":"reference/models/#mindcv.models.volo.OutlookAttention","title":"<code>mindcv.models.volo.OutlookAttention</code>","text":"<p>               Bases: <code>Cell</code></p> <p>Implementation of outlook attention --dim: hidden dim --num_heads: number of heads --kernel_size: kernel size in each window for outlook attention return: token features after outlook attention</p> Source code in <code>mindcv\\models\\volo.py</code> <pre><code>class OutlookAttention(nn.Cell):\n    \"\"\"\n    Implementation of outlook attention\n    --dim: hidden dim\n    --num_heads: number of heads\n    --kernel_size: kernel size in each window for outlook attention\n    return: token features after outlook attention\n    \"\"\"\n\n    def __init__(\n        self,\n        dim,\n        num_heads,\n        kernel_size=3,\n        padding=1,\n        stride=1,\n        qkv_bias=False,\n        qk_scale=None,\n        attn_drop=0.0,\n        proj_drop=0.0,\n    ) -&gt; None:\n        super().__init__()\n        head_dim = dim // num_heads\n        self.num_heads = num_heads\n        self.kernel_size = kernel_size\n        self.padding = padding\n        self.stride = stride\n        self.scale = qk_scale or head_dim**-0.5\n\n        self.v = nn.Dense(dim, dim, has_bias=qkv_bias)\n        self.attn = nn.Dense(dim, kernel_size**4 * num_heads)\n\n        self.attn_drop = Dropout(p=attn_drop)\n        self.proj = nn.Dense(dim, dim)\n        self.proj_drop = Dropout(p=proj_drop)\n\n        self.unfold = nn.Unfold(ksizes=[1, kernel_size, kernel_size, 1], strides=[1, stride, stride, 1],\n                                rates=[1, 1, 1, 1])\n        self.pool = nn.AvgPool2d(kernel_size=stride, stride=stride)\n        self.softmax = nn.Softmax(axis=-1)\n        self.batch_mat_mul = ops.BatchMatMul()\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        B, H, W, C = x.shape\n\n        v = ops.transpose(self.v(x), (0, 3, 1, 2))  # B, C, H, W\n\n        h = int((H - 1) / self.stride + 1)\n        w = int((W - 1) / self.stride + 1)\n        v = ops.pad(v, ((0, 0), (0, 0), (1, 1), (1, 1)))\n        v = self.unfold(v)\n        v = ops.reshape(v, (B, self.num_heads, C // self.num_heads, self.kernel_size * self.kernel_size, h * w))\n        v = ops.transpose(v, (0, 1, 4, 3, 2))  # B,H,N,kxk,C/H\n\n        attn = self.pool(ops.transpose(x, (0, 3, 1, 2)))\n        attn = ops.transpose(attn, (0, 2, 3, 1))\n        attn = ops.reshape(self.attn(attn), (B, h * w, self.num_heads, self.kernel_size * self.kernel_size,\n                           self.kernel_size * self.kernel_size))\n        attn = ops.transpose(attn, (0, 2, 1, 3, 4))  # B,H,N,kxk,kxk\n        attn = attn * self.scale\n        attn = self.softmax(attn)\n        attn = self.attn_drop(attn)\n\n        x = ops.transpose(self.batch_mat_mul(attn, v), (0, 1, 4, 3, 2))\n        x = ops.reshape(x, (B, C * self.kernel_size * self.kernel_size, h * w))\n        fold = Fold(C, (H, W), self.kernel_size, padding=self.padding, stride=self.stride)\n        x = fold(x)\n        x = self.proj(ops.transpose(x, (0, 2, 3, 1)))\n        x = self.proj_drop(x)\n\n        return x\n</code></pre>"},{"location":"reference/models/#mindcv.models.volo.Outlooker","title":"<code>mindcv.models.volo.Outlooker</code>","text":"<p>               Bases: <code>Cell</code></p> <p>Implementation of outlooker layer: which includes outlook attention + MLP Outlooker is the first stage in our VOLO --dim: hidden dim --num_heads: number of heads --mlp_ratio: mlp ratio --kernel_size: kernel size in each window for outlook attention return: outlooker layer</p> Source code in <code>mindcv\\models\\volo.py</code> <pre><code>class Outlooker(nn.Cell):\n    \"\"\"\n    Implementation of outlooker layer: which includes outlook attention + MLP\n    Outlooker is the first stage in our VOLO\n    --dim: hidden dim\n    --num_heads: number of heads\n    --mlp_ratio: mlp ratio\n    --kernel_size: kernel size in each window for outlook attention\n    return: outlooker layer\n    \"\"\"\n    def __init__(\n        self,\n        dim,\n        kernel_size,\n        padding,\n        stride=1,\n        num_heads=1,\n        mlp_ratio=3.,\n        attn_drop=0.0,\n        drop_path=0.0,\n        act_layer=nn.GELU,\n        norm_layer=nn.LayerNorm,\n        qkv_bias=False,\n        qk_scale=None,\n    ) -&gt; None:\n        super().__init__()\n        self.norm1 = norm_layer([dim])\n        self.attn = OutlookAttention(dim, num_heads, kernel_size=kernel_size,\n                                     padding=padding, stride=stride,\n                                     qkv_bias=qkv_bias, qk_scale=qk_scale,\n                                     attn_drop=attn_drop)\n\n        self.drop_path = DropPath(\n            drop_path) if drop_path &gt; 0.0 else Identity()\n\n        self.norm2 = norm_layer([dim])\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = Mlp(in_features=dim,\n                       hidden_features=mlp_hidden_dim,\n                       act_layer=act_layer)\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        x = x + self.drop_path(self.attn(self.norm1(x)))\n        x = x + self.drop_path(self.mlp(self.norm2(x)))\n        return x\n</code></pre>"},{"location":"reference/models/#mindcv.models.volo.PatchEmbed","title":"<code>mindcv.models.volo.PatchEmbed</code>","text":"<p>               Bases: <code>Cell</code></p> <p>Image to Patch Embedding. Different with ViT use 1 conv layer, we use 4 conv layers to do patch embedding</p> Source code in <code>mindcv\\models\\volo.py</code> <pre><code>class PatchEmbed(nn.Cell):\n    \"\"\"\n    Image to Patch Embedding.\n    Different with ViT use 1 conv layer, we use 4 conv layers to do patch embedding\n    \"\"\"\n\n    def __init__(\n        self,\n        img_size=224,\n        stem_conv=False,\n        stem_stride=1,\n        patch_size=8,\n        in_channels=3,\n        hidden_dim=64,\n        embed_dim=384,\n    ) -&gt; None:\n        super().__init__()\n        assert patch_size in [4, 8, 16]\n\n        self.stem_conv = stem_conv\n        if stem_conv:\n            self.conv = nn.SequentialCell(\n                nn.Conv2d(in_channels, hidden_dim, 7, stem_stride,\n                          pad_mode='pad', padding=3),  # 112x112\n                nn.BatchNorm2d(hidden_dim),\n                nn.ReLU(),\n                nn.Conv2d(hidden_dim, hidden_dim, 3, 1,\n                          pad_mode='pad', padding=1),  # 112x112\n                nn.BatchNorm2d(hidden_dim),\n                nn.ReLU(),\n                nn.Conv2d(hidden_dim, hidden_dim, 3, 1,\n                          pad_mode='pad', padding=1),  # 112x112\n                nn.BatchNorm2d(hidden_dim),\n                nn.ReLU(),\n            )\n\n        self.proj = nn.Conv2d(hidden_dim,\n                              embed_dim,\n                              kernel_size=patch_size // stem_stride,\n                              stride=patch_size // stem_stride, has_bias=True)\n        self.num_patches = (img_size // patch_size) * (img_size // patch_size)\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        if self.stem_conv:\n            x = self.conv(x)\n        x = self.proj(x)  # B, C, H, W\n        return x\n</code></pre>"},{"location":"reference/models/#mindcv.models.volo.Transformer","title":"<code>mindcv.models.volo.Transformer</code>","text":"<p>               Bases: <code>Cell</code></p> <p>Implementation of Transformer, Transformer is the second stage in our VOLO</p> Source code in <code>mindcv\\models\\volo.py</code> <pre><code>class Transformer(nn.Cell):\n    \"\"\"\n    Implementation of Transformer,\n    Transformer is the second stage in our VOLO\n    \"\"\"\n    def __init__(\n        self,\n        dim,\n        num_heads,\n        mlp_ratio=4.,\n        qkv_bias=False,\n        qk_scale=None,\n        attn_drop=0.0,\n        drop_path=0.0,\n        act_layer=nn.GELU,\n        norm_layer=nn.LayerNorm,\n    ) -&gt; None:\n        super().__init__()\n        self.norm1 = norm_layer([dim])\n        self.attn = Attention(dim, num_heads=num_heads, qkv_bias=qkv_bias,\n                              qk_scale=qk_scale, attn_drop=attn_drop)\n\n        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n        self.drop_path = DropPath(\n            drop_path) if drop_path &gt; 0.0 else Identity()\n\n        self.norm2 = norm_layer([dim])\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = Mlp(in_features=dim,\n                       hidden_features=mlp_hidden_dim,\n                       act_layer=act_layer)\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        x = x + self.drop_path(self.attn(self.norm1(x)))\n        x = x + self.drop_path(self.mlp(self.norm2(x)))\n        return x\n</code></pre>"},{"location":"reference/models/#mindcv.models.volo.VOLO","title":"<code>mindcv.models.volo.VOLO</code>","text":"<p>               Bases: <code>Cell</code></p> <p>Vision Outlooker, the main class of our model --layers: [x,x,x,x], four blocks in two stages, the first block is outlooker, the           other three are transformer, we set four blocks, which are easily           applied to downstream tasks --img_size, --in_channels, --num_classes: these three are very easy to understand --patch_size: patch_size in outlook attention --stem_hidden_dim: hidden dim of patch embedding, d1-d4 is 64, d5 is 128 --embed_dims, --num_heads: embedding dim, number of heads in each block --downsamples: flags to apply downsampling or not --outlook_attention: flags to apply outlook attention or not --mlp_ratios, --qkv_bias, --qk_scale, --drop_rate: easy to undertand --attn_drop_rate, --drop_path_rate, --norm_layer: easy to undertand --post_layers: post layers like two class attention layers using [ca, ca],               if yes, return_mean=False --return_mean: use mean of all feature tokens for classification, if yes, no class token --return_dense: use token labeling, details are here:                 https://github.com/zihangJiang/TokenLabeling --mix_token: mixing tokens as token labeling, details are here:                 https://github.com/zihangJiang/TokenLabeling --pooling_scale: pooling_scale=2 means we downsample 2x --out_kernel, --out_stride, --out_padding: kerner size,                                            stride, and padding for outlook attention</p> Source code in <code>mindcv\\models\\volo.py</code> <pre><code>class VOLO(nn.Cell):\n    \"\"\"\n    Vision Outlooker, the main class of our model\n    --layers: [x,x,x,x], four blocks in two stages, the first block is outlooker, the\n              other three are transformer, we set four blocks, which are easily\n              applied to downstream tasks\n    --img_size, --in_channels, --num_classes: these three are very easy to understand\n    --patch_size: patch_size in outlook attention\n    --stem_hidden_dim: hidden dim of patch embedding, d1-d4 is 64, d5 is 128\n    --embed_dims, --num_heads: embedding dim, number of heads in each block\n    --downsamples: flags to apply downsampling or not\n    --outlook_attention: flags to apply outlook attention or not\n    --mlp_ratios, --qkv_bias, --qk_scale, --drop_rate: easy to undertand\n    --attn_drop_rate, --drop_path_rate, --norm_layer: easy to undertand\n    --post_layers: post layers like two class attention layers using [ca, ca],\n                  if yes, return_mean=False\n    --return_mean: use mean of all feature tokens for classification, if yes, no class token\n    --return_dense: use token labeling, details are here:\n                    https://github.com/zihangJiang/TokenLabeling\n    --mix_token: mixing tokens as token labeling, details are here:\n                    https://github.com/zihangJiang/TokenLabeling\n    --pooling_scale: pooling_scale=2 means we downsample 2x\n    --out_kernel, --out_stride, --out_padding: kerner size,\n                                               stride, and padding for outlook attention\n    \"\"\"\n    def __init__(\n        self,\n        layers,\n        img_size=224,\n        in_channels=3,\n        num_classes=1000,\n        patch_size=8,\n        stem_hidden_dim=64,\n        embed_dims=None,\n        num_heads=None,\n        downsamples=None,\n        outlook_attention=None,\n        mlp_ratios=None,\n        qkv_bias=False,\n        qk_scale=None,\n        drop_rate=0.0,\n        attn_drop_rate=0.0,\n        drop_path_rate=0.0,\n        norm_layer=nn.LayerNorm,\n        post_layers=None,\n        return_mean=False,\n        return_dense=True,\n        mix_token=True,\n        pooling_scale=2,\n        out_kernel=3,\n        out_stride=2,\n        out_padding=1,\n    ) -&gt; None:\n\n        super().__init__()\n        self.num_classes = num_classes\n        self.patch_embed = PatchEmbed(stem_conv=True, stem_stride=2, patch_size=patch_size,\n                                      in_channels=in_channels, hidden_dim=stem_hidden_dim,\n                                      embed_dim=embed_dims[0])\n        # inital positional encoding, we add positional encoding after outlooker blocks\n        self.pos_embed = Parameter(\n            ops.zeros((1, img_size // patch_size // pooling_scale,\n                      img_size // patch_size // pooling_scale,\n                      embed_dims[-1]), mstype.float32))\n\n        self.pos_drop = Dropout(p=drop_rate)\n\n        # set the main block in network\n        network = []\n        for i in range(len(layers)):\n            if outlook_attention[i]:\n                # stage 1\n                stage = outlooker_blocks(Outlooker, i, embed_dims[i], layers,\n                                         downsample=downsamples[i], num_heads=num_heads[i],\n                                         kernel_size=out_kernel, stride=out_stride,\n                                         padding=out_padding, mlp_ratio=mlp_ratios[i],\n                                         qkv_bias=qkv_bias, qk_scale=qk_scale,\n                                         attn_drop=attn_drop_rate, norm_layer=norm_layer)\n                network.append(stage)\n            else:\n                # stage 2\n                stage = transformer_blocks(Transformer, i, embed_dims[i], layers,\n                                           num_heads[i], mlp_ratio=mlp_ratios[i],\n                                           qkv_bias=qkv_bias, qk_scale=qk_scale,\n                                           drop_path_rate=drop_path_rate,\n                                           attn_drop=attn_drop_rate,\n                                           norm_layer=norm_layer)\n                network.append(stage)\n\n            if downsamples[i]:\n                # downsampling between two stages\n                network.append(Downsample(embed_dims[i], embed_dims[i + 1], 2))\n\n        self.network = nn.CellList(network)\n\n        # set post block, for example, class attention layers\n        self.post_network = None\n        if post_layers is not None:\n            self.post_network = nn.CellList([\n                get_block(post_layers[i],\n                          dim=embed_dims[-1],\n                          num_heads=num_heads[-1],\n                          mlp_ratio=mlp_ratios[-1],\n                          qkv_bias=qkv_bias,\n                          qk_scale=qk_scale,\n                          attn_drop=attn_drop_rate,\n                          drop_path=0.0,\n                          norm_layer=norm_layer)\n                for i in range(len(post_layers))\n            ])\n            self.cls_token = Parameter(ops.zeros((1, 1, embed_dims[-1]), mstype.float32))\n            self.cls_token.set_data(init.initializer(init.TruncatedNormal(sigma=.02), self.cls_token.data.shape))\n\n        # set output type\n        self.return_mean = return_mean  # if yes, return mean, not use class token\n        self.return_dense = return_dense  # if yes, return class token and all feature tokens\n        if return_dense:\n            assert not return_mean, \"cannot return both mean and dense\"\n        self.mix_token = mix_token\n        self.pooling_scale = pooling_scale\n        if mix_token:  # enable token mixing, see token labeling for details.\n            self.beta = 1.0\n            assert return_dense, \"return all tokens if mix_token is enabled\"\n        if return_dense:\n            self.aux_head = nn.Dense(\n                embed_dims[-1],\n                num_classes) if num_classes &gt; 0 else Identity()\n        self.norm = norm_layer([embed_dims[-1]])\n\n        # Classifier head\n        self.head = nn.Dense(\n            embed_dims[-1], num_classes) if num_classes &gt; 0 else Identity()\n\n        self.pos_embed.set_data(init.initializer(init.TruncatedNormal(sigma=.02), self.pos_embed.data.shape))\n        self._init_weights()\n\n    def _init_weights(self) -&gt; None:\n        for name, m in self.cells_and_names():\n            if isinstance(m, nn.Dense):\n                m.weight.set_data(init.initializer(init.TruncatedNormal(sigma=.02), m.weight.data.shape))\n                if m.bias is not None:\n                    m.bias.set_data(init.initializer(init.Constant(0), m.bias.shape))\n            elif isinstance(m, nn.LayerNorm):\n                m.gamma.set_data(init.initializer(init.Constant(1), m.gamma.shape))\n                m.beta.set_data(init.initializer(init.Constant(0), m.beta.shape))\n\n    def forward_embeddings(self, x: Tensor) -&gt; Tensor:\n        # patch embedding\n        x = self.patch_embed(x)\n        # B,C,H,W-&gt; B,H,W,C\n        x = ops.transpose(x, (0, 2, 3, 1))\n        return x\n\n    def forward_tokens(self, x: Tensor) -&gt; Tensor:\n        for idx, block in enumerate(self.network):\n            if idx == 2:  # add positional encoding after outlooker blocks\n                x = x + self.pos_embed\n                x = self.pos_drop(x)\n            x = block(x)\n\n        B, H, W, C = x.shape\n        x = ops.reshape(x, (B, -1, C))\n        return x\n\n    def forward_cls(self, x: Tensor) -&gt; Tensor:\n        # B, N, C = x.shape\n        cls_tokens = ops.broadcast_to(self.cls_token, (x.shape[0], -1, -1))\n        x = ops.Cast()(x, cls_tokens.dtype)\n        x = ops.concat([cls_tokens, x], 1)\n        for block in self.post_network:\n            x = block(x)\n        return x\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        # step1: patch embedding\n        x = self.forward_embeddings(x)\n\n        # step2: tokens learning in the two stages\n        x = self.forward_tokens(x)\n\n        # step3: post network, apply class attention or not\n        if self.post_network is not None:\n            x = self.forward_cls(x)\n        x = self.norm(x)\n\n        if self.return_mean:  # if no class token, return mean\n            return self.head(ops.mean(x, 1))\n\n        x_cls = self.head(x[:, 0])\n        if not self.return_dense:\n            return x_cls\n\n        return x_cls\n</code></pre>"},{"location":"reference/models/#mindcv.models.volo.get_block","title":"<code>mindcv.models.volo.get_block(block_type, **kargs)</code>","text":"<p>get block by name, specifically for class attention block in here</p> Source code in <code>mindcv\\models\\volo.py</code> <pre><code>def get_block(block_type, **kargs) -&gt; ClassBlock:\n    \"\"\"\n    get block by name, specifically for class attention block in here\n    \"\"\"\n    if block_type == 'ca':\n        return ClassBlock(**kargs)\n</code></pre>"},{"location":"reference/models/#mindcv.models.volo.outlooker_blocks","title":"<code>mindcv.models.volo.outlooker_blocks(block_fn, index, dim, layers, num_heads=1, kernel_size=3, padding=1, stride=1, mlp_ratio=3.0, qkv_bias=False, qk_scale=None, attn_drop=0.0, drop_path_rate=0.0, **kwargs)</code>","text":"<p>generate outlooker layer in stage1 return: outlooker layers</p> Source code in <code>mindcv\\models\\volo.py</code> <pre><code>def outlooker_blocks(block_fn, index, dim, layers, num_heads=1, kernel_size=3,\n                     padding=1, stride=1, mlp_ratio=3., qkv_bias=False, qk_scale=None,\n                     attn_drop=0.0, drop_path_rate=0.0, **kwargs) -&gt; nn.SequentialCell:\n    \"\"\"\n    generate outlooker layer in stage1\n    return: outlooker layers\n    \"\"\"\n    blocks = []\n    for block_idx in range(layers[index]):\n        block_dpr = drop_path_rate * (block_idx +\n                                      sum(layers[:index])) / (sum(layers) - 1)\n        blocks.append(block_fn(dim, kernel_size=kernel_size, padding=padding,\n                               stride=stride, num_heads=num_heads, mlp_ratio=mlp_ratio,\n                               qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop,\n                               drop_path=block_dpr))\n\n    blocks = nn.SequentialCell(*blocks)\n\n    return blocks\n</code></pre>"},{"location":"reference/models/#mindcv.models.volo.transformer_blocks","title":"<code>mindcv.models.volo.transformer_blocks(block_fn, index, dim, layers, num_heads, mlp_ratio=3.0, qkv_bias=False, qk_scale=None, attn_drop=0, drop_path_rate=0.0, **kwargs)</code>","text":"<p>generate transformer layers in stage2 return: transformer layers</p> Source code in <code>mindcv\\models\\volo.py</code> <pre><code>def transformer_blocks(block_fn, index, dim, layers, num_heads, mlp_ratio=3.,\n                       qkv_bias=False, qk_scale=None, attn_drop=0,\n                       drop_path_rate=0.0, **kwargs) -&gt; nn.SequentialCell:\n    \"\"\"\n    generate transformer layers in stage2\n    return: transformer layers\n    \"\"\"\n    blocks = []\n    for block_idx in range(layers[index]):\n        block_dpr = drop_path_rate * (block_idx +\n                                      sum(layers[:index])) / (sum(layers) - 1)\n        blocks.append(\n            block_fn(dim, num_heads,\n                     mlp_ratio=mlp_ratio,\n                     qkv_bias=qkv_bias,\n                     qk_scale=qk_scale,\n                     attn_drop=attn_drop,\n                     drop_path=block_dpr))\n\n    blocks = nn.SequentialCell(*blocks)\n\n    return blocks\n</code></pre>"},{"location":"reference/models/#mindcv.models.volo.volo_d1","title":"<code>mindcv.models.volo.volo_d1(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>VOLO-D1 model, Params: 27M --layers: [x,x,x,x], four blocks in two stages, the first stage(block) is outlooker,         the other three blocks are transformer, we set four blocks, which are easily          applied to downstream tasks --embed_dims, --num_heads,: embedding dim, number of heads in each block --downsamples: flags to apply downsampling or not in four blocks --outlook_attention: flags to apply outlook attention or not --mlp_ratios: mlp ratio in four blocks --post_layers: post layers like two class attention layers using [ca, ca] See detail for all args in the class VOLO()</p> Source code in <code>mindcv\\models\\volo.py</code> <pre><code>@register_model\ndef volo_d1(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs):\n    \"\"\"\n    VOLO-D1 model, Params: 27M\n    --layers: [x,x,x,x], four blocks in two stages, the first stage(block) is outlooker,\n            the other three blocks are transformer, we set four blocks, which are easily\n             applied to downstream tasks\n    --embed_dims, --num_heads,: embedding dim, number of heads in each block\n    --downsamples: flags to apply downsampling or not in four blocks\n    --outlook_attention: flags to apply outlook attention or not\n    --mlp_ratios: mlp ratio in four blocks\n    --post_layers: post layers like two class attention layers using [ca, ca]\n    See detail for all args in the class VOLO()\n    \"\"\"\n    default_cfg = default_cfgs['volo_d1']\n\n    # first block is outlooker (stage1), the other three are transformer (stage2)\n    model = VOLO(layers=[4, 4, 8, 2],\n                 in_channels=in_channels,\n                 num_classes=num_classes,\n                 embed_dims=[192, 384, 384, 384],\n                 num_heads=[6, 12, 12, 12],\n                 mlp_ratios=[3, 3, 3, 3],\n                 downsamples=[True, False, False, False],\n                 outlook_attention=[True, False, False, False],\n                 post_layers=['ca', 'ca'],\n                 **kwargs)\n    model.default_cfg = default_cfg\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n    return model\n</code></pre>"},{"location":"reference/models/#mindcv.models.volo.volo_d2","title":"<code>mindcv.models.volo.volo_d2(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>VOLO-D2 model, Params: 59M</p> Source code in <code>mindcv\\models\\volo.py</code> <pre><code>@register_model\ndef volo_d2(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs):\n    \"\"\"\n    VOLO-D2 model, Params: 59M\n    \"\"\"\n    default_cfg = default_cfgs['volo_d2']\n    model = VOLO(layers=[6, 4, 10, 4],\n                 in_channels=in_channels,\n                 num_classes=num_classes,\n                 embed_dims=[256, 512, 512, 512],\n                 num_heads=[8, 16, 16, 16],\n                 mlp_ratios=[3, 3, 3, 3],\n                 downsamples=[True, False, False, False],\n                 outlook_attention=[True, False, False, False],\n                 post_layers=['ca', 'ca'],\n                 **kwargs)\n    model.default_cfg = default_cfg\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n    return model\n</code></pre>"},{"location":"reference/models/#mindcv.models.volo.volo_d3","title":"<code>mindcv.models.volo.volo_d3(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>VOLO-D3 model, Params: 86M</p> Source code in <code>mindcv\\models\\volo.py</code> <pre><code>@register_model\ndef volo_d3(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs):\n    \"\"\"\n    VOLO-D3 model, Params: 86M\n    \"\"\"\n    default_cfg = default_cfgs['volo_d3']\n    model = VOLO(layers=[8, 8, 16, 4],\n                 in_channels=in_channels,\n                 num_classes=num_classes,\n                 embed_dims=[256, 512, 512, 512],\n                 num_heads=[8, 16, 16, 16],\n                 mlp_ratios=[3, 3, 3, 3],\n                 downsamples=[True, False, False, False],\n                 outlook_attention=[True, False, False, False],\n                 post_layers=['ca', 'ca'],\n                 **kwargs)\n    model.default_cfg = default_cfg\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n    return model\n</code></pre>"},{"location":"reference/models/#mindcv.models.volo.volo_d4","title":"<code>mindcv.models.volo.volo_d4(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>VOLO-D4 model, Params: 193M</p> Source code in <code>mindcv\\models\\volo.py</code> <pre><code>@register_model\ndef volo_d4(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs):\n    \"\"\"\n    VOLO-D4 model, Params: 193M\n    \"\"\"\n    default_cfg = default_cfgs['volo_d4']\n    model = VOLO(layers=[8, 8, 16, 4],\n                 in_channels=in_channels,\n                 num_classes=num_classes,\n                 embed_dims=[384, 768, 768, 768],\n                 num_heads=[12, 16, 16, 16],\n                 mlp_ratios=[3, 3, 3, 3],\n                 downsamples=[True, False, False, False],\n                 outlook_attention=[True, False, False, False],\n                 post_layers=['ca', 'ca'],\n                 **kwargs)\n    model.default_cfg = default_cfg\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n    return model\n</code></pre>"},{"location":"reference/models/#mindcv.models.volo.volo_d5","title":"<code>mindcv.models.volo.volo_d5(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>VOLO-D5 model, Params: 296M stem_hidden_dim=128, the dim in patch embedding is 128 for VOLO-D5</p> Source code in <code>mindcv\\models\\volo.py</code> <pre><code>@register_model\ndef volo_d5(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs):\n    \"\"\"\n    VOLO-D5 model, Params: 296M\n    stem_hidden_dim=128, the dim in patch embedding is 128 for VOLO-D5\n    \"\"\"\n    default_cfg = default_cfgs['volo_d5']\n    model = VOLO(layers=[12, 12, 20, 4],\n                 embed_dims=[384, 768, 768, 768],\n                 num_heads=[12, 16, 16, 16],\n                 mlp_ratios=[4, 4, 4, 4],\n                 downsamples=[True, False, False, False],\n                 outlook_attention=[True, False, False, False],\n                 post_layers=['ca', 'ca'],\n                 stem_hidden_dim=128,\n                 **kwargs)\n    model.default_cfg = default_cfg\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n    return model\n</code></pre>"},{"location":"reference/models/#xcit","title":"xcit","text":""},{"location":"reference/models/#mindcv.models.xcit","title":"<code>mindcv.models.xcit</code>","text":"<p>MindSpore implementation of XCiT Refer to: XCiT: Cross-Covariance Image Transformers</p>"},{"location":"reference/models/#mindcv.models.xcit.ClassAttention","title":"<code>mindcv.models.xcit.ClassAttention</code>","text":"<p>               Bases: <code>Cell</code></p> <p>Class Attention Layer as in CaiT https://arxiv.org/abs/2103.17239</p> Source code in <code>mindcv\\models\\xcit.py</code> <pre><code>class ClassAttention(nn.Cell):\n    \"\"\"Class Attention Layer as in CaiT https://arxiv.org/abs/2103.17239\n    \"\"\"\n\n    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n        super().__init__()\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        self.scale = qk_scale or head_dim ** -0.5\n\n        self.qkv = nn.Dense(\n            in_channels=dim, out_channels=dim * 3, has_bias=qkv_bias)\n        self.attn_drop = Dropout(p=attn_drop)\n        self.proj = nn.Dense(in_channels=dim, out_channels=dim)\n        self.proj_drop = Dropout(p=proj_drop)\n        self.softmax = nn.Softmax(axis=-1)\n\n        self.attn_matmul_v = ops.BatchMatMul()\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        B, N, C = x.shape\n\n        qkv = self.qkv(x)\n        qkv = ops.reshape(qkv, (B, N, 3, self.num_heads, C // self.num_heads))\n        qkv = ops.transpose(qkv, (2, 0, 3, 1, 4))\n        q, k, v = ops.unstack(qkv, axis=0)\n        qc = q[:, :, 0:1]\n        attn_cls = (qc * k).sum(-1) * self.scale\n        attn_cls = self.softmax(attn_cls)\n        attn_cls = self.attn_drop(attn_cls)\n\n        attn_cls = ops.expand_dims(attn_cls, 2)\n        cls_tkn = self.attn_matmul_v(attn_cls, v)\n        cls_tkn = ops.transpose(cls_tkn, (0, 2, 1, 3))\n        cls_tkn = ops.reshape(cls_tkn, (B, 1, C))\n        cls_tkn = self.proj(cls_tkn)\n        x = ops.concat((self.proj_drop(cls_tkn), x[:, 1:]), axis=1)\n        return x\n</code></pre>"},{"location":"reference/models/#mindcv.models.xcit.ClassAttentionBlock","title":"<code>mindcv.models.xcit.ClassAttentionBlock</code>","text":"<p>               Bases: <code>Cell</code></p> <p>Class Attention Layer as in CaiT https://arxiv.org/abs/2103.17239</p> Source code in <code>mindcv\\models\\xcit.py</code> <pre><code>class ClassAttentionBlock(nn.Cell):\n    \"\"\"Class Attention Layer as in CaiT https://arxiv.org/abs/2103.17239\n    \"\"\"\n\n    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0.,\n                 attn_drop=0., drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm, eta=None,\n                 tokens_norm=False):\n        super().__init__()\n        self.norm1 = norm_layer([dim])\n        self.attn = ClassAttention(\n            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop,\n            proj_drop=drop\n        )\n\n        self.drop_path = DropPath(\n            drop_path) if drop_path &gt; 0. else ops.Identity()\n        self.norm2 = norm_layer([dim])\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer,\n                       drop=drop)\n\n        # LayerScale Initialization (no layerscale when None)\n        if eta is not None:\n            self.gamma1 = Parameter(\n                eta * ops.Ones()((dim), mstype.float32), requires_grad=True)\n            self.gamma2 = Parameter(\n                eta * ops.Ones()((dim), mstype.float32), requires_grad=True)\n        else:\n            self.gamma1, self.gamma2 = 1.0, 1.0\n\n        # FIXME: A hack for models pre-trained with layernorm over all the tokens not just the CLS\n        self.tokens_norm = tokens_norm\n\n    def construct(self, x, H, W, mask=None):\n\n        x = x + self.drop_path(self.gamma1 * self.attn(self.norm1(x)))\n\n        if self.tokens_norm:\n            x = self.norm2(x)\n        else:\n            x[:, 0:1] = self.norm2(x[:, 0:1])\n        x_res = x\n        cls_token = x[:, 0:1]\n        cls_token = self.gamma2 * self.mlp(cls_token)\n        x = ops.concat((cls_token, x[:, 1:]), axis=1)\n        x = x_res + self.drop_path(x)\n        return x\n</code></pre>"},{"location":"reference/models/#mindcv.models.xcit.ConvPatchEmbed","title":"<code>mindcv.models.xcit.ConvPatchEmbed</code>","text":"<p>               Bases: <code>Cell</code></p> <p>Image to Patch Embedding using multiple convolutional layers</p> Source code in <code>mindcv\\models\\xcit.py</code> <pre><code>class ConvPatchEmbed(nn.Cell):\n    \"\"\" Image to Patch Embedding using multiple convolutional layers\n    \"\"\"\n\n    def __init__(self,\n                 img_size: int = 224,\n                 patch_size: int = 16,\n                 in_chans: int = 3,\n                 embed_dim: int = 768\n                 ) -&gt; None:\n        super().__init__()\n        img_size = to_2tuple(img_size)\n        patch_size = to_2tuple(patch_size)\n        num_patches = (img_size[1] // patch_size[1]) * \\\n            (img_size[0] // patch_size[0])\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.num_patches = num_patches\n\n        if patch_size[0] == 16:\n            self.proj = nn.SequentialCell([\n                conv3x3(3, embed_dim // 8, 2),\n                nn.GELU(),\n                conv3x3(embed_dim // 8, embed_dim // 4, 2),\n                nn.GELU(),\n                conv3x3(embed_dim // 4, embed_dim // 2, 2),\n                nn.GELU(),\n                conv3x3(embed_dim // 2, embed_dim, 2),\n            ])\n        elif patch_size[0] == 8:\n            self.proj = nn.SequentialCell([\n                conv3x3(3, embed_dim // 4, 2),\n                nn.GELU(),\n                conv3x3(embed_dim // 4, embed_dim // 2, 2),\n                nn.GELU(),\n                conv3x3(embed_dim // 2, embed_dim, 2),\n            ])\n        else:\n            raise ValueError(\n                \"For convolutional projection, patch size has to be in [8, 16]\")\n\n    def construct(self, x, padding_size=None) -&gt; Tensor:\n        x = self.proj(x)\n        B, C, Hp, Wp = x.shape\n        x = ops.reshape(x, (B, C, Hp * Wp))\n        x = x.transpose(0, 2, 1)\n\n        return x, (Hp, Wp)\n</code></pre>"},{"location":"reference/models/#mindcv.models.xcit.LPI","title":"<code>mindcv.models.xcit.LPI</code>","text":"<p>               Bases: <code>Cell</code></p> <p>Local Patch Interaction module that allows explicit communication between tokens in 3x3 windows to augment the implicit communcation performed by the block diagonal scatter attention. Implemented using 2 layers of separable 3x3 convolutions with GeLU and BatchNorm2d</p> Source code in <code>mindcv\\models\\xcit.py</code> <pre><code>class LPI(nn.Cell):\n    \"\"\"\n    Local Patch Interaction module that allows explicit communication between tokens in 3x3 windows\n    to augment the implicit communcation performed by the block diagonal scatter attention.\n    Implemented using 2 layers of separable 3x3 convolutions with GeLU and BatchNorm2d\n    \"\"\"\n\n    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU,\n                 drop=0., kernel_size=3) -&gt; None:\n        super().__init__()\n        out_features = out_features or in_features\n\n        padding = kernel_size // 2\n\n        self.conv1 = nn.Conv2d(in_features, out_features, kernel_size=kernel_size,\n                               padding=padding, pad_mode='pad', group=out_features, has_bias=True)\n        self.act = act_layer()\n        self.bn = nn.BatchNorm2d(in_features)\n        self.conv2 = nn.Conv2d(in_features, out_features, kernel_size=kernel_size,\n                               padding=padding, pad_mode='pad', group=out_features, has_bias=True)\n\n    def construct(self, x, H, W) -&gt; Tensor:\n        B, N, C = x.shape\n        x = ops.reshape(ops.transpose(x, (0, 2, 1)), (B, C, H, W))\n        x = self.conv1(x)\n        x = self.act(x)\n        x = self.bn(x)\n        x = self.conv2(x)\n        x = ops.transpose(ops.reshape(x, (B, C, N)), (0, 2, 1))\n\n        return x\n</code></pre>"},{"location":"reference/models/#mindcv.models.xcit.PositionalEncodingFourier","title":"<code>mindcv.models.xcit.PositionalEncodingFourier</code>","text":"<p>               Bases: <code>Cell</code></p> <p>Positional encoding relying on a fourier kernel matching the one used in the \"Attention is all of Need\" paper. The implementation builds on DeTR code https://github.com/facebookresearch/detr/blob/master/models/position_encoding.py</p> Source code in <code>mindcv\\models\\xcit.py</code> <pre><code>class PositionalEncodingFourier(nn.Cell):\n    \"\"\"\n    Positional encoding relying on a fourier kernel matching the one used in the\n    \"Attention is all of Need\" paper. The implementation builds on DeTR code\n    https://github.com/facebookresearch/detr/blob/master/models/position_encoding.py\n    \"\"\"\n\n    def __init__(self,\n                 hidden_dim: int = 32,\n                 dim: int = 768,\n                 temperature=10000\n                 ) -&gt; None:\n        super().__init__()\n        self.token_projection = nn.Conv2d(\n            hidden_dim * 2, dim, kernel_size=1, has_bias=True)\n        self.scale = 2 * np.pi\n        self.temperature = temperature\n        self.hidden_dim = hidden_dim\n        self.dim = dim\n\n    def construct(self, B, H, W) -&gt; Tensor:\n        mask = Tensor(np.zeros((B, H, W)).astype(bool))\n        not_mask = ~mask\n        y_embed = not_mask.cumsum(1, dtype=mstype.float32)\n        x_embed = not_mask.cumsum(2, dtype=mstype.float32)\n        eps = 1e-6\n        y_embed = y_embed / (y_embed[:, -1:, :] + eps) * self.scale\n        x_embed = x_embed / (x_embed[:, :, -1:] + eps) * self.scale\n\n        dim_t = numpy.arange(self.hidden_dim, dtype=mstype.float32)\n        dim_t = self.temperature ** (2 * (dim_t // 2) / self.hidden_dim)\n\n        pos_x = x_embed[:, :, :, None] / dim_t\n        pos_y = y_embed[:, :, :, None] / dim_t\n        pos_x = ops.stack((ops.sin(pos_x[:, :, :, 0::2]),\n                           ops.cos(pos_x[:, :, :, 1::2])), 4)\n        x1, x2, x3, x4, x5 = pos_x.shape\n        pos_x = ops.reshape(pos_x, (x1, x2, x3, x4 * x5))\n        pos_y = ops.stack((ops.sin(pos_y[:, :, :, 0::2]),\n                           ops.cos(pos_y[:, :, :, 1::2])), 4)\n        y1, y2, y3, y4, y5 = pos_y.shape\n        pos_y = ops.reshape(pos_y, (y1, y2, y3, y4 * y5))\n        pos = ops.transpose(ops.concat((pos_y, pos_x), 3), (0, 3, 1, 2))\n        pos = self.token_projection(pos)\n        return pos\n</code></pre>"},{"location":"reference/models/#mindcv.models.xcit.XCA","title":"<code>mindcv.models.xcit.XCA</code>","text":"<p>               Bases: <code>Cell</code></p> <p>Cross-Covariance Attention (XCA) operation where the channels are updated using a weighted  sum. The weights are obtained from the (softmax normalized) Cross-covariance matrix (Q^T K \\in d_h \\times d_h)</p> Source code in <code>mindcv\\models\\xcit.py</code> <pre><code>class XCA(nn.Cell):\n\n    \"\"\" Cross-Covariance Attention (XCA) operation where the channels are updated using a weighted\n     sum. The weights are obtained from the (softmax normalized) Cross-covariance\n    matrix (Q^T K \\\\in d_h \\\\times d_h)\n    \"\"\"\n\n    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n        super().__init__()\n        self.num_heads = num_heads\n        self.temperature = Parameter(\n            ops.Ones()((num_heads, 1, 1), mstype.float32))\n        self.qkv = nn.Dense(\n            in_channels=dim, out_channels=dim * 3, has_bias=qkv_bias)\n        self.q_matmul_k = ops.BatchMatMul(transpose_b=True)\n        self.softmax = nn.Softmax(axis=-1)\n        self.attn_drop = Dropout(p=attn_drop)\n        self.attn_matmul_v = ops.BatchMatMul()\n        self.proj = nn.Dense(in_channels=dim, out_channels=dim)\n        self.proj_drop = Dropout(p=proj_drop)\n\n    def construct(self, x):\n        B, N, C = x.shape\n\n        qkv = ops.reshape(\n            self.qkv(x), (B, N, 3, self.num_heads, C // self.num_heads))\n        qkv = ops.transpose(qkv, (2, 0, 3, 1, 4))\n        q, k, v = ops.unstack(qkv, axis=0)\n\n        q = ops.transpose(q, (0, 1, 3, 2))\n        k = ops.transpose(k, (0, 1, 3, 2))\n        v = ops.transpose(v, (0, 1, 3, 2))\n\n        attn = self.q_matmul_k(q, k) * self.temperature\n        attn = self.softmax(attn)\n        attn = self.attn_drop(attn)\n        x = self.attn_matmul_v(attn, v)\n        x = ops.transpose(x, (0, 3, 1, 2))\n        x = ops.reshape(x, (B, N, C))\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n</code></pre>"},{"location":"reference/models/#mindcv.models.xcit.XCiT","title":"<code>mindcv.models.xcit.XCiT</code>","text":"<p>               Bases: <code>Cell</code></p> <p>XCiT model class, based on <code>\"XCiT: Cross-Covariance Image Transformers\" &lt;https://arxiv.org/abs/2106.09681&gt;</code>_ Args:     img_size (int, tuple): input image size     patch_size (int, tuple): patch size     in_chans (int): number of input channels     num_classes (int): number of classes for classification head     embed_dim (int): embedding dimension     depth (int): depth of transformer     num_heads (int): number of attention heads     mlp_ratio (int): ratio of mlp hidden dim to embedding dim     qkv_bias (bool): enable bias for qkv if True     qk_scale (float): override default qk scale of head_dim ** -0.5 if set     drop_rate (float): dropout rate     attn_drop_rate (float): attention dropout rate     drop_path_rate (float): stochastic depth rate     norm_layer: (nn.Module): normalization layer     cls_attn_layers: (int) Depth of Class attention layers     use_pos: (bool) whether to use positional encoding     eta: (float) layerscale initialization value     tokens_norm: (bool) Whether to normalize all tokens or just the cls_token in the CA</p> Source code in <code>mindcv\\models\\xcit.py</code> <pre><code>class XCiT(nn.Cell):\n    r\"\"\"XCiT model class, based on\n    `\"XCiT: Cross-Covariance Image Transformers\" &lt;https://arxiv.org/abs/2106.09681&gt;`_\n    Args:\n        img_size (int, tuple): input image size\n        patch_size (int, tuple): patch size\n        in_chans (int): number of input channels\n        num_classes (int): number of classes for classification head\n        embed_dim (int): embedding dimension\n        depth (int): depth of transformer\n        num_heads (int): number of attention heads\n        mlp_ratio (int): ratio of mlp hidden dim to embedding dim\n        qkv_bias (bool): enable bias for qkv if True\n        qk_scale (float): override default qk scale of head_dim ** -0.5 if set\n        drop_rate (float): dropout rate\n        attn_drop_rate (float): attention dropout rate\n        drop_path_rate (float): stochastic depth rate\n        norm_layer: (nn.Module): normalization layer\n        cls_attn_layers: (int) Depth of Class attention layers\n        use_pos: (bool) whether to use positional encoding\n        eta: (float) layerscale initialization value\n        tokens_norm: (bool) Whether to normalize all tokens or just the cls_token in the CA\n    \"\"\"\n\n    def __init__(self,\n                 img_size: int = 224,\n                 patch_size: int = 16,\n                 in_chans: int = 3,\n                 num_classes: int = 1000,\n                 embed_dim: int = 768,\n                 depth: int = 12,\n                 num_heads: int = 12,\n                 mlp_ratio: int = 4.,\n                 qkv_bias: bool = True,\n                 qk_scale: float = None,\n                 drop_rate: float = 0.,\n                 attn_drop_rate: float = 0.,\n                 drop_path_rate: float = 0.,\n                 norm_layer: nn.Cell = None,\n                 cls_attn_layers: int = 2,\n                 use_pos: bool = True,\n                 patch_proj: str = 'linear',\n                 eta: float = None,\n                 tokens_norm: bool = False):\n        super().__init__()\n\n        self.num_classes = num_classes\n        self.num_features = self.embed_dim = embed_dim\n        norm_layer = norm_layer or partial(nn.LayerNorm, epsilon=1e-6)\n\n        self.patch_embed = ConvPatchEmbed(img_size=img_size, embed_dim=embed_dim,\n                                          patch_size=patch_size)\n\n        num_patches = self.patch_embed.num_patches\n\n        self.cls_token = Parameter(\n            ops.zeros((1, 1, embed_dim), mstype.float32))\n        self.pos_drop = Dropout(p=drop_rate)\n\n        dpr = [drop_path_rate for i in range(depth)]\n        self.blocks = nn.CellList([\n            XCABlock(\n                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias,\n                qk_scale=qk_scale, drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i],\n                norm_layer=norm_layer, num_tokens=num_patches, eta=eta)\n            for i in range(depth)])\n\n        self.cls_attn_blocks = nn.CellList([\n            ClassAttentionBlock(\n                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias,\n                qk_scale=qk_scale, drop=drop_rate, attn_drop=attn_drop_rate, norm_layer=norm_layer,\n                eta=eta, tokens_norm=tokens_norm)\n            for i in range(cls_attn_layers)])\n        self.norm = norm_layer([embed_dim])\n        self.head = nn.Dense(\n            in_channels=embed_dim, out_channels=num_classes) if num_classes &gt; 0 else ops.Identity()\n\n        self.pos_embeder = PositionalEncodingFourier(dim=embed_dim)\n        self.use_pos = use_pos\n\n        # Classifier head\n        self.cls_token.set_data(weight_init.initializer(weight_init.TruncatedNormal(sigma=0.02),\n                                                        self.cls_token.shape,\n                                                        self.cls_token.dtype))\n        self._init_weights()\n\n    def _init_weights(self) -&gt; None:\n        for name, m in self.cells_and_names():\n            if isinstance(m, nn.Dense):\n                m.weight = weight_init.initializer(weight_init.TruncatedNormal(\n                    sigma=0.02), m.weight.shape, mindspore.float32)\n                if m.bias is not None:\n                    m.bias.set_data(weight_init.initializer(\n                        weight_init.Constant(0), m.bias.shape))\n            elif isinstance(m, nn.LayerNorm):\n                m.beta.set_data(weight_init.initializer(\n                    weight_init.Constant(0), m.beta.shape))\n                m.gamma.set_data(weight_init.initializer(\n                    weight_init.Constant(1), m.gamma.shape))\n\n    def forward_features(self, x):\n        B, C, H, W = x.shape\n        x, (Hp, Wp) = self.patch_embed(x)\n        if self.use_pos:\n            pos_encoding = self.pos_embeder(B, Hp, Wp).reshape(\n                B, -1, x.shape[1]).transpose(0, 2, 1)\n            x = x + pos_encoding\n        x = self.pos_drop(x)\n        for blk in self.blocks:\n            x = blk(x, Hp, Wp)\n        cls_tokens = ops.broadcast_to(self.cls_token, (B, -1, -1))\n        cls_tokens = ops.cast(cls_tokens, x.dtype)\n        x = ops.concat((cls_tokens, x), 1)\n\n        for blk in self.cls_attn_blocks:\n            x = blk(x, Hp, Wp)\n        return self.norm(x)[:, 0]\n\n    def construct(self, x):\n        x = self.forward_features(x)\n        x = self.head(x)\n        return x\n</code></pre>"},{"location":"reference/models/#mindcv.models.xcit.conv3x3","title":"<code>mindcv.models.xcit.conv3x3(in_planes, out_planes, stride=1)</code>","text":"<p>3x3 convolution with padding</p> Source code in <code>mindcv\\models\\xcit.py</code> <pre><code>def conv3x3(in_planes, out_planes, stride=1):\n    \"\"\"3x3 convolution with padding\"\"\"\n    return nn.SequentialCell([\n        nn.Conv2d(\n            in_planes, out_planes, kernel_size=3, stride=stride, padding=1, pad_mode='pad', has_bias=False\n        ),\n        nn.BatchNorm2d(out_planes)\n    ])\n</code></pre>"},{"location":"reference/models/#mindcv.models.xcit.xcit_tiny_12_p16_224","title":"<code>mindcv.models.xcit.xcit_tiny_12_p16_224(pretrained=False, num_classes=1000, in_channels=3, **kwargs)</code>","text":"<p>Get xcit_tiny_12_p16_224 model. Refer to the base class 'models.XCiT' for more details.</p> Source code in <code>mindcv\\models\\xcit.py</code> <pre><code>@register_model\ndef xcit_tiny_12_p16_224(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; XCiT:\n    \"\"\"Get xcit_tiny_12_p16_224 model.\n    Refer to the base class 'models.XCiT' for more details.\n    \"\"\"\n    default_cfg = default_cfgs['xcit_tiny_12_p16_224']\n    model = XCiT(\n        patch_size=16, num_classes=num_classes, embed_dim=192, depth=12, num_heads=4, mlp_ratio=4, qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, epsilon=1e-6), eta=1.0, tokens_norm=True, **kwargs)\n    if pretrained:\n        load_pretrained(model, default_cfg,\n                        num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre>"},{"location":"reference/optim/","title":"Optimizer","text":""},{"location":"reference/optim/#optimizer-factory","title":"Optimizer Factory","text":""},{"location":"reference/optim/#mindcv.optim.optim_factory.create_optimizer","title":"<code>mindcv.optim.optim_factory.create_optimizer(model_or_params, opt='adam', lr=0.001, weight_decay=0, momentum=0.9, nesterov=False, weight_decay_filter='disable', layer_decay=None, loss_scale=1.0, schedule_decay=0.004, checkpoint_path='', eps=1e-10, **kwargs)</code>","text":"<p>Creates optimizer by name.</p> PARAMETER DESCRIPTION <code>model_or_params</code> <p>network or network parameters. Union[list[Parameter],list[dict], nn.Cell], which must be the list of parameters or list of dicts or nn.Cell. When the list element is a dictionary, the key of the dictionary can be \"params\", \"lr\", \"weight_decay\",\"grad_centralization\" and \"order_params\".</p> <p> </p> <code>opt</code> <p>wrapped optimizer. You could choose like 'sgd', 'nesterov', 'momentum', 'adam', 'adamw', 'lion', 'rmsprop', 'adagrad', 'lamb'. 'adam' is the default choose for convolution-based networks. 'adamw' is recommended for ViT-based networks. Default: 'adam'.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'adam'</code> </p> <code>lr</code> <p>learning rate: float or lr scheduler. Fixed and dynamic learning rate are supported. Default: 1e-3.</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>0.001</code> </p> <code>weight_decay</code> <p>weight decay factor. It should be noted that weight decay can be a constant value or a Cell. It is a Cell only when dynamic weight decay is applied. Dynamic weight decay is similar to dynamic learning rate, users need to customize a weight decay schedule only with global step as input, and during training, the optimizer calls the instance of WeightDecaySchedule to get the weight decay value of current step. Default: 0.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0</code> </p> <code>momentum</code> <p>momentum if the optimizer supports. Default: 0.9.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.9</code> </p> <code>nesterov</code> <p>Whether to use Nesterov Accelerated Gradient (NAG) algorithm to update the gradients. Default: False.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>weight_decay_filter</code> <p>filters to filter parameters from weight_decay. - \"disable\": No parameters to filter. - \"auto\": We do not apply weight decay filtering to any parameters. However, MindSpore currently         automatically filters the parameters of Norm layer from weight decay. - \"norm_and_bias\": Filter the paramters of Norm layer and Bias from weight decay.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'disable'</code> </p> <code>layer_decay</code> <p>for apply layer-wise learning rate decay.</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>None</code> </p> <code>loss_scale</code> <p>A floating point value for the loss scale, which must be larger than 0.0. Default: 1.0.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> RETURNS DESCRIPTION <p>Optimizer object</p> Source code in <code>mindcv\\optim\\optim_factory.py</code> <pre><code>def create_optimizer(\n    model_or_params,\n    opt: str = \"adam\",\n    lr: Optional[float] = 1e-3,\n    weight_decay: float = 0,\n    momentum: float = 0.9,\n    nesterov: bool = False,\n    weight_decay_filter: str = \"disable\",\n    layer_decay: Optional[float] = None,\n    loss_scale: float = 1.0,\n    schedule_decay: float = 4e-3,\n    checkpoint_path: str = \"\",\n    eps: float = 1e-10,\n    **kwargs,\n):\n    r\"\"\"Creates optimizer by name.\n\n    Args:\n        model_or_params: network or network parameters. Union[list[Parameter],list[dict], nn.Cell], which must be\n            the list of parameters or list of dicts or nn.Cell. When the list element is a dictionary, the key of\n            the dictionary can be \"params\", \"lr\", \"weight_decay\",\"grad_centralization\" and \"order_params\".\n        opt: wrapped optimizer. You could choose like 'sgd', 'nesterov', 'momentum', 'adam', 'adamw', 'lion',\n            'rmsprop', 'adagrad', 'lamb'. 'adam' is the default choose for convolution-based networks.\n            'adamw' is recommended for ViT-based networks. Default: 'adam'.\n        lr: learning rate: float or lr scheduler. Fixed and dynamic learning rate are supported. Default: 1e-3.\n        weight_decay: weight decay factor. It should be noted that weight decay can be a constant value or a Cell.\n            It is a Cell only when dynamic weight decay is applied. Dynamic weight decay is similar to\n            dynamic learning rate, users need to customize a weight decay schedule only with global step as input,\n            and during training, the optimizer calls the instance of WeightDecaySchedule to get the weight decay value\n            of current step. Default: 0.\n        momentum: momentum if the optimizer supports. Default: 0.9.\n        nesterov: Whether to use Nesterov Accelerated Gradient (NAG) algorithm to update the gradients. Default: False.\n        weight_decay_filter: filters to filter parameters from weight_decay.\n            - \"disable\": No parameters to filter.\n            - \"auto\": We do not apply weight decay filtering to any parameters. However, MindSpore currently\n                    automatically filters the parameters of Norm layer from weight decay.\n            - \"norm_and_bias\": Filter the paramters of Norm layer and Bias from weight decay.\n        layer_decay: for apply layer-wise learning rate decay.\n        loss_scale: A floating point value for the loss scale, which must be larger than 0.0. Default: 1.0.\n\n    Returns:\n        Optimizer object\n    \"\"\"\n\n    no_weight_decay = {}\n    if isinstance(model_or_params, nn.Cell):\n        # a model was passed in, extract parameters and add weight decays to appropriate layers\n        if hasattr(model_or_params, \"no_weight_decay\"):\n            no_weight_decay = model_or_params.no_weight_decay()\n        params = model_or_params.trainable_params()\n\n    else:\n        params = model_or_params\n\n    if weight_decay_filter == \"auto\":\n        _logger.warning(\n            \"You are using AUTO weight decay filter, which means the weight decay filter isn't explicitly pass in \"\n            \"when creating an mindspore.nn.Optimizer instance. \"\n            \"NOTE: mindspore.nn.Optimizer will filter Norm parmas from weight decay. \"\n        )\n    elif layer_decay is not None and isinstance(model_or_params, nn.Cell):\n        params = param_groups_layer_decay(\n            model_or_params,\n            lr=lr,\n            weight_decay=weight_decay,\n            layer_decay=layer_decay,\n            no_weight_decay_list=no_weight_decay,\n        )\n        weight_decay = 0.0\n    elif weight_decay_filter == \"disable\" or \"norm_and_bias\":\n        params = init_group_params(params, weight_decay, weight_decay_filter, no_weight_decay)\n        weight_decay = 0.0\n    else:\n        raise ValueError(\n            f\"weight decay filter only support ['disable', 'auto', 'norm_and_bias'], but got{weight_decay_filter}.\"\n        )\n\n    opt = opt.lower()\n    opt_args = dict(**kwargs)\n    # if lr is not None:\n    #    opt_args.setdefault('lr', lr)\n\n    # non-adaptive: SGD, momentum, and nesterov\n    if opt == \"sgd\":\n        # note: nn.Momentum may perform better if momentum &gt; 0.\n        optimizer = nn.SGD(\n            params=params,\n            learning_rate=lr,\n            momentum=momentum,\n            weight_decay=weight_decay,\n            nesterov=nesterov,\n            loss_scale=loss_scale,\n            **opt_args,\n        )\n    elif opt in [\"momentum\", \"nesterov\"]:\n        optimizer = nn.Momentum(\n            params=params,\n            learning_rate=lr,\n            momentum=momentum,\n            weight_decay=weight_decay,\n            use_nesterov=nesterov,\n            loss_scale=loss_scale,\n        )\n    # adaptive\n    elif opt == \"adam\":\n        optimizer = nn.Adam(\n            params=params,\n            learning_rate=lr,\n            weight_decay=weight_decay,\n            loss_scale=loss_scale,\n            use_nesterov=nesterov,\n            **opt_args,\n        )\n    elif opt == \"adamw\":\n        optimizer = AdamW(\n            params=params,\n            learning_rate=lr,\n            weight_decay=weight_decay,\n            loss_scale=loss_scale,\n            **opt_args,\n        )\n    elif opt == \"lion\":\n        optimizer = Lion(\n            params=params,\n            learning_rate=lr,\n            weight_decay=weight_decay,\n            loss_scale=loss_scale,\n            **opt_args,\n        )\n    elif opt == \"nadam\":\n        optimizer = NAdam(\n            params=params,\n            learning_rate=lr,\n            weight_decay=weight_decay,\n            loss_scale=loss_scale,\n            schedule_decay=schedule_decay,\n            **opt_args,\n        )\n    elif opt == \"adan\":\n        optimizer = Adan(\n            params=params,\n            learning_rate=lr,\n            weight_decay=weight_decay,\n            loss_scale=loss_scale,\n            **opt_args,\n        )\n    elif opt == \"rmsprop\":\n        optimizer = nn.RMSProp(\n            params=params,\n            learning_rate=lr,\n            momentum=momentum,\n            weight_decay=weight_decay,\n            loss_scale=loss_scale,\n            epsilon=eps,\n            **opt_args,\n        )\n    elif opt == \"adagrad\":\n        optimizer = nn.Adagrad(\n            params=params,\n            learning_rate=lr,\n            weight_decay=weight_decay,\n            loss_scale=loss_scale,\n            **opt_args,\n        )\n    elif opt == \"lamb\":\n        assert loss_scale == 1.0, \"Loss scaler is not supported by Lamb optimizer\"\n        optimizer = nn.Lamb(\n            params=params,\n            learning_rate=lr,\n            weight_decay=weight_decay,\n            **opt_args,\n        )\n    else:\n        raise ValueError(f\"Invalid optimizer: {opt}\")\n\n    if os.path.exists(checkpoint_path):\n        param_dict = load_checkpoint(checkpoint_path)\n        load_param_into_net(optimizer, param_dict)\n\n    return optimizer\n</code></pre>"},{"location":"reference/optim/#adamw","title":"AdamW","text":""},{"location":"reference/optim/#mindcv.optim.adamw.AdamW","title":"<code>mindcv.optim.adamw.AdamW</code>","text":"<p>               Bases: <code>Optimizer</code></p> <p>Implements the gradient clipping by norm for a AdamWeightDecay optimizer.</p> Source code in <code>mindcv\\optim\\adamw.py</code> <pre><code>class AdamW(Optimizer):\n    \"\"\"\n    Implements the gradient clipping by norm for a AdamWeightDecay optimizer.\n    \"\"\"\n\n    @opt_init_args_register\n    def __init__(\n        self,\n        params,\n        learning_rate=1e-3,\n        beta1=0.9,\n        beta2=0.999,\n        eps=1e-8,\n        weight_decay=0.0,\n        loss_scale=1.0,\n        clip=False,\n    ):\n        super().__init__(learning_rate, params, weight_decay)\n        _check_param_value(beta1, beta2, eps, self.cls_name)\n        self.beta1 = Tensor(np.array([beta1]).astype(np.float32))\n        self.beta2 = Tensor(np.array([beta2]).astype(np.float32))\n        self.eps = Tensor(np.array([eps]).astype(np.float32))\n        self.moments1 = self.parameters.clone(prefix=\"adam_m\", init=\"zeros\")\n        self.moments2 = self.parameters.clone(prefix=\"adam_v\", init=\"zeros\")\n        self.hyper_map = ops.HyperMap()\n        self.beta1_power = Parameter(initializer(1, [1], ms.float32), name=\"beta1_power\")\n        self.beta2_power = Parameter(initializer(1, [1], ms.float32), name=\"beta2_power\")\n\n        self.reciprocal_scale = Tensor(1.0 / loss_scale, ms.float32)\n        self.clip = clip\n\n    def get_lr(self):\n        \"\"\"\n        The optimizer calls this interface to get the learning rate for the current step. User-defined optimizers based\n        on :class:`mindspore.nn.Optimizer` can also call this interface before updating the parameters.\n\n        Returns:\n            float, the learning rate of current step.\n        \"\"\"\n        lr = self.learning_rate\n        if self.dynamic_lr:\n            if self.is_group_lr:\n                lr = ()\n                for learning_rate in self.learning_rate:\n                    current_dynamic_lr = learning_rate(self.global_step).reshape(())\n                    lr += (current_dynamic_lr,)\n            else:\n                lr = self.learning_rate(self.global_step).reshape(())\n        if self._is_dynamic_lr_or_weight_decay():\n            self.assignadd(self.global_step, self.global_step_increase_tensor)\n        return lr\n\n    @jit\n    def construct(self, gradients):\n        lr = self.get_lr()\n        gradients = scale_grad(gradients, self.reciprocal_scale)\n        if self.clip:\n            gradients = ops.clip_by_global_norm(gradients, 5.0, None)\n\n        beta1_power = self.beta1_power * self.beta1\n        self.beta1_power = beta1_power\n        beta2_power = self.beta2_power * self.beta2\n        self.beta2_power = beta2_power\n\n        if self.is_group:\n            if self.is_group_lr:\n                optim_result = self.hyper_map(\n                    ops.partial(_adam_opt, beta1_power, beta2_power, self.beta1, self.beta2, self.eps),\n                    lr,\n                    self.weight_decay,\n                    self.parameters,\n                    self.moments1,\n                    self.moments2,\n                    gradients,\n                    self.decay_flags,\n                    self.optim_filter,\n                )\n            else:\n                optim_result = self.hyper_map(\n                    ops.partial(_adam_opt, beta1_power, beta2_power, self.beta1, self.beta2, self.eps, lr),\n                    self.weight_decay,\n                    self.parameters,\n                    self.moments1,\n                    self.moments2,\n                    gradients,\n                    self.decay_flags,\n                    self.optim_filter,\n                )\n        else:\n            optim_result = self.hyper_map(\n                ops.partial(\n                    _adam_opt, beta1_power, beta2_power, self.beta1, self.beta2, self.eps, lr, self.weight_decay\n                ),\n                self.parameters,\n                self.moments1,\n                self.moments2,\n                gradients,\n                self.decay_flags,\n                self.optim_filter,\n            )\n        if self.use_parallel:\n            self.broadcast_params(optim_result)\n        return optim_result\n</code></pre>"},{"location":"reference/optim/#mindcv.optim.adamw.AdamW.get_lr","title":"<code>mindcv.optim.adamw.AdamW.get_lr()</code>","text":"<p>The optimizer calls this interface to get the learning rate for the current step. User-defined optimizers based on :class:<code>mindspore.nn.Optimizer</code> can also call this interface before updating the parameters.</p> RETURNS DESCRIPTION <p>float, the learning rate of current step.</p> Source code in <code>mindcv\\optim\\adamw.py</code> <pre><code>def get_lr(self):\n    \"\"\"\n    The optimizer calls this interface to get the learning rate for the current step. User-defined optimizers based\n    on :class:`mindspore.nn.Optimizer` can also call this interface before updating the parameters.\n\n    Returns:\n        float, the learning rate of current step.\n    \"\"\"\n    lr = self.learning_rate\n    if self.dynamic_lr:\n        if self.is_group_lr:\n            lr = ()\n            for learning_rate in self.learning_rate:\n                current_dynamic_lr = learning_rate(self.global_step).reshape(())\n                lr += (current_dynamic_lr,)\n        else:\n            lr = self.learning_rate(self.global_step).reshape(())\n    if self._is_dynamic_lr_or_weight_decay():\n        self.assignadd(self.global_step, self.global_step_increase_tensor)\n    return lr\n</code></pre>"},{"location":"reference/optim/#adan","title":"Adan","text":""},{"location":"reference/optim/#mindcv.optim.adan.Adan","title":"<code>mindcv.optim.adan.Adan</code>","text":"<p>               Bases: <code>Optimizer</code></p> <p>The Adan (ADAptive Nesterov momentum algorithm) Optimizer from https://arxiv.org/abs/2208.06677</p> <p>Note: it is an experimental version.</p> Source code in <code>mindcv\\optim\\adan.py</code> <pre><code>class Adan(Optimizer):\n    \"\"\"\n    The Adan (ADAptive Nesterov momentum algorithm) Optimizer from https://arxiv.org/abs/2208.06677\n\n    Note: it is an experimental version.\n    \"\"\"\n\n    @opt_init_args_register\n    def __init__(\n        self,\n        params,\n        learning_rate=1e-3,\n        beta1=0.98,\n        beta2=0.92,\n        beta3=0.99,\n        eps=1e-8,\n        use_locking=False,\n        weight_decay=0.0,\n        loss_scale=1.0,\n    ):\n        super().__init__(\n            learning_rate, params, weight_decay=weight_decay, loss_scale=loss_scale\n        )  # Optimized inherit weight decay is bloaked. weight decay is computed in this py.\n\n        _check_param_value(beta1, beta2, eps, self.cls_name)\n        assert isinstance(use_locking, bool), f\"For {self.cls_name}, use_looking should be bool\"\n\n        self.beta1 = Tensor(beta1, mstype.float32)\n        self.beta2 = Tensor(beta2, mstype.float32)\n        self.beta3 = Tensor(beta3, mstype.float32)\n\n        self.eps = Tensor(eps, mstype.float32)\n        self.use_locking = use_locking\n        self.moment1 = self._parameters.clone(prefix=\"moment1\", init=\"zeros\")  # m\n        self.moment2 = self._parameters.clone(prefix=\"moment2\", init=\"zeros\")  # v\n        self.moment3 = self._parameters.clone(prefix=\"moment3\", init=\"zeros\")  # n\n        self.prev_gradient = self._parameters.clone(prefix=\"prev_gradient\", init=\"zeros\")\n\n        self.weight_decay = Tensor(weight_decay, mstype.float32)\n\n    def get_lr(self):\n        \"\"\"\n        The optimizer calls this interface to get the learning rate for the current step. User-defined optimizers based\n        on :class:`mindspore.nn.Optimizer` can also call this interface before updating the parameters.\n\n        Returns:\n            float, the learning rate of current step.\n        \"\"\"\n        lr = self.learning_rate\n        if self.dynamic_lr:\n            if self.is_group_lr:\n                lr = ()\n                for learning_rate in self.learning_rate:\n                    current_dynamic_lr = learning_rate(self.global_step).reshape(())\n                    lr += (current_dynamic_lr,)\n            else:\n                lr = self.learning_rate(self.global_step).reshape(())\n        if self._is_dynamic_lr_or_weight_decay():\n            self.assignadd(self.global_step, self.global_step_increase_tensor)\n        return lr\n\n    @jit\n    def construct(self, gradients):\n        params = self._parameters\n        moment1 = self.moment1\n        moment2 = self.moment2\n        moment3 = self.moment3\n\n        gradients = self.flatten_gradients(gradients)\n        gradients = self.gradients_centralization(gradients)\n        gradients = self.scale_grad(gradients)\n        gradients = self._grad_sparse_indices_deduplicate(gradients)\n        lr = self.get_lr()\n\n        # TODO: currently not support dist\n        success = self.map_(\n            ops.partial(_adan_opt, self.beta1, self.beta2, self.beta3, self.eps, lr, self.weight_decay),\n            params,\n            moment1,\n            moment2,\n            moment3,\n            gradients,\n            self.prev_gradient,\n        )\n\n        return success\n\n    @Optimizer.target.setter\n    def target(self, value):\n        \"\"\"\n        If the input value is set to \"CPU\", the parameters will be updated on the host using the Fused\n        optimizer operation.\n        \"\"\"\n        self._set_base_target(value)\n</code></pre>"},{"location":"reference/optim/#mindcv.optim.adan.Adan.get_lr","title":"<code>mindcv.optim.adan.Adan.get_lr()</code>","text":"<p>The optimizer calls this interface to get the learning rate for the current step. User-defined optimizers based on :class:<code>mindspore.nn.Optimizer</code> can also call this interface before updating the parameters.</p> RETURNS DESCRIPTION <p>float, the learning rate of current step.</p> Source code in <code>mindcv\\optim\\adan.py</code> <pre><code>def get_lr(self):\n    \"\"\"\n    The optimizer calls this interface to get the learning rate for the current step. User-defined optimizers based\n    on :class:`mindspore.nn.Optimizer` can also call this interface before updating the parameters.\n\n    Returns:\n        float, the learning rate of current step.\n    \"\"\"\n    lr = self.learning_rate\n    if self.dynamic_lr:\n        if self.is_group_lr:\n            lr = ()\n            for learning_rate in self.learning_rate:\n                current_dynamic_lr = learning_rate(self.global_step).reshape(())\n                lr += (current_dynamic_lr,)\n        else:\n            lr = self.learning_rate(self.global_step).reshape(())\n    if self._is_dynamic_lr_or_weight_decay():\n        self.assignadd(self.global_step, self.global_step_increase_tensor)\n    return lr\n</code></pre>"},{"location":"reference/optim/#mindcv.optim.adan.Adan.target","title":"<code>mindcv.optim.adan.Adan.target(value)</code>","text":"<p>If the input value is set to \"CPU\", the parameters will be updated on the host using the Fused optimizer operation.</p> Source code in <code>mindcv\\optim\\adan.py</code> <pre><code>@Optimizer.target.setter\ndef target(self, value):\n    \"\"\"\n    If the input value is set to \"CPU\", the parameters will be updated on the host using the Fused\n    optimizer operation.\n    \"\"\"\n    self._set_base_target(value)\n</code></pre>"},{"location":"reference/optim/#lion","title":"Lion","text":""},{"location":"reference/optim/#mindcv.optim.lion.Lion","title":"<code>mindcv.optim.lion.Lion</code>","text":"<p>               Bases: <code>Optimizer</code></p> <p>Implementation of Lion optimizer from paper 'https://arxiv.org/abs/2302.06675'. Additionally, this implementation is with gradient clipping.</p> <p>Notes: lr is usually 3-10x smaller than adamw. weight decay is usually 3-10x larger than adamw.</p> Source code in <code>mindcv\\optim\\lion.py</code> <pre><code>class Lion(Optimizer):\n    \"\"\"\n    Implementation of Lion optimizer from paper 'https://arxiv.org/abs/2302.06675'.\n    Additionally, this implementation is with gradient clipping.\n\n    Notes:\n    lr is usually 3-10x smaller than adamw.\n    weight decay is usually 3-10x larger than adamw.\n    \"\"\"\n\n    @opt_init_args_register\n    def __init__(\n        self,\n        params,\n        learning_rate=2e-4,\n        beta1=0.9,\n        beta2=0.99,\n        weight_decay=0.0,\n        loss_scale=1.0,\n        clip=False,\n    ):\n        super().__init__(learning_rate, params, weight_decay)\n        _check_param_value(beta1, beta2, self.cls_name)\n        self.beta1 = Tensor(np.array([beta1]).astype(np.float32))\n        self.beta2 = Tensor(np.array([beta2]).astype(np.float32))\n        self.moments1 = self.parameters.clone(prefix=\"lion_m\", init=\"zeros\")\n        self.hyper_map = ops.HyperMap()\n        self.beta1_power = Parameter(initializer(1, [1], ms.float32), name=\"beta1_power\")\n        self.beta2_power = Parameter(initializer(1, [1], ms.float32), name=\"beta2_power\")\n\n        self.reciprocal_scale = Tensor(1.0 / loss_scale, ms.float32)\n        self.clip = clip\n\n    def get_lr(self):\n        \"\"\"\n        The optimizer calls this interface to get the learning rate for the current step. User-defined optimizers based\n        on :class:`mindspore.nn.Optimizer` can also call this interface before updating the parameters.\n\n        Returns:\n            float, the learning rate of current step.\n        \"\"\"\n        lr = self.learning_rate\n        if self.dynamic_lr:\n            if self.is_group_lr:\n                lr = ()\n                for learning_rate in self.learning_rate:\n                    current_dynamic_lr = learning_rate(self.global_step).reshape(())\n                    lr += (current_dynamic_lr,)\n            else:\n                lr = self.learning_rate(self.global_step).reshape(())\n        if self._is_dynamic_lr_or_weight_decay():\n            self.assignadd(self.global_step, self.global_step_increase_tensor)\n        return lr\n\n    @jit\n    def construct(self, gradients):\n        lr = self.get_lr()\n        gradients = scale_grad(gradients, self.reciprocal_scale)\n        if self.clip:\n            gradients = ops.clip_by_global_norm(gradients, 5.0, None)\n\n        beta1_power = self.beta1_power * self.beta1\n        self.beta1_power = beta1_power\n        beta2_power = self.beta2_power * self.beta2\n        self.beta2_power = beta2_power\n\n        if self.is_group:\n            if self.is_group_lr:\n                optim_result = self.hyper_map(\n                    ops.partial(_lion_opt, beta1_power, beta2_power, self.beta1, self.beta2),\n                    lr,\n                    self.weight_decay,\n                    self.parameters,\n                    self.moments1,\n                    gradients,\n                    self.decay_flags,\n                    self.optim_filter,\n                )\n            else:\n                optim_result = self.hyper_map(\n                    ops.partial(_lion_opt, beta1_power, beta2_power, self.beta1, self.beta2, lr),\n                    self.weight_decay,\n                    self.parameters,\n                    self.moments1,\n                    gradients,\n                    self.decay_flags,\n                    self.optim_filter,\n                )\n        else:\n            optim_result = self.hyper_map(\n                ops.partial(_lion_opt, beta1_power, beta2_power, self.beta1, self.beta2, lr, self.weight_decay),\n                self.parameters,\n                self.moments1,\n                gradients,\n                self.decay_flags,\n                self.optim_filter,\n            )\n        if self.use_parallel:\n            self.broadcast_params(optim_result)\n        return optim_result\n</code></pre>"},{"location":"reference/optim/#mindcv.optim.lion.Lion.get_lr","title":"<code>mindcv.optim.lion.Lion.get_lr()</code>","text":"<p>The optimizer calls this interface to get the learning rate for the current step. User-defined optimizers based on :class:<code>mindspore.nn.Optimizer</code> can also call this interface before updating the parameters.</p> RETURNS DESCRIPTION <p>float, the learning rate of current step.</p> Source code in <code>mindcv\\optim\\lion.py</code> <pre><code>def get_lr(self):\n    \"\"\"\n    The optimizer calls this interface to get the learning rate for the current step. User-defined optimizers based\n    on :class:`mindspore.nn.Optimizer` can also call this interface before updating the parameters.\n\n    Returns:\n        float, the learning rate of current step.\n    \"\"\"\n    lr = self.learning_rate\n    if self.dynamic_lr:\n        if self.is_group_lr:\n            lr = ()\n            for learning_rate in self.learning_rate:\n                current_dynamic_lr = learning_rate(self.global_step).reshape(())\n                lr += (current_dynamic_lr,)\n        else:\n            lr = self.learning_rate(self.global_step).reshape(())\n    if self._is_dynamic_lr_or_weight_decay():\n        self.assignadd(self.global_step, self.global_step_increase_tensor)\n    return lr\n</code></pre>"},{"location":"reference/optim/#nadam","title":"NAdam","text":""},{"location":"reference/optim/#mindcv.optim.nadam.NAdam","title":"<code>mindcv.optim.nadam.NAdam</code>","text":"<p>               Bases: <code>Optimizer</code></p> <p>Implements NAdam algorithm (a variant of Adam based on Nesterov momentum).</p> Source code in <code>mindcv\\optim\\nadam.py</code> <pre><code>class NAdam(Optimizer):\n    \"\"\"\n    Implements NAdam algorithm (a variant of Adam based on Nesterov momentum).\n    \"\"\"\n\n    @opt_init_args_register\n    def __init__(\n        self,\n        params,\n        learning_rate=2e-3,\n        beta1=0.9,\n        beta2=0.999,\n        eps=1e-8,\n        weight_decay=0.0,\n        loss_scale=1.0,\n        schedule_decay=4e-3,\n    ):\n        super().__init__(learning_rate, params, weight_decay, loss_scale)\n        _check_param_value(beta1, beta2, eps, self.cls_name)\n        self.beta1 = Tensor(np.array([beta1]).astype(np.float32))\n        self.beta2 = Tensor(np.array([beta2]).astype(np.float32))\n        self.eps = Tensor(np.array([eps]).astype(np.float32))\n        self.moments1 = self.parameters.clone(prefix=\"nadam_m\", init=\"zeros\")\n        self.moments2 = self.parameters.clone(prefix=\"nadam_v\", init=\"zeros\")\n        self.schedule_decay = Tensor(np.array([schedule_decay]).astype(np.float32))\n        self.mu_schedule = Parameter(initializer(1, [1], ms.float32), name=\"mu_schedule\")\n        self.beta2_power = Parameter(initializer(1, [1], ms.float32), name=\"beta2_power\")\n\n    def get_lr(self):\n        \"\"\"\n        The optimizer calls this interface to get the learning rate for the current step. User-defined optimizers based\n        on :class:`mindspore.nn.Optimizer` can also call this interface before updating the parameters.\n\n        Returns:\n            float, the learning rate of current step.\n        \"\"\"\n        lr = self.learning_rate\n        if self.dynamic_lr:\n            if self.is_group_lr:\n                lr = ()\n                for learning_rate in self.learning_rate:\n                    current_dynamic_lr = learning_rate(self.global_step).reshape(())\n                    lr += (current_dynamic_lr,)\n            else:\n                lr = self.learning_rate(self.global_step).reshape(())\n        if self._is_dynamic_lr_or_weight_decay():\n            self.assignadd(self.global_step, self.global_step_increase_tensor)\n        return lr\n\n    @jit\n    def construct(self, gradients):\n        lr = self.get_lr()\n        params = self.parameters\n        step = self.global_step + _scaler_one\n        gradients = self.decay_weight(gradients)\n        mu = self.beta1 * (\n            _scaler_one - Tensor(0.5, ms.float32) * ops.pow(Tensor(0.96, ms.float32), step * self.schedule_decay)\n        )\n        mu_next = self.beta1 * (\n            _scaler_one\n            - Tensor(0.5, ms.float32) * ops.pow(Tensor(0.96, ms.float32), (step + _scaler_one) * self.schedule_decay)\n        )\n        mu_schedule = self.mu_schedule * mu\n        mu_schedule_next = self.mu_schedule * mu * mu_next\n        self.mu_schedule = mu_schedule\n        beta2_power = self.beta2_power * self.beta2\n        self.beta2_power = beta2_power\n\n        num_params = len(params)\n        for i in range(num_params):\n            ops.assign(self.moments1[i], self.beta1 * self.moments1[i] + (_scaler_one - self.beta1) * gradients[i])\n            ops.assign(\n                self.moments2[i], self.beta2 * self.moments2[i] + (_scaler_one - self.beta2) * ops.square(gradients[i])\n            )\n\n            regulate_m = mu_next * self.moments1[i] / (_scaler_one - mu_schedule_next) + (_scaler_one - mu) * gradients[\n                i\n            ] / (_scaler_one - mu_schedule)\n            regulate_v = self.moments2[i] / (_scaler_one - beta2_power)\n\n            update = params[i] - lr * regulate_m / (self.eps + ops.sqrt(regulate_v))\n            ops.assign(params[i], update)\n\n        return params\n</code></pre>"},{"location":"reference/optim/#mindcv.optim.nadam.NAdam.get_lr","title":"<code>mindcv.optim.nadam.NAdam.get_lr()</code>","text":"<p>The optimizer calls this interface to get the learning rate for the current step. User-defined optimizers based on :class:<code>mindspore.nn.Optimizer</code> can also call this interface before updating the parameters.</p> RETURNS DESCRIPTION <p>float, the learning rate of current step.</p> Source code in <code>mindcv\\optim\\nadam.py</code> <pre><code>def get_lr(self):\n    \"\"\"\n    The optimizer calls this interface to get the learning rate for the current step. User-defined optimizers based\n    on :class:`mindspore.nn.Optimizer` can also call this interface before updating the parameters.\n\n    Returns:\n        float, the learning rate of current step.\n    \"\"\"\n    lr = self.learning_rate\n    if self.dynamic_lr:\n        if self.is_group_lr:\n            lr = ()\n            for learning_rate in self.learning_rate:\n                current_dynamic_lr = learning_rate(self.global_step).reshape(())\n                lr += (current_dynamic_lr,)\n        else:\n            lr = self.learning_rate(self.global_step).reshape(())\n    if self._is_dynamic_lr_or_weight_decay():\n        self.assignadd(self.global_step, self.global_step_increase_tensor)\n    return lr\n</code></pre>"},{"location":"reference/scheduler/","title":"Learning Rate Scheduler","text":""},{"location":"reference/scheduler/#scheduler-factory","title":"Scheduler Factory","text":""},{"location":"reference/scheduler/#mindcv.scheduler.scheduler_factory.create_scheduler","title":"<code>mindcv.scheduler.scheduler_factory.create_scheduler(steps_per_epoch, scheduler='constant', lr=0.01, min_lr=1e-06, warmup_epochs=3, warmup_factor=0.0, decay_epochs=10, decay_rate=0.9, milestones=None, num_epochs=200, num_cycles=1, cycle_decay=1.0, lr_epoch_stair=False)</code>","text":"<p>Creates learning rate scheduler by name.</p> PARAMETER DESCRIPTION <code>steps_per_epoch</code> <p>number of steps per epoch.</p> <p> TYPE: <code>int</code> </p> <code>scheduler</code> <p>scheduler name like 'constant', 'cosine_decay', 'step_decay', 'exponential_decay', 'polynomial_decay', 'multi_step_decay'. Default: 'constant'.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'constant'</code> </p> <code>lr</code> <p>learning rate value. Default: 0.01.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.01</code> </p> <code>min_lr</code> <p>lower lr bound for 'cosine_decay' schedulers. Default: 1e-6.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1e-06</code> </p> <code>warmup_epochs</code> <p>epochs to warmup LR, if scheduler supports. Default: 3.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>warmup_factor</code> <p>the warmup phase of scheduler is a linearly increasing lr, the beginning factor is <code>warmup_factor</code>, i.e., the lr of the first step/epoch is lr*warmup_factor, and the ending lr in the warmup phase is lr. Default: 0.0</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>decay_epochs</code> <p>for 'cosine_decay' schedulers, decay LR to min_lr in <code>decay_epochs</code>. For 'step_decay' scheduler, decay LR by a factor of <code>decay_rate</code> every <code>decay_epochs</code>. Default: 10.</p> <p> TYPE: <code>int</code> DEFAULT: <code>10</code> </p> <code>decay_rate</code> <p>LR decay rate. Default: 0.9.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.9</code> </p> <code>milestones</code> <p>list of epoch milestones for 'multi_step_decay' scheduler. Must be increasing. Default: None</p> <p> TYPE: <code>list</code> DEFAULT: <code>None</code> </p> <code>num_epochs</code> <p>Number of total epochs. Default: 200.</p> <p> TYPE: <code>int</code> DEFAULT: <code>200</code> </p> <code>num_cycles</code> <p>Number of cycles for cosine decay and cyclic. Default: 1.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>cycle_decay</code> <p>Decay rate of lr max in each cosine cycle. Default: 1.0.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>lr_epoch_stair</code> <p>If True, LR will be updated in the beginning of each new epoch and the LR will be consistent for each batch in one epoch. Otherwise, learning rate will be updated dynamically in each step. Default: False.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Source code in <code>mindcv\\scheduler\\scheduler_factory.py</code> <pre><code>def create_scheduler(\n    steps_per_epoch: int,\n    scheduler: str = \"constant\",\n    lr: float = 0.01,\n    min_lr: float = 1e-6,\n    warmup_epochs: int = 3,\n    warmup_factor: float = 0.0,\n    decay_epochs: int = 10,\n    decay_rate: float = 0.9,\n    milestones: list = None,\n    num_epochs: int = 200,\n    num_cycles: int = 1,\n    cycle_decay: float = 1.0,\n    lr_epoch_stair: bool = False,\n):\n    r\"\"\"Creates learning rate scheduler by name.\n\n    Args:\n        steps_per_epoch: number of steps per epoch.\n        scheduler: scheduler name like 'constant', 'cosine_decay', 'step_decay',\n            'exponential_decay', 'polynomial_decay', 'multi_step_decay'. Default: 'constant'.\n        lr: learning rate value. Default: 0.01.\n        min_lr: lower lr bound for 'cosine_decay' schedulers. Default: 1e-6.\n        warmup_epochs: epochs to warmup LR, if scheduler supports. Default: 3.\n        warmup_factor: the warmup phase of scheduler is a linearly increasing lr,\n            the beginning factor is `warmup_factor`, i.e., the lr of the first step/epoch is lr*warmup_factor,\n            and the ending lr in the warmup phase is lr. Default: 0.0\n        decay_epochs: for 'cosine_decay' schedulers, decay LR to min_lr in `decay_epochs`.\n            For 'step_decay' scheduler, decay LR by a factor of `decay_rate` every `decay_epochs`. Default: 10.\n        decay_rate: LR decay rate. Default: 0.9.\n        milestones: list of epoch milestones for 'multi_step_decay' scheduler. Must be increasing. Default: None\n        num_epochs: Number of total epochs. Default: 200.\n        num_cycles: Number of cycles for cosine decay and cyclic. Default: 1.\n        cycle_decay: Decay rate of lr max in each cosine cycle. Default: 1.0.\n        lr_epoch_stair: If True, LR will be updated in the beginning of each new epoch\n            and the LR will be consistent for each batch in one epoch.\n            Otherwise, learning rate will be updated dynamically in each step. Default: False.\n    Returns:\n        Cell object for computing LR with input of current global steps\n    \"\"\"\n    # check params\n    if milestones is None:\n        milestones = []\n\n    if warmup_epochs + decay_epochs &gt; num_epochs:\n        _logger.warning(\"warmup_epochs + decay_epochs &gt; num_epochs. Please check and reduce decay_epochs!\")\n\n    # lr warmup phase\n    warmup_lr_scheduler = []\n    if warmup_epochs &gt; 0:\n        if warmup_factor == 0 and lr_epoch_stair:\n            _logger.warning(\n                \"The warmup factor is set to 0, lr of 0-th epoch is always zero! \" \"Recommend value is 0.01.\"\n            )\n        warmup_func = linear_lr if lr_epoch_stair else linear_refined_lr\n        warmup_lr_scheduler = warmup_func(\n            start_factor=warmup_factor,\n            end_factor=1.0,\n            total_iters=warmup_epochs,\n            lr=lr,\n            steps_per_epoch=steps_per_epoch,\n            epochs=warmup_epochs,\n        )\n\n    # lr decay phase\n    main_epochs = num_epochs - warmup_epochs\n    if scheduler in [\"cosine_decay\", \"warmup_cosine_decay\"]:\n        cosine_func = cosine_decay_lr if lr_epoch_stair else cosine_decay_refined_lr\n        main_lr_scheduler = cosine_func(\n            decay_epochs=decay_epochs,\n            eta_min=min_lr,\n            eta_max=lr,\n            steps_per_epoch=steps_per_epoch,\n            epochs=main_epochs,\n            num_cycles=num_cycles,\n            cycle_decay=cycle_decay,\n        )\n    elif scheduler == \"one_cycle\":\n        if lr_epoch_stair or warmup_epochs &gt; 0:\n            raise ValueError(\n                \"OneCycle scheduler doesn't support learning rate varies with epoch and warmup_epochs &gt; 0.\"\n            )\n        div_factor = 25.0\n        initial_lr = lr / div_factor\n        final_div_factor = initial_lr / min_lr\n        main_lr_scheduler = one_cycle_lr(\n            max_lr=lr,\n            final_div_factor=final_div_factor,\n            steps_per_epoch=steps_per_epoch,\n            epochs=main_epochs,\n        )\n    elif scheduler == \"cyclic\":\n        if lr_epoch_stair or warmup_epochs &gt; 0:\n            raise ValueError(\"Cyclic scheduler doesn't support learning rate varies with epoch and warmup_epochs &gt; 0.\")\n        num_steps = steps_per_epoch * main_epochs\n        step_size_up = int(num_steps / num_cycles / 2)\n        main_lr_scheduler = cyclic_lr(\n            base_lr=min_lr,\n            max_lr=lr,\n            step_size_up=step_size_up,\n            steps_per_epoch=steps_per_epoch,\n            epochs=main_epochs,\n        )\n    elif scheduler == \"exponential_decay\":\n        exponential_func = exponential_lr if lr_epoch_stair else exponential_refined_lr\n        main_lr_scheduler = exponential_func(\n            gamma=decay_rate, lr=lr, steps_per_epoch=steps_per_epoch, epochs=main_epochs\n        )\n    elif scheduler == \"polynomial_decay\":\n        polynomial_func = polynomial_lr if lr_epoch_stair else polynomial_refined_lr\n        main_lr_scheduler = polynomial_func(\n            total_iters=main_epochs, power=decay_rate, lr=lr, steps_per_epoch=steps_per_epoch, epochs=main_epochs\n        )\n    elif scheduler == \"step_decay\":\n        main_lr_scheduler = step_lr(\n            step_size=decay_epochs, gamma=decay_rate, lr=lr, steps_per_epoch=steps_per_epoch, epochs=main_epochs\n        )\n    elif scheduler == \"multi_step_decay\":\n        main_lr_scheduler = multi_step_lr(\n            milestones=milestones, gamma=decay_rate, lr=lr, steps_per_epoch=steps_per_epoch, epochs=main_epochs\n        )\n    elif scheduler == \"constant\":\n        main_lr_scheduler = [lr for _ in range(steps_per_epoch * main_epochs)]\n    else:\n        raise ValueError(f\"Invalid scheduler: {scheduler}\")\n\n    # combine\n    lr_scheduler = warmup_lr_scheduler + main_lr_scheduler\n\n    return lr_scheduler\n</code></pre>"},{"location":"reference/scheduler/#mindcv.scheduler.dynamic_lr","title":"<code>mindcv.scheduler.dynamic_lr</code>","text":"<p>Meta learning rate scheduler.</p> <p>This module implements exactly the same learning rate scheduler as native PyTorch, see <code>\"torch.optim.lr_scheduler\" &lt;https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate&gt;</code>_. At present, only <code>constant_lr</code>, <code>linear_lr</code>, <code>polynomial_lr</code>, <code>exponential_lr</code>, <code>step_lr</code>, <code>multi_step_lr</code>, <code>cosine_annealing_lr</code>, <code>cosine_annealing_warm_restarts_lr</code>, <code>one_cycle_lr</code>, <code>cyclic_lr</code> are implemented. The number, name and usage of the Positional Arguments are exactly the same as those of native PyTorch.</p> <p>However, due to the constraint of having to explicitly return the learning rate at each step, we have to introduce additional Keyword Arguments. There are only three Keyword Arguments introduced, namely <code>lr</code>, <code>steps_per_epoch</code> and <code>epochs</code>, explained as follows: <code>lr</code>: the basic learning rate when creating optim in torch. <code>steps_per_epoch</code>: the number of steps(iterations) of each epoch. <code>epochs</code>: the number of epoch. It and <code>steps_per_epoch</code> determine the length of the returned lrs.</p> <p>In all schedulers, <code>one_cycle_lr</code> and <code>cyclic_lr</code> only need two Keyword Arguments except <code>lr</code>, since when creating optim in torch, <code>lr</code> argument will have no effect if using the two schedulers above.</p> <p>Since most scheduler in PyTorch are coarse-grained, that is the learning rate is constant within a single epoch. For non-stepwise scheduler, we introduce several fine-grained variation, that is the learning rate is also changed within a single epoch. The function name of these variants have the <code>refined</code> keyword. The implemented fine-grained variation are list as follows: <code>linear_refined_lr</code>, <code>polynomial_refined_lr</code>, etc.</p>"},{"location":"reference/scheduler/#mindcv.scheduler.dynamic_lr.cosine_decay_lr","title":"<code>mindcv.scheduler.dynamic_lr.cosine_decay_lr(decay_epochs, eta_min, *, eta_max, steps_per_epoch, epochs, num_cycles=1, cycle_decay=1.0)</code>","text":"<p>update every epoch</p> Source code in <code>mindcv\\scheduler\\dynamic_lr.py</code> <pre><code>def cosine_decay_lr(decay_epochs, eta_min, *, eta_max, steps_per_epoch, epochs, num_cycles=1, cycle_decay=1.0):\n    \"\"\"update every epoch\"\"\"\n    tot_steps = steps_per_epoch * epochs\n    lrs = []\n\n    for c in range(num_cycles):\n        lr_max = eta_max * (cycle_decay**c)\n        delta = 0.5 * (lr_max - eta_min)\n        for i in range(steps_per_epoch * decay_epochs):\n            t_cur = math.floor(i / steps_per_epoch)\n            t_cur = min(t_cur, decay_epochs)\n            lr_cur = eta_min + delta * (1.0 + math.cos(math.pi * t_cur / decay_epochs))\n            if len(lrs) &lt; tot_steps:\n                lrs.append(lr_cur)\n            else:\n                break\n\n    if epochs &gt; num_cycles * decay_epochs:\n        for i in range((epochs - (num_cycles * decay_epochs)) * steps_per_epoch):\n            lrs.append(eta_min)\n\n    return lrs\n</code></pre>"},{"location":"reference/scheduler/#mindcv.scheduler.dynamic_lr.cosine_decay_refined_lr","title":"<code>mindcv.scheduler.dynamic_lr.cosine_decay_refined_lr(decay_epochs, eta_min, *, eta_max, steps_per_epoch, epochs, num_cycles=1, cycle_decay=1.0)</code>","text":"<p>update every step</p> Source code in <code>mindcv\\scheduler\\dynamic_lr.py</code> <pre><code>def cosine_decay_refined_lr(decay_epochs, eta_min, *, eta_max, steps_per_epoch, epochs, num_cycles=1, cycle_decay=1.0):\n    \"\"\"update every step\"\"\"\n    tot_steps = steps_per_epoch * epochs\n    lrs = []\n\n    for c in range(num_cycles):\n        lr_max = eta_max * (cycle_decay**c)\n        delta = 0.5 * (lr_max - eta_min)\n        for i in range(steps_per_epoch * decay_epochs):\n            t_cur = i / steps_per_epoch\n            t_cur = min(t_cur, decay_epochs)\n            lr_cur = eta_min + delta * (1.0 + math.cos(math.pi * t_cur / decay_epochs))\n            if len(lrs) &lt; tot_steps:\n                lrs.append(lr_cur)\n            else:\n                break\n\n    if epochs &gt; num_cycles * decay_epochs:\n        for i in range((epochs - (num_cycles * decay_epochs)) * steps_per_epoch):\n            lrs.append(eta_min)\n\n    return lrs\n</code></pre>"},{"location":"reference/scheduler/#mindcv.scheduler.dynamic_lr.cyclic_lr","title":"<code>mindcv.scheduler.dynamic_lr.cyclic_lr(base_lr, max_lr, step_size_up=2000, step_size_down=None, mode='triangular', gamma=1.0, scale_fn=None, scale_mode='cycle', *, steps_per_epoch, epochs)</code>","text":"<p>Cyclic learning rate scheduler based on '\"Cyclical Learning Rates for Training Neural Networks\" https://arxiv.org/abs/1708.07120'</p> PARAMETER DESCRIPTION <code>base_lr</code> <p>Lower learning rate boundaries in each cycle.</p> <p> TYPE: <code>float</code> </p> <code>max_lr</code> <p>Upper learning rate boundaries in each cycle.</p> <p> TYPE: <code>float</code> </p> <code>step_size_up</code> <p>Number of steps in the increasing half in each cycle. Default: 2000.</p> <p> TYPE: <code>int</code> DEFAULT: <code>2000</code> </p> <code>step_size_down</code> <p>Number of steps in the increasing half in each cycle. If step_size_down is None, it's set to step_size_up. Default: None.</p> <p> DEFAULT: <code>None</code> </p> <code>div_factor</code> <p>Initial learning rate via initial_lr = max_lr / div_factor. Default: 25.0.</p> <p> </p> <code>final_div_factor</code> <p>Minimum learning rate at the end via min_lr = initial_lr / final_div_factor. Default: 10000.0.</p> <p> </p> <code>mode</code> <p>One of {triangular, triangular2, exp_range}. If scale_fn is not None, it's set to None. Default: 'triangular'.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'triangular'</code> </p> <code>gamma</code> <p>Constant in 'exp_range' calculating fuction: gamma**(cycle_iterations). Default: 1.0</p> <p> DEFAULT: <code>1.0</code> </p> <code>scale_fn</code> <p>Custom scaling policy defined by a single argument lambda function. If it's not None, 'mode' is ignored. Default: None</p> <p> DEFAULT: <code>None</code> </p> <code>scale_mode</code> <p>One of {'cycle', 'iterations'}. Determine scale_fn is evaluated on cycle number or cycle iterations. Default: 'cycle'</p> <p> DEFAULT: <code>'cycle'</code> </p> <code>steps_per_epoch</code> <p>Number of steps per epoch.</p> <p> TYPE: <code>int</code> </p> <code>epochs</code> <p>Number of total epochs.</p> <p> TYPE: <code>int</code> </p> Source code in <code>mindcv\\scheduler\\dynamic_lr.py</code> <pre><code>def cyclic_lr(\n    base_lr: float,\n    max_lr: float,\n    step_size_up: int = 2000,\n    step_size_down=None,\n    mode: str = \"triangular\",\n    gamma=1.0,\n    scale_fn=None,\n    scale_mode=\"cycle\",\n    *,\n    steps_per_epoch: int,\n    epochs: int,\n):\n    \"\"\"\n    Cyclic learning rate scheduler based on\n    '\"Cyclical Learning Rates for Training Neural Networks\" &lt;https://arxiv.org/abs/1708.07120&gt;'\n\n    Args:\n        base_lr: Lower learning rate boundaries in each cycle.\n        max_lr: Upper learning rate boundaries in each cycle.\n        step_size_up: Number of steps in the increasing half in each cycle. Default: 2000.\n        step_size_down: Number of steps in the increasing half in each cycle. If step_size_down\n            is None, it's set to step_size_up. Default: None.\n        div_factor: Initial learning rate via initial_lr = max_lr / div_factor.\n            Default: 25.0.\n        final_div_factor: Minimum learning rate at the end via\n            min_lr = initial_lr / final_div_factor. Default: 10000.0.\n        mode: One of {triangular, triangular2, exp_range}. If scale_fn is not None, it's set to\n            None. Default: 'triangular'.\n        gamma: Constant in 'exp_range' calculating fuction: gamma**(cycle_iterations).\n            Default: 1.0\n        scale_fn: Custom scaling policy defined by a single argument lambda function. If it's\n            not None, 'mode' is ignored. Default: None\n        scale_mode: One of {'cycle', 'iterations'}. Determine scale_fn is evaluated on cycle\n            number or cycle iterations. Default: 'cycle'\n        steps_per_epoch: Number of steps per epoch.\n        epochs: Number of total epochs.\n    \"\"\"\n\n    def _triangular_scale_fn(x):\n        return 1.0\n\n    def _triangular2_scale_fn(x):\n        return 1 / (2.0**(x - 1))\n\n    def _exp_range_scale_fn(x):\n        return gamma**x\n\n    steps = steps_per_epoch * epochs\n    step_size_up = float(step_size_up)\n    step_size_down = float(step_size_down) if step_size_down is not None else step_size_up\n    total_size = step_size_up + step_size_down\n    step_ratio = step_size_up / total_size\n    if scale_fn is None:\n        if mode == \"triangular\":\n            scale_fn = _triangular_scale_fn\n            scale_mode = \"cycle\"\n        elif mode == \"triangular2\":\n            scale_fn = _triangular2_scale_fn\n            scale_mode = \"cycle\"\n        elif mode == \"exp_range\":\n            scale_fn = _exp_range_scale_fn\n            scale_mode = \"iterations\"\n    lrs = []\n    for i in range(steps):\n        cycle = math.floor(1 + i / total_size)\n        x = 1.0 + i / total_size - cycle\n        if x &lt;= step_ratio:\n            scale_factor = x / step_ratio\n        else:\n            scale_factor = (x - 1) / (step_ratio - 1)\n        base_height = (max_lr - base_lr) * scale_factor\n        if scale_mode == \"cycle\":\n            lrs.append(base_lr + base_height * scale_fn(cycle))\n        else:\n            lrs.append(base_lr + base_height * scale_fn(i))\n    return lrs\n</code></pre>"},{"location":"reference/scheduler/#mindcv.scheduler.dynamic_lr.one_cycle_lr","title":"<code>mindcv.scheduler.dynamic_lr.one_cycle_lr(max_lr, pct_start=0.3, anneal_strategy='cos', div_factor=25.0, final_div_factor=10000.0, three_phase=False, *, steps_per_epoch, epochs)</code>","text":"<p>OneCycle learning rate scheduler based on '\"Super-Convergence: Very Fast Training of Neural Networks Using Large Learning Rates\" https://arxiv.org/abs/1708.07120'</p> PARAMETER DESCRIPTION <code>max_lr</code> <p>Upper learning rate boundaries in the cycle.</p> <p> TYPE: <code>float</code> </p> <code>pct_start</code> <p>The percentage of the number of steps of increasing learning rate in the cycle. Default: 0.3.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.3</code> </p> <code>anneal_strategy</code> <p>Define the annealing strategy: \"cos\" for cosine annealing, \"linear\" for linear annealing. Default: \"cos\".</p> <p> TYPE: <code>str</code> DEFAULT: <code>'cos'</code> </p> <code>div_factor</code> <p>Initial learning rate via initial_lr = max_lr / div_factor. Default: 25.0.</p> <p> TYPE: <code>float</code> DEFAULT: <code>25.0</code> </p> <code>final_div_factor</code> <p>Minimum learning rate at the end via min_lr = initial_lr / final_div_factor. Default: 10000.0.</p> <p> TYPE: <code>float</code> DEFAULT: <code>10000.0</code> </p> <code>three_phase</code> <p>If True, learning rate will be updated by three-phase according to \"final_div_factor\". Otherwise, learning rate will be updated by two-phase. Default: False.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>steps_per_epoch</code> <p>Number of steps per epoch.</p> <p> TYPE: <code>int</code> </p> <code>epochs</code> <p>Number of total epochs.</p> <p> TYPE: <code>int</code> </p> Source code in <code>mindcv\\scheduler\\dynamic_lr.py</code> <pre><code>def one_cycle_lr(\n    max_lr: float,\n    pct_start: float = 0.3,\n    anneal_strategy: str = \"cos\",\n    div_factor: float = 25.0,\n    final_div_factor: float = 10000.0,\n    three_phase: bool = False,\n    *,\n    steps_per_epoch: int,\n    epochs: int,\n):\n    \"\"\"\n    OneCycle learning rate scheduler based on\n    '\"Super-Convergence: Very Fast Training of Neural Networks Using Large Learning Rates\"\n    &lt;https://arxiv.org/abs/1708.07120&gt;'\n\n    Args:\n        max_lr: Upper learning rate boundaries in the cycle.\n        pct_start: The percentage of the number of steps of increasing learning rate\n            in the cycle. Default: 0.3.\n        anneal_strategy: Define the annealing strategy: \"cos\" for cosine annealing,\n            \"linear\" for linear annealing. Default: \"cos\".\n        div_factor: Initial learning rate via initial_lr = max_lr / div_factor.\n            Default: 25.0.\n        final_div_factor: Minimum learning rate at the end via\n            min_lr = initial_lr / final_div_factor. Default: 10000.0.\n        three_phase: If True, learning rate will be updated by three-phase according to\n            \"final_div_factor\". Otherwise, learning rate will be updated by two-phase.\n            Default: False.\n        steps_per_epoch: Number of steps per epoch.\n        epochs: Number of total epochs.\n    \"\"\"\n\n    def _annealing_cos(start, end, pct):\n        cos_out = math.cos(math.pi * pct) + 1\n        return end + (start - end) / 2.0 * cos_out\n\n    def _annealing_linear(start, end, pct):\n        return (end - start) * pct + start\n\n    initial_lr = max_lr / div_factor\n    min_lr = initial_lr / final_div_factor\n    steps = steps_per_epoch * epochs\n    step_size_up = float(pct_start * steps) - 1\n    step_size_down = float(2 * pct_start * steps) - 2\n    step_size_end = float(steps) - 1\n    if anneal_strategy == \"cos\":\n        anneal_func = _annealing_cos\n    elif anneal_strategy == \"linear\":\n        anneal_func = _annealing_linear\n    else:\n        raise ValueError(f\"anneal_strategy must be one of 'cos' or 'linear', but got {anneal_strategy}\")\n    lrs = []\n    for i in range(steps):\n        if three_phase:\n            if i &lt;= step_size_up:\n                lrs.append(anneal_func(initial_lr, max_lr, i / step_size_up))\n            elif step_size_up &lt; i &lt;= step_size_down:\n                lrs.append(anneal_func(max_lr, initial_lr, (i - step_size_up) / (step_size_down - step_size_up)))\n            else:\n                lrs.append(anneal_func(initial_lr, min_lr, (i - step_size_down) / (step_size_end - step_size_down)))\n        else:\n            if i &lt;= step_size_up:\n                lrs.append(anneal_func(initial_lr, max_lr, i / step_size_up))\n            else:\n                lrs.append(anneal_func(max_lr, min_lr, (i - step_size_up) / (step_size_end - step_size_up)))\n    return lrs\n</code></pre>"},{"location":"reference/utils/","title":"Utility","text":""},{"location":"reference/utils/#logger","title":"Logger","text":""},{"location":"reference/utils/#mindcv.utils.logger.set_logger","title":"<code>mindcv.utils.logger.set_logger(name=None, output_dir=None, rank=0, log_level=logging.INFO, color=True)</code>","text":"<p>Initialize the logger.</p> <p>If the logger has not been initialized, this method will initialize the logger by adding one or two handlers, otherwise the initialized logger will be directly returned. During initialization, only logger of the master process is added console handler. If <code>output_dir</code> is specified, all loggers will be added file handler.</p> PARAMETER DESCRIPTION <code>name</code> <p>Logger name. Defaults to None to set up root logger.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>output_dir</code> <p>The directory to save log.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>rank</code> <p>Process rank in the distributed training. Defaults to 0.</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>log_level</code> <p>Verbosity level of the logger. Defaults to <code>logging.INFO</code>.</p> <p> TYPE: <code>int</code> DEFAULT: <code>INFO</code> </p> <code>color</code> <p>If True, color the output. Defaults to True.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>Logger</code> <p>logging.Logger: A initialized logger.</p> Source code in <code>mindcv\\utils\\logger.py</code> <pre><code>def set_logger(\n    name: Optional[str] = None,\n    output_dir: Optional[str] = None,\n    rank: int = 0,\n    log_level: int = logging.INFO,\n    color: bool = True,\n) -&gt; logging.Logger:\n    \"\"\"Initialize the logger.\n\n    If the logger has not been initialized, this method will initialize the\n    logger by adding one or two handlers, otherwise the initialized logger will\n    be directly returned. During initialization, only logger of the master\n    process is added console handler. If ``output_dir`` is specified, all loggers\n    will be added file handler.\n\n    Args:\n        name: Logger name. Defaults to None to set up root logger.\n        output_dir: The directory to save log.\n        rank: Process rank in the distributed training. Defaults to 0.\n        log_level: Verbosity level of the logger. Defaults to ``logging.INFO``.\n        color: If True, color the output. Defaults to True.\n\n    Returns:\n        logging.Logger: A initialized logger.\n    \"\"\"\n    rank = 0 if rank is None else rank\n    if name in logger_initialized:\n        return logger_initialized[name]\n\n    # get root logger if name is None\n    logger = logging.getLogger(name)\n    logger.setLevel(log_level)\n    # the messages of this logger will not be propagated to its parent\n    logger.propagate = False\n\n    fmt = \"%(asctime)s %(name)s %(levelname)s - %(message)s\"\n    datefmt = \"[%Y-%m-%d %H:%M:%S]\"\n\n    # create console handler for master process\n    if rank == 0:\n        if color:\n            if has_rich:\n                console_handler = RichHandler(level=log_level, log_time_format=datefmt)\n            elif has_termcolor:\n                console_handler = logging.StreamHandler(stream=sys.stdout)\n                console_handler.setLevel(log_level)\n                console_handler.setFormatter(_ColorfulFormatter(fmt=fmt, datefmt=datefmt))\n            else:\n                raise NotImplementedError(\"If you want color, 'rich' or 'termcolor' has to be installed!\")\n        else:\n            console_handler = logging.StreamHandler(stream=sys.stdout)\n            console_handler.setLevel(log_level)\n            console_handler.setFormatter(logging.Formatter(fmt=fmt, datefmt=datefmt))\n        logger.addHandler(console_handler)\n\n    if output_dir is not None:\n        os.makedirs(output_dir, exist_ok=True)\n        file_handler = logging.FileHandler(os.path.join(output_dir, f\"rank{rank}.log\"))\n        file_handler.setLevel(log_level)\n        file_handler.setFormatter(logging.Formatter(fmt=fmt, datefmt=datefmt))\n        logger.addHandler(file_handler)\n\n    logger_initialized[name] = logger\n    return logger\n</code></pre>"},{"location":"reference/utils/#callbacks","title":"Callbacks","text":""},{"location":"reference/utils/#mindcv.utils.callbacks.StateMonitor","title":"<code>mindcv.utils.callbacks.StateMonitor</code>","text":"<p>               Bases: <code>Callback</code></p> <p>Train loss and validation accuracy monitor, after each epoch save the best checkpoint file with the highest validation accuracy.</p> Source code in <code>mindcv\\utils\\callbacks.py</code> <pre><code>class StateMonitor(Callback):\n    \"\"\"\n    Train loss and validation accuracy monitor, after each epoch save the\n    best checkpoint file with the highest validation accuracy.\n    \"\"\"\n\n    def __init__(\n        self,\n        model,\n        model_name=\"\",\n        model_ema=False,\n        last_epoch=0,\n        dataset_sink_mode=True,\n        dataset_val=None,\n        metric_name=(\"accuracy\",),\n        val_interval=1,\n        val_start_epoch=1,\n        save_best_ckpt=True,\n        ckpt_save_dir=\"./\",\n        ckpt_save_interval=1,\n        ckpt_save_policy=None,\n        ckpt_keep_max=10,\n        summary_dir=\"./\",\n        log_interval=100,\n        rank_id=None,\n        device_num=None,\n    ):\n        super().__init__()\n        # model\n        self.model = model\n        self.model_name = model_name\n        self.model_ema = model_ema\n        self.last_epoch = last_epoch\n        self.dataset_sink_mode = dataset_sink_mode\n        # evaluation\n        self.dataset_val = dataset_val\n        self.metric_name = metric_name\n        self.val_interval = val_interval\n        self.val_start_epoch = val_start_epoch\n        # logging\n        self.best_res = 0\n        self.best_epoch = -1\n        self.save_best_ckpt = save_best_ckpt\n        self.ckpt_save_dir = ckpt_save_dir\n        self.ckpt_save_interval = ckpt_save_interval\n        self.ckpt_save_policy = ckpt_save_policy\n        self.ckpt_keep_max = ckpt_keep_max\n        self.ckpt_manager = CheckpointManager(ckpt_save_policy=self.ckpt_save_policy)\n        self._need_flush_from_cache = True\n        self.summary_dir = summary_dir\n        self.log_interval = log_interval\n        # system\n        self.rank_id = rank_id if rank_id is not None else 0\n        self.device_num = device_num if rank_id is not None else 1\n        if self.rank_id in [0, None]:\n            os.makedirs(ckpt_save_dir, exist_ok=True)\n            self.log_file = os.path.join(ckpt_save_dir, \"result.log\")\n            log_line = \"\".join(\n                f\"{s:&lt;20}\" for s in [\"Epoch\", \"TrainLoss\", *metric_name, \"TrainTime\", \"EvalTime\", \"TotalTime\"]\n            )\n            with open(self.log_file, \"w\", encoding=\"utf-8\") as fp:  # writing the title of result.log\n                fp.write(log_line + \"\\n\")\n        if self.device_num &gt; 1:\n            self.all_reduce = AllReduceSum()\n        # timestamp\n        self.step_ts = None\n        self.epoch_ts = None\n        self.step_time_accum = 0\n        # model_ema\n        if self.model_ema:\n            self.hyper_map = ops.HyperMap()\n            self.online_params = ParameterTuple(self.model.train_network.get_parameters())\n            self.swap_params = self.online_params.clone(\"swap\", \"zeros\")\n\n    def __enter__(self):\n        self.summary_record = SummaryRecord(self.summary_dir)\n        return self\n\n    def __exit__(self, *exc_args):\n        self.summary_record.close()\n\n    def apply_eval(self, run_context):\n        \"\"\"Model evaluation, return validation accuracy.\"\"\"\n        if self.model_ema:\n            cb_params = run_context.original_args()\n            self.hyper_map(ops.assign, self.swap_params, self.online_params)\n            ema_dict = dict()\n            net = self._get_network_from_cbp(cb_params)\n            for param in net.get_parameters():\n                if param.name.startswith(\"ema\"):\n                    new_name = param.name.split(\"ema.\")[1]\n                    ema_dict[new_name] = param.data\n            load_param_into_net(self.model.train_network.network, ema_dict)\n            res_dict = self.model.eval(self.dataset_val, dataset_sink_mode=False)\n            self.hyper_map(ops.assign, self.online_params, self.swap_params)\n        else:\n            res_dict = self.model.eval(self.dataset_val, dataset_sink_mode=False)\n        res_array = ms.Tensor(list(res_dict.values()), ms.float32)\n        if self.device_num &gt; 1:\n            res_array = self.all_reduce(res_array)\n            res_array /= self.device_num\n        res_array = res_array.asnumpy()\n        return res_array\n\n    def on_train_step_begin(self, run_context):\n        self.step_ts = time()\n\n    def on_train_epoch_begin(self, run_context):\n        self.epoch_ts = time()\n\n    def on_train_step_end(self, run_context):\n        cb_params = run_context.original_args()\n        num_epochs = cb_params.epoch_num\n        num_batches = cb_params.batch_num\n        # num_steps = num_batches * num_epochs\n        # cur_x start from 1, end at num_xs, range: [1, num_xs]\n        cur_step = cb_params.cur_step_num + self.last_epoch * num_batches\n        cur_epoch = cb_params.cur_epoch_num + self.last_epoch\n        cur_batch = (cur_step - 1) % num_batches + 1\n\n        self.step_time_accum += time() - self.step_ts\n        if cur_batch % self.log_interval == 0 or cur_batch == num_batches or cur_batch == 1:\n            lr = self._get_lr_from_cbp(cb_params)\n            loss = self._get_loss_from_cbp(cb_params)\n            _logger.info(\n                f\"Epoch: [{cur_epoch}/{num_epochs}], \"\n                f\"batch: [{cur_batch}/{num_batches}], \"\n                f\"loss: {loss.asnumpy():.6f}, \"\n                f\"lr: {lr.asnumpy():.6f}, \"\n                f\"time: {self.step_time_accum:.6f}s\"\n            )\n            self.step_time_accum = 0\n\n    def on_train_epoch_end(self, run_context):\n        \"\"\"\n        After epoch, print train loss and val accuracy,\n        save the best ckpt file with the highest validation accuracy.\n        \"\"\"\n        cb_params = run_context.original_args()\n        num_epochs = cb_params.epoch_num\n        num_batches = cb_params.batch_num\n        cur_step = cb_params.cur_step_num + self.last_epoch * num_batches\n        cur_epoch = cb_params.cur_epoch_num + self.last_epoch\n        cur_batch = (cur_step - 1) % num_batches + 1\n\n        train_time = time() - self.epoch_ts\n        loss = self._get_loss_from_cbp(cb_params)\n\n        val_time = 0\n        res = np.zeros(len(self.metric_name), dtype=np.float32)\n        # val while training if validation loader is not None\n        if (\n            self.dataset_val is not None\n            and cur_epoch &gt;= self.val_start_epoch\n            and (cur_epoch - self.val_start_epoch) % self.val_interval == 0\n        ):\n            val_time = time()\n            res = self.apply_eval(run_context)\n            val_time = time() - val_time\n            # record val acc\n            metric_str = \"Validation \"\n            for i in range(len(self.metric_name)):\n                metric_str += f\"{self.metric_name[i]}: {res[i]:.4%}, \"\n            metric_str += f\"time: {val_time:.6f}s\"\n            _logger.info(metric_str)\n            # save the best ckpt file\n            if res[0] &gt; self.best_res:\n                self.best_res = res[0]\n                self.best_epoch = cur_epoch\n                _logger.info(f\"=&gt; New best val acc: {res[0]:.4%}\")\n\n        # save checkpoint\n        if self.rank_id in [0, None]:\n            if self.save_best_ckpt and self.best_epoch == cur_epoch:  # always save ckpt if cur epoch got best acc\n                best_ckpt_save_path = os.path.join(self.ckpt_save_dir, f\"{self.model_name}_best.ckpt\")\n                save_checkpoint(cb_params.train_network, best_ckpt_save_path, async_save=True)\n            if (cur_epoch % self.ckpt_save_interval == 0) or (cur_epoch == num_epochs):\n                if self._need_flush_from_cache:\n                    self._flush_from_cache(cb_params)\n                # save optim for resume\n                optimizer = self._get_optimizer_from_cbp(cb_params)\n                optim_save_path = os.path.join(self.ckpt_save_dir, f\"optim_{self.model_name}.ckpt\")\n                save_checkpoint(optimizer, optim_save_path, async_save=True)\n                # keep checkpoint files number equal max number.\n                ckpt_save_path = os.path.join(self.ckpt_save_dir, f\"{self.model_name}-{cur_epoch}_{cur_batch}.ckpt\")\n                _logger.info(f\"Saving model to {ckpt_save_path}\")\n                self.ckpt_manager.save_ckpoint(\n                    cb_params.train_network,\n                    num_ckpt=self.ckpt_keep_max,\n                    metric=res[0],\n                    save_path=ckpt_save_path,\n                )\n\n        # logging\n        total_time = time() - self.epoch_ts\n        _logger.info(\n            f\"Total time since last epoch: {total_time:.6f}(train: {train_time:.6f}, val: {val_time:.6f})s, \"\n            f\"ETA: {(num_epochs - cur_epoch) * total_time:.6f}s\"\n        )\n        _logger.info(\"-\" * 80)\n        if self.rank_id in [0, None]:\n            log_line = \"\".join(\n                f\"{s:&lt;20}\"\n                for s in [\n                    f\"{cur_epoch}\",\n                    f\"{loss.asnumpy():.6f}\",\n                    *[f\"{i:.4%}\" for i in res],\n                    f\"{train_time:.2f}\",\n                    f\"{val_time:.2f}\",\n                    f\"{total_time:.2f}\",\n                ]\n            )\n            with open(self.log_file, \"a\", encoding=\"utf-8\") as fp:\n                fp.write(log_line + \"\\n\")\n\n        # summary\n        self.summary_record.add_value(\"scalar\", f\"train_loss_{self.rank_id}\", loss)\n        for i in range(len(res)):\n            self.summary_record.add_value(\n                \"scalar\", f\"val_{self.metric_name[i]}_{self.rank_id}\", Tensor(res[i], dtype=ms.float32)\n            )\n        self.summary_record.record(cur_step)\n\n    def on_train_end(self, run_context):\n        _logger.info(\"Finish training!\")\n        if self.dataset_val is not None:\n            _logger.info(\n                f\"The best validation {self.metric_name[0]} is: {self.best_res:.4%} at epoch {self.best_epoch}.\"\n            )\n        _logger.info(\"=\" * 80)\n\n    def _get_network_from_cbp(self, cb_params):\n        if self.dataset_sink_mode:\n            network = cb_params.train_network.network\n        else:\n            network = cb_params.train_network\n        return network\n\n    def _get_optimizer_from_cbp(self, cb_params):\n        if cb_params.optimizer is not None:\n            optimizer = cb_params.optimizer\n        elif self.dataset_sink_mode:\n            optimizer = cb_params.train_network.network.optimizer\n        else:\n            optimizer = cb_params.train_network.optimizer\n        return optimizer\n\n    def _get_lr_from_cbp(self, cb_params):\n        optimizer = self._get_optimizer_from_cbp(cb_params)\n        if optimizer.global_step &lt; 1:\n            _logger.warning(\n                \"`global_step` of optimizer is less than 1. It seems to be a overflow at the first step. \"\n                \"If you keep seeing this message, it means that the optimizer never actually called.\"\n            )\n            optim_step = Tensor((0,), ms.int32)\n        else:  # if the optimizer is successfully called, the global_step will actually be the value of next step.\n            optim_step = optimizer.global_step - 1\n        if optimizer.dynamic_lr:\n            if isinstance(optimizer.learning_rate, ms.nn.CellList):\n                # return the learning rates of the first parameter if dynamic_lr\n                lr = optimizer.learning_rate[0](optim_step)[0]\n            else:\n                lr = optimizer.learning_rate(optim_step)[0]\n        else:\n            lr = optimizer.learning_rate\n        return lr\n\n    def _get_loss_from_cbp(self, cb_params):\n        \"\"\"\n        Get loss from the network output.\n        Args:\n            cb_params (_InternalCallbackParam): Callback parameters.\n        Returns:\n            Union[Tensor, None], if parse loss success, will return a Tensor value(shape is [1]), else return None.\n        \"\"\"\n        output = cb_params.net_outputs\n        if output is None:\n            _logger.warning(\"Can not find any output by this network, so SummaryCollector will not collect loss.\")\n            return None\n\n        if isinstance(output, (int, float, Tensor)):\n            loss = output\n        elif isinstance(output, (list, tuple)) and output:\n            # If the output is a list, since the default network returns loss first,\n            # we assume that the first one is loss.\n            loss = output[0]\n        else:\n            _logger.warning(\n                \"The output type could not be identified, expect type is one of \"\n                \"[int, float, Tensor, list, tuple], so no loss was recorded in SummaryCollector.\"\n            )\n            return None\n\n        if not isinstance(loss, Tensor):\n            loss = Tensor(loss)\n\n        loss = Tensor(np.mean(loss.asnumpy()))\n        return loss\n\n    def _flush_from_cache(self, cb_params):\n        \"\"\"Flush cache data to host if tensor is cache enable.\"\"\"\n        has_cache_params = False\n        params = cb_params.train_network.get_parameters()\n        for param in params:\n            if param.cache_enable:\n                has_cache_params = True\n                Tensor(param).flush_from_cache()\n        if not has_cache_params:\n            self._need_flush_from_cache = False\n</code></pre>"},{"location":"reference/utils/#mindcv.utils.callbacks.StateMonitor.apply_eval","title":"<code>mindcv.utils.callbacks.StateMonitor.apply_eval(run_context)</code>","text":"<p>Model evaluation, return validation accuracy.</p> Source code in <code>mindcv\\utils\\callbacks.py</code> <pre><code>def apply_eval(self, run_context):\n    \"\"\"Model evaluation, return validation accuracy.\"\"\"\n    if self.model_ema:\n        cb_params = run_context.original_args()\n        self.hyper_map(ops.assign, self.swap_params, self.online_params)\n        ema_dict = dict()\n        net = self._get_network_from_cbp(cb_params)\n        for param in net.get_parameters():\n            if param.name.startswith(\"ema\"):\n                new_name = param.name.split(\"ema.\")[1]\n                ema_dict[new_name] = param.data\n        load_param_into_net(self.model.train_network.network, ema_dict)\n        res_dict = self.model.eval(self.dataset_val, dataset_sink_mode=False)\n        self.hyper_map(ops.assign, self.online_params, self.swap_params)\n    else:\n        res_dict = self.model.eval(self.dataset_val, dataset_sink_mode=False)\n    res_array = ms.Tensor(list(res_dict.values()), ms.float32)\n    if self.device_num &gt; 1:\n        res_array = self.all_reduce(res_array)\n        res_array /= self.device_num\n    res_array = res_array.asnumpy()\n    return res_array\n</code></pre>"},{"location":"reference/utils/#mindcv.utils.callbacks.StateMonitor.on_train_epoch_end","title":"<code>mindcv.utils.callbacks.StateMonitor.on_train_epoch_end(run_context)</code>","text":"<p>After epoch, print train loss and val accuracy, save the best ckpt file with the highest validation accuracy.</p> Source code in <code>mindcv\\utils\\callbacks.py</code> <pre><code>def on_train_epoch_end(self, run_context):\n    \"\"\"\n    After epoch, print train loss and val accuracy,\n    save the best ckpt file with the highest validation accuracy.\n    \"\"\"\n    cb_params = run_context.original_args()\n    num_epochs = cb_params.epoch_num\n    num_batches = cb_params.batch_num\n    cur_step = cb_params.cur_step_num + self.last_epoch * num_batches\n    cur_epoch = cb_params.cur_epoch_num + self.last_epoch\n    cur_batch = (cur_step - 1) % num_batches + 1\n\n    train_time = time() - self.epoch_ts\n    loss = self._get_loss_from_cbp(cb_params)\n\n    val_time = 0\n    res = np.zeros(len(self.metric_name), dtype=np.float32)\n    # val while training if validation loader is not None\n    if (\n        self.dataset_val is not None\n        and cur_epoch &gt;= self.val_start_epoch\n        and (cur_epoch - self.val_start_epoch) % self.val_interval == 0\n    ):\n        val_time = time()\n        res = self.apply_eval(run_context)\n        val_time = time() - val_time\n        # record val acc\n        metric_str = \"Validation \"\n        for i in range(len(self.metric_name)):\n            metric_str += f\"{self.metric_name[i]}: {res[i]:.4%}, \"\n        metric_str += f\"time: {val_time:.6f}s\"\n        _logger.info(metric_str)\n        # save the best ckpt file\n        if res[0] &gt; self.best_res:\n            self.best_res = res[0]\n            self.best_epoch = cur_epoch\n            _logger.info(f\"=&gt; New best val acc: {res[0]:.4%}\")\n\n    # save checkpoint\n    if self.rank_id in [0, None]:\n        if self.save_best_ckpt and self.best_epoch == cur_epoch:  # always save ckpt if cur epoch got best acc\n            best_ckpt_save_path = os.path.join(self.ckpt_save_dir, f\"{self.model_name}_best.ckpt\")\n            save_checkpoint(cb_params.train_network, best_ckpt_save_path, async_save=True)\n        if (cur_epoch % self.ckpt_save_interval == 0) or (cur_epoch == num_epochs):\n            if self._need_flush_from_cache:\n                self._flush_from_cache(cb_params)\n            # save optim for resume\n            optimizer = self._get_optimizer_from_cbp(cb_params)\n            optim_save_path = os.path.join(self.ckpt_save_dir, f\"optim_{self.model_name}.ckpt\")\n            save_checkpoint(optimizer, optim_save_path, async_save=True)\n            # keep checkpoint files number equal max number.\n            ckpt_save_path = os.path.join(self.ckpt_save_dir, f\"{self.model_name}-{cur_epoch}_{cur_batch}.ckpt\")\n            _logger.info(f\"Saving model to {ckpt_save_path}\")\n            self.ckpt_manager.save_ckpoint(\n                cb_params.train_network,\n                num_ckpt=self.ckpt_keep_max,\n                metric=res[0],\n                save_path=ckpt_save_path,\n            )\n\n    # logging\n    total_time = time() - self.epoch_ts\n    _logger.info(\n        f\"Total time since last epoch: {total_time:.6f}(train: {train_time:.6f}, val: {val_time:.6f})s, \"\n        f\"ETA: {(num_epochs - cur_epoch) * total_time:.6f}s\"\n    )\n    _logger.info(\"-\" * 80)\n    if self.rank_id in [0, None]:\n        log_line = \"\".join(\n            f\"{s:&lt;20}\"\n            for s in [\n                f\"{cur_epoch}\",\n                f\"{loss.asnumpy():.6f}\",\n                *[f\"{i:.4%}\" for i in res],\n                f\"{train_time:.2f}\",\n                f\"{val_time:.2f}\",\n                f\"{total_time:.2f}\",\n            ]\n        )\n        with open(self.log_file, \"a\", encoding=\"utf-8\") as fp:\n            fp.write(log_line + \"\\n\")\n\n    # summary\n    self.summary_record.add_value(\"scalar\", f\"train_loss_{self.rank_id}\", loss)\n    for i in range(len(res)):\n        self.summary_record.add_value(\n            \"scalar\", f\"val_{self.metric_name[i]}_{self.rank_id}\", Tensor(res[i], dtype=ms.float32)\n        )\n    self.summary_record.record(cur_step)\n</code></pre>"},{"location":"reference/utils/#mindcv.utils.callbacks.ValCallback","title":"<code>mindcv.utils.callbacks.ValCallback</code>","text":"<p>               Bases: <code>Callback</code></p> Source code in <code>mindcv\\utils\\callbacks.py</code> <pre><code>class ValCallback(Callback):\n    def __init__(self, log_interval=100):\n        super().__init__()\n        self.log_interval = log_interval\n        self.ts = time()\n\n    def on_eval_step_end(self, run_context):\n        cb_params = run_context.original_args()\n        num_batches = cb_params.batch_num\n        cur_step = cb_params.cur_step_num\n\n        if cur_step % self.log_interval == 0 or cur_step == num_batches:\n            print(f\"batch: {cur_step}/{num_batches}, time: {time() - self.ts:.6f}s\")\n            self.ts = time()\n</code></pre>"},{"location":"reference/utils/#train-step","title":"Train Step","text":""},{"location":"reference/utils/#mindcv.utils.train_step.TrainStep","title":"<code>mindcv.utils.train_step.TrainStep</code>","text":"<p>               Bases: <code>TrainOneStepWithLossScaleCell</code></p> <p>Training step with loss scale.</p> The customized trainOneStepCell also supported following algorithms <ul> <li>Exponential Moving Average (EMA)</li> <li>Gradient Clipping</li> <li>Gradient Accumulation</li> </ul> Source code in <code>mindcv\\utils\\train_step.py</code> <pre><code>class TrainStep(nn.TrainOneStepWithLossScaleCell):\n    \"\"\"Training step with loss scale.\n\n    The customized trainOneStepCell also supported following algorithms:\n        * Exponential Moving Average (EMA)\n        * Gradient Clipping\n        * Gradient Accumulation\n    \"\"\"\n\n    def __init__(\n        self,\n        network,\n        optimizer,\n        scale_sense=1.0,\n        ema=False,\n        ema_decay=0.9999,\n        clip_grad=False,\n        clip_value=15.0,\n        gradient_accumulation_steps=1,\n    ):\n        super(TrainStep, self).__init__(network, optimizer, scale_sense)\n        self.ema = ema\n        self.ema_decay = ema_decay\n        self.updates = Parameter(Tensor(0.0, ms.float32))\n        self.clip_grad = clip_grad\n        self.clip_value = clip_value\n        if self.ema:\n            self.weights_all = ms.ParameterTuple(list(network.get_parameters()))\n            self.ema_weight = self.weights_all.clone(\"ema\", init=\"same\")\n\n        self.accumulate_grad = gradient_accumulation_steps &gt; 1\n        if self.accumulate_grad:\n            self.gradient_accumulation = GradientAccumulation(gradient_accumulation_steps, optimizer, self.grad_reducer)\n\n    def ema_update(self):\n        self.updates += 1\n        # ema factor is corrected by (1 - exp(-t/T)), where `t` means time and `T` means temperature.\n        ema_decay = self.ema_decay * (1 - F.exp(-self.updates / 2000))\n        # update trainable parameters\n        success = self.hyper_map(F.partial(_ema_op, ema_decay), self.ema_weight, self.weights_all)\n        return success\n\n    def construct(self, *inputs):\n        weights = self.weights\n        loss = self.network(*inputs)\n        scaling_sens = self.scale_sense\n\n        status, scaling_sens = self.start_overflow_check(loss, scaling_sens)\n\n        scaling_sens_filled = ops.ones_like(loss) * F.cast(scaling_sens, F.dtype(loss))\n        grads = self.grad(self.network, weights)(*inputs, scaling_sens_filled)\n        grads = self.hyper_map(F.partial(_grad_scale, scaling_sens), grads)\n\n        # todo: When to clip grad? Do we need to clip grad after grad reduction? What if grad accumulation is needed?\n        if self.clip_grad:\n            grads = ops.clip_by_global_norm(grads, clip_norm=self.clip_value)\n\n        if self.loss_scaling_manager:  # scale_sense = update_cell: Cell --&gt; TrainOneStepWithLossScaleCell.construct\n            if self.accumulate_grad:\n                # todo: GradientAccumulation only call grad_reducer at the step where the accumulation is completed.\n                #  So checking the overflow status is after gradient reduction, is this correct?\n                # get the overflow buffer\n                cond = self.get_overflow_status(status, grads)\n                overflow = self.process_loss_scale(cond)\n                # if there is no overflow, do optimize\n                if not overflow:\n                    loss = self.gradient_accumulation(loss, grads)\n                    if self.ema:\n                        loss = F.depend(loss, self.ema_update())\n            else:\n                # apply grad reducer on grads\n                grads = self.grad_reducer(grads)\n                # get the overflow buffer\n                cond = self.get_overflow_status(status, grads)\n                overflow = self.process_loss_scale(cond)\n                # if there is no overflow, do optimize\n                if not overflow:\n                    loss = F.depend(loss, self.optimizer(grads))\n                    if self.ema:\n                        loss = F.depend(loss, self.ema_update())\n        else:  # scale_sense = loss_scale: Tensor --&gt; TrainOneStepCell.construct\n            if self.accumulate_grad:\n                loss = self.gradient_accumulation(loss, grads)\n            else:\n                grads = self.grad_reducer(grads)\n                loss = F.depend(loss, self.optimizer(grads))\n\n            if self.ema:\n                loss = F.depend(loss, self.ema_update())\n\n        return loss\n</code></pre>"},{"location":"reference/utils/#trainer-factory","title":"Trainer Factory","text":""},{"location":"reference/utils/#mindcv.utils.trainer_factory.create_trainer","title":"<code>mindcv.utils.trainer_factory.create_trainer(network, loss, optimizer, metrics, amp_level, amp_cast_list, loss_scale_type, loss_scale=1.0, drop_overflow_update=False, ema=False, ema_decay=0.9999, clip_grad=False, clip_value=15.0, gradient_accumulation_steps=1)</code>","text":"<p>Create Trainer.</p> PARAMETER DESCRIPTION <code>network</code> <p>The backbone network to train, evaluate or predict.</p> <p> TYPE: <code>Cell</code> </p> <code>loss</code> <p>The function of calculating loss.</p> <p> TYPE: <code>Cell</code> </p> <code>optimizer</code> <p>The optimizer for training.</p> <p> TYPE: <code>Cell</code> </p> <code>metrics</code> <p>The metrics for model evaluation.</p> <p> TYPE: <code>Union[dict, set]</code> </p> <code>amp_level</code> <p>The level of auto mixing precision training.</p> <p> TYPE: <code>str</code> </p> <code>amp_cast_list</code> <p>At the cell level, custom casting the cell to FP16.</p> <p> TYPE: <code>str</code> </p> <code>loss_scale_type</code> <p>The type of loss scale.</p> <p> TYPE: <code>str</code> </p> <code>loss_scale</code> <p>The value of loss scale.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>drop_overflow_update</code> <p>Whether to execute optimizer if there is an overflow.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>ema</code> <p>Whether to use exponential moving average of model weights.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>ema_decay</code> <p>Decay factor for model weights moving average.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.9999</code> </p> <code>clip_grad</code> <p>whether to gradient clip.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>clip_value</code> <p>The value at which to clip gradients.</p> <p> TYPE: <code>float</code> DEFAULT: <code>15.0</code> </p> <code>gradient_accumulation_steps</code> <p>Accumulate the gradients of n batches before update.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> RETURNS DESCRIPTION <p>mindspore.Model</p> Source code in <code>mindcv\\utils\\trainer_factory.py</code> <pre><code>def create_trainer(\n    network: nn.Cell,\n    loss: nn.Cell,\n    optimizer: nn.Cell,\n    metrics: Union[dict, set],\n    amp_level: str,\n    amp_cast_list: str,\n    loss_scale_type: str,\n    loss_scale: float = 1.0,\n    drop_overflow_update: bool = False,\n    ema: bool = False,\n    ema_decay: float = 0.9999,\n    clip_grad: bool = False,\n    clip_value: float = 15.0,\n    gradient_accumulation_steps: int = 1,\n):\n    \"\"\"Create Trainer.\n\n    Args:\n        network: The backbone network to train, evaluate or predict.\n        loss: The function of calculating loss.\n        optimizer: The optimizer for training.\n        metrics: The metrics for model evaluation.\n        amp_level: The level of auto mixing precision training.\n        amp_cast_list: At the cell level, custom casting the cell to FP16.\n        loss_scale_type: The type of loss scale.\n        loss_scale: The value of loss scale.\n        drop_overflow_update: Whether to execute optimizer if there is an overflow.\n        ema: Whether to use exponential moving average of model weights.\n        ema_decay: Decay factor for model weights moving average.\n        clip_grad: whether to gradient clip.\n        clip_value: The value at which to clip gradients.\n        gradient_accumulation_steps: Accumulate the gradients of n batches before update.\n\n    Returns:\n        mindspore.Model\n\n    \"\"\"\n    if loss_scale &lt; 1.0:\n        raise ValueError(\"Loss scale cannot be less than 1.0!\")\n\n    if drop_overflow_update is False and loss_scale_type.lower() == \"dynamic\":\n        raise ValueError(\"DynamicLossScale ALWAYS drop overflow!\")\n\n    if gradient_accumulation_steps &lt; 1:\n        raise ValueError(\"`gradient_accumulation_steps` must be &gt;= 1!\")\n\n    if not require_customized_train_step(ema, clip_grad, gradient_accumulation_steps, amp_cast_list):\n        mindspore_kwargs = dict(\n            network=network,\n            loss_fn=loss,\n            optimizer=optimizer,\n            metrics=metrics,\n            amp_level=amp_level,\n        )\n        if loss_scale_type.lower() == \"fixed\":\n            mindspore_kwargs[\"loss_scale_manager\"] = FixedLossScaleManager(\n                loss_scale=loss_scale, drop_overflow_update=drop_overflow_update\n            )\n        elif loss_scale_type.lower() == \"dynamic\":\n            mindspore_kwargs[\"loss_scale_manager\"] = DynamicLossScaleManager(\n                init_loss_scale=loss_scale, scale_factor=2, scale_window=2000\n            )\n        elif loss_scale_type.lower() == \"auto\":\n            # We don't explicitly construct LossScaleManager\n            _logger.warning(\n                \"You are using AUTO loss scale, which means the LossScaleManager isn't explicitly pass in \"\n                \"when creating a mindspore.Model instance. \"\n                \"NOTE: mindspore.Model may use LossScaleManager silently. See mindspore.train.amp for details.\"\n            )\n        else:\n            raise ValueError(f\"Loss scale type only support ['fixed', 'dynamic', 'auto'], but got{loss_scale_type}.\")\n        model = Model(**mindspore_kwargs)\n    else:  # require customized train step\n        eval_network = nn.WithEvalCell(network, loss, amp_level in [\"O2\", \"O3\", \"auto\"])\n        auto_mixed_precision(network, amp_level, amp_cast_list)\n        net_with_loss = add_loss_network(network, loss, amp_level)\n        train_step_kwargs = dict(\n            network=net_with_loss,\n            optimizer=optimizer,\n            ema=ema,\n            ema_decay=ema_decay,\n            clip_grad=clip_grad,\n            clip_value=clip_value,\n            gradient_accumulation_steps=gradient_accumulation_steps,\n        )\n        if loss_scale_type.lower() == \"fixed\":\n            loss_scale_manager = FixedLossScaleManager(loss_scale=loss_scale, drop_overflow_update=drop_overflow_update)\n        elif loss_scale_type.lower() == \"dynamic\":\n            loss_scale_manager = DynamicLossScaleManager(init_loss_scale=loss_scale, scale_factor=2, scale_window=2000)\n        else:\n            raise ValueError(f\"Loss scale type only support ['fixed', 'dynamic'], but got{loss_scale_type}.\")\n        update_cell = loss_scale_manager.get_update_cell()\n        # 1. loss_scale_type=\"fixed\", drop_overflow_update=False\n        # --&gt; update_cell=None, TrainStep=TrainOneStepCell(scale_sense=loss_scale)\n        # 2. loss_scale_type: fixed, drop_overflow_update: True\n        # --&gt; update_cell=FixedLossScaleUpdateCell, TrainStep=TrainOneStepWithLossScaleCell(scale_sense=update_cell)\n        # 3. loss_scale_type: dynamic, drop_overflow_update: True\n        # --&gt; update_cell=DynamicLossScaleUpdateCell, TrainStep=TrainOneStepWithLossScaleCell(scale_sense=update_cell)\n        if update_cell is None:\n            train_step_kwargs[\"scale_sense\"] = Tensor(loss_scale, dtype=ms.float32)\n        else:\n            if not context.get_context(\"enable_ge\") and context.get_context(\"device_target\") == \"CPU\":\n                raise ValueError(\n                    \"Only `loss_scale_type` is `fixed` and `drop_overflow_update` is `False`\"\n                    \"are supported on device `CPU`.\"\n                )\n            train_step_kwargs[\"scale_sense\"] = update_cell\n        train_step_cell = TrainStep(**train_step_kwargs).set_train()\n        model = Model(train_step_cell, eval_network=eval_network, metrics=metrics, eval_indexes=[0, 1, 2])\n        # todo: do we need to set model._loss_scale_manager\n    return model\n</code></pre>"},{"location":"tutorials/configuration/","title":"Configuration","text":"<p>MindCV can parse the yaml file of the model through the argparse library and PyYAML library to configure parameters. Let's use squeezenet_1.0 model as an example to explain how to configure the corresponding parameters.</p>"},{"location":"tutorials/configuration/#basic-environment","title":"Basic Environment","text":"<ol> <li>Parameter description</li> </ol> <ul> <li> <p>mode: Use graph mode (0) or pynative mode (1).</p> </li> <li> <p>distribute: Whether to use distributed.</p> </li> </ul> <ol> <li> <p>Sample yaml file</p> <pre><code>mode: 0\ndistribute: True\n...\n</code></pre> </li> <li> <p>Parse parameter setting</p> <pre><code>python train.py --mode 0 --distribute False ...\n</code></pre> </li> <li> <p>Corresponding code example</p> <p><code>args.mode</code> represents the parameter <code>mode</code>, <code>args.distribute</code> represents the parameter <code>distribute</code>.</p> <pre><code>def train(args):\n    ms.set_context(mode=args.mode)\n\n    if args.distribute:\n        init()\n        device_num = get_group_size()\n        rank_id = get_rank()\n        ms.set_auto_parallel_context(device_num=device_num,\n                                     parallel_mode='data_parallel',\n                                     gradients_mean=True)\n    else:\n        device_num = None\n        rank_id = None\n    ...\n</code></pre> </li> </ol>"},{"location":"tutorials/configuration/#dataset","title":"Dataset","text":"<ol> <li>Parameter description</li> </ol> <ul> <li> <p>dataset: dataset name.</p> </li> <li> <p>data_dir: Path of dataset file.</p> </li> <li> <p>shuffle: whether to shuffle the dataset.</p> </li> <li> <p>dataset_download: whether to download the dataset.</p> </li> <li> <p>batch_size: The number of rows in each batch.</p> </li> <li> <p>drop_remainder: Determines whether to drop the last block whose data row number is less than the batch size.</p> </li> <li> <p>num_parallel_workers: Number of workers(threads) to process the dataset in parallel.</p> </li> </ul> <ol> <li> <p>Sample yaml file</p> <pre><code>dataset: 'imagenet'\ndata_dir: './imagenet2012'\nshuffle: True\ndataset_download: False\nbatch_size: 32\ndrop_remainder: True\nnum_parallel_workers: 8\n...\n</code></pre> </li> <li> <p>Parse parameter setting</p> <pre><code>python train.py ... --dataset imagenet --data_dir ./imagenet2012 --shuffle True \\\n    --dataset_download False --batch_size 32 --drop_remainder True \\\n    --num_parallel_workers 8 ...\n</code></pre> </li> <li> <p>Corresponding code example</p> <pre><code>def train(args):\n    ...\n    dataset_train = create_dataset(\n        name=args.dataset,\n        root=args.data_dir,\n        split='train',\n        shuffle=args.shuffle,\n        num_samples=args.num_samples,\n        num_shards=device_num,\n        shard_id=rank_id,\n        num_parallel_workers=args.num_parallel_workers,\n        download=args.dataset_download,\n        num_aug_repeats=args.aug_repeats)\n\n    ...\n    target_transform = transforms.OneHot(num_classes) if args.loss == 'BCE' else None\n\n    loader_train = create_loader(\n        dataset=dataset_train,\n        batch_size=args.batch_size,\n        drop_remainder=args.drop_remainder,\n        is_training=True,\n        mixup=args.mixup,\n        cutmix=args.cutmix,\n        cutmix_prob=args.cutmix_prob,\n        num_classes=args.num_classes,\n        transform=transform_list,\n        target_transform=target_transform,\n        num_parallel_workers=args.num_parallel_workers,\n    )\n    ...\n</code></pre> </li> </ol>"},{"location":"tutorials/configuration/#data-augmentation","title":"Data Augmentation","text":"<ol> <li>Parameter description</li> </ol> <ul> <li> <p>image_resize: the image size after resizing for adapting to the network.</p> </li> <li> <p>scale: random resize scale.</p> </li> <li> <p>ratio: random resize aspect ratio.</p> </li> <li> <p>hfilp: horizontal flip training aug probability.</p> </li> <li> <p>interpolation: image interpolation mode for resize operator.</p> </li> <li> <p>crop_pct: input image center crop percent.</p> </li> <li> <p>color_jitter: color jitter factor.</p> </li> <li> <p>re_prob: the probability of performing erasing.</p> </li> </ul> <ol> <li> <p>Sample yaml file</p> <pre><code>image_resize: 224\nscale: [0.08, 1.0]\nratio: [0.75, 1.333]\nhflip: 0.5\ninterpolation: 'bilinear'\ncrop_pct: 0.875\ncolor_jitter: [0.4, 0.4, 0.4]\nre_prob: 0.5\n...\n</code></pre> </li> <li> <p>Parse parameter setting</p> <pre><code>python train.py ... --image_resize 224 --scale [0.08, 1.0] --ratio [0.75, 1.333] \\\n    --hflip 0.5 --interpolation \"bilinear\" --crop_pct 0.875 \\\n    --color_jitter [0.4, 0.4, 0.4] --re_prob 0.5 ...\n</code></pre> </li> <li> <p>Corresponding code example</p> <pre><code>def train(args):\n    ...\n    transform_list = create_transforms(\n        dataset_name=args.dataset,\n        is_training=True,\n        image_resize=args.image_resize,\n        scale=args.scale,\n        ratio=args.ratio,\n        hflip=args.hflip,\n        vflip=args.vflip,\n        color_jitter=args.color_jitter,\n        interpolation=args.interpolation,\n        auto_augment=args.auto_augment,\n        mean=args.mean,\n        std=args.std,\n        re_prob=args.re_prob,\n        re_scale=args.re_scale,\n        re_ratio=args.re_ratio,\n        re_value=args.re_value,\n        re_max_attempts=args.re_max_attempts\n    )\n    ...\n</code></pre> </li> </ol>"},{"location":"tutorials/configuration/#model","title":"Model","text":"<ol> <li>Parameter description</li> </ol> <ul> <li> <p>model: model name.</p> </li> <li> <p>num_classes: number of label classes.</p> </li> <li> <p>pretrained: whether load pretrained model.</p> </li> <li> <p>ckpt_path: initialize model from this checkpoint.</p> </li> <li> <p>keep_checkpoint_max: max number of checkpoint files.</p> </li> <li> <p>ckpt_save_dir: the path of checkpoint.</p> </li> <li> <p>epoch_size: train epoch size.</p> </li> <li> <p>dataset_sink_mode: the dataset sink mode.</p> </li> <li> <p>amp_level: auto mixed precision level for saving memory and acceleration.</p> </li> </ul> <ol> <li> <p>Sample yaml file</p> <pre><code>model: 'squeezenet1_0'\nnum_classes: 1000\npretrained: False\nckpt_path: './squeezenet1_0_gpu.ckpt'\nkeep_checkpoint_max: 10\nckpt_save_dir: './ckpt/'\nepoch_size: 200\ndataset_sink_mode: True\namp_level: 'O0'\n...\n</code></pre> </li> <li> <p>Parse parameter setting</p> <pre><code>python train.py ... --model squeezenet1_0 --num_classes 1000 --pretrained False \\\n    --ckpt_path ./squeezenet1_0_gpu.ckpt --keep_checkpoint_max 10 \\\n    --ckpt_save_path ./ckpt/ --epoch_size 200 --dataset_sink_mode True \\\n    --amp_level O0 ...\n</code></pre> </li> <li> <p>Corresponding code example</p> <pre><code>def train(args):\n    ...\n    network = create_model(model_name=args.model,\n        num_classes=args.num_classes,\n        in_channels=args.in_channels,\n        drop_rate=args.drop_rate,\n        drop_path_rate=args.drop_path_rate,\n        pretrained=args.pretrained,\n        checkpoint_path=args.ckpt_path,\n        ema=args.ema\n    )\n    ...\n</code></pre> </li> </ol>"},{"location":"tutorials/configuration/#loss-function","title":"Loss Function","text":"<ol> <li>Parameter description</li> </ol> <ul> <li> <p>loss: name of loss function, BCE (BinaryCrossEntropy) or CE (CrossEntropy).</p> </li> <li> <p>label_smoothing: use label smoothing.</p> </li> </ul> <ol> <li> <p>Sample yaml file</p> <pre><code>loss: 'CE'\nlabel_smoothing: 0.1\n...\n</code></pre> </li> <li> <p>Parse parameter setting</p> <pre><code>python train.py ... --loss CE --label_smoothing 0.1 ...\n</code></pre> </li> <li> <p>Corresponding code example</p> <pre><code>def train(args):\n    ...\n    loss = create_loss(name=args.loss,\n        reduction=args.reduction,\n        label_smoothing=args.label_smoothing,\n        aux_factor=args.aux_factor\n     )\n    ...\n</code></pre> </li> </ol>"},{"location":"tutorials/configuration/#learning-rate-scheduler","title":"Learning Rate Scheduler","text":"<ol> <li>Parameter description</li> </ol> <ul> <li> <p>scheduler: name of scheduler.</p> </li> <li> <p>min_lr: the minimum value of learning rate if the scheduler supports.</p> </li> <li> <p>lr: learning rate.</p> </li> <li> <p>warmup_epochs: warmup epochs.</p> </li> <li> <p>decay_epochs: decay epochs.</p> </li> </ul> <ol> <li> <p>Sample yaml file</p> <pre><code>scheduler: 'cosine_decay'\nmin_lr: 0.0\nlr: 0.01\nwarmup_epochs: 0\ndecay_epochs: 200\n...\n</code></pre> </li> <li> <p>Parse parameter setting</p> <pre><code>python train.py ... --scheduler cosine_decay --min_lr 0.0 --lr 0.01 \\\n    --warmup_epochs 0 --decay_epochs 200 ...\n</code></pre> </li> <li> <p>Corresponding code example</p> <pre><code>def train(args):\n    ...\n    lr_scheduler = create_scheduler(num_batches,\n        scheduler=args.scheduler,\n        lr=args.lr,\n        min_lr=args.min_lr,\n        warmup_epochs=args.warmup_epochs,\n        warmup_factor=args.warmup_factor,\n        decay_epochs=args.decay_epochs,\n        decay_rate=args.decay_rate,\n        milestones=args.multi_step_decay_milestones,\n        num_epochs=args.epoch_size,\n        lr_epoch_stair=args.lr_epoch_stair\n    )\n    ...\n</code></pre> </li> </ol>"},{"location":"tutorials/configuration/#optimizer","title":"Optimizer","text":"<ol> <li>Parameter description</li> </ol> <ul> <li> <p>opt: name of optimizer.</p> </li> <li> <p>weight_decay_filter: weight decay filter (filter parameters from weight decay).</p> </li> <li> <p>momentum: Hyperparameter of type float, means momentum for the moving average.</p> </li> <li> <p>weight_decay: weight decay (L2 penalty).</p> </li> <li> <p>loss_scale: gradient scaling factor</p> </li> <li> <p>use_nesterov: whether enables the Nesterov momentum</p> </li> </ul> <ol> <li> <p>Sample yaml file</p> <pre><code>opt: 'momentum'\nweight_decay_filter: 'norm_and_bias'\nmomentum: 0.9\nweight_decay: 0.00007\nloss_scale: 1024\nuse_nesterov: False\n...\n</code></pre> </li> <li> <p>Parse parameter setting</p> <pre><code>python train.py ... --opt momentum --weight_decay_filter 'norm_and_bias' --weight_decay 0.00007 \\\n    --loss_scale 1024 --use_nesterov False ...\n</code></pre> </li> <li> <p>Corresponding code example</p> <pre><code>def train(args):\n    ...\n    if args.ema:\n        optimizer = create_optimizer(network.trainable_params(),\n            opt=args.opt,\n            lr=lr_scheduler,\n            weight_decay=args.weight_decay,\n            momentum=args.momentum,\n            nesterov=args.use_nesterov,\n            weight_decay_filter=args.weight_decay_filter,\n            loss_scale=args.loss_scale,\n            checkpoint_path=opt_ckpt_path,\n            eps=args.eps\n        )\n    else:\n        optimizer = create_optimizer(network.trainable_params(),\n            opt=args.opt,\n            lr=lr_scheduler,\n            weight_decay=args.weight_decay,\n            momentum=args.momentum,\n            nesterov=args.use_nesterov,\n            weight_decay_filter=args.weight_decay_filter,\n            checkpoint_path=opt_ckpt_path,\n            eps=args.eps\n        )\n    ...\n</code></pre> </li> </ol>"},{"location":"tutorials/configuration/#combination-of-yaml-and-parse","title":"Combination of Yaml and Parse","text":"<p>You can override the parameter settings in the yaml file by using parse to set parameters. Take the following shell command as an example,</p> <pre><code>python train.py -c ./configs/squeezenet/squeezenet_1.0_gpu.yaml --data_dir ./data\n</code></pre> <p>The above command overwrites the value of <code>args.data_dir</code> parameter from <code>./imagenet2012</code> in yaml file to <code>./data</code>.</p>"},{"location":"tutorials/deployment/","title":"Inference Service Deployment","text":"<p>MindSpore Serving is a lightweight and high-performance service module that helps MindSpore developers efficiently deploy online inference services in the production environment. After completing model training on MindSpore, you can export the MindSpore model and use MindSpore Serving to create an inference service for the model.</p> <p>This tutorial uses mobilenet_v2_100 network as an example to describe how to deploy the Inference Service based on MindSpore Serving.</p>"},{"location":"tutorials/deployment/#environment-preparation","title":"Environment Preparation","text":"<p>Before deploying, ensure that MindSpore Serving has been properly installed and the environment variables are configured. To install and configure MindSpore Serving on your PC, go to the MindSpore Serving installation page.</p>"},{"location":"tutorials/deployment/#exporting-the-model","title":"Exporting the Model","text":"<p>To implement cross-platform or hardware inference (e.g., Ascend AI processor, MindSpore device side, GPU, etc.), the model file of MindIR format should be generated by network definition and CheckPoint. In MindSpore, the function of exporting the network model is <code>export</code> and the main parameters are as follows:</p> <ul> <li><code>net</code>: MindSpore network structure.</li> <li><code>inputs</code>: Network input, the supported input type is <code>Tensor</code>. If multiple values are input, the values should be input at the same time, for example, <code>ms.export(network, ms.Tensor(input1), ms.Tensor(input2), file_name='network', file_format='MINDIR')</code>.</li> <li><code>file_name</code>: Name of the exported model file. If <code>file_name</code> doesn't contain the corresponding suffix (for example, .mindir), the system will automatically add one after <code>file_format</code> is set.</li> <li><code>file_format</code>: MindSpore currently supports \u2018AIR\u2019, \u2018ONNX\u2019 and \u2018MINDIR\u2019 format for exported models.</li> </ul> <p>The following code uses mobilenet_v2_100 as an example to export the pretrained network model of MindCV and obtain the model file in MindIR format.</p> <pre><code>from mindcv.models import create_model\nimport numpy as np\nimport mindspore as ms\n\nmodel = create_model(model_name='mobilenet_v2_100', num_classes=1000, pretrained=True)\n\ninput_np = np.random.uniform(0.0, 1.0, size=[1, 3, 224, 224]).astype(np.float32)\n\n# Export mobilenet_v2_100.mindir to the current folder.\nms.export(model, ms.Tensor(input_np), file_name='mobilenet_v2_100', file_format='MINDIR')\n</code></pre>"},{"location":"tutorials/deployment/#deploying-the-serving-inference-service","title":"Deploying the Serving Inference Service","text":""},{"location":"tutorials/deployment/#configuring-the-service","title":"Configuring the Service","text":"<p>Start Serving with the following files:</p> <pre><code>demo\n\u251c\u2500\u2500 mobilenet_v2_100\n\u2502   \u251c\u2500\u2500 1\n\u2502   \u2502   \u2514\u2500\u2500 mobilenet_v2_100.mindir\n\u2502   \u2514\u2500\u2500 servable_config.py\n\u2502\u2500\u2500 serving_server.py\n\u251c\u2500\u2500 serving_client.py\n\u251c\u2500\u2500 imagenet1000_clsidx_to_labels.txt\n\u2514\u2500\u2500 test_image\n    \u251c\u2500 dog\n    \u2502   \u251c\u2500 dog.jpg\n    \u2502   \u2514\u2500 \u2026\u2026\n    \u2514\u2500 \u2026\u2026\n</code></pre> <ul> <li><code>mobilenet_v2_100</code>: Model folder. The folder name is the model name.</li> <li><code>mobilenet_v2_100.mindir</code>: Model file generated by the network in the previous step, which is stored in folder 1 (the number indicates the version number). Different versions are stored in different folders. The version number must be a string of digits. By default, the latest model file is started.</li> <li><code>servable_config.py</code>: Model configuration script. Declare the model and specify the input and output parameters of the model.</li> <li><code>serving_server.py</code>: Script to start the Serving server.</li> <li><code>serving_client.py</code>: Script to start the Python client.</li> <li><code>imagenet1000_clsidx_to_labels.txt</code>: Index of 1000 labels for the ImageNet dataset, available at examples/data/.</li> <li><code>test_image</code>: Test images, available at README.</li> </ul> <p>Content of the configuration file <code>servable_config.py</code>:</p> <pre><code>from mindspore_serving.server import register\n\n# Declare the model. The parameter model_file indicates the name of the model file and model_format indicates the model type.\nmodel = register.declare_model(model_file=\"mobilenet_v2_100.mindir\", model_format=\"MindIR\")\n\n# The input parameters of the Servable method are specified by the input parameters of the Python method. The output parameters of the Servable method are specified by the output_names of register_method.\n@register.register_method(output_names=[\"score\"])\ndef predict(image):\n    x = register.add_stage(model, image, outputs_count=1)\n    return x\n</code></pre>"},{"location":"tutorials/deployment/#starting-the-service","title":"Starting the Service","text":"<p>The <code>server</code> function of MindSpore can provide deployment service through either gRPC or RESTful. The following uses gRPC as an example. The service startup script <code>serving_server.py</code> deploys the <code>mobilenet_v2_100</code> in the local directory to device 0 and starts the gRPC server at 127.0.0.1:5500. Content of the script:</p> <pre><code>import os\nimport sys\nfrom mindspore_serving import server\n\ndef start():\n    servable_dir = os.path.dirname(os.path.realpath(sys.argv[0]))\n\n    servable_config = server.ServableStartConfig(servable_directory=servable_dir, servable_name=\"mobilenet_v2_100\",\n                                                 device_ids=0)\n    server.start_servables(servable_configs=servable_config)\n    server.start_grpc_server(address=\"127.0.0.1:5500\")\n\nif __name__ == \"__main__\":\n    start()\n</code></pre> <p>If the following log information is displayed on the server, the gRPC service is started successfully.</p> <pre><code>Serving gRPC server start success, listening on 127.0.0.1:5500\n</code></pre>"},{"location":"tutorials/deployment/#inference-execution","title":"Inference Execution","text":"<p>Start the Python client by using <code>serving_client.py</code>. The client script uses the <code>create_transforms</code>, <code>create_dataset</code> and <code>create_loader</code> functions of <code>mindcv.data</code> to preprocess the image and send the image to the serving server, then postprocesse the result returned by the server and prints the prediction label of the image.</p> <pre><code>import os\nfrom mindspore_serving.client import Client\nimport numpy as np\nfrom mindcv.data import create_transforms, create_dataset, create_loader\n\nnum_workers = 1\n\n# Dataset directory path\ndata_dir = \"./test_image/\"\n\ndataset = create_dataset(root=data_dir, split='', num_parallel_workers=num_workers)\ntransforms_list = create_transforms(dataset_name='ImageNet', is_training=False)\ndata_loader = create_loader(\n    dataset=dataset,\n    batch_size=1,\n    is_training=False,\n    num_classes=1000,\n    transform=transforms_list,\n    num_parallel_workers=num_workers\n)\nwith open(\"imagenet1000_clsidx_to_labels.txt\") as f:\n    idx2label = eval(f.read())\n\ndef postprocess(score):\n    max_idx = np.argmax(score)\n    return idx2label[max_idx]\n\ndef predict():\n    client = Client(\"127.0.0.1:5500\", \"mobilenet_v2_100\", \"predict\")\n    instances = []\n    images, _ = next(data_loader.create_tuple_iterator())\n    image_np = images.asnumpy().squeeze()\n    instances.append({\"image\": image_np})\n    result = client.infer(instances)\n\n    for instance in result:\n        label = postprocess(instance[\"score\"])\n        print(label)\n\nif __name__ == '__main__':\n    predict()\n</code></pre> <p>If the following information is displayed, serving service has correctly executed the inference of the mobilenet_v2_100 model: <pre><code>Labrador retriever\n</code></pre></p>"},{"location":"tutorials/finetune/","title":"Model Fine-Tuning Training","text":"<p>In this tutorial, you will learn how to use MindCV for transfer Learning to solve the problem of image classification on custom datasets. In the deep learning task, we often encounter the problem of insufficient training data. At this time, it is difficult to train the entire network directly to achieve the desired accuracy. A better approach is to use a pretrained model on a large dataset (close to the task data), and then use the model to initialize the network's weight parameters or apply it to specific tasks as a fixed feature extractor.</p> <p>This tutorial will use the DenseNet model pretrained on ImageNet as an example to introduce two different fine-tuning strategies to solve the image classification problem of wolves and dogs in the case of small samples:</p> <ol> <li>Overall model fine-tuning.</li> <li>Freeze backbone and only fine-tune the classifier.</li> </ol> <p>For details of transfer learning, see Stanford University CS231n</p>"},{"location":"tutorials/finetune/#data-preparation","title":"Data Preparation","text":""},{"location":"tutorials/finetune/#download-dataset","title":"Download Dataset","text":"<p>Download the dog and wolf classification dataset used in the case. Each category has 120 training images and 30 verification images. Use the <code>mindcv.utils.download</code> interface to download the dataset, and automatically unzip the downloaded dataset to the current directory.</p> <pre><code>import os\nfrom mindcv.utils.download import DownLoad\n\ndataset_url = \"https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/notebook/datasets/intermediate/Canidae_data.zip\"\nroot_dir = \"./\"\n\nif not os.path.exists(os.path.join(root_dir, 'data/Canidae')):\n    DownLoad().download_and_extract_archive(dataset_url, root_dir)\n</code></pre> <p>The directory structure of the dataset is as follows:</p> <pre><code>data/\n\u2514\u2500\u2500 Canidae\n    \u251c\u2500\u2500 train\n    \u2502   \u251c\u2500\u2500 dogs\n    \u2502   \u2514\u2500\u2500 wolves\n    \u2514\u2500\u2500 val\n        \u251c\u2500\u2500 dogs\n        \u2514\u2500\u2500 wolves\n</code></pre>"},{"location":"tutorials/finetune/#dataset-loading-and-processing","title":"Dataset Loading and Processing","text":""},{"location":"tutorials/finetune/#loading-custom-datasets","title":"Loading Custom Datasets","text":"<p>By calling the <code>create_dataset</code> function in <code>mindcv.data</code>, we can easily load preset and customized datasets.</p> <ul> <li>When the parameter <code>name</code> is set to null, it is specified as a user-defined dataset. (Default)</li> <li>When the parameter <code>name</code> is set to be <code>MNIST</code>, <code>CIFAR10</code> or other standard dataset names, it is specified as the preset dataset.</li> </ul> <p>At the same time, we need to set the path <code>data_dir</code> of the dataset and the name <code>split</code> of the data segmentation (such as train, val) to load the corresponding training set or validation set.</p> <pre><code>from mindcv.data import create_dataset, create_transforms, create_loader\n\nnum_workers = 8\n\n# path of dataset\ndata_dir = \"./data/Canidae/\"\n\n# load dataset\ndataset_train = create_dataset(root=data_dir, split='train', num_parallel_workers=num_workers)\ndataset_val = create_dataset(root=data_dir, split='val', num_parallel_workers=num_workers)\n</code></pre> <p>Note: The directory structure of the custom dataset should be the same as ImageNet, that is, the hierarchy of root -&gt;split -&gt;class -&gt;image</p> <pre><code>DATASET_NAME\n    \u251c\u2500\u2500 split1(e.g. train)/\n    \u2502  \u251c\u2500\u2500 class1/\n    \u2502  \u2502   \u251c\u2500\u2500 000001.jpg\n    \u2502  \u2502   \u251c\u2500\u2500 000002.jpg\n    \u2502  \u2502   \u2514\u2500\u2500 ....\n    \u2502  \u2514\u2500\u2500 class2/\n    \u2502      \u251c\u2500\u2500 000001.jpg\n    \u2502      \u251c\u2500\u2500 000002.jpg\n    \u2502      \u2514\u2500\u2500 ....\n    \u2514\u2500\u2500 split2/\n       \u251c\u2500\u2500 class1/\n       \u2502   \u251c\u2500\u2500 000001.jpg\n       \u2502   \u251c\u2500\u2500 000002.jpg\n       \u2502   \u2514\u2500\u2500 ....\n       \u2514\u2500\u2500 class2/\n           \u251c\u2500\u2500 000001.jpg\n           \u251c\u2500\u2500 000002.jpg\n           \u2514\u2500\u2500 ....\n</code></pre>"},{"location":"tutorials/finetune/#data-processing-and-augmentation","title":"Data Processing and Augmentation","text":"<p>First, we call the <code>create_transforms</code> function to obtain the preset data processing and augmentation strategy (transform list). In this task, because the file structure of the wolf-dog dataset is consistent with that of the ImageNet dataset, we specify the parameter <code>dataset_name</code> as ImageNet, and directly use the preset ImageNet data processing and image augmentation strategy. <code>create_transforms</code> also supports a variety of customized processing and enhancement operations, as well as automatic enhancement policies (AutoAug). See API description for details.</p> <p>We will transfer the obtained transform list to the <code>create_loader()</code>, specify <code>batch_size</code> and other parameters to complete the preparation of training and validation data, and return the <code>Dataset</code> Object as the input of the model.</p> <pre><code># Define and acquire data processing and augment operations\ntrans_train = create_transforms(dataset_name='ImageNet', is_training=True)\ntrans_val = create_transforms(dataset_name='ImageNet',is_training=False)\n\nloader_train = create_loader(\n    dataset=dataset_train,\n    batch_size=16,\n    is_training=True,\n    num_classes=2,\n    transform=trans_train,\n    num_parallel_workers=num_workers,\n)\nloader_val = create_loader(\n    dataset=dataset_val,\n    batch_size=5,\n    is_training=True,\n    num_classes=2,\n    transform=trans_val,\n    num_parallel_workers=num_workers,\n)\n</code></pre>"},{"location":"tutorials/finetune/#dataset-visualization","title":"Dataset Visualization","text":"<p>For the Dataset object returned by the <code>create_loader</code> interface to complete data loading, we can create a data iterator through the <code>create_tuple_iterator</code> interface, access the dataset using the <code>next</code> iteration, and read a batch of data.</p> <pre><code>images, labels = next(loader_train.create_tuple_iterator())\nprint(\"Tensor of image\", images.shape)\nprint(\"Labels:\", labels)\n</code></pre> <pre><code>Tensor of image (16, 3, 224, 224)\nLabels: [0 1 1 1 1 0 0 0 0 0 1 0 1 0 1 1]\n</code></pre> <p>Visualize the acquired image and label data, and the title is the label name corresponding to the image.</p> <pre><code>import matplotlib.pyplot as plt\nimport numpy as np\n\n# class_name corresponds to label, and labels are marked in the order of folder string from small to large\nclass_name = {0: \"dogs\", 1: \"wolves\"}\n\nplt.figure(figsize=(15, 7))\nfor i in range(len(labels)):\n    # Get the image and its corresponding label\n    data_image = images[i].asnumpy()\n    data_label = labels[i]\n    # Process images for display\n    data_image = np.transpose(data_image, (1, 2, 0))\n    mean = np.array([0.485, 0.456, 0.406])\n    std = np.array([0.229, 0.224, 0.225])\n    data_image = std * data_image + mean\n    data_image = np.clip(data_image, 0, 1)\n    # Show Image\n    plt.subplot(3, 6, i + 1)\n    plt.imshow(data_image)\n    plt.title(class_name[int(labels[i].asnumpy())])\n    plt.axis(\"off\")\n\nplt.show()\n</code></pre> <p></p>"},{"location":"tutorials/finetune/#model-fine-tuning","title":"Model Fine-Tuning","text":""},{"location":"tutorials/finetune/#1-overall-model-fine-tuning","title":"1. Overall Model Fine-Tuning","text":""},{"location":"tutorials/finetune/#pretraining-model-loading","title":"Pretraining Model Loading","text":"<p>We use <code>mindcv.models.densenet</code> to define the DenseNet121 network. When the <code>pretrained</code> parameter in the interface is set to True, the network weight can be automatically downloaded. Since the pretraining model is used to classify 1000 categories in the ImageNet dataset, we set <code>num_classes=2</code>, and the output of DenseNet's classifier (the last FC layer) is adjusted to two dimensions. At this time, only the pre-trained weights of the backbone are loaded, while the classifier uses the initial value.</p> <pre><code>from mindcv.models import create_model\n\nnetwork = create_model(model_name='densenet121', num_classes=2, pretrained=True)\n</code></pre> <p>For the specific structure of DenseNet, see the DenseNet paper.</p>"},{"location":"tutorials/finetune/#model-training","title":"Model Training","text":"<p>Use the loaded and processed wolf and dog images with tags to fine-tune the DenseNet network. Note that smaller learning rates should be used when fine-tuning the overall model.</p> <pre><code>from mindcv.loss import create_loss\nfrom mindcv.optim import create_optimizer\nfrom mindcv.scheduler import create_scheduler\nfrom mindspore import Model, LossMonitor, TimeMonitor\n\n# Define optimizer and loss function\nopt = create_optimizer(network.trainable_params(), opt='adam', lr=1e-4)\nloss = create_loss(name='CE')\n\n# Instantiated model\nmodel = Model(network, loss_fn=loss, optimizer=opt, metrics={'accuracy'})\nmodel.train(10, loader_train, callbacks=[LossMonitor(5), TimeMonitor(5)], dataset_sink_mode=False)\n</code></pre> <pre><code>epoch: 1 step: 5, loss is 0.5195528864860535\nepoch: 1 step: 10, loss is 0.2654373049736023\nepoch: 1 step: 15, loss is 0.28758567571640015\nTrain epoch time: 17270.144 ms, per step time: 1151.343 ms\nepoch: 2 step: 5, loss is 0.1807008981704712\nepoch: 2 step: 10, loss is 0.1700802594423294\nepoch: 2 step: 15, loss is 0.09752683341503143\nTrain epoch time: 1372.549 ms, per step time: 91.503 ms\nepoch: 3 step: 5, loss is 0.13594701886177063\nepoch: 3 step: 10, loss is 0.03628234937787056\nepoch: 3 step: 15, loss is 0.039737217128276825\nTrain epoch time: 1453.237 ms, per step time: 96.882 ms\nepoch: 4 step: 5, loss is 0.014213413000106812\nepoch: 4 step: 10, loss is 0.030747078359127045\nepoch: 4 step: 15, loss is 0.0798817127943039\nTrain epoch time: 1331.237 ms, per step time: 88.749 ms\nepoch: 5 step: 5, loss is 0.009510636329650879\nepoch: 5 step: 10, loss is 0.02603740245103836\nepoch: 5 step: 15, loss is 0.051846928894519806\nTrain epoch time: 1312.737 ms, per step time: 87.516 ms\nepoch: 6 step: 5, loss is 0.1163717582821846\nepoch: 6 step: 10, loss is 0.02439398318529129\nepoch: 6 step: 15, loss is 0.02564268559217453\nTrain epoch time: 1434.704 ms, per step time: 95.647 ms\nepoch: 7 step: 5, loss is 0.013310655951499939\nepoch: 7 step: 10, loss is 0.02289542555809021\nepoch: 7 step: 15, loss is 0.1992517113685608\nTrain epoch time: 1275.935 ms, per step time: 85.062 ms\nepoch: 8 step: 5, loss is 0.015928998589515686\nepoch: 8 step: 10, loss is 0.011409260332584381\nepoch: 8 step: 15, loss is 0.008141174912452698\nTrain epoch time: 1323.102 ms, per step time: 88.207 ms\nepoch: 9 step: 5, loss is 0.10395607352256775\nepoch: 9 step: 10, loss is 0.23055407404899597\nepoch: 9 step: 15, loss is 0.04896317049860954\nTrain epoch time: 1261.067 ms, per step time: 84.071 ms\nepoch: 10 step: 5, loss is 0.03162381425499916\nepoch: 10 step: 10, loss is 0.13094250857830048\nepoch: 10 step: 15, loss is 0.020028553903102875\nTrain epoch time: 1217.958 ms, per step time: 81.197 ms\n</code></pre>"},{"location":"tutorials/finetune/#model-evaluation","title":"Model Evaluation","text":"<p>After the training, we evaluate the accuracy of the model on the validation set.</p> <pre><code>res = model.eval(loader_val)\nprint(res)\n</code></pre> <pre><code>{'accuracy': 1.0}\n</code></pre>"},{"location":"tutorials/finetune/#visual-model-inference-results","title":"Visual Model Inference Results","text":"<p>Define <code>visualize_mode</code> function and visualize model prediction.</p> <pre><code>import matplotlib.pyplot as plt\nimport mindspore as ms\n\ndef visualize_model(model, val_dl, num_classes=2):\n    # Load the data of the validation set for validation\n    images, labels= next(val_dl.create_tuple_iterator())\n    # Predict image class\n    output = model.predict(images)\n    pred = np.argmax(output.asnumpy(), axis=1)\n    # Display images and their predicted values\n    images = images.asnumpy()\n    labels = labels.asnumpy()\n    class_name = {0: \"dogs\", 1: \"wolves\"}\n    plt.figure(figsize=(15, 7))\n    for i in range(len(labels)):\n        plt.subplot(3, 6, i + 1)\n        # If the prediction is correct, it is displayed in blue; If the prediction is wrong, it is displayed in red\n        color = 'blue' if pred[i] == labels[i] else 'red'\n        plt.title('predict:{}'.format(class_name[pred[i]]), color=color)\n        picture_show = np.transpose(images[i], (1, 2, 0))\n        mean = np.array([0.485, 0.456, 0.406])\n        std = np.array([0.229, 0.224, 0.225])\n        picture_show = std * picture_show + mean\n        picture_show = np.clip(picture_show, 0, 1)\n        plt.imshow(picture_show)\n        plt.axis('off')\n\n    plt.show()\n</code></pre> <p>Use the finely tuned model piece to predict the wolf and dog image data of the verification set. If the prediction font is blue, the prediction is correct; if the prediction font is red, the prediction is wrong.</p> <pre><code>visualize_model(model, loader_val)\n</code></pre> <p></p>"},{"location":"tutorials/finetune/#2-freeze-backbone-and-fine-tune-the-classifier","title":"2. Freeze Backbone and Fine-Tune the Classifier","text":""},{"location":"tutorials/finetune/#freezing-backbone-parameters","title":"Freezing Backbone Parameters","text":"<p>First, we need to freeze all network layers except the last layer classifier, that is, set the <code>requires_grad</code> attribute of the corresponding layer parameter to False, so that it does not calculate the gradient and update the parameters in the backpropagation.</p> <p>Because all models in <code>mindcv.models</code> use a <code>classifier</code> to identify and name the classifier of the model (i.e., the Dense layer), the parameters of each layer outside the classifier can be filtered through <code>classifier.weight</code> and <code>classifier.bias</code>, and its <code>requires_grad</code> attribute is set to False.</p> <pre><code># freeze backbone\nfor param in network.get_parameters():\n    if param.name not in [\"classifier.weight\", \"classifier.bias\"]:\n        param.requires_grad = False\n</code></pre>"},{"location":"tutorials/finetune/#fine-tune-classifier","title":"Fine-Tune Classifier","text":"<p>Because the feature network has been fixed, we don't have to worry about distortpratised features in the training process. Therefore, compared with the first method, we can increase the learning rate.</p> <p>Compared with no pretraining model, it will save more than half of the time, because partial gradient can not be calculated at this time.</p> <pre><code># dataset load\ndataset_train = create_dataset(root=data_dir, split='train', num_parallel_workers=num_workers)\nloader_train = create_loader(\n    dataset=dataset_train,\n    batch_size=16,\n    is_training=True,\n    num_classes=2,\n    transform=trans_train,\n    num_parallel_workers=num_workers,\n)\n\n# Define optimizer and loss function\nopt = create_optimizer(network.trainable_params(), opt='adam', lr=1e-3)\nloss = create_loss(name='CE')\n\n# Instantiated model\nmodel = Model(network, loss_fn=loss, optimizer=opt, metrics={'accuracy'})\n\nmodel.train(10, loader_train, callbacks=[LossMonitor(5), TimeMonitor(5)], dataset_sink_mode=False)\n</code></pre> <pre><code>epoch: 1 step: 5, loss is 0.051333948969841\nepoch: 1 step: 10, loss is 0.02043312042951584\nepoch: 1 step: 15, loss is 0.16161368787288666\nTrain epoch time: 10228.601 ms, per step time: 681.907 ms\nepoch: 2 step: 5, loss is 0.002121545374393463\nepoch: 2 step: 10, loss is 0.0009798109531402588\nepoch: 2 step: 15, loss is 0.015776708722114563\nTrain epoch time: 562.543 ms, per step time: 37.503 ms\nepoch: 3 step: 5, loss is 0.008056879043579102\nepoch: 3 step: 10, loss is 0.0009347647428512573\nepoch: 3 step: 15, loss is 0.028648357838392258\nTrain epoch time: 523.249 ms, per step time: 34.883 ms\nepoch: 4 step: 5, loss is 0.001014217734336853\nepoch: 4 step: 10, loss is 0.0003159046173095703\nepoch: 4 step: 15, loss is 0.0007699579000473022\nTrain epoch time: 508.886 ms, per step time: 33.926 ms\nepoch: 5 step: 5, loss is 0.0015687644481658936\nepoch: 5 step: 10, loss is 0.012090332806110382\nepoch: 5 step: 15, loss is 0.004598274827003479\nTrain epoch time: 507.243 ms, per step time: 33.816 ms\nepoch: 6 step: 5, loss is 0.010022152215242386\nepoch: 6 step: 10, loss is 0.0066385045647621155\nepoch: 6 step: 15, loss is 0.0036080628633499146\nTrain epoch time: 517.646 ms, per step time: 34.510 ms\nepoch: 7 step: 5, loss is 0.01344013586640358\nepoch: 7 step: 10, loss is 0.0008538365364074707\nepoch: 7 step: 15, loss is 0.14135593175888062\nTrain epoch time: 511.513 ms, per step time: 34.101 ms\nepoch: 8 step: 5, loss is 0.01626245677471161\nepoch: 8 step: 10, loss is 0.02871556021273136\nepoch: 8 step: 15, loss is 0.010110966861248016\nTrain epoch time: 545.678 ms, per step time: 36.379 ms\nepoch: 9 step: 5, loss is 0.008498094975948334\nepoch: 9 step: 10, loss is 0.2588501274585724\nepoch: 9 step: 15, loss is 0.0014278888702392578\nTrain epoch time: 499.243 ms, per step time: 33.283 ms\nepoch: 10 step: 5, loss is 0.021337147802114487\nepoch: 10 step: 10, loss is 0.00829876959323883\nepoch: 10 step: 15, loss is 0.008352771401405334\nTrain epoch time: 465.600 ms, per step time: 31.040 ms\n</code></pre>"},{"location":"tutorials/finetune/#model-evaluation_1","title":"Model Evaluation","text":"<p>After the training, we evaluate the accuracy of the model on the validation set.</p> <pre><code>dataset_val = create_dataset(root=data_dir, split='val', num_parallel_workers=num_workers)\nloader_val = create_loader(\n    dataset=dataset_val,\n    batch_size=5,\n    is_training=True,\n    num_classes=2,\n    transform=trans_val,\n    num_parallel_workers=num_workers,\n)\n\nres = model.eval(loader_val)\nprint(res)\n</code></pre> <pre><code>{'accuracy': 1.0}\n</code></pre>"},{"location":"tutorials/finetune/#visual-model-prediction","title":"Visual Model Prediction","text":"<p>Use the finely tuned model piece to predict the wolf and dog image data of the verification set. If the prediction font is blue, the prediction is correct; if the prediction font is red, the prediction is wrong.</p> <pre><code>visualize_model(model, loader_val)\n</code></pre> <p></p> <p>The prediction results of wolf/dog after fine-tuning are correct.</p>"},{"location":"tutorials/inference/","title":"Image Classification Prediction","text":"<p>This tutorial introduces how to call the pretraining model in MindCV to make classification prediction on the test image.</p>"},{"location":"tutorials/inference/#model-loading","title":"Model Loading","text":""},{"location":"tutorials/inference/#view-all-available-models","title":"View All Available Models","text":"<p>By calling the <code>registry.list_models</code> function in <code>mindcv.models</code>, the names of all network models can be printed. The models of a network in different parameter configurations will also be printed, such as resnet18 / resnet34 / resnet50 / resnet101 / resnet152.</p> <pre><code>import sys\nsys.path.append(\"..\")\nfrom mindcv.models import registry\nregistry.list_models()\n</code></pre> <pre><code>['BiT_resnet50',\n 'repmlp_b224',\n 'repmlp_b256',\n 'repmlp_d256',\n 'repmlp_l256',\n 'repmlp_t224',\n 'repmlp_t256',\n 'convit_base',\n 'convit_base_plus',\n 'convit_small',\n ...\n 'visformer_small',\n 'visformer_small_v2',\n 'visformer_tiny',\n 'visformer_tiny_v2',\n 'vit_b_16_224',\n 'vit_b_16_384',\n 'vit_b_32_224',\n 'vit_b_32_384',\n 'vit_l_16_224',\n 'vit_l_16_384',\n 'vit_l_32_224',\n 'xception']\n</code></pre>"},{"location":"tutorials/inference/#load-pretraining-model","title":"Load Pretraining Model","text":"<p>Taking the resnet50 model as an example, we introduce two methods to load the model checkpoint using the <code>create_model</code> function in <code>mindcv.models</code>.</p> <p>1). When the <code>pretrained</code> parameter in the interface is set to True, network weights can be automatically downloaded.</p> <pre><code>from mindcv.models import create_model\nmodel = create_model(model_name='resnet50', num_classes=1000, pretrained=True)\n# Switch the execution logic of the network to the inference scenario\nmodel.set_train(False)\n</code></pre> <pre><code>102453248B [00:16, 6092186.31B/s]\n\nResNet&lt;\n  (conv1): Conv2d&lt;input_channels=3, output_channels=64, kernel_size=(7, 7), stride=(2, 2), pad_mode=pad, padding=3, dilation=(1, 1), group=1, has_bias=False, weight_init=normal, bias_init=zeros, format=NCHW&gt;\n  (bn1): BatchNorm2d&lt;num_features=64, eps=1e-05, momentum=0.9, gamma=Parameter (name=bn1.gamma, shape=(64,), dtype=Float32, requires_grad=True), beta=Parameter (name=bn1.beta, shape=(64,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=bn1.moving_mean, shape=(64,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=bn1.moving_variance, shape=(64,), dtype=Float32, requires_grad=False)&gt;\n  (relu): ReLU&lt;&gt;\n  (max_pool): MaxPool2d&lt;kernel_size=3, stride=2, pad_mode=SAME&gt;\n  ...\n  (pool): GlobalAvgPooling&lt;&gt;\n  (classifier): Dense&lt;input_channels=2048, output_channels=1000, has_bias=True&gt;\n  &gt;\n</code></pre> <p>2). When the <code>checkpoint_path</code> parameter in the interface is set to the file path, the model parameter file with the <code>.ckpt</code> can be loaded.</p> <pre><code>from mindcv.models import create_model\nmodel = create_model(model_name='resnet50', num_classes=1000, checkpoint_path='./resnet50_224.ckpt')\n# Switch the execution logic of the network to the inference scenario\nmodel.set_train(False)\n</code></pre>"},{"location":"tutorials/inference/#data-preparation","title":"Data Preparation","text":""},{"location":"tutorials/inference/#create-dataset","title":"Create Dataset","text":"<p>Here, we download a Wikipedia image as a test image, and use the <code>create_dataset</code> function in <code>mindcv.data</code> to construct a custom dataset for a single image.</p> <pre><code>from mindcv.data import create_dataset\nnum_workers = 1\n# path of dataset\ndata_dir = \"./data/\"\ndataset = create_dataset(root=data_dir, split='test', num_parallel_workers=num_workers)\n# Image visualization\nfrom PIL import Image\nImage.open(\"./data/test/dog/dog.jpg\")\n</code></pre> <p></p>"},{"location":"tutorials/inference/#data-preprocessing","title":"Data Preprocessing","text":"<p>Call the <code>create_transforms</code> function to obtain the data processing strategy (transform list) of the ImageNet dataset used by the pre-trained model.</p> <p>We pass the obtained transform list into the <code>create_loader</code> function, specify <code>batch_size=1</code> and other parameters, and then complete the preparation of test data. The <code>Dataset</code> object is returned as the input of the model.</p> <pre><code>from mindcv.data import create_transforms, create_loader\ntransforms_list = create_transforms(dataset_name='imagenet', is_training=False)\ndata_loader = create_loader(\n    dataset=dataset,\n    batch_size=1,\n    is_training=False,\n    num_classes=1000,\n    transform=transforms_list,\n    num_parallel_workers=num_workers\n)\n</code></pre>"},{"location":"tutorials/inference/#model-inference","title":"Model Inference","text":"<p>The picture of the user-defined dataset is transferred to the model to obtain the inference result. Here, use the <code>Squeeze</code> function of <code>mindspore.ops</code> to remove the batch dimension.</p> <pre><code>import mindspore.ops as P\nimport numpy as np\nimages, _ = next(data_loader.create_tuple_iterator())\noutput = P.Squeeze()(model(images))\npred = np.argmax(output.asnumpy())\n</code></pre> <pre><code>with open(\"imagenet1000_clsidx_to_labels.txt\") as f:\n    idx2label = eval(f.read())\nprint('predict: {}'.format(idx2label[pred]))\n</code></pre> <pre><code>predict: Labrador retriever\n</code></pre>"},{"location":"tutorials/quick_start/","title":"Quick Start","text":"<p>MindCV is an open-source toolbox for computer vision research and development based on MindSpore. It collects a series of classic and SoTA vision models, such as ResNet and SwinTransformer, along with their pretrained weights. SoTA methods such as AutoAugment are also provided for performance improvement. With the decoupled module design, it is easy to apply or adapt MindCV to your own CV tasks. In this tutorial, we will provide a quick start guideline for MindCV.</p> <p>This tutorial will take DenseNet classification model as an example to implement transfer training on CIFAR-10 dataset and explain the usage of MindCV modules in this process.</p>"},{"location":"tutorials/quick_start/#environment-setting","title":"Environment Setting","text":"<p>See Installation for details.</p>"},{"location":"tutorials/quick_start/#data","title":"Data","text":""},{"location":"tutorials/quick_start/#dataset","title":"Dataset","text":"<p>Through the <code>create_dataset</code> module in <code>mindcv.data</code>, we can quickly load standard datasets or customized datasets.</p> <pre><code>import os\nfrom mindcv.data import create_dataset, create_transforms, create_loader\n\ncifar10_dir = './datasets/cifar/cifar-10-batches-bin'  # your dataset path\nnum_classes = 10  # num of classes\nnum_workers = 8  # num of parallel workers\n\n# create dataset\ndataset_train = create_dataset(\n    name='cifar10', root=cifar10_dir, split='train', shuffle=True, num_parallel_workers=num_workers\n)\n</code></pre>"},{"location":"tutorials/quick_start/#transform","title":"Transform","text":"<p>Through the <code>create_transforms</code> function, you can directly obtain the appropriate data processing augmentation strategies (transform list) for standard datasets, including common data processing strategies on Cifar10 and Imagenet.</p> <pre><code># create transforms\ntrans = create_transforms(dataset_name='cifar10', image_resize=224)\n</code></pre>"},{"location":"tutorials/quick_start/#loader","title":"Loader","text":"<p>The <code>mindcv.data.create_loader</code> function is used for data conversion and batch split loading. We need to pass in the transform_list returned by <code>create_transforms</code>.</p> <pre><code># Perform data augmentation operations to generate the required dataset.\nloader_train = create_loader(dataset=dataset_train,\n                             batch_size=64,\n                             is_training=True,\n                             num_classes=num_classes,\n                             transform=trans,\n                             num_parallel_workers=num_workers)\n\nnum_batches = loader_train.get_dataset_size()\n</code></pre> <p>Avoid repeatedly executing a single cell of <code>create_loader</code> in notebook, or execute again after executing <code>create_dataset</code>.</p>"},{"location":"tutorials/quick_start/#model","title":"Model","text":"<p>Use the <code>create_model</code> interface to obtain the instantiated DenseNet and load the pretraining weight(obtained from ImageNet dataset training).</p> <pre><code>from mindcv.models import create_model\n\n# instantiate the DenseNet121 model and load the pretraining weights.\nnetwork = create_model(model_name='densenet121', num_classes=num_classes, pretrained=True)\n</code></pre> <p>Because the number of classes required by CIFAR-10 and ImageNet datasets is different, the classifier parameters cannot be shared, and the warning that the classifier parameters cannot be loaded does not affect the fine-tuning.</p>"},{"location":"tutorials/quick_start/#loss","title":"Loss","text":"<p>By <code>create_loss</code> interface obtains loss function.</p> <pre><code>from mindcv.loss import create_loss\n\nloss = create_loss(name='CE')\n</code></pre>"},{"location":"tutorials/quick_start/#learning-rate-scheduler","title":"Learning Rate Scheduler","text":"<p>Use <code>create_scheduler</code> interface to set the learning rate scheduler.</p> <pre><code>from mindcv.scheduler import create_scheduler\n\n# learning rate scheduler\nlr_scheduler = create_scheduler(steps_per_epoch=num_batches,\n                                scheduler='constant',\n                                lr=0.0001)\n</code></pre>"},{"location":"tutorials/quick_start/#optimizer","title":"Optimizer","text":"<p>Use <code>create_optimizer</code> interface creates an optimizer.</p> <pre><code>from mindcv.optim import create_optimizer\n\n# create optimizer\nopt = create_optimizer(network.trainable_params(), opt='adam', lr=lr_scheduler)\n</code></pre>"},{"location":"tutorials/quick_start/#training","title":"Training","text":"<p>Use the mindspore.Model interface to encapsulate trainable instances according to the parameters passed in by the user.</p> <pre><code>from mindspore import Model\n\n# Encapsulates examples that can be trained or inferred\nmodel = Model(network, loss_fn=loss, optimizer=opt, metrics={'accuracy'})\n</code></pre> <p>Use the <code>mindspore.Model.train</code> interface for model training.</p> <pre><code>from mindspore import LossMonitor, TimeMonitor, CheckpointConfig, ModelCheckpoint\n\n# Set the callback function for saving network parameters during training.\nckpt_save_dir = './ckpt'\nckpt_config = CheckpointConfig(save_checkpoint_steps=num_batches)\nckpt_cb = ModelCheckpoint(prefix='densenet121-cifar10',\n                          directory=ckpt_save_dir,\n                          config=ckpt_config)\n\nmodel.train(5, loader_train, callbacks=[LossMonitor(num_batches//5), TimeMonitor(num_batches//5), ckpt_cb], dataset_sink_mode=False)\n</code></pre> <pre><code>[WARNING] DEVICE(1769,ffff87c70ac0,python):2022-12-21-16:04:30.001.890 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Default/network-WithLossCell/_loss_fn-CrossEntropySmooth/GatherD-op5273] don't support int64, reduce precision from int64 to int32.\n\n\nepoch: 1 step: 156, loss is 2.0816354751586914\nepoch: 1 step: 312, loss is 1.4474115371704102\nepoch: 1 step: 468, loss is 0.8935483694076538\nepoch: 1 step: 624, loss is 0.5588696002960205\nepoch: 1 step: 780, loss is 0.3161369860172272\n\n\n[WARNING] DEVICE(1769,ffff87c70ac0,python):2022-12-21-16:09:20.261.851 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Default/network-WithLossCell/_loss_fn-CrossEntropySmooth/GatherD-op16720] don't support int64, reduce precision from int64 to int32.\n\n\nTrain epoch time: 416429.509 ms, per step time: 532.519 ms\nepoch: 2 step: 154, loss is 0.19752007722854614\nepoch: 2 step: 310, loss is 0.14635677635669708\nepoch: 2 step: 466, loss is 0.3511860966682434\nepoch: 2 step: 622, loss is 0.12542471289634705\nepoch: 2 step: 778, loss is 0.22351759672164917\nTrain epoch time: 156746.872 ms, per step time: 200.444 ms\nepoch: 3 step: 152, loss is 0.08965137600898743\nepoch: 3 step: 308, loss is 0.22765043377876282\nepoch: 3 step: 464, loss is 0.19035443663597107\nepoch: 3 step: 620, loss is 0.06591956317424774\nepoch: 3 step: 776, loss is 0.0934530645608902\nTrain epoch time: 156574.210 ms, per step time: 200.223 ms\nepoch: 4 step: 150, loss is 0.03782692924141884\nepoch: 4 step: 306, loss is 0.023876197636127472\nepoch: 4 step: 462, loss is 0.038690414279699326\nepoch: 4 step: 618, loss is 0.15388774871826172\nepoch: 4 step: 774, loss is 0.1581358164548874\nTrain epoch time: 158398.108 ms, per step time: 202.555 ms\nepoch: 5 step: 148, loss is 0.06556802988052368\nepoch: 5 step: 304, loss is 0.006707251071929932\nepoch: 5 step: 460, loss is 0.02353120595216751\nepoch: 5 step: 616, loss is 0.014183484017848969\nepoch: 5 step: 772, loss is 0.09367241710424423\nTrain epoch time: 154978.618 ms, per step time: 198.182 ms\n</code></pre>"},{"location":"tutorials/quick_start/#evaluation","title":"Evaluation","text":"<p>Now, let's evaluate the trained model on the validation set of CIFAR-10.</p> <pre><code># Load validation dataset\ndataset_val = create_dataset(\n    name='cifar10', root=cifar10_dir, split='test', shuffle=True, num_parallel_workers=num_workers\n)\n\n# Perform data enhancement operations to generate the required dataset.\nloader_val = create_loader(dataset=dataset_val,\n                           batch_size=64,\n                           is_training=False,\n                           num_classes=num_classes,\n                           transform=trans,\n                           num_parallel_workers=num_workers)\n</code></pre> <p>Load the fine-tuning parameter file (densenet121-cifar10-5_782.ckpt) to the model.</p> <p>Encapsulate inferable instances according to the parameters passed in by the user, load the validation dataset and verify the precision of the fine-tuned DenseNet121 model.</p> <pre><code># Verify the accuracy of DenseNet121 after fine-tune\nacc = model.eval(loader_val, dataset_sink_mode=False)\nprint(acc)\n</code></pre> <pre><code>[WARNING] DEVICE(1769,ffff87c70ac0,python):2022-12-21-16:24:11.927.472 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Default/network-WithLossCell/_loss_fn-CrossEntropySmooth/GatherD-op24314] don't support int64, reduce precision from int64 to int32.\n\n\n{'accuracy': 0.951}\n\n\n[WARNING] DEVICE(1769,ffff87c70ac0,python):2022-12-21-16:25:01.871.273 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Default/network-WithLossCell/_loss_fn-CrossEntropySmooth/GatherD-op27139] don't support int64, reduce precision from int64 to int32.\n</code></pre>"},{"location":"tutorials/quick_start/#use-yaml-files-for-model-training-and-validation","title":"Use YAML files for model training and validation","text":"<p>We can also use the yaml file with the model parameters set directly to quickly train and verify the model through <code>train.py</code> and <code>validate.py</code> scripts. The following is an example of training SqueezenetV1 on ImageNet (you need to download ImageNet to the directory in advance).</p> <p>For detailed tutorials, please refer to the tutorial.</p> <pre><code># standalone training on a CPU/GPU/Ascend device\npython train.py -c configs/squeezenet/squeezenet_1.0_gpu.yaml --data_dir /path/to/dataset --distribute False\n</code></pre> <pre><code>python validate.py -c configs/squeezenet/squeezenet_1.0_gpu.yaml --data_dir /path/to/dataset --ckpt_path /path/to/ckpt\n</code></pre>"},{"location":"zh/","title":"\u4e3b\u9875","text":""},{"location":"zh/#_1","title":"\u7b80\u4ecb","text":"<p>MindCV\u662f\u4e00\u4e2a\u57fa\u4e8e MindSpore \u5f00\u53d1\u7684\uff0c\u81f4\u529b\u4e8e\u8ba1\u7b97\u673a\u89c6\u89c9\u76f8\u5173\u6280\u672f\u7814\u53d1\u7684\u5f00\u6e90\u5de5\u5177\u7bb1\u3002\u5b83\u63d0\u4f9b\u5927\u91cf\u7684\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u7684\u7ecf\u5178\u6a21\u578b\u548cSoTA\u6a21\u578b\u4ee5\u53ca\u5b83\u4eec\u7684\u9884\u8bad\u7ec3\u6743\u91cd\u548c\u8bad\u7ec3\u7b56\u7565\u3002\u540c\u65f6\uff0c\u8fd8\u63d0\u4f9b\u4e86\u81ea\u52a8\u589e\u5f3a\u7b49SoTA\u7b97\u6cd5\u6765\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\u3002\u901a\u8fc7\u89e3\u8026\u7684\u6a21\u5757\u8bbe\u8ba1\uff0c\u60a8\u53ef\u4ee5\u8f7b\u677e\u5730\u5c06MindCV\u5e94\u7528\u5230\u60a8\u81ea\u5df1\u7684CV\u4efb\u52a1\u4e2d\u3002</p>"},{"location":"zh/#_2","title":"\u4e3b\u8981\u7279\u6027","text":"<ul> <li> <p>\u9ad8\u6613\u7528\u6027 MindCV\u5c06\u89c6\u89c9\u4efb\u52a1\u5206\u89e3\u4e3a\u5404\u79cd\u53ef\u914d\u7f6e\u7684\u7ec4\u4ef6\uff0c\u7528\u6237\u53ef\u4ee5\u8f7b\u677e\u5730\u6784\u5efa\u81ea\u5df1\u7684\u6570\u636e\u5904\u7406\u548c\u6a21\u578b\u8bad\u7ec3\u6d41\u7a0b\u3002</p> <pre><code>&gt;&gt;&gt; import mindcv\n# \u521b\u5efa\u6570\u636e\u96c6\n&gt;&gt;&gt; dataset = mindcv.create_dataset('cifar10', download=True)\n# \u521b\u5efa\u6a21\u578b\n&gt;&gt;&gt; network = mindcv.create_model('resnet50', pretrained=True)\n</code></pre> <p>\u7528\u6237\u53ef\u901a\u8fc7\u9884\u5b9a\u4e49\u7684\u8bad\u7ec3\u548c\u5fae\u8c03\u811a\u672c\uff0c\u5feb\u901f\u914d\u7f6e\u5e76\u5b8c\u6210\u8bad\u7ec3\u6216\u8fc1\u79fb\u5b66\u4e60\u4efb\u52a1\u3002</p> <pre><code># \u914d\u7f6e\u548c\u542f\u52a8\u8fc1\u79fb\u5b66\u4e60\u4efb\u52a1\npython train.py --model swin_tiny --pretrained --opt=adamw --lr=0.001 --data_dir=/path/to/dataset\n</code></pre> </li> <li> <p>\u9ad8\u6027\u80fd MindCV\u96c6\u6210\u4e86\u5927\u91cf\u57fa\u4e8eCNN\u548cTransformer\u7684\u9ad8\u6027\u80fd\u6a21\u578b\uff0c\u5982SwinTransformer\uff0c\u5e76\u63d0\u4f9b\u9884\u8bad\u7ec3\u6743\u91cd\u3001\u8bad\u7ec3\u7b56\u7565\u548c\u6027\u80fd\u62a5\u544a\uff0c\u5e2e\u52a9\u7528\u6237\u5feb\u901f\u9009\u578b\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u89c6\u89c9\u6a21\u578b\u3002</p> </li> <li> <p>\u7075\u6d3b\u9ad8\u6548 MindCV\u57fa\u4e8e\u9ad8\u6548\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6MindSpore\u5f00\u53d1\uff0c\u5177\u6709\u81ea\u52a8\u5e76\u884c\u548c\u81ea\u52a8\u5fae\u5206\u7b49\u7279\u6027\uff0c\u652f\u6301\u4e0d\u540c\u786c\u4ef6\u5e73\u53f0\u4e0a\uff08CPU/GPU/Ascend\uff09\uff0c\u540c\u65f6\u652f\u6301\u6548\u7387\u4f18\u5316\u7684\u9759\u6001\u56fe\u6a21\u5f0f\u548c\u8c03\u8bd5\u7075\u6d3b\u7684\u52a8\u6001\u56fe\u6a21\u5f0f\u3002</p> </li> </ul>"},{"location":"zh/#_3","title":"\u6a21\u578b\u652f\u6301","text":"<p>\u57fa\u4e8eMindCV\u8fdb\u884c\u6a21\u578b\u5b9e\u73b0\u548c\u91cd\u8bad\u7ec3\u7684\u6c47\u603b\u7ed3\u679c\u8be6\u89c1\u6a21\u578b\u4ed3\u5e93, \u6240\u7528\u5230\u7684\u8bad\u7ec3\u7b56\u7565\u548c\u8bad\u7ec3\u540e\u7684\u6a21\u578b\u6743\u91cd\u5747\u53ef\u901a\u8fc7\u8868\u4e2d\u94fe\u63a5\u83b7\u53d6\u3002</p> <p>\u5404\u6a21\u578b\u8bb2\u89e3\u548c\u8bad\u7ec3\u8bf4\u660e\u8be6\u89c1configs\u76ee\u5f55\u3002</p>"},{"location":"zh/#_4","title":"\u5b89\u88c5","text":"<p>\u8be6\u60c5\u8bf7\u89c1\u5b89\u88c5\u9875\u9762\u3002</p>"},{"location":"zh/#_5","title":"\u5feb\u901f\u5165\u95e8","text":""},{"location":"zh/#_6","title":"\u4e0a\u624b\u6559\u7a0b","text":"<p>\u5728\u5f00\u59cb\u4e0a\u624bMindCV\u524d\uff0c\u53ef\u4ee5\u9605\u8bfbMindCV\u7684\u5feb\u901f\u5f00\u59cb\uff0c\u8be5\u6559\u7a0b\u53ef\u4ee5\u5e2e\u52a9\u7528\u6237\u5feb\u901f\u4e86\u89e3MindCV\u7684\u5404\u4e2a\u91cd\u8981\u7ec4\u4ef6\u4ee5\u53ca\u8bad\u7ec3\u3001\u9a8c\u8bc1\u3001\u6d4b\u8bd5\u6d41\u7a0b\u3002</p> <p>\u4ee5\u4e0b\u662f\u4e00\u4e9b\u4f9b\u60a8\u5feb\u901f\u4f53\u9a8c\u7684\u4ee3\u7801\u6837\u4f8b\u3002</p> <pre><code>&gt;&gt;&gt; import mindcv\n# \u5217\u51fa\u6ee1\u8db3\u6761\u4ef6\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u540d\u79f0\n&gt;&gt;&gt; mindcv.list_models(\"swin*\", pretrained=True)\n['swin_tiny']\n# \u521b\u5efa\u6a21\u578b\n&gt;&gt;&gt; network = mindcv.create_model('swin_tiny', pretrained=True)\n# \u9a8c\u8bc1\u6a21\u578b\u7684\u51c6\u786e\u7387\n&gt;&gt;&gt; !python validate.py --model=swin_tiny --pretrained --dataset=imagenet --val_split=validation\n{'Top_1_Accuracy': 0.80824, 'Top_5_Accuracy': 0.94802, 'loss': 1.7331367141008378}\n</code></pre> \u56fe\u7247\u5206\u7c7b\u793a\u4f8b <p>\u53f3\u952e\u70b9\u51fb\u5982\u4e0b\u56fe\u7247\uff0c\u53e6\u5b58\u4e3a<code>dog.jpg</code>\u3002</p> <p><p> </p></p> <p>\u4f7f\u7528\u52a0\u8f7d\u4e86\u9884\u8bad\u7ec3\u53c2\u6570\u7684SoTA\u6a21\u578b\u5bf9\u56fe\u7247\u8fdb\u884c\u63a8\u7406\u3002</p> <pre><code>&gt;&gt;&gt; !python infer.py --model=swin_tiny --image_path='./dog.jpg'\n{'Labrador retriever': 0.5700152, 'golden retriever': 0.034551315, 'kelpie': 0.010108651, 'Chesapeake Bay retriever': 0.008229004, 'Walker hound, Walker foxhound': 0.007791956}\n</code></pre> <p>\u9884\u6d4b\u7ed3\u679c\u6392\u540d\u524d1\u7684\u662f\u62c9\u5e03\u62c9\u591a\u72ac\uff0c\u6b63\u662f\u8fd9\u5f20\u56fe\u7247\u91cc\u7684\u72d7\u72d7\u7684\u54c1\u79cd\u3002</p>"},{"location":"zh/#_7","title":"\u6a21\u578b\u8bad\u7ec3","text":"<p>\u901a\u8fc7<code>train.py</code>\uff0c\u7528\u6237\u53ef\u4ee5\u5f88\u5bb9\u6613\u5730\u5728\u6807\u51c6\u6570\u636e\u96c6\u6216\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u6a21\u578b\uff0c\u7528\u6237\u53ef\u4ee5\u901a\u8fc7\u5916\u90e8\u53d8\u91cf\u6216\u8005yaml\u914d\u7f6e\u6587\u4ef6\u6765\u8bbe\u7f6e\u8bad\u7ec3\u7b56\u7565\uff08\u5982\u6570\u636e\u589e\u5f3a\u3001\u5b66\u4e60\u7387\u7b56\u7565\uff09\u3002</p> <ul> <li> <p>\u5355\u5361\u8bad\u7ec3</p> <pre><code># \u5355\u5361\u8bad\u7ec3\npython train.py --model resnet50 --dataset cifar10 --dataset_download\n</code></pre> <p>\u4ee5\u4e0a\u4ee3\u7801\u662f\u5728CIFAR10\u6570\u636e\u96c6\u4e0a\u5355\u5361\uff08CPU/GPU/Ascend\uff09\u8bad\u7ec3ResNet\u7684\u793a\u4f8b\uff0c\u901a\u8fc7<code>model</code>\u548c<code>dataset</code>\u53c2\u6570\u5206\u522b\u6307\u5b9a\u9700\u8981\u8bad\u7ec3\u7684\u6a21\u578b\u548c\u6570\u636e\u96c6\u3002</p> </li> <li> <p>\u5206\u5e03\u5f0f\u8bad\u7ec3</p> <p>\u5bf9\u4e8e\u50cfImageNet\u8fd9\u6837\u7684\u5927\u578b\u6570\u636e\u96c6\uff0c\u6709\u5fc5\u8981\u5728\u591a\u4e2a\u8bbe\u5907\u4e0a\u4ee5\u5206\u5e03\u5f0f\u6a21\u5f0f\u8fdb\u884c\u8bad\u7ec3\u3002\u57fa\u4e8eMindSpore\u5bf9\u5206\u5e03\u5f0f\u76f8\u5173\u529f\u80fd\u7684\u826f\u597d\u652f\u6301\uff0c\u7528\u6237\u53ef\u4ee5\u4f7f\u7528<code>mpirun</code>\u6765\u8fdb\u884c\u6a21\u578b\u7684\u5206\u5e03\u5f0f\u8bad\u7ec3\u3002</p> <pre><code># \u5206\u5e03\u5f0f\u8bad\u7ec3\n# \u5047\u8bbe\u4f60\u67094\u5f20GPU\u6216\u8005NPU\u5361\nmpirun --allow-run-as-root -n 4 python train.py --distribute \\\n    --model densenet121 --dataset imagenet --data_dir ./datasets/imagenet\n</code></pre> <p>\u5b8c\u6574\u7684\u53c2\u6570\u5217\u8868\u53ca\u8bf4\u660e\u5728<code>config.py</code>\u4e2d\u5b9a\u4e49\uff0c\u53ef\u8fd0\u884c<code>python train.py --help</code>\u5feb\u901f\u67e5\u770b\u3002</p> <p>\u5982\u9700\u6062\u590d\u8bad\u7ec3\uff0c\u8bf7\u6307\u5b9a<code>--ckpt_path</code>\u548c<code>--ckpt_save_dir</code>\u53c2\u6570\uff0c\u811a\u672c\u5c06\u52a0\u8f7d\u8def\u5f84\u4e2d\u7684\u6a21\u578b\u6743\u91cd\u548c\u4f18\u5316\u5668\u72b6\u6001\uff0c\u5e76\u6062\u590d\u4e2d\u65ad\u7684\u8bad\u7ec3\u8fdb\u7a0b\u3002</p> </li> <li> <p>\u8d85\u53c2\u914d\u7f6e\u548c\u9884\u8bad\u7ec3\u7b56\u7565</p> <p>\u60a8\u53ef\u4ee5\u7f16\u5199yaml\u6587\u4ef6\u6216\u8bbe\u7f6e\u5916\u90e8\u53c2\u6570\u6765\u6307\u5b9a\u914d\u7f6e\u6570\u636e\u3001\u6a21\u578b\u3001\u4f18\u5316\u5668\u7b49\u7ec4\u4ef6\u53ca\u5176\u8d85\u53c2\u3002\u4ee5\u4e0b\u662f\u4f7f\u7528\u9884\u8bbe\u7684\u8bad\u7ec3\u7b56\u7565\uff08yaml\u6587\u4ef6\uff09\u8fdb\u884c\u6a21\u578b\u8bad\u7ec3\u7684\u793a\u4f8b\u3002</p> <pre><code>mpirun --allow-run-as-root -n 4 python train.py -c configs/squeezenet/squeezenet_1.0_gpu.yaml\n</code></pre> <p>\u9884\u5b9a\u4e49\u7684\u8bad\u7ec3\u7b56\u7565</p> <p>MindCV\u76ee\u524d\u63d0\u4f9b\u4e86\u8d85\u8fc720\u79cd\u6a21\u578b\u8bad\u7ec3\u7b56\u7565\uff0c\u5728ImageNet\u53d6\u5f97SoTA\u6027\u80fd\u3002 \u5177\u4f53\u7684\u53c2\u6570\u914d\u7f6e\u548c\u8be6\u7ec6\u7cbe\u5ea6\u6027\u80fd\u6c47\u603b\u8bf7\u89c1<code>configs</code>\u6587\u4ef6\u5939\u3002 \u60a8\u53ef\u4ee5\u4fbf\u6377\u5730\u5c06\u8fd9\u4e9b\u8bad\u7ec3\u7b56\u7565\u7528\u4e8e\u60a8\u7684\u6a21\u578b\u8bad\u7ec3\u4e2d\u4ee5\u63d0\u9ad8\u6027\u80fd\uff08\u590d\u7528\u6216\u4fee\u6539\u76f8\u5e94\u7684yaml\u6587\u4ef6\u5373\u53ef\uff09\u3002</p> </li> <li> <p>\u5728ModelArts/OpenI\u5e73\u53f0\u4e0a\u8bad\u7ec3</p> <p>\u5728ModelArts\u6216OpenI\u4e91\u5e73\u53f0\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u9700\u8981\u6267\u884c\u4ee5\u4e0b\u64cd\u4f5c\uff1a</p> <pre><code>1\u3001\u5728\u4e91\u5e73\u53f0\u4e0a\u521b\u5efa\u65b0\u7684\u8bad\u7ec3\u4efb\u52a1\u3002\n2\u3001\u5728\u7f51\u7ad9UI\u754c\u9762\u6dfb\u52a0\u8fd0\u884c\u53c2\u6570`config`\uff0c\u5e76\u6307\u5b9ayaml\u914d\u7f6e\u6587\u4ef6\u7684\u8def\u5f84\u3002\n3\u3001\u5728\u7f51\u7ad9UI\u754c\u9762\u6dfb\u52a0\u8fd0\u884c\u53c2\u6570`enable_modelarts`\u5e76\u8bbe\u7f6e\u4e3aTrue\u3002\n4\u3001\u5728\u7f51\u7ad9\u4e0a\u586b\u5199\u5176\u4ed6\u8bad\u7ec3\u4fe1\u606f\u5e76\u542f\u52a8\u8bad\u7ec3\u4efb\u52a1\u3002\n</code></pre> </li> </ul> <p>\u9759\u6001\u56fe\u548c\u52a8\u6001\u56fe\u6a21\u5f0f</p> <p>\u5728\u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0c\u6a21\u578b\u8bad\u7ec3\uff08<code>train.py</code>\uff09\u5728MindSpore\u4e0a\u4ee5\u56fe\u6a21\u5f0f \u8fd0\u884c\uff0c\u8be5\u6a21\u5f0f\u5bf9\u4f7f\u7528\u9759\u6001\u56fe\u7f16\u8bd1\u5bf9\u6027\u80fd\u548c\u5e76\u884c\u8ba1\u7b97\u8fdb\u884c\u4e86\u4f18\u5316\u3002 \u76f8\u6bd4\u4e4b\u4e0b\uff0cpynative\u6a21\u5f0f\u7684\u4f18\u52bf\u5728\u4e8e\u7075\u6d3b\u6027\u548c\u6613\u4e8e\u8c03\u8bd5\u3002\u4e3a\u4e86\u65b9\u4fbf\u8c03\u8bd5\uff0c\u60a8\u53ef\u4ee5\u5c06\u53c2\u6570<code>--mode</code>\u8bbe\u4e3a1\u4ee5\u5c06\u8fd0\u884c\u6a21\u5f0f\u8bbe\u7f6e\u4e3a\u8c03\u8bd5\u6a21\u5f0f\u3002</p> <p>\u6df7\u5408\u6a21\u5f0f</p> <p>\u57fa\u4e8emindspore.jit\u7684\u6df7\u5408\u6a21\u5f0f \u662f\u517c\u987e\u4e86MindSpore\u7684\u6548\u7387\u548c\u7075\u6d3b\u7684\u6df7\u5408\u6a21\u5f0f\u3002\u7528\u6237\u53ef\u901a\u8fc7\u4f7f\u7528<code>train_with_func.py</code>\u6587\u4ef6\u6765\u4f7f\u7528\u8be5\u6df7\u5408\u6a21\u5f0f\u8fdb\u884c\u8bad\u7ec3\u3002</p> <pre><code>python train_with_func.py --model=resnet50 --dataset=cifar10 --dataset_download --epoch_size=10\n</code></pre> <p>\u6ce8\uff1a\u6b64\u4e3a\u8bd5\u9a8c\u6027\u8d28\u7684\u8bad\u7ec3\u811a\u672c\uff0c\u4ecd\u5728\u6539\u8fdb\uff0c\u5728MindSpore 1.8.1\u6216\u66f4\u65e9\u7248\u672c\u4e0a\u4f7f\u7528\u6b64\u6a21\u5f0f\u76ee\u524d\u5e76\u4e0d\u7a33\u5b9a\u3002</p>"},{"location":"zh/#_8","title":"\u6a21\u578b\u9a8c\u8bc1","text":"<p>\u4f7f\u7528<code>validate.py</code>\u53ef\u4ee5\u4fbf\u6377\u5730\u9a8c\u8bc1\u8bad\u7ec3\u597d\u7684\u6a21\u578b\u3002</p> <pre><code># \u9a8c\u8bc1\u6a21\u578b\npython validate.py --model=resnet50 --dataset=imagenet --data_dir=/path/to/data --ckpt_path=/path/to/model.ckpt\n</code></pre> <p>\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u8fdb\u884c\u9a8c\u8bc1</p> <p>\u5f53\u9700\u8981\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0c\u8ddf\u8e2a\u6a21\u578b\u5728\u6d4b\u8bd5\u96c6\u4e0a\u7cbe\u5ea6\u7684\u53d8\u5316\u65f6\uff0c\u8bf7\u542f\u7528\u53c2\u6570<code>--val_while_train</code>\uff0c\u5982\u4e0b</p> <pre><code>python train.py --model=resnet50 --dataset=cifar10 \\\n    --val_while_train --val_split=test --val_interval=1\n</code></pre> <p>\u5404\u8f6e\u6b21\u7684\u8bad\u7ec3\u635f\u5931\u548c\u6d4b\u8bd5\u7cbe\u5ea6\u5c06\u4fdd\u5b58\u5728<code>{ckpt_save_dir}/results.log</code>\u4e2d\u3002</p> <p>\u66f4\u591a\u8bad\u7ec3\u548c\u9a8c\u8bc1\u7684\u793a\u4f8b\u8bf7\u89c1\u793a\u4f8b\u3002</p>"},{"location":"zh/#_9","title":"\u6559\u7a0b","text":"<p>\u6211\u4eec\u63d0\u4f9b\u4e86\u7cfb\u5217\u6559\u7a0b\uff0c\u5e2e\u52a9\u7528\u6237\u5b66\u4e60\u5982\u4f55\u4f7f\u7528MindCV.</p> <ul> <li>\u4e86\u89e3\u6a21\u578b\u914d\u7f6e</li> <li>\u6a21\u578b\u63a8\u7406</li> <li>\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\u4e0a\u7684\u6a21\u578b\u5fae\u8c03\u8bad\u7ec3</li> <li>\u5982\u4f55\u81ea\u5b9a\u4e49\u6a21\u578b //coming soon</li> <li>\u89c6\u89c9transformer\u6027\u80fd\u4f18\u5316 //coming soon</li> <li>\u90e8\u7f72\u63a8\u7406\u670d\u52a1</li> </ul>"},{"location":"zh/#_10","title":"\u652f\u6301\u7b97\u6cd5","text":"\u652f\u6301\u7b97\u6cd5\u5217\u8868  <ul> <li>\u6570\u636e\u589e\u5f3a<ul> <li>AutoAugment</li> <li>RandAugment</li> <li>Repeated Augmentation</li> <li>RandErasing (Cutout)</li> <li>CutMix</li> <li>MixUp</li> <li>RandomResizeCrop</li> <li>Color Jitter, Flip, etc</li> </ul> </li> <li>\u4f18\u5316\u5668<ul> <li>Adam</li> <li>AdamW</li> <li>Lion</li> <li>Adan (experimental)</li> <li>AdaGrad</li> <li>LAMB</li> <li>Momentum</li> <li>RMSProp</li> <li>SGD</li> <li>NAdam</li> </ul> </li> <li>\u5b66\u4e60\u7387\u8c03\u5ea6\u5668<ul> <li>Warmup Cosine Decay</li> <li>Step LR</li> <li>Polynomial Decay</li> <li>Exponential Decay</li> </ul> </li> <li>\u6b63\u5219\u5316<ul> <li>Weight Decay</li> <li>Label Smoothing</li> <li>Stochastic Depth (depends on networks)</li> <li>Dropout (depends on networks)</li> </ul> </li> <li>\u635f\u5931\u51fd\u6570<ul> <li>Cross Entropy (w/ class weight and auxiliary logit support)</li> <li>Binary Cross Entropy  (w/ class weight and auxiliary logit support)</li> <li>Soft Cross Entropy Loss (automatically enabled if mixup or label smoothing is used)</li> <li>Soft Binary Cross Entropy Loss (automatically enabled if mixup or label smoothing is used)</li> </ul> </li> <li>\u6a21\u578b\u878d\u5408<ul> <li>Warmup EMA (Exponential Moving Average)</li> </ul> </li> </ul>"},{"location":"zh/#_11","title":"\u8d21\u732e\u65b9\u5f0f","text":"<p>\u6b22\u8fce\u5f00\u53d1\u8005\u7528\u6237\u63d0issue\u6216\u63d0\u4ea4\u4ee3\u7801PR\uff0c\u6216\u8d21\u732e\u66f4\u591a\u7684\u7b97\u6cd5\u548c\u6a21\u578b\uff0c\u4e00\u8d77\u8ba9MindCV\u53d8\u5f97\u66f4\u597d\u3002</p> <p>\u6709\u5173\u8d21\u732e\u6307\u5357\uff0c\u8bf7\u53c2\u9605\u8d21\u732e\u3002 \u8bf7\u9075\u5faa\u6a21\u578b\u7f16\u5199\u6307\u5357\u6240\u89c4\u5b9a\u7684\u89c4\u5219\u6765\u8d21\u732e\u6a21\u578b\u63a5\u53e3\uff1a)</p>"},{"location":"zh/#_12","title":"\u8bb8\u53ef\u8bc1","text":"<p>\u672c\u9879\u76ee\u9075\u5faaApache License 2.0\u5f00\u6e90\u534f\u8bae\u3002</p>"},{"location":"zh/#_13","title":"\u81f4\u8c22","text":"<p>MindCV\u662f\u7531MindSpore\u56e2\u961f\u3001\u897f\u5b89\u7535\u5b50\u79d1\u6280\u5927\u5b66\u3001\u897f\u5b89\u4ea4\u901a\u5927\u5b66\u8054\u5408\u5f00\u53d1\u7684\u5f00\u6e90\u9879\u76ee\u3002 \u8877\u5fc3\u611f\u8c22\u6240\u6709\u53c2\u4e0e\u7684\u7814\u7a76\u4eba\u5458\u548c\u5f00\u53d1\u4eba\u5458\u4e3a\u8fd9\u4e2a\u9879\u76ee\u6240\u4ed8\u51fa\u7684\u52aa\u529b\u3002 \u5341\u5206\u611f\u8c22 OpenI \u5e73\u53f0\u6240\u63d0\u4f9b\u7684\u7b97\u529b\u8d44\u6e90\u3002</p>"},{"location":"zh/#_14","title":"\u5f15\u7528","text":"<p>\u5982\u679c\u4f60\u89c9\u5f97MindCV\u5bf9\u4f60\u7684\u9879\u76ee\u6709\u5e2e\u52a9\uff0c\u8bf7\u8003\u8651\u5f15\u7528\uff1a</p> <pre><code>@misc{MindSpore Computer Vision 2022,\n    title={{MindSpore Computer  Vision}:MindSpore Computer Vision Toolbox and Benchmark},\n    author={MindSpore Vision Contributors},\n    howpublished = {\\url{https://github.com/mindspore-lab/mindcv/}},\n    year={2022}\n}\n</code></pre>"},{"location":"zh/installation/","title":"\u5b89\u88c5","text":""},{"location":"zh/installation/#_1","title":"\u4f9d\u8d56","text":"<ul> <li>mindspore &gt;= 1.8.1</li> <li>numpy &gt;= 1.17.0</li> <li>pyyaml &gt;= 5.3</li> <li>tqdm</li> <li>openmpi 4.0.3 (\u5206\u5e03\u5f0f\u8bad\u7ec3\u6240\u9700)</li> </ul> <p>\u4e3a\u4e86\u5b89\u88c5<code>python</code>\u76f8\u5173\u5e93\u4f9d\u8d56\uff0c\u53ea\u9700\u8fd0\u884c\uff1a</p> <pre><code>pip install -r requirements.txt\n</code></pre> <p>Tip</p> <p>MindSpore\u53ef\u4ee5\u901a\u8fc7\u9075\u5faa\u5b98\u65b9\u6307\u5f15\uff0c\u5728\u4e0d\u540c\u7684\u786c\u4ef6\u5e73\u53f0\u4e0a\u83b7\u5f97\u6700\u4f18\u7684\u5b89\u88c5\u4f53\u9a8c\u3002 \u4e3a\u4e86\u5728\u5206\u5e03\u5f0f\u6a21\u5f0f\u4e0b\u8fd0\u884c\uff0c\u60a8\u8fd8\u9700\u8981\u5b89\u88c5OpenMPI\u3002</p> <p>\u5982\u4e0b\u7684\u6307\u5f15\u5047\u8bbe\u60a8\u5df2\u7ecf\u5b8c\u6210\u4e86\u6240\u6709\u4f9d\u8d56\u5e93\u7684\u5b89\u88c5\u3002</p>"},{"location":"zh/installation/#pypi","title":"PyPI\u6e90\u5b89\u88c5","text":"<p>MindCV\u88ab\u53d1\u5e03\u4e3a\u4e00\u4e2aPython\u5305\u5e76\u80fd\u591f\u901a\u8fc7<code>pip</code>\u8fdb\u884c\u5b89\u88c5\u3002\u6211\u4eec\u63a8\u8350\u60a8\u5728\u865a\u62df\u73af\u5883\u5b89\u88c5\u4f7f\u7528\u3002 \u6253\u5f00\u7ec8\u7aef\uff0c\u8f93\u5165\u4ee5\u4e0b\u6307\u4ee4\u6765\u5b89\u88c5MindCV:</p> stablenightly <pre><code>pip install mindcv\n</code></pre> <pre><code># \u6682\u4e0d\u652f\u6301\n</code></pre> <p>\u4e0a\u8ff0\u547d\u4ee4\u4f1a\u81ea\u52a8\u5b89\u88c5\u4f9d\u8d56\uff1aNumPy\uff0cPyYAML \u548c tqdm\u7684\u517c\u5bb9\u7248\u672c\u3002</p> <p>Tip</p> <p>\u5982\u679c\u60a8\u4e4b\u524d\u6ca1\u6709\u4f7f\u7528 Python \u7684\u7ecf\u9a8c\uff0c\u6211\u4eec\u5efa\u8bae\u60a8\u9605\u8bfb\u4f7f\u7528Python\u7684pip\u6765\u7ba1\u7406\u60a8\u7684\u9879\u76ee\u7684\u4f9d\u8d56\u5173\u7cfb\uff0c \u8fd9\u662f\u5bf9 Python \u5305\u7ba1\u7406\u673a\u5236\u7684\u4e00\u4e2a\u5f88\u597d\u7684\u4ecb\u7ecd\uff0c\u5e76\u4e14\u53ef\u4ee5\u5e2e\u52a9\u60a8\u5728\u9047\u5230\u9519\u8bef\u65f6\u8fdb\u884c\u6545\u969c\u6392\u9664\u3002</p> <p>Warning</p> <p>\u4e0a\u8ff0\u547d\u4ee4 \u4e0d\u4f1a \u5b89\u88c5MindSpore. \u6211\u4eec\u5f3a\u70c8\u63a8\u8350\u60a8\u901a\u8fc7\u5b98\u65b9\u6307\u5f15\u6765\u5b89\u88c5MindSpore\u3002</p>"},{"location":"zh/installation/#_2","title":"\u6e90\u7801\u5b89\u88c5 (\u672a\u7ecf\u6d4b\u8bd5\u7248\u672c)","text":""},{"location":"zh/installation/#vcs","title":"VCS\u6e90\u7801\u5b89\u88c5","text":"<pre><code>pip install git+https://github.com/mindspore-lab/mindcv.git\n</code></pre>"},{"location":"zh/installation/#_3","title":"\u672c\u5730\u6e90\u7801\u5b89\u88c5","text":"<p>Tip</p> <p>\u7531\u4e8e\u672c\u9879\u76ee\u5904\u4e8e\u6d3b\u8dc3\u5f00\u53d1\u9636\u6bb5\uff0c\u5982\u679c\u60a8\u662f\u5f00\u53d1\u8005\u6216\u8005\u8d21\u732e\u8005\uff0c\u8bf7\u4f18\u5148\u9009\u62e9\u6b64\u5b89\u88c5\u65b9\u5f0f\u3002</p> <p>MindCV\u53ef\u4ee5\u5728\u7531 GitHub \u514b\u9686\u4ed3\u5e93\u5230\u672c\u5730\u6587\u4ef6\u5939\u540e\u76f4\u63a5\u4f7f\u7528\u3002 \u8fd9\u5bf9\u4e8e\u60f3\u4f7f\u7528\u6700\u65b0\u7248\u672c\u7684\u5f00\u53d1\u8005\u5341\u5206\u65b9\u4fbf:</p> <pre><code>git clone https://github.com/mindspore-lab/mindcv.git\n</code></pre> <p>\u5728\u514b\u9686\u5230\u672c\u5730\u4e4b\u540e\uff0c\u63a8\u8350\u60a8\u4f7f\u7528\"\u53ef\u7f16\u8f91\"\u6a21\u5f0f\u8fdb\u884c\u5b89\u88c5\uff0c\u8fd9\u6709\u52a9\u4e8e\u89e3\u51b3\u6f5c\u5728\u7684\u6a21\u5757\u5bfc\u5165\u95ee\u9898\u3002</p> <pre><code>cd mindcv\npip install -e .\n</code></pre>"},{"location":"zh/modelzoo/","title":"\u6a21\u578b\u4ed3\u5e93","text":"Model Context Top-1 (%) Top-5 (%) Params(M) Recipe Download bit_resnet50 D910x8-G 76.81 93.17 25.55 yaml weights bit_resnet50x3 D910x8-G 80.63 95.12 217.31 yaml weights bit_resnet101 D910x8-G 77.93 93.75 44.54 yaml weights cmt_small D910x8-G 83.24 96.41 26.09 yaml weights coat_lite_tiny D910x8-G 77.35 93.43 5.72 yaml weights coat_lite_mini D910x8-G 78.51 93.84 11.01 yaml weights coat_tiny D910x8-G 79.67 94.88 5.50 yaml weights coat_mini D910x8-G 81.08 95.34 10.34 yaml weights convit_tiny D910x8-G 73.66 91.72 5.71 yaml weights convit_tiny_plus D910x8-G 77.00 93.60 9.97 yaml weights convit_small D910x8-G 81.63 95.59 27.78 yaml weights convit_small_plus D910x8-G 81.80 95.42 48.98 yaml weights convit_base D910x8-G 82.10 95.52 86.54 yaml weights convit_base_plus D910x8-G 81.96 95.04 153.13 yaml weights convnext_tiny D910x64-G 81.91 95.79 28.59 yaml weights convnext_small D910x64-G 83.40 96.36 50.22 yaml weights convnext_base D910x64-G 83.32 96.24 88.59 yaml weights convnextv2_tiny D910x8-G 82.43 95.98 28.64 yaml weights crossvit_9 D910x8-G 73.56 91.79 8.55 yaml weights crossvit_15 D910x8-G 81.08 95.33 27.27 yaml weights crossvit_18 D910x8-G 81.93 95.75 43.27 yaml weights densenet121 D910x8-G 75.64 92.84 8.06 yaml weights densenet161 D910x8-G 79.09 94.66 28.90 yaml weights densenet169 D910x8-G 77.26 93.71 14.31 yaml weights densenet201 D910x8-G 78.14 94.08 20.24 yaml weights dpn92 D910x8-G 79.46 94.49 37.79 yaml weights dpn98 D910x8-G 79.94 94.57 61.74 yaml weights dpn107 D910x8-G 80.05 94.74 87.13 yaml weights dpn131 D910x8-G 80.07 94.72 79.48 yaml weights edgenext_xx_small D910x8-G 71.02 89.99 1.33 yaml weights edgenext_x_small D910x8-G 75.14 92.50 2.34 yaml weights edgenext_small D910x8-G 79.15 94.39 5.59 yaml weights edgenext_base D910x8-G 82.24 95.94 18.51 yaml weights efficientnet_b0 D910x64-G 76.89 93.16 5.33 yaml weights efficientnet_b1 D910x64-G 78.95 94.34 7.86 yaml weights ghostnet_050 D910x8-G 66.03 86.64 2.60 yaml weights ghostnet_100 D910x8-G 73.78 91.66 5.20 yaml weights ghostnet_130 D910x8-G 75.50 92.56 7.39 yaml weights googlenet D910x8-G 72.68 90.89 6.99 yaml weights halonet_50t D910X8-G 79.53 94.79 22.79 yaml weights hrnet_w32 D910x8-G 80.64 95.44 41.30 yaml weights hrnet_w48 D910x8-G 81.19 95.69 77.57 yaml weights inception_v3 D910x8-G 79.11 94.40 27.20 yaml weights inception_v4 D910x8-G 80.88 95.34 42.74 yaml weights mixnet_s D910x8-G 75.52 92.52 4.17 yaml weights mixnet_m D910x8-G 76.64 93.05 5.06 yaml weights mixnet_l D910x8-G 78.73 94.31 7.38 yaml weights mnasnet_050 D910x8-G 68.07 88.09 2.14 yaml weights mnasnet_075 D910x8-G 71.81 90.53 3.20 yaml weights mnasnet_100 D910x8-G 74.28 91.70 4.42 yaml weights mnasnet_130 D910x8-G 75.65 92.64 6.33 yaml weights mnasnet_140 D910x8-G 76.01 92.83 7.16 yaml weights mobilenet_v1_025 D910x8-G 53.87 77.66 0.47 yaml weights mobilenet_v1_050 D910x8-G 65.94 86.51 1.34 yaml weights mobilenet_v1_075 D910x8-G 70.44 89.49 2.60 yaml weights mobilenet_v1_100 D910x8-G 72.95 91.01 4.25 yaml weights mobilenet_v2_075 D910x8-G 69.98 89.32 2.66 yaml weights mobilenet_v2_100 D910x8-G 72.27 90.72 3.54 yaml weights mobilenet_v2_140 D910x8-G 75.56 92.56 6.15 yaml weights mobilenet_v3_small_100 D910x8-G 68.10 87.86 2.55 yaml weights mobilenet_v3_large_100 D910x8-G 75.23 92.31 5.51 yaml weights mobilevit_xx_small D910x8-G 68.91 88.91 1.27 yaml weights mobilevit_x_small D910x8-G 74.99 92.32 2.32 yaml weights mobilevit_small D910x8-G 78.47 94.18 5.59 yaml weights nasnet_a_4x1056 D910x8-G 73.65 91.25 5.33 yaml weights pit_ti D910x8-G 72.96 91.33 4.85 yaml weights pit_xs D910x8-G 78.41 94.06 10.61 yaml weights pit_s D910x8-G 80.56 94.80 23.46 yaml weights pit_b D910x8-G 81.87 95.04 73.76 yaml weights poolformer_s12 D910x8-G 77.33 93.34 11.92 yaml weights pvt_tiny D910x8-G 74.81 92.18 13.23 yaml weights pvt_small D910x8-G 79.66 94.71 24.49 yaml weights pvt_medium D910x8-G 81.82 95.81 44.21 yaml weights pvt_large D910x8-G 81.75 95.70 61.36 yaml weights pvt_v2_b0 D910x8-G 71.50 90.60 3.67 yaml weights pvt_v2_b1 D910x8-G 78.91 94.49 14.01 yaml weights pvt_v2_b2 D910x8-G 81.99 95.74 25.35 yaml weights pvt_v2_b3 D910x8-G 82.84 96.24 45.24 yaml weights pvt_v2_b4 D910x8-G 83.14 96.27 62.56 yaml weights regnet_x_200mf D910x8-G 68.74 88.38 2.68 yaml weights regnet_x_400mf D910x8-G 73.16 91.35 5.16 yaml weights regnet_x_600mf D910x8-G 74.34 92.00 6.20 yaml weights regnet_x_800mf D910x8-G 76.04 92.97 7.26 yaml weights regnet_y_200mf D910x8-G 70.30 89.61 3.16 yaml weights regnet_y_400mf D910x8-G 73.91 91.84 4.34 yaml weights regnet_y_600mf D910x8-G 75.69 92.50 6.06 yaml weights regnet_y_800mf D910x8-G 76.52 93.10 6.26 yaml weights regnet_y_16gf D910x8-G 82.92 96.29 83.71 yaml weights repmlp_t224 D910x8-G 76.71 93.30 38.30 yaml weights repvgg_a0 D910x8-G 72.19 90.75 9.13 yaml weights repvgg_a1 D910x8-G 74.19 91.89 14.12 yaml weights repvgg_a2 D910x8-G 76.63 93.42 28.25 yaml weights repvgg_b0 D910x8-G 74.99 92.40 15.85 yaml weights repvgg_b1 D910x8-G 78.81 94.37 57.48 yaml weights repvgg_b2 D910x64-G 79.29 94.66 89.11 yaml weights repvgg_b3 D910x64-G 80.46 95.34 123.19 yaml weights repvgg_b1g2 D910x8-G 78.03 94.09 45.85 yaml weights repvgg_b1g4 D910x8-G 77.64 94.03 40.03 yaml weights repvgg_b2g4 D910x8-G 78.8 94.36 61.84 yaml weights res2net50 D910x8-G 79.35 94.64 25.76 yaml weights res2net101 D910x8-G 79.56 94.70 45.33 yaml weights res2net50_v1b D910x8-G 80.32 95.09 25.77 yaml weights res2net101_v1b D910x8-G 81.14 95.41 45.35 yaml weights resnest50 D910x8-G 80.81 95.16 27.55 yaml weights resnest101 D910x8-G 82.90 96.12 48.41 yaml weights resnet18 D910x8-G 70.21 89.62 11.70 yaml weights resnet34 D910x8-G 74.15 91.98 21.81 yaml weights resnet50 D910x8-G 76.69 93.50 25.61 yaml weights resnet101 D910x8-G 78.24 94.09 44.65 yaml weights resnet152 D910x8-G 78.72 94.45 60.34 yaml weights resnetv2_50 D910x8-G 76.90 93.37 25.60 yaml weights resnetv2_101 D910x8-G 78.48 94.23 44.55 yaml weights resnext50_32x4d D910x8-G 78.53 94.10 25.10 yaml weights resnext101_32x4d D910x8-G 79.83 94.80 44.32 yaml weights resnext101_64x4d D910x8-G 80.30 94.82 83.66 yaml weights resnext152_64x4d D910x8-G 80.52 95.00 115.27 yaml weights rexnet_09 D910x8-G 77.06 93.41 4.13 yaml weights rexnet_10 D910x8-G 77.38 93.60 4.84 yaml weights rexnet_13 D910x8-G 79.06 94.28 7.61 yaml weights rexnet_15 D910x8-G 79.95 94.74 9.79 yaml weights rexnet_20 D910x8-G 80.64 94.99 16.45 yaml weights seresnet18 D910x8-G 71.81 90.49 11.80 yaml weights seresnet34 D910x8-G 75.38 92.50 21.98 yaml weights seresnet50 D910x8-G 78.32 94.07 28.14 yaml weights seresnext26_32x4d D910x8-G 77.17 93.42 16.83 yaml weights seresnext50_32x4d D910x8-G 78.71 94.36 27.63 yaml weights shufflenet_v1_g3_05 D910x8-G 57.05 79.73 0.73 yaml weights shufflenet_v1_g3_10 D910x8-G 67.77 87.73 1.89 yaml weights shufflenet_v2_x0_5 D910x8-G 60.53 82.11 1.37 yaml weights shufflenet_v2_x1_0 D910x8-G 69.47 88.88 2.29 yaml weights shufflenet_v2_x1_5 D910x8-G 72.79 90.93 3.53 yaml weights shufflenet_v2_x2_0 D910x8-G 75.07 92.08 7.44 yaml weights skresnet18 D910x8-G 73.09 91.20 11.97 yaml weights skresnet34 D910x8-G 76.71 93.10 22.31 yaml weights skresnext50_32x4d D910x8-G 79.08 94.60 37.31 yaml weights squeezenet1_0 D910x8-G 59.01 81.01 1.25 yaml weights squeezenet1_0 GPUx8-G 58.83 81.08 1.25 yaml weights squeezenet1_1 D910x8-G 58.44 80.84 1.24 yaml weights squeezenet1_1 GPUx8-G 59.18 81.41 1.24 yaml weights swin_tiny D910x8-G 80.82 94.80 33.38 yaml weights swinv2_tiny_window8 D910x8-G 81.42 95.43 28.78 yaml weights vgg11 D910x8-G 71.86 90.50 132.86 yaml weights vgg13 D910x8-G 72.87 91.02 133.04 yaml weights vgg16 D910x8-G 74.61 91.87 138.35 yaml weights vgg19 D910x8-G 75.21 92.56 143.66 yaml weights visformer_tiny D910x8-G 78.28 94.15 10.33 yaml weights visformer_tiny_v2 D910x8-G 78.82 94.41 9.38 yaml weights visformer_small D910x8-G 81.76 95.88 40.25 yaml weights visformer_small_v2 D910x8-G 82.17 95.90 23.52 yaml weights vit_b_32_224 D910x8-G 75.86 92.08 87.46 yaml weights vit_l_16_224 D910x8-G 76.34 92.79 303.31 yaml weights vit_l_32_224 D910x8-G 73.71 90.92 305.52 yaml weights volo_d1 D910x8-G 82.59 95.99 27 yaml weights xception D910x8-G 79.01 94.25 22.91 yaml weights xcit_tiny_12_p16_224 D910x8-G 77.67 93.79 7.00 yaml weights"},{"location":"zh/modelzoo/#_2","title":"\u8bf4\u660e","text":"<ul> <li>Context\uff1aTraining context\uff0c\u8868\u793a\u4e3a{\u8bbe\u5907}x{\u6570\u91cf}-{MS\u6a21\u5f0f}\uff0c\u5176\u4e2dmindspore\u6a21\u5f0f\u53ef\u4ee5\u662f G-graph \u6a21\u5f0f\u6216\u5e26ms\u529f\u80fd\u7684 F-pynative   \u6a21\u5f0f\u3002\u4f8b\u5982\uff0cD910x8-G\u662f\u4f7f\u7528 graph \u6a21\u5f0f\u57288\u5757Ascend 910 NPU\u4e0a\u8fdb\u884c\u8bad\u7ec3\u3002</li> <li>Top-1 \u548c Top-5\uff1a\u5728 ImageNet-1K \u9a8c\u8bc1\u96c6\u4e0a\u62a5\u544a\u7684Accuracy\u3002</li> </ul>"},{"location":"zh/how_to_guides/feature_extraction/","title":"\u591a\u5c3a\u5ea6\u7279\u5f81\u63d0\u53d6","text":"<p>\u5728\u672c\u6559\u7a0b\u4e2d\uff0c\u60a8\u5c06\u5b66\u4e60\u5982\u4f55\u5bf9MindCV\u4e2d\u7684\u6a21\u578b\u8fdb\u884c\u591a\u5c3a\u5ea6\u7279\u5f81\u62bd\u53d6\u3002 \u5728\u5b9e\u9645\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u9879\u76ee\u4e2d\uff0c\u6211\u4eec\u7ecf\u5e38\u4f7f\u7528\u7ecf\u5178\u7684\u8ba1\u7b97\u673a\u89c6\u89c9\u9aa8\u5e72\u7f51\u7edc\uff0c\u5982ResNet\u3001VGG\uff0c\u4ee5\u83b7\u5f97\u66f4\u597d\u7684\u6027\u80fd\u548c\u66f4\u5feb\u7684\u5f00\u53d1\u8fdb\u5ea6\u3002\u901a\u5e38\u60c5\u51b5\u4e0b\uff0c\u4ec5\u4f7f\u7528\u9aa8\u5e72\u7f51\u7edc\u7684\u6700\u7ec8\u8f93\u51fa\u5f80\u5f80\u662f\u4e0d\u591f\u7684\u3002 \u6211\u4eec\u9700\u8981\u6765\u81ea\u4e2d\u95f4\u5c42\u7684\u8f93\u51fa\uff0c\u5b83\u4eec\u4f5c\u4e3a\u8f93\u5165\u7684\u591a\u5c3a\u5ea6\u62bd\u8c61\uff0c\u53ef\u4ee5\u5e2e\u52a9\u8fdb\u4e00\u6b65\u63d0\u5347\u6211\u4eec\u4e0b\u6e38\u4efb\u52a1\u7684\u6027\u80fd\u3002 \u4e3a\u6b64\uff0c\u6211\u4eec\u5728MindCV\u4e2d\u8bbe\u8ba1\u4e86\u4e00\u79cd\u4ece\u9aa8\u5e72\u7f51\u7edc\u4e2d\u62bd\u53d6\u591a\u5c3a\u5ea6\u7279\u5f81\u7684\u673a\u5236\u3002\u5728\u7f16\u5199\u672c\u6559\u7a0b\u65f6\uff0cMindCV\u5df2\u7ecf\u652f\u6301\u4eceResNet\u3001MobileNetV3\u3001ConvNeXt\u3001ResNeST\u3001EfficientNet\u3001RepVGG\u3001HRNet\u548cReXNet\u4e2d\u4f7f\u7528\u8fd9\u79cd\u673a\u5236\u62bd\u53d6\u7279\u5f81\u3002\u6709\u5173\u7279\u5f81\u62bd\u53d6\u673a\u5236\u7684\u66f4\u591a\u8be6\u7ec6\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605<code>FeatureExtractWrapper</code>.</p> <p>\u672c\u6559\u7a0b\u5c06\u5e2e\u52a9\u60a8\u5b66\u4e60\u5982\u4f55\u6dfb\u52a0\u4e00\u4e9b\u4ee3\u7801\u6765\u4ece\u5176\u4f59\u9aa8\u5e72\u7f51\u7edc\u4e2d\u62bd\u53d6\u591a\u5c3a\u5ea6\u7279\u5f81\u3002\u4e3b\u8981\u6709\u4e24\u4e2a\u6b65\u9aa4\uff1a</p> <ol> <li>\u5728\u6a21\u578b\u7684<code>__init__()</code>\u4e2d\uff0c\u6ce8\u518c\u9700\u8981\u63d0\u53d6\u8f93\u51fa\u7279\u5f81\u7684\u4e2d\u95f4\u5c42\uff0c\u5c06\u5176\u6dfb\u52a0\u5230<code>self.feature_info</code>\u4e2d\u3002</li> <li>\u6dfb\u52a0\u4e00\u4e2a\u7528\u4e8e\u6a21\u578b\u521b\u5efa\u7684\u5c01\u88c5\u51fd\u6570\u3002</li> <li>\u5c06\u53c2\u6570<code>feature_only=True</code>\u548c<code>out_indices</code>\u4f20\u5165<code>create_model()</code>.</li> </ol>"},{"location":"zh/how_to_guides/feature_extraction/#_2","title":"\u6ce8\u518c\u4e2d\u95f4\u5c42","text":"<p>\u5728MindCV\u4e2d\u5b9e\u73b0\u7279\u5f81\u62bd\u53d6\u4ee3\u7801\u65f6\uff0c\u4e3b\u8981\u6709\u4e09\u79cd\u6a21\u578b\u5b9e\u73b0\u60c5\u51b5\uff0c\u5373\uff1a * \u5bf9\u4e8e\u6bcf\u4e2a\u4e2d\u95f4\u5c42\uff0c\u6a21\u578b\u90fd\u6709\u5355\u72ec\u5bf9\u5e94\u7684\u987a\u5e8f\u6a21\u5757\u3002 * \u6a21\u578b\u6240\u6709\u5c42\u90fd\u5305\u542b\u5728\u540c\u4e00\u4e2a\u987a\u5e8f\u6a21\u5757\u4e2d\u3002 * \u6a21\u578b\u4e2d\u95f4\u5c42\u662f\u975e\u987a\u5e8f\u6a21\u5757\u3002</p>"},{"location":"zh/how_to_guides/feature_extraction/#1","title":"\u573a\u666f1\uff1a\u6bcf\u4e2a\u4e2d\u95f4\u5c42\u6709\u5355\u72ec\u5bf9\u5e94\u7684\u987a\u5e8f\u6a21\u5757","text":"<p>\u573a\u666f1\u7684\u793a\u4f8b\u5982\u4e0b\u3002</p> <pre><code>class DummyNet1(nn.Cell):\n    def __init__(self, **kwargs):\n        super().__init__()\n\n        # \u6bcf\u4e2a\u5c42\u90fd\u6709\u5355\u72ec\u7684\u987a\u5e8f\u6a21\u5757\n        self.layer1 = Layer()\n        self.layer2 = Layer()\n        self.layer3 = Layer()\n        self.layer4 = Layer()\n\n    def forward_features(self, x):\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        return x\n\n    def construct(self, x):\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n</code></pre> <p>\u4e3a\u4e86\u5b9e\u73b0\u573a\u666f1\u7684\u7279\u5f81\u62bd\u53d6\uff0c\u6211\u4eec\u5728<code>__init__()</code>\u4e2d\u6dfb\u52a0\u4e86\u6210\u5458\u53d8\u91cf<code>self.feature_info</code>\u7528\u4e8e\u6ce8\u518c\u53ef\u63d0\u53d6\u7684\u4e2d\u95f4\u5c42\uff0c\u4f8b\u5982\uff0c</p> <pre><code>class DummyNet1(nn.Cell):\n    def __init__(self, **kwargs):\n        super().__init__()\n        self.feature_info = []   # \u7528\u4e8e\u4e2d\u95f4\u5c42\u6ce8\u518c\n\n        self.layer1 = Layer()\n        self.feature_info.append(dict(chs=, reduction=, name=\u201dlayer1\u201d))  # \u6ce8\u518c\u4e2d\u95f4\u5c42\n        self.layer2 = Layer()\n        self.feature_info.append(dict(chs=, reduction=, name=\u201dlayer2\u201d))  # \u6ce8\u518c\u4e2d\u95f4\u5c42\n        self.layer3 = Layer()\n        self.feature_info.append(dict(chs=, reduction=, name=\u201dlayer3\u201d))  # \u6ce8\u518c\u4e2d\u95f4\u5c42\n        self.layer4 = Layer()\n        self.feature_info.append(dict(chs=, reduction=, name=\u201dlayer4\u201d))  # \u6ce8\u518c\u4e2d\u95f4\u5c42\n</code></pre> <p>\u5982\u4e0a\u6240\u793a\uff0c<code>self.feature_info</code>\u662f\u4e00\u4e2a\u5305\u542b\u591a\u4e2a\u5b57\u5178\u7684\u5217\u8868\uff0c\u6bcf\u4e2a\u5b57\u5178\u5305\u542b\u4e09\u5bf9\u952e\u503c\u5bf9\u3002\u5177\u4f53\u800c\u8a00, <code>chs</code> \u8868\u793a\u751f\u6210\u7279\u5f81\u7684\u901a\u9053\u6570\uff0c<code>reduction</code>\u8868\u793a\u5f53\u524d\u5c42\u7684\u603b<code>stide</code>\u6570\uff0c<code>name</code>\u8868\u793a\u8be5\u4e2d\u95f4\u5c42\u5728\u6a21\u578b\u53c2\u6570\u4e2d\u7684\u540d\u79f0\u3002 \u6b64\u540d\u79f0\u53ef\u7528<code>get_parameters()</code>\u627e\u5230\u3002</p> <p>\u6709\u5173\u6b64\u573a\u666f\u7684\u771f\u5b9e\u793a\u4f8b\uff0c\u8bf7\u53c2\u9605ResNet\u3002</p>"},{"location":"zh/how_to_guides/feature_extraction/#2","title":"\u573a\u666f2\uff1a\u6240\u6709\u4e2d\u95f4\u5c42\u5728\u540c\u4e00\u4e2a\u987a\u5e8f\u6a21\u5757\u4e2d","text":"<p>\u5bf9\u4e8e\u67d0\u4e9b\u6a21\u578b\uff0c\u6240\u6709\u4e2d\u95f4\u5c42\u90fd\u88ab\u5305\u542b\u5728\u540c\u4e00\u4e2a\u987a\u5e8f\u6a21\u5757\u4e2d\u3002\u573a\u666f2\u7684\u793a\u4f8b\u5982\u4e0b\u3002</p> <pre><code>class DummyNet2(nn.Cell):\n    def __init__(self, **kwargs):\n        super().__init__()\n        layers = []\n\n        for i in range(4):\n            layers.append(Layer())\n\n        # \u6240\u6709\u4e2d\u95f4\u5c42\u5728\u540c\u4e00\u4e2a\u987a\u5e8f\u6a21\u5757\u4e2d\n        self.layers = nn.SequentialCell(layers)\n\n    def forward_features(self, x):\n        x = self.layers(x)\n        return x\n\n    def construct(self, x):\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n</code></pre> <p>\u4e3a\u4e86\u5b9e\u73b0\u573a\u666f2\u7684\u7279\u5f81\u62bd\u53d6\uff0c\u6211\u4eec\u540c\u6837\u9700\u8981\u5728<code>__init__()</code>\u4e2d\u6dfb\u52a0<code>self.feature_info</code>\uff0c\u540c\u65f6\u8fd8\u8981\u521b\u5efa\u4e00\u4e2a\u6210\u5458\u53d8\u91cf<code>self.flatten_sequential = True</code>\uff0c\u7528\u4e8e\u8868\u793a\u5728\u63d0\u53d6\u7279\u5f81\u4e4b\u524d\u9700\u8981\u5c06\u6b64\u6a21\u578b\u4e2d\u7684\u987a\u5e8f\u6a21\u5757\u5c55\u5f00\uff0c\u4f8b\u5982\uff0c</p> <pre><code>class DummyNet2(nn.Cell):\n    def __init__(self, **kwargs):\n        super().__init__()\n        self.feature_info = []  # \u7528\u4e8e\u4e2d\u95f4\u5c42\u6ce8\u518c\n        self.flatten_sequential = True  # \u8868\u793a\u9700\u8981\u5c55\u5f00\u987a\u5e8f\u6a21\u5757\n\n        layers = []\n\n        for i in range(4):\n            layers.append(Layer())\n            self.feature_info.append(dict(chs=, reduction=, name=f\u201dlayer{i}\u201d))  # \u6ce8\u518c\u4e2d\u95f4\u5c42\n\n        self.layers = nn.SequentialCell(layers)\n</code></pre> <p>\u8bf7\u6ce8\u610f\uff0c<code>__init__()</code>\u4e2d\u6a21\u5757\u5b9e\u4f8b\u5316\u7684\u987a\u5e8f\u975e\u5e38\u91cd\u8981\u3002\u987a\u5e8f\u5fc5\u987b\u4e0e\u5728<code>forward_features()</code>\u548c<code>construct()</code>\u4e2d\u8c03\u7528\u8fd9\u4e9b\u6a21\u5757\u7684\u987a\u5e8f\u4fdd\u6301\u4e00\u81f4\u3002\u6b64\u5916\uff0c\u53ea\u6709\u5728<code>forward_features()</code>\u548c<code>construct()</code>\u4e2d\u88ab\u8c03\u7528\u7684<code>nn.Cell</code>\u7c7b\u578b\u6a21\u5757\uff0c\u624d\u80fd\u4f5c\u4e3a\u6210\u5458\u53d8\u91cf\u88ab\u5b9e\u4f8b\u5316\u3002\u5426\u5219\uff0c\u7279\u5f81\u62bd\u53d6\u673a\u5236\u5c06\u65e0\u6cd5\u6b63\u5e38\u5de5\u4f5c\u3002</p> <p>\u6709\u5173\u6b64\u573a\u666f\u7684\u771f\u5b9e\u793a\u4f8b\uff0c\u8bf7\u53c2\u9605MobileNetV3\u3002</p>"},{"location":"zh/how_to_guides/feature_extraction/#3","title":"\u573a\u666f3\uff1a\u6a21\u578b\u4e2d\u95f4\u5c42\u4e3a\u975e\u987a\u5e8f\u6a21\u5757","text":"<p>\u6a21\u578b\u4e2d\u95f4\u5c42\u6709\u65f6\u662f\u975e\u987a\u5e8f\u6a21\u5757\u3002\u573a\u666f3\u7684\u793a\u4f8b\u5982\u4e0b\u3002</p> <pre><code>class DummyNet3(nn.Cell):\n    def __init__(self, **kwargs):\n        super().__init__()\n        layer1 = []\n\n        for i in range(3):\n                layer1.append(Layer())\n\n        # self.layer1 \u4e2d\u7684\u5c42\u4e0d\u662f\u987a\u5e8f\u6a21\u5757\n        self.layer1 = nn.CellList(layer1)\n\n        self.stage1 = Stage()\n\n        layer2 = []\n\n        for i in range(3):\n                layer2.append(Layer())\n\n        # self.layer2 \u4e2d\u7684\u5c42\u4e0d\u662f\u987a\u5e8f\u6a21\u5757\n        self.layer2 = nn.CellList(layer2)\n\n        self.stage2 = Stage()\n\n    def forward_features(self, x):\n        x_list = []\n\n        # \u4e2d\u95f4\u5c42\u662f\u5e76\u884c\u7684\uff0c\u800c\u4e0d\u662f\u987a\u5e8f\u7684\n        for i in range(3):\n                x_list.append(self.layer1[i](x))\n\n        x = self.stage1(x_list)\n\n        x_list = []\n\n        # \u4e2d\u95f4\u5c42\u662f\u5e76\u884c\u7684\uff0c\u800c\u4e0d\u662f\u987a\u5e8f\u7684\n        for i in range(3):\n                x_list.append(self.layer2[i](x))\n\n        x = self.stage2(x_list)\n\n        return x\n\n    def construct(self, x):\n        x = self.forward_features(x)\n        x = self.forward_head(x)\n        return x\n</code></pre> <p>\u4e3a\u4e86\u5b9e\u73b0\u573a\u666f3\u7684\u7279\u5f81\u62bd\u53d6\uff0c\u6211\u4eec\u9700\u8981\u9996\u5148\u901a\u8fc7\u7ee7\u627f\u539f\u59cb\u6a21\u578b\u7c7b\u521b\u5efa\u4e00\u4e2a\u65b0\u7684\u6a21\u578b\u7279\u5f81\u7c7b\u3002\u7136\u540e\uff0c\u5728<code>__init__()</code>\u4e2d\u6dfb\u52a0<code>self.feature_info</code>\u548c\u4e00\u4e2a\u6210\u5458\u53d8\u91cf<code>self.is_rewritten = True</code>\uff0c\u4ee5\u6307\u793a\u8be5\u7c7b\u662f\u4e3a\u7279\u5f81\u62bd\u53d6\u800c\u91cd\u5199\u7684\u3002\u6700\u540e\uff0c\u6211\u4eec\u4f7f\u7528\u7279\u5f81\u62bd\u53d6\u903b\u8f91\u91cd\u65b0\u5b9e\u73b0<code>forward_features()</code>\u548c<code>construct()</code>\u3002\u4ee5\u4e0b\u662f\u4e00\u4e2a\u793a\u4f8b\u3002</p> <pre><code>class DummyFeatureNet3(DummyNet3):\n    def __init__(self, **kwargs):\n        super(DummyFeatureNet3, self).__init__(**kwargs)\n        self.feature_info = []  # \u7528\u4e8e\u4e2d\u95f4\u5c42\u6ce8\u518c\n        self.is_rewritten = True  # \u8868\u793a\u4e3a\u7279\u5f81\u62bd\u53d6\u800c\u91cd\u5199\u7684\n\n        self.feature_info.append(dict(chs=, reduction=, name=\u201dstage1\u201d)  # \u6ce8\u518c\u4e2d\u95f4\u5c42\n        self.feature_info.append(dict(chs=, reduction=, name=\u201dstage2\u201d)  # \u6ce8\u518c\u4e2d\u95f4\u5c42\n\n    def forward_features(self, x):  # \u91cd\u65b0\u5b9e\u73b0\u7279\u5f81\u62bd\u53d6\u903b\u8f91\n        out = []\n        x_list = []\n\n        for i in range(3):\n                x_list.append(self.layer1[i](x))\n\n        x = self.stage1(x_list)\n        out.append(x)\n\n        x_list = []\n\n        for i in range(3):\n                x_list.append(self.layer2[i](x))\n\n        x = self.stage2(x_list)\n        out.append(x)\n\n        return out\n\n    def construct(self, x):\n        x = self.forward_features(x)\n        return x\n</code></pre> <p>\u6709\u5173\u6b64\u60c5\u51b5\u7684\u771f\u5b9e\u793a\u4f8b\uff0c\u8bf7\u53c2\u9605HRNet\u3002</p>"},{"location":"zh/how_to_guides/feature_extraction/#_3","title":"\u6dfb\u52a0\u6a21\u578b\u521b\u5efa\u7684\u5c01\u88c5\u51fd\u6570","text":"<p>\u5728\u6dfb\u52a0\u4e2d\u95f4\u5c42\u6ce8\u518c\u540e\uff0c\u6211\u4eec\u9700\u8981\u518d\u6dfb\u52a0\u4e00\u4e2a\u7b80\u5355\u7684\u6a21\u578b\u521b\u5efa\u5c01\u88c5\u51fd\u6570\uff0c\u4ee5\u4fbf\u5c06\u6a21\u578b\u5b9e\u4f8b\u4f20\u9012\u7ed9<code>build_model_with_cfg()</code>\u8fdb\u884c\u7279\u5f81\u62bd\u53d6\u3002</p> <p>\u901a\u5e38\uff0cMindCV\u4e2d\u6a21\u578b\u7684\u539f\u59cb\u521b\u5efa\u51fd\u6570\u5982\u4e0b\u6240\u793a\uff1a</p> <pre><code>@register_model\ndef dummynet18(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; DummyNet:\n    default_cfg = default_cfgs[\"dummynet18\"]\n    model = DummyNet(..., num_classes=num_classes, in_channels=in_channels, **kwargs)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n\n    return model\n</code></pre> <p>\u5bf9\u4e8e\u5c5e\u4e8e\u573a\u666f1\u548c\u573a\u666f2\u7684\u6a21\u578b\uff0c\u5728\u6dfb\u52a0\u7684\u6a21\u578b\u521b\u5efa\u5c01\u88c5\u51fd\u6570\u4e2d\uff0c\u53ea\u9700\u5c06\u539f\u6765\u7684\u53c2\u6570\u4f20\u7ed9<code>build_model_with_cfg()</code>\u5373\u53ef\uff0c\u4f8b\u5982\uff0c</p> <pre><code>def _create_dummynet(pretrained=False, **kwargs):\n    return build_model_with_cfg(DummyNet, pretrained, **kwargs)\n\n@register_model\ndef dummynet18(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; DummyNet:\n    default_cfg = default_cfgs[\"dummynet18\"]\n    model_args = dict(..., num_classes=num_classes, in_channels=in_channels, **kwargs)\n    return _create_dummynet(pretrained, **dict(default_cfg=default_cfg, **model_args))\n</code></pre> <p>\u5bf9\u4e8e\u5c5e\u4e8e\u573a\u666f3\u7684\u6a21\u578b\uff0c\u5c01\u88c5\u51fd\u6570\u5927\u81f4\u4e0e\u573a\u666f1\u548c\u573a\u666f2\u7c7b\u4f3c\u3002\u4e0d\u540c\u4e4b\u5904\u5728\u4e8e\u9700\u8981\u51b3\u5b9a\u5b9e\u4f8b\u5316\u54ea\u4e2a\u6a21\u578b\u7c7b\uff0c\u8fd9\u53d6\u51b3\u4e8e<code>feature_only</code>\u3002\u4f8b\u5982\uff0c</p> <pre><code>def _create_dummynet(pretrained=False, **kwargs):\n    if not kwargs.get(\"features_only\", False):\n        return build_model_with_cfg(DummyNet3, pretrained, **kwargs)\n    else:\n        return build_model_with_cfg(DummyFeatureNet3, pretrained, **kwargs)\n\n@register_model\ndef dummynet18(pretrained: bool = False, num_classes: int = 1000, in_channels=3, **kwargs) -&gt; DummyNet:\n    default_cfg = default_cfgs[\"dummynet18\"]\n    model_args = dict(..., num_classes=num_classes, in_channels=in_channels, **kwargs)\n    return _create_dummynet(pretrained, **dict(default_cfg=default_cfg, **model_args))\n</code></pre> <p>\u6709\u5173\u5b9e\u9645\u793a\u4f8b\uff0c\u8bf7\u53c2\u9605ResNet\u4e0eHRNet\u3002</p>"},{"location":"zh/how_to_guides/feature_extraction/#create_model","title":"<code>create_model()</code>\u4f20\u5165\u53c2\u6570","text":"<p>\u5b8c\u6210\u524d\u9762\u4e24\u4e2a\u6b65\u9aa4\u540e\uff0c\u6211\u4eec\u53ef\u5c06<code>feature_only=True</code>\u548c<code>out_indices</code>\u4f20\u9012\u7ed9<code>create_model()</code>\u6765\u521b\u5efa\u53ef\u8f93\u51fa\u6240\u9700\u7279\u5f81\u7684\u9aa8\u5e72\u7f51\u7edc\uff0c\u4f8b\u5982\uff0c</p> <pre><code>from mindcv.models import create_model\n\n\nbackbone = create_model(\n    model_name=\"dummynet18\",\n    features_only=True,  # \u8bbe\u7f6efeatures_only\u4e3a True\n    out_indices=[0, 1, 2],  # \u6307\u5b9a\u7279\u5f81\u62bd\u53d6\u7684\u4e2d\u95f4\u5c42\u5728feature_info\u4e2d\u7684\u7d22\u5f15\n)\n</code></pre> <p>\u6b64\u5916\uff0c\u5982\u679c\u6211\u4eec\u60f3\u8981\u5c06checkpoint\u52a0\u8f7d\u5230\u7528\u4e8e\u7279\u5f81\u62bd\u53d6\u7684\u9aa8\u5e72\u7f51\u7edc\u4e2d\uff0c\u5e76\u4e14\u6b64\u9aa8\u5e72\u7f51\u7edc\u5c5e\u4e8e\u573a\u666f2\uff0c\u90a3\u4e48\uff0c\u6211\u4eec\u8fd8\u9700\u8bbe\u7f6e<code>auto_mapping=True</code>\uff0c\u4f8b\u5982\uff0c</p> <pre><code>from mindcv.models import create_model\n\n\nbackbone = create_model(\n    model_name=\"dummynet18\",\n    checkpoint_path=\"/path/to/dummynet18.ckpt\",\n    auto_mapping=True,  # \u5f53\u4e3a\u573a\u666f2\u7684\u6a21\u578b\u52a0\u8f7dcheckpoint\u65f6\uff0c\u5c06auto_mapping\u8bbe\u4e3aTrue\n    features_only=True,  # \u8bbe\u7f6efeatures_only\u4e3a True\n    out_indices=[0, 1, 2],  # \u6307\u5b9a\u7279\u5f81\u62bd\u53d6\u7684\u4e2d\u95f4\u5c42\u5728feature_info\u4e2d\u7684\u7d22\u5f15\n)\n</code></pre> <p>\u606d\u559c\u60a8\uff01\u73b0\u5728\u60a8\u5df2\u7ecf\u5b66\u4f1a\u4e86\u5982\u4f55\u5bf9MindCV\u4e2d\u7684\u6a21\u578b\u8fdb\u884c\u591a\u5c3a\u5ea6\u7279\u5f81\u62bd\u53d6\u3002</p>"},{"location":"zh/how_to_guides/finetune_with_a_custom_dataset/","title":"\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\u7684\u6a21\u578b\u5fae\u8c03\u6307\u5357","text":"<p>\u672c\u6587\u6863\u63d0\u4f9b\u4e86\u4f7f\u7528MindCV\u5728\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\u4e0a\u5fae\u8c03\u7684\u53c2\u8003\u6d41\u7a0b\u4ee5\u53ca\u5728\u7ebf\u8bfb\u53d6\u6570\u636e\u96c6\u3001\u5206\u5c42\u8bbe\u7f6e\u5b66\u4e60\u7387\u3001\u51bb\u7ed3\u90e8\u5206\u7279\u5f81\u7f51\u7edc\u7b49\u5fae\u8c03\u6280\u5de7\u7684\u5b9e\u73b0\u65b9\u6cd5\uff0c\u4e3b\u8981\u4ee3\u7801\u5b9e\u73b0\u96c6\u6210\u5728./example/finetune.py\u4e2d\uff0c\u60a8\u53ef\u4ee5\u57fa\u4e8e\u6b64\u6559\u7a0b\u6839\u636e\u9700\u8981\u81ea\u884c\u6539\u52a8\u3002</p> <p>\u63a5\u4e0b\u6765\u5c06\u4ee5FGVC-Aircraft\u6570\u636e\u96c6\u4e3a\u4f8b\u5c55\u793a\u5982\u4f55\u5bf9\u9884\u8bad\u7ec3\u6a21\u578bmobilenet v3-small\u8fdb\u884c\u5fae\u8c03\u3002Fine-Grained Visual Classification of Aircraft\u662f\u5e38\u7528\u7684\u7ec6\u7c92\u5ea6\u56fe\u50cf\u5206\u7c7b\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5305\u542b 10000 \u5f20\u98de\u673a\u56fe\u7247\uff0c100 \u79cd\u4e0d\u540c\u7684\u98de\u673a\u578b\u53f7(variant)\uff0c\u5176\u4e2d\u6bcf\u79cd\u98de\u673a\u578b\u53f7\u5747\u6709 100 \u5f20\u56fe\u7247\u3002</p> <p>\u9996\u5148\u5c06\u4e0b\u8f7d\u540e\u7684\u6570\u636e\u96c6\u89e3\u538b\u5230./data\u6587\u4ef6\u5939\u4e0b\uff0cAircraft\u6570\u636e\u96c6\u7684\u76ee\u5f55\u4e3a\uff1a</p> <pre><code>aircraft\n\u2514\u2500\u2500 data\n    \u251c\u2500\u2500 images\n    \u2502   \u251c\u2500\u2500 image1.jpg\n    \u2502   \u251c\u2500\u2500 image2.jpg\n    \u2502   \u2514\u2500\u2500 ....\n    \u251c\u2500\u2500 images_variant_test.txt\n    \u251c\u2500\u2500 images_variant_trainval.txt\n    \u2514\u2500\u2500 ....\n</code></pre> <p>\u5176\u4e2dimages\u6587\u4ef6\u5939\u5305\u542b\u5168\u90e810000\u5f20\u56fe\u7247\uff0c\u6bcf\u5f20\u56fe\u7247\u6240\u5c5e\u7684\u98de\u673a\u578b\u53f7\u548c\u5b50\u96c6\u7531images_variant_*.txt\u6807\u6ce8\u3002\u5728\u6a21\u578b\u5fae\u8c03\u9636\u6bb5\uff0c\u8bad\u7ec3\u96c6\u4e00\u822c\u7531images_variant_trainval.txt \u786e\u5b9a\u3002\u7ecf\u8fc7\u62c6\u5206\u540e\uff0c\u8bad\u7ec3\u96c6\u5e94\u5f53\u5305\u542b6667\u5f20\u56fe\u7247\uff0c\u6d4b\u8bd5\u96c6\u5305\u542b3333\u5f20\u56fe\u7247\u3002</p>"},{"location":"zh/how_to_guides/finetune_with_a_custom_dataset/#_2","title":"\u6570\u636e\u9884\u5904\u7406","text":""},{"location":"zh/how_to_guides/finetune_with_a_custom_dataset/#_3","title":"\u8bfb\u53d6\u6570\u636e\u96c6","text":"<p>\u5bf9\u4e8e\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\u800c\u8a00\uff0c\u65e2\u53ef\u4ee5\u5148\u5728\u672c\u5730\u5c06\u6570\u636e\u6587\u4ef6\u76ee\u5f55\u6574\u7406\u6210\u4e0eImageNet\u7c7b\u4f3c\u7684\u6811\u72b6\u7ed3\u6784\uff0c\u518d\u4f7f\u7528<code>create_dataset</code>\u8bfb\u53d6\u6570\u636e\u96c6\uff08\u79bb\u7ebf\u65b9\u5f0f\uff0c\u4ec5\u9002\u7528\u4e8e\u5c0f\u578b\u6570\u636e\u96c6\uff09\uff0c\u53c8\u53ef\u4ee5\u76f4\u63a5\u5c06\u539f\u59cb\u6570\u636e\u96c6\u8bfb\u53d6\u6210\u53ef\u8fed\u4ee3/\u53ef\u6620\u5c04\u5bf9\u8c61\uff0c\u66ff\u4ee3\u6587\u4ef6\u62c6\u5206\u4e0e<code>create_dataset</code>\u6b65\u9aa4\uff08\u5728\u7ebf\u65b9\u5f0f\uff09\u3002</p>"},{"location":"zh/how_to_guides/finetune_with_a_custom_dataset/#_4","title":"\u79bb\u7ebf\u65b9\u5f0f","text":"<p>MindCV\u7684<code>create_dataset</code>\u63a5\u53e3\u4f7f\u7528<code>mindspore.dataset.ImageFolderDataset</code>\u51fd\u6570\u6784\u5efa\u6570\u636e\u5bf9\u8c61\uff0c\u540c\u4e00\u4e2a\u6587\u4ef6\u5939\u5185\u7684\u6240\u6709\u56fe\u7247\u5c06\u4f1a\u6839\u636e\u6587\u4ef6\u5939\u540d\u5b57\u88ab\u5206\u914d\u76f8\u540c\u7684\u6807\u7b7e\u3002\u56e0\u6b64\uff0c\u4f7f\u7528\u8be5\u6d41\u7a0b\u7684\u524d\u63d0\u6761\u4ef6\u662f\u6e90\u6570\u636e\u96c6\u7684\u6587\u4ef6\u76ee\u5f55\u5e94\u5f53\u9075\u5faa\u5982\u4e0b\u6811\u72b6\u7ed3\u6784\uff1a</p> <pre><code>DATASET_NAME\n    \u251c\u2500\u2500 split1(e.g. train)/\n    \u2502  \u251c\u2500\u2500 class1/\n    \u2502  \u2502   \u251c\u2500\u2500 000001.jpg\n    \u2502  \u2502   \u251c\u2500\u2500 000002.jpg\n    \u2502  \u2502   \u2514\u2500\u2500 ....\n    \u2502  \u2514\u2500\u2500 class2/\n    \u2502      \u251c\u2500\u2500 000001.jpg\n    \u2502      \u251c\u2500\u2500 000002.jpg\n    \u2502      \u2514\u2500\u2500 ....\n    \u2514\u2500\u2500 split2/\n       \u251c\u2500\u2500 class1/\n       \u2502   \u251c\u2500\u2500 000001.jpg\n       \u2502   \u251c\u2500\u2500 000002.jpg\n       \u2502   \u2514\u2500\u2500 ....\n       \u2514\u2500\u2500 class2/\n           \u251c\u2500\u2500 000001.jpg\n           \u251c\u2500\u2500 000002.jpg\n           \u2514\u2500\u2500 ....\n</code></pre> <p>\u63a5\u4e0b\u6765\u4ee5\u8bf4\u660e\u6587\u4ef6./aircraft/data/images_variant_trainval.txt \u4e3a\u4f8b\uff0c\u5728\u672c\u5730\u751f\u6210\u6ee1\u8db3\u524d\u8ff0\u6811\u72b6\u7ed3\u6784\u7684\u8bad\u7ec3\u96c6\u6587\u4ef6 ./aircraft/data/images/trainval/\u3002</p> <pre><code>\"\"\" Extract images and generate ImageNet-style dataset directory \"\"\"\nimport os\nimport shutil\n\n\n# only for Aircraft dataset but not a general one\ndef extract_images(images_path, subset_name, annotation_file_path, copy=True):\n    # read the annotation file to get the label of each image\n    def annotations(annotation_file_path):\n        image_label = {}\n        with open(annotation_file_path, \"r\") as f:\n            lines = f.readlines()\n            for line in lines:\n                label = \" \".join(line.split(\" \")[1:]).replace(\"\\n\", \"\").replace(\"/\", \"_\")\n                if label not in image_label.keys():\n                    image_label[label] = []\n                    image_label[label].append(line.split(\" \")[0])\n                else:\n                    image_label[label].append(line.split(\" \")[0])\n        return image_label\n\n    # make a new folder for subset\n    subset_path = images_path + subset_name\n    os.mkdir(subset_path)\n\n    # extract and copy/move images to the new folder\n    image_label = annotations(annotation_file_path)\n    for label in image_label.keys():\n        label_folder = subset_path + \"/\" + label\n        os.mkdir(label_folder)\n        for image in image_label[label]:\n            image_name = image + \".jpg\"\n            if copy:\n                shutil.copy(images_path + image_name, label_folder + image_name)\n            else:\n                shutil.move(images_path + image_name, label_folder)\n\n\n# take train set of aircraft dataset as an example\nimages_path = \"./aircraft/data/images/\"\nsubset_name = \"trainval\"\nannotation_file_path = \"./aircraft/data/images_variant_trainval.txt\"\nextract_images(images_path, subset_name, annotation_file_path)\n</code></pre> <p>\u6d4b\u8bd5\u96c6\u7684\u62c6\u5206\u65b9\u5f0f\u4e0e\u8bad\u7ec3\u96c6\u4e00\u81f4\uff0c\u6574\u7406\u5b8c\u6210\u7684Aircraft\u6570\u636e\u96c6\u6587\u4ef6\u7ed3\u6784\u5e94\u4e3a\uff1a</p> <pre><code>aircraft\n\u2514\u2500\u2500 data\n    \u2514\u2500\u2500 images\n        \u251c\u2500\u2500 trainval\n        \u2502   \u251c\u2500\u2500 707-320\n        \u2502   \u2502   \u251c\u2500\u2500 0056978.jpg\n        \u2502   \u2502   \u2514\u2500\u2500 ....\n        \u2502   \u251c\u2500\u2500 727-200\n        \u2502   \u2502   \u251c\u2500\u2500 0048341.jpg\n        \u2502   \u2502   \u2514\u2500\u2500 ....\n        \u2502   \u2514\u2500\u2500 ....\n        \u2514\u2500\u2500 test\n            \u251c\u2500\u2500 707-320\n            \u2502   \u251c\u2500\u2500 0062765.jpg\n            \u2502   \u2514\u2500\u2500 ....\n            \u251c\u2500\u2500 727-200\n            \u2502   \u251c\u2500\u2500 0061581.jpg\n            \u2502   \u2514\u2500\u2500 ....\n            \u2514\u2500\u2500 ....\n</code></pre> <p>\u7531\u4e8e\u6a21\u578b\u5fae\u8c03\u6587\u4ef6./example/finetune.py\u4e2d\u96c6\u6210\u4e86<code>create_dataset</code>-&gt;<code>create_transforms</code>-&gt;<code>create_loader</code>-&gt;<code>create_model</code>-&gt;...\u7b49\u6240\u6709\u4ece\u9884\u5904\u7406\u5230\u5efa\u7acb\u3001\u9a8c\u8bc1\u6a21\u578b\u7684\u8bad\u7ec3\u6d41\u7a0b\uff0c\u4f7f\u7528\u79bb\u7ebf\u65b9\u5f0f\u6574\u7406\u5b8c\u6587\u4ef6\u76ee\u5f55\u7ed3\u6784\u7684\u6570\u636e\u96c6\u53ef\u4ee5**\u76f4\u63a5\u901a\u8fc7\u8fd0\u884c<code>python ./example/finetune.py</code>\u547d\u4ee4\u5b8c\u6210\u540e\u7eed\u8bfb\u53d6\u6570\u636e\u4e0e\u8bad\u7ec3\u6a21\u578b**\u8fd9\u4e00\u6574\u5957\u64cd\u4f5c\u3002\u5bf9\u4e8e\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\u800c\u8a00\uff0c\u8fd8\u9700\u6ce8\u610f\u63d0\u524d\u5c06\u914d\u7f6e\u6587\u4ef6\u4e2d\u7684<code>dataset</code>\u53c2\u6570\u8bbe\u7f6e\u4e3a\u7a7a\u5b57\u7b26\u4e32<code>\"\"</code>\u3002</p>"},{"location":"zh/how_to_guides/finetune_with_a_custom_dataset/#_5","title":"\u5728\u7ebf\u65b9\u5f0f","text":"<p>\u79bb\u7ebf\u65b9\u5f0f\u7684\u6570\u636e\u8bfb\u53d6\u4f1a\u5728\u672c\u5730\u5360\u7528\u989d\u5916\u7684\u78c1\u76d8\u7a7a\u95f4\u5b58\u50a8\u65b0\u751f\u6210\u7684\u6570\u636e\u6587\u4ef6\uff0c\u56e0\u6b64\u5728\u672c\u5730\u5b58\u50a8\u7a7a\u95f4\u4e0d\u8db3\u6216\u65e0\u6cd5\u5c06\u6570\u636e\u5907\u4efd\u5230\u672c\u5730\u7b49\u5176\u4ed6\u7279\u6b8a\u60c5\u51b5\u4e0b\uff0c\u65e0\u6cd5\u76f4\u63a5\u4f7f\u7528<code>create_dataset</code>\u63a5\u53e3\u8bfb\u53d6\u672c\u5730\u6570\u636e\u6587\u4ef6\u65f6\uff0c\u53ef\u4ee5\u91c7\u7528\u5728\u7ebf\u65b9\u5f0f\u81ea\u884c\u7f16\u5199\u51fd\u6570\u8bfb\u53d6\u6570\u636e\u96c6\u3002</p> <p>\u4ee5\u751f\u6210\u50a8\u5b58\u8bad\u7ec3\u96c6\u56fe\u7247\u548c\u7d22\u5f15\u5230\u56fe\u7247\u6837\u672c\u6620\u5c04\u7684\u53ef\u968f\u673a\u8bbf\u95ee\u6570\u636e\u96c6\u4e3a\u4f8b\uff1a</p> <ul> <li> <p>\u9996\u5148\u5b9a\u4e49\u4e00\u4e2a\u8bfb\u53d6\u539f\u59cb\u6570\u636e\u5e76\u5c06\u5176\u8f6c\u6362\u6210\u53ef\u968f\u673a\u8bbf\u95ee\u7684\u6570\u636e\u96c6\u5bf9\u8c61<code>ImageClsDataset</code>\uff1a</p> <ul> <li> <p>\u5728\u8be5\u7c7b\u7684\u521d\u59cb\u5316\u51fd\u6570<code>__init__()</code>\u4e2d\uff0c\u4ee5./aircraft/data/images_variant_trainval.txt\u4e3a\u4f8b\u7684\u6807\u6ce8\u6587\u4ef6\u7684\u6587\u4ef6\u8def\u5f84\u5c06\u88ab\u5f53\u505a\u8f93\u5165\uff0c\u7528\u4e8e\u751f\u6210\u50a8\u5b58\u56fe\u7247\u4e0e\u6807\u7b7e\u4e00\u4e00\u5bf9\u5e94\u5173\u7cfb\u7684\u5b57\u5178<code>self.annotation</code>\uff1b</p> </li> <li> <p>\u7531\u4e8e\u5728<code>create_loader</code>\u4e2d\u5c06\u4f1a\u5bf9\u6b64\u5bf9\u8c61\u8fdb\u884cmap\u64cd\u4f5c\uff0c\u800c\u8be5\u64cd\u4f5c\u4e0d\u652f\u6301\u5b57\u7b26\u4e32\u683c\u5f0f\u7684\u6807\u7b7e\uff0c\u56e0\u6b64\u8fd8\u9700\u8981\u751f\u6210<code>self.label2id</code>\u5e76\u5c06<code>self.annotation</code>\u4e2d\u5b57\u7b26\u4e32\u683c\u5f0f\u7684\u6807\u7b7e\u8f6c\u6362\u6210\u6574\u6570\u683c\u5f0f\uff1b</p> </li> <li> <p>\u6839\u636e<code>self.annotation</code>\u4e2d\u50a8\u5b58\u7684\u4fe1\u606f\uff0c\u4ece\u6587\u4ef6\u5939./aircraft/data/images/\u4e2d\u5c06\u8bad\u7ec3\u96c6\u56fe\u7247\u8bfb\u53d6\u6210\u4e00\u7ef4\u6570\u7ec4\u5f62\u5f0f\uff08\u7531\u4e8e<code>create_loader</code>\u4e2dmap\u64cd\u4f5c\u9650\u5236\uff0c\u6b64\u5904\u56fe\u7247\u6570\u636e\u5fc5\u987b\u88ab\u8bfb\u53d6\u4e3a\u4e00\u7ef4\u683c\u5f0f\uff09\uff0c\u5e76\u5c06\u56fe\u7247\u4fe1\u606f\u4e0e\u6807\u7b7e\u5206\u522b\u5b58\u653e\u5230<code>self._data</code>\u4e0e<code>self._label</code>\u4e2d\uff1b</p> </li> <li> <p>\u63a5\u4e0b\u6765\u4f7f\u7528<code>__getitem__</code>\u65b9\u6cd5\u6784\u9020\u53ef\u968f\u673a\u8bbf\u95ee\u7684\u6570\u636e\u96c6\u5bf9\u8c61\u3002</p> </li> </ul> </li> </ul> <p>-   \u6784\u9020\u5b8c<code>ImageClsDataset</code>\u7c7b\u4e4b\u540e\uff0c\u5411\u5176\u4f20\u5165\u6807\u6ce8\u6587\u4ef6\u7684\u8def\u5f84\u4ee5\u5b9e\u4f8b\u5316\u8be5\u7c7b\uff0c\u5e76\u901a\u8fc7<code>mindspore.dataset.GeneratorDataset</code>\u51fd\u6570\u5c06\u8be5\u53ef\u6620\u5c04\u5bf9\u8c61\u52a0\u8f7d\u6210\u6570\u636e\u96c6\u5373\u53ef\uff0c\u6ce8\u610f\u8be5\u51fd\u6570\u7684\u53c2\u6570<code>column_names</code>\u5fc5\u987b\u88ab\u8bbe\u7f6e\u4e3a[\"image\", \"label\"]\u4ee5\u4fbf\u540e\u7eed\u5176\u4ed6\u63a5\u53e3\u8bfb\u53d6\u6570\u636e\uff0c\u6b64\u65f6\u5f97\u5230\u7684<code>dataset_train</code>\u5e94\u5f53\u4e0e\u901a\u8fc7<code>create_dataset</code>\u8bfb\u53d6\u7684\u8bad\u7ec3\u96c6\u5b8c\u5168\u4e00\u81f4\u3002</p> <pre><code>import numpy as np\n\nfrom mindspore.dataset import GeneratorDataset\n\n\nclass ImageClsDataset:\n    def __init__(self, annotation_dir, images_dir):\n        # Read annotations\n        self.annotation = {}\n        with open(annotation_dir, \"r\") as f:\n            lines = f.readlines()\n            for line in lines:\n                image_label = line.replace(\"\\n\", \"\").replace(\"/\", \"_\").split(\" \")\n                image = image_label[0] + \".jpg\"\n                label = \" \".join(image_label[1:])\n                self.annotation[image] = label\n\n        # Transfer string-type label to int-type label\n        self.label2id = {}\n        labels = sorted(list(set(self.annotation.values())))\n        for i in labels:\n            self.label2id[i] = labels.index(i)\n\n        for image, label in self.annotation.items():\n            self.annotation[image] = self.label2id[label]\n\n        # Read image-labels as mappable object\n        label2images = {key: [] for key in self.label2id.values()}\n        for image, label in self.annotation.items():\n            read_image = np.fromfile(images_dir + image, dtype=np.uint8)\n            label2images[label].append(read_image)\n\n        self._data = sum(list(label2images.values()), [])\n        self._label = sum([[i] * len(label2images[i]) for i in label2images.keys()], [])\n\n    # make class ImageClsDataset a mappable object\n    def __getitem__(self, index):\n        return self._data[index], self._label[index]\n\n    def __len__(self):\n        return len(self._data)\n\n\n# take aircraft dataset as an example\nannotation_dir = \"./aircraft/data/images_variant_trainval.txt\"\nimages_dir = \"./aircraft/data/images/\"\ndataset = ImageClsDataset(annotation_dir, images_dir)\ndataset_train = GeneratorDataset(source=dataset, column_names=[\"image\", \"label\"], shuffle=True)\n</code></pre> <p>\u4e0e\u79bb\u7ebf\u65b9\u5f0f\u8bfb\u53d6\u6570\u636e\u96c6\u76f8\u6bd4\uff0c\u5728\u7ebf\u8bfb\u53d6\u65b9\u5f0f\u7701\u7565\u4e86\u5728\u672c\u5730\u62c6\u5206\u6570\u636e\u6587\u4ef6\u5e76\u7528<code>create_dataset</code>\u63a5\u53e3\u8bfb\u53d6\u672c\u5730\u6587\u4ef6\u7684\u6b65\u9aa4\uff0c\u56e0\u6b64\u5728\u540e\u7eed\u7684\u8bad\u7ec3\u4e2d\uff0c\u53ea\u9700**\u5c06finetune.py\u4e2d\u4f7f\u7528<code>create_dataset</code>\u63a5\u53e3\u7684\u90e8\u5206\u66ff\u6362\u6210\u4e0a\u8ff0\u4ee3\u7801**\uff0c\u5c31\u53ef\u4ee5\u4e0e\u79bb\u7ebf\u65b9\u5f0f\u4e00\u6837\uff0c\u76f4\u63a5\u8fd0\u884cfinetune.py\u5f00\u59cb\u8bad\u7ec3\u3002</p>"},{"location":"zh/how_to_guides/finetune_with_a_custom_dataset/#_6","title":"\u6570\u636e\u589e\u5f3a\u4e0e\u5206\u6279","text":"<p>MindCV\u4f7f\u7528<code>create_loader</code>\u51fd\u6570\u5bf9\u4e0a\u4e00\u7ae0\u8282\u8bfb\u53d6\u7684\u6570\u636e\u96c6\u8fdb\u884c\u56fe\u50cf\u589e\u5f3a\u4e0e\u5206\u6279\u5904\u7406\uff0c\u56fe\u50cf\u589e\u5f3a\u7b56\u7565\u901a\u8fc7<code>create_transforms</code>\u51fd\u6570\u4e8b\u5148\u5b9a\u4e49\uff0c\u5206\u6279\u5904\u7406\u64cd\u4f5c\u901a\u8fc7<code>create_loader</code>\u51fd\u6570\u4e2d\u7684\u53c2\u6570<code>batch_size</code>\u5b9a\u4e49\uff0c\u4ee5\u4e0a\u6d89\u53ca\u5230\u7684**\u6240\u6709\u8d85\u53c2\u6570\u5747\u53ef\u4ee5\u901a\u8fc7\u6a21\u578b\u914d\u7f6e\u6587\u4ef6\u4f20\u9012**\uff0c\u8d85\u53c2\u6570\u5177\u4f53\u4f7f\u7528\u65b9\u6cd5\u89c1API\u8bf4\u660e\u3002</p> <p>\u5bf9\u4e8e\u89c4\u6a21\u8f83\u5c0f\u7684\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\uff0c\u5efa\u8bae\u53ef\u4ee5\u5728\u8fd9\u4e00\u90e8\u5206\u5bf9\u8bad\u7ec3\u96c6\u505a\u989d\u5916\u7684\u6570\u636e\u589e\u5f3a\u5904\u7406\uff0c\u4ee5\u589e\u5f3a\u6a21\u578b\u7684\u6cdb\u5316\u6027\uff0c\u9632\u6b62\u8fc7\u62df\u5408\u3002\u5bf9\u4e8e\u7ec6\u7c92\u5ea6\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u7684\u6570\u636e\u96c6\uff0c\u6bd4\u5982\u672c\u6587\u4e2d\u7684Aircraft\u6570\u636e\u96c6\uff0c\u7531\u4e8e\u6570\u636e\u7c7b\u5185\u65b9\u5dee\u8f83\u5927\u53ef\u80fd\u5bfc\u81f4\u5206\u7c7b\u6548\u679c\u8f83\u5dee\uff0c\u8fd8\u53ef\u4ee5\u901a\u8fc7\u8c03\u6574\u8d85\u53c2\u6570<code>image_resize</code>\u9002\u5f53\u589e\u5927\u56fe\u7247\u5c3a\u5bf8\uff08\u5982\uff1a448\u3001512\u3001600\u7b49\u7b49\uff09\u3002</p>"},{"location":"zh/how_to_guides/finetune_with_a_custom_dataset/#_7","title":"\u6a21\u578b\u5fae\u8c03","text":"<p>\u53c2\u8003Stanford University CS231n\uff0c\u6574\u4f53\u5fae\u8c03\u3001\u51bb\u7ed3\u7279\u5f81\u7f51\u7edc\u5fae\u8c03\u3001\u4e0e**\u5206\u5c42\u8bbe\u7f6e\u5b66\u4e60\u7387\u5fae\u8c03**\u662f\u5e38\u7528\u7684\u5fae\u8c03\u6a21\u5f0f\u3002\u6a21\u578b\u7684\u6574\u4f53\u5fae\u8c03\u4f7f\u7528\u9884\u8bad\u7ec3\u6743\u91cd\u521d\u59cb\u5316\u76ee\u6807\u6a21\u578b\u7684\u53c2\u6570\u5e76\u5728\u6b64\u57fa\u7840\u4e0a\u9488\u5bf9\u65b0\u6570\u636e\u96c6\u7ee7\u7eed\u8bad\u7ec3\u3001\u66f4\u65b0\u6240\u6709\u53c2\u6570\uff0c\u56e0\u6b64\u8ba1\u7b97\u91cf\u8f83\u5927\uff0c\u8017\u65f6\u8f83\u957f\u4f46\u4e00\u822c\u7cbe\u5ea6\u8f83\u9ad8\uff1b\u51bb\u7ed3\u7279\u5f81\u7f51\u7edc\u5219\u5206\u4e3a\u51bb\u7ed3\u6240\u6709\u7279\u5f81\u7f51\u7edc\u4e0e\u51bb\u7ed3\u90e8\u5206\u7279\u5f81\u7f51\u7edc\u4e24\u79cd\uff0c\u524d\u8005\u5c06\u9884\u8bad\u7ec3\u6a21\u578b\u4f5c\u4e3a\u7279\u5f81\u63d0\u53d6\u5668\uff0c\u4ec5\u66f4\u65b0\u5168\u8fde\u63a5\u5c42\u53c2\u6570\uff0c\u8017\u65f6\u77ed\u4f46\u7cbe\u5ea6\u4f4e\uff0c\u540e\u8005\u4e00\u822c\u56fa\u5b9a\u5b66\u4e60\u57fa\u7840\u7279\u5f81\u7684\u6d45\u5c42\u53c2\u6570\uff0c\u53ea\u66f4\u65b0\u5b66\u4e60\u7cbe\u7ec6\u7279\u5f81\u7684\u6df1\u5c42\u7f51\u7edc\u53c2\u6570\u4e0e\u5168\u8fde\u63a5\u5c42\u53c2\u6570\uff1b\u5206\u5c42\u8bbe\u7f6e\u5b66\u4e60\u7387\u4e0e\u4e4b\u76f8\u4f3c\uff0c\u4f46\u662f\u66f4\u52a0\u7cbe\u7ec6\u5730\u6307\u5b9a\u4e86\u7f51\u7edc\u5185\u90e8\u67d0\u4e9b\u7279\u5b9a\u5c42\u5728\u8bad\u7ec3\u4e2d\u66f4\u65b0\u53c2\u6570\u6240\u4f7f\u7528\u7684\u5b66\u4e60\u7387\u3002</p> <p>\u5bf9\u4e8e\u5b9e\u9645\u5fae\u8c03\u8bad\u7ec3\u4e2d\u6240\u4f7f\u7528\u7684\u7684\u8d85\u53c2\u6570\u914d\u7f6e\uff0c\u53ef\u4ee5\u53c2\u8003./configs\u4e2d\u57fa\u4e8eImageNet-1k\u6570\u636e\u96c6\u9884\u8bad\u7ec3\u7684\u914d\u7f6e\u6587\u4ef6\u3002\u6ce8\u610f\u5bf9\u6a21\u578b\u5fae\u8c03\u800c\u8a00\uff0c\u5e94\u4e8b\u5148\u5c06\u8d85\u53c2\u6570<code>pretrained</code>\u8bbe\u7f6e\u4e3a<code>True</code>\u4ee5\u52a0\u8f7d\u9884\u8bad\u7ec3\u6743\u91cd\uff0c\u5c06<code>num_classes</code>\u8bbe\u7f6e\u4e3a\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\u7684\u6807\u7b7e\u4e2a\u6570\uff08\u6bd4\u5982Aircfrat\u6570\u636e\u96c6\u662f100\uff09\uff0c\u8fd8\u53ef\u4ee5\u57fa\u4e8e\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\u89c4\u6a21\uff0c\u9002\u5f53\u8c03\u5c0f<code>batch_size</code>\u4e0e<code>epoch_size</code>\u3002\u6b64\u5916\uff0c\u7531\u4e8e\u9884\u8bad\u7ec3\u6743\u91cd\u4e2d\u5df2\u7ecf\u5305\u542b\u4e86\u8bb8\u591a\u8bc6\u522b\u56fe\u50cf\u7684\u521d\u59cb\u4fe1\u606f\uff0c\u4e3a\u4e86\u4e0d\u8fc7\u5206\u7834\u574f\u8fd9\u4e9b\u4fe1\u606f\uff0c\u8fd8\u9700\u5c06\u5b66\u4e60\u7387<code>lr</code>\u8c03\u5c0f\uff0c\u5efa\u8bae\u81f3\u591a\u4ece\u9884\u8bad\u7ec3\u5b66\u4e60\u7387\u7684\u5341\u5206\u4e4b\u4e00\u62160.0001\u5f00\u59cb\u8bad\u7ec3\u3001\u8c03\u53c2\u3002\u8fd9\u4e9b\u53c2\u6570\u90fd\u53ef\u4ee5\u5728\u914d\u7f6e\u6587\u4ef6\u4e2d\u4fee\u6539\uff0c\u4e5f\u53ef\u4ee5\u5982\u4e0b\u6240\u793a\u5728shell\u547d\u4ee4\u4e2d\u6dfb\u52a0\uff0c\u8bad\u7ec3\u7ed3\u679c\u53ef\u5728./ckpt/results.txt\u6587\u4ef6\u4e2d\u67e5\u770b\u3002</p> <pre><code>python .examples/finetune/finetune.py --config=./configs/mobilenetv3/mobilnet_v3_small_ascend.yaml --data_dir=./aircraft/data --pretrained=True\n</code></pre> <p>\u672c\u6587\u5728\u57fa\u4e8eAircraft\u6570\u636e\u96c6\u5bf9mobilenet v3-small\u5fae\u8c03\u65f6\u4e3b\u8981\u5bf9\u8d85\u53c2\u6570\u505a\u4e86\u5982\u4e0b\u6539\u52a8\uff1a</p> Hyper-parameter Pretrain Fine-tune dataset \"imagenet\" \"\" batch_size 75 8 image_resize 224 600 auto_augment - \"randaug-m7-mstd0.5\" num_classes 1000 100 pretrained False True epoch_size 470 50 lr 0.77 0.002"},{"location":"zh/how_to_guides/finetune_with_a_custom_dataset/#_8","title":"\u6574\u4f53\u5fae\u8c03","text":"<p>\u7531\u4e8e\u6574\u4f53\u5fae\u8c03\u7684\u8bad\u7ec3\u6d41\u7a0b\u4e0e\u4ece\u5934\u8bad\u7ec3\u4e00\u81f4\uff0c\u56e0\u6b64\u53ea\u9700\u901a\u8fc7**\u8fd0\u884cfinetune.py\u542f\u52a8\u8bad\u7ec3**\u5e76\u8ddf\u4ece\u5934\u8bad\u7ec3\u4e00\u6837\u8c03\u53c2\u5373\u53ef\u3002</p>"},{"location":"zh/how_to_guides/finetune_with_a_custom_dataset/#_9","title":"\u51bb\u7ed3\u7279\u5f81\u7f51\u7edc","text":""},{"location":"zh/how_to_guides/finetune_with_a_custom_dataset/#_10","title":"\u51bb\u7ed3\u6240\u6709\u7279\u5f81\u7f51\u7edc","text":"<p>\u6211\u4eec\u901a\u8fc7\u5bf9\u9664\u5168\u8fde\u63a5\u5c42\u5916\u7684\u6240\u6709\u53c2\u6570\u8bbe\u7f6e<code>requires_grad=False</code>\u6765\u9632\u6b62\u5176\u53c2\u6570\u66f4\u65b0\u3002\u5728finetune.py\u4e2d\uff0c\u53ea\u9700\u5728\u521b\u5efa\u6a21\u578b<code>create_model</code>\u4e4b\u540e\u52a0\u5165\u5982\u4e0b\u4ee3\u7801\u5373\u53ef\u5b9e\u73b0\uff1a</p> <pre><code>from mindcv.models.registry import _model_pretrained_cfgs\n\n# ...create_model()\n\n# number of parameters to be updated\nnum_params = 2\n\n# read names of parameters in FC layer\nclassifier_names = [_model_pretrained_cfgs[args.model][\"classifier\"] + \".weight\",\n                    _model_pretrained_cfgs[args.model][\"classifier\"] + \".bias\"]\n\n# prevent parameters in network(except the classifier) from updating\nfor param in network.trainable_params():\n    if param.name not in classifier_names:\n        param.requires_grad = False\n</code></pre>"},{"location":"zh/how_to_guides/finetune_with_a_custom_dataset/#_11","title":"\u51bb\u7ed3\u90e8\u5206\u7279\u5f81\u7f51\u7edc","text":"<p>\u4e3a\u4e86\u5e73\u8861\u5fae\u8c03\u8bad\u7ec3\u7684\u901f\u5ea6\u548c\u7cbe\u5ea6\uff0c\u6211\u4eec\u8fd8\u53ef\u4ee5\u56fa\u5b9a\u90e8\u5206\u76ee\u6807\u7f51\u7edc\u53c2\u6570\uff0c\u6709\u9488\u5bf9\u6027\u5730\u8bad\u7ec3\u7f51\u7edc\u4e2d\u7684\u6df1\u5c42\u53c2\u6570\u3002\u5b9e\u73b0\u8fd9\u4e00\u64cd\u4f5c\u53ea\u9700\u8981\u63d0\u53d6\u51fa\u8981\u51bb\u7ed3\u7684\u5c42\u4e2d\u7684\u53c2\u6570\u540d\u79f0\uff0c\u5e76\u5728\u4e0a\u8ff0\u51bb\u7ed3\u6240\u6709\u7279\u5f81\u7f51\u7edc\u7684\u4ee3\u7801\u57fa\u7840\u4e0a\u7a0d\u4f5c\u4fee\u6539\u5373\u53ef\u3002\u901a\u8fc7\u6253\u5370<code>create_model</code>\u7684\u7ed3\u679c\u2014\u2014<code>network</code>\u53ef\u77e5\uff0cMindCV\u4e2d\u5bf9mobilenet v3-small\u7684\u6bcf\u5c42\u7f51\u7edc\u547d\u540d\u4e3a<code>\"features.*\"</code>\uff0c\u5047\u8bbe\u6211\u4eec\u4ec5\u51bb\u7ed3\u7f51\u7edc\u524d7\u5c42\uff0c\u5728finetune.py\u4e2d\u521b\u5efa\u6a21\u578b<code>create_model</code>\u540e\u52a0\u5165\u5982\u4e0b\u4ee3\u7801\u5373\u53ef\uff1a</p> <pre><code># ...create_model()\n\n# read names of network layers\nfreeze_layer=[\"features.\"+str(i) for i in range(7)]\n\n# prevent parameters in first 7 layers of network from updating\nfor param in network.trainable_params():\n    for layer in freeze_layer:\n        if layer in param.name:\n            param.requires_grad = False\n</code></pre>"},{"location":"zh/how_to_guides/finetune_with_a_custom_dataset/#_12","title":"\u5206\u5c42\u8bbe\u7f6e\u5b66\u4e60\u7387","text":"<p>\u4e3a\u4e86\u8fdb\u4e00\u6b65\u63d0\u5347\u5fae\u8c03\u7f51\u7edc\u7684\u8bad\u7ec3\u6548\u679c\uff0c\u8fd8\u53ef\u4ee5\u5206\u5c42\u8bbe\u7f6e\u8bad\u7ec3\u4e2d\u7684\u5b66\u4e60\u7387\u3002\u8fd9\u662f\u7531\u4e8e\u6d45\u5c42\u7f51\u7edc\u4e00\u822c\u662f\u8bc6\u522b\u901a\u7528\u7684\u8f6e\u5ed3\u7279\u5f81\uff0c\u6240\u4ee5\u5373\u4fbf\u91cd\u65b0\u66f4\u65b0\u8be5\u90e8\u5206\u53c2\u6570\uff0c\u5b66\u4e60\u7387\u4e5f\u5e94\u8be5\u88ab\u8bbe\u7f6e\u5f97\u6bd4\u8f83\u5c0f\uff1b\u6df1\u5c42\u90e8\u5206\u4e00\u822c\u8bc6\u522b\u7269\u4f53\u7cbe\u7ec6\u7684\u4e2a\u6027\u7279\u5f81\uff0c\u5b66\u4e60\u7387\u4e5f\u56e0\u6b64\u53ef\u4ee5\u8bbe\u7f6e\u5f97\u6bd4\u8f83\u5927\uff1b\u800c\u76f8\u5bf9\u4e8e\u9700\u8981\u5c3d\u91cf\u4fdd\u7559\u9884\u8bad\u7ec3\u4fe1\u606f\u7684\u7279\u5f81\u7f51\u7edc\u800c\u8a00\uff0c\u5206\u7c7b\u5668\u9700\u8981\u4ece\u5934\u5f00\u59cb\u8bad\u7ec3\uff0c\u4e5f\u53ef\u4ee5\u9002\u5f53\u5c06\u5b66\u4e60\u7387\u8c03\u5927\u3002\u7531\u4e8e\u9488\u5bf9\u7279\u5b9a\u7f51\u7edc\u5c42\u7684\u5b66\u4e60\u7387\u8c03\u6574\u64cd\u4f5c\u6bd4\u8f83\u7cbe\u7ec6\uff0c\u6211\u4eec\u9700\u8981\u8fdb\u5165finetune.py\u4e2d\u81ea\u884c\u6307\u5b9a\u53c2\u6570\u540d\u4e0e\u5bf9\u5e94\u7684\u5b66\u4e60\u7387\u3002</p> <p>MindCV\u4f7f\u7528<code>create_optimizer</code>\u51fd\u6570\u6784\u9020\u4f18\u5316\u5668\uff0c\u5e76\u5c06\u5b66\u4e60\u7387\u4f20\u5230\u4f18\u5316\u5668\u4e2d\u53bb\u3002\u8981\u8bbe\u7f6e\u5206\u5c42\u5b66\u4e60\u7387\uff0c\u53ea\u9700**\u5c06finetune.py\u4e2d<code>create_optimizer</code>\u51fd\u6570\u7684<code>params</code>\u53c2\u6570\u4ece<code>network.trainable_params()</code>\u6539\u4e3a\u5305\u542b\u7279\u5b9a\u5c42\u53c2\u6570\u540d\u4e0e\u5bf9\u5e94\u5b66\u4e60\u7387\u7684\u5217\u8868\u5373\u53ef**\uff0c\u53c2\u8003MindSpore\u5404\u4f18\u5316\u5668\u8bf4\u660e\u6587\u6863\uff0c\u5176\u4e2d\u7f51\u7edc\u5177\u4f53\u7ed3\u6784\u4e0e\u6bcf\u5c42\u4e2d\u7684\u53c2\u6570\u540d\u5747\u53ef\u4ee5\u901a\u8fc7\u6253\u5370<code>create_model</code>\u7684\u7ed3\u679c\u2014\u2014<code>network</code>\u67e5\u770b\u3002</p> <p>Tips: \u60a8\u8fd8\u53ef\u4ee5\u4f7f\u7528\u540c\u6837\u7684\u64cd\u4f5c\u5206\u5c42\u8bbe\u7f6eweight_decay.</p>"},{"location":"zh/how_to_guides/finetune_with_a_custom_dataset/#_13","title":"\u5355\u72ec\u8c03\u6574\u5206\u7c7b\u5668\u7684\u5b66\u4e60\u7387","text":"<p>\u4ee5mobilenet v3-small\u4e3a\u4f8b\uff0c\u8be5\u6a21\u578b\u5206\u7c7b\u5668\u540d\u79f0\u4ee5\u201cclassifier\u201d\u5f00\u5934\uff0c\u56e0\u6b64\u5982\u679c\u4ec5\u8c03\u5927\u5206\u7c7b\u5668\u7684\u5b66\u4e60\u7387\uff0c\u6211\u4eec\u9700\u8981\u6307\u5b9a\u5206\u7c7b\u5668\u5728\u6bcf\u4e00\u6b65\u8bad\u7ec3\u4e2d\u7684\u5b66\u4e60\u7387\u3002<code>lr_scheduler</code>\u662f\u7531<code>create_scheduler</code>\u751f\u6210\u7684\u5b66\u4e60\u7387\u8c03\u6574\u7b56\u7565\uff0c\u662f\u4e00\u4e2a\u5305\u542b\u7f51\u7edc\u6bcf\u6b65\u8bad\u7ec3\u4e2d\u5177\u4f53\u5b66\u4e60\u7387\u503c\u7684\u5217\u8868\uff0c\u5047\u8bbe\u6211\u4eec\u5c06\u5206\u7c7b\u5668\u7684\u5b66\u4e60\u7387\u8c03\u6574\u81f3\u7279\u5f81\u7f51\u7edc\u5b66\u4e60\u7387\u76841.2\u500d\uff0cfinetune.py\u4e2d\u521b\u5efa\u4f18\u5316\u5668\u90e8\u5206\u4ee3\u7801\u7684\u6539\u52a8\u5982\u4e0b\uff1a</p> <pre><code># ...\n\n\n# Note: a)the params-lr dict must contain all the parameters. b)Also, you're recommended to set a dict with a key \"order_params\" to make sure the parameters will be updated in a right order.\nparams_lr_group = [{\"params\": list(filter(lambda x: 'classifier' in x.name, network.trainable_params())),\n                    \"lr\": [i*1.2 for i in lr_scheduler]},\n                   {\"params\": list(filter(lambda x: 'classifier' not in x.name, network.trainable_params())),\n                    \"lr\": lr_scheduler},\n                   {\"order_params\": network.trainable_params()}]\n\noptimizer = create_optimizer(params_lr_group,\n                             opt=args.opt,\n                             lr=lr_scheduler,\n                             ...)\n</code></pre>"},{"location":"zh/how_to_guides/finetune_with_a_custom_dataset/#_14","title":"\u8bbe\u7f6e\u7279\u5f81\u7f51\u7edc\u4efb\u610f\u5c42\u7684\u5b66\u4e60\u7387","text":"<p>\u4e0e\u5355\u72ec\u8c03\u6574\u5206\u7c7b\u5668\u7684\u5b66\u4e60\u7387\u7c7b\u4f3c\uff0c\u5206\u5c42\u8bbe\u7f6e\u7279\u5f81\u7f51\u7edc\u5b66\u4e60\u7387\u9700\u8981\u6307\u5b9a\u7279\u5b9a\u5c42\u7684\u5b66\u4e60\u7387\u53d8\u5316\u5217\u8868\u3002\u5047\u8bbe\u6211\u4eec\u4ec5\u589e\u5927\u7279\u5f81\u7f51\u7edc\u6700\u540e\u4e09\u5c42\u53c2\u6570\uff08features.13, features.14, features.15\uff09\u66f4\u65b0\u7684\u5b66\u4e60\u7387\uff0c\u5bf9finetune.py\u4e2d\u521b\u5efa\u4f18\u5316\u5668\u90e8\u5206\u4ee3\u7801\u7684\u6539\u52a8\u5982\u4e0b\uff1a</p> <pre><code># ...\n\n\n# Note: a)the params-lr dict must contain all the parameters. b)Also, you're recommended to set a dict with a key \"order_params\" to make sure the parameters will be updated in a right order.\nparams_lr_group = [{\"params\": list(filter(lambda x: 'features.13' in x.name, network.trainable_params())),\n                    \"lr\": [i * 1.05 for i in lr_scheduler]},\n                   {\"params\": list(filter(lambda x: 'features.14' in x.name, network.trainable_params())),\n                    \"lr\": [i * 1.1 for i in lr_scheduler]},\n                   {\"params\": list(filter(lambda x: 'features.15' in x.name, network.trainable_params())),\n                    \"lr\": [i * 1.15 for i in lr_scheduler]},\n                   {\"params\": list(filter(\n                       lambda x: \".\".join(x.name.split(\".\")[:2]) not in [\"features.13\", \"features.14\", \"features.15\"],\n                       network.trainable_params())),\n                    \"lr\": lr_scheduler},\n                   {\"order_params\": network.trainable_params()}]\n\noptimizer = create_optimizer(params_lr_group,\n                             opt=args.opt,\n                             lr=lr_scheduler,\n                             ...)\n</code></pre>"},{"location":"zh/how_to_guides/finetune_with_a_custom_dataset/#_15","title":"\u6a21\u578b\u8bc4\u4f30","text":"<p>\u8bad\u7ec3\u7ed3\u675f\u540e\uff0c\u4f7f\u7528./ckpt\u6587\u4ef6\u5939\u4e2d\u4ee5<code>*_best.ckpt</code>\u683c\u5f0f\u50a8\u5b58\u7684\u6a21\u578b\u6743\u91cd\u6765\u8bc4\u4f30\u6a21\u578b\u5728\u6d4b\u8bd5\u96c6\u4e0a\u7684\u6700\u4f18\u8868\u73b0\uff0c\u53ea\u9700**\u76f4\u63a5\u8fd0\u884cvalidate.py**\u5e76\u5411\u5176\u4f20\u5165\u6a21\u578b\u914d\u7f6e\u6587\u4ef6\u8def\u5f84\u4e0e\u6743\u91cd\u7684\u6587\u4ef6\u8def\u5f84\u5373\u53ef\uff1a</p> <pre><code>python validate.py --config=./configs/mobilenetv3/mobilnet_v3_small_ascend.yaml --data_dir=./aircraft/data --ckpt_path=./ckpt/mobilenet_v3_small_100_best.ckpt\n</code></pre> <p>\u6a21\u578b\u5fae\u8c03\u7ae0\u8282\u5c55\u793a\u4e86\u591a\u79cd\u5fae\u8c03\u6280\u5de7\uff0c\u4e0b\u8868\u603b\u7ed3\u4e86\u5728\u4f7f\u7528\u76f8\u540c\u8bad\u7ec3\u914d\u7f6e\u4e0d\u540c\u5fae\u8c03\u65b9\u5f0f\u4e0bmobilenet v3-small\u6a21\u578b\u5728Aircraft\u6570\u636e\u96c6\u4e0a\u7684Top-1 \u7cbe\u5ea6\u8868\u73b0\uff1a</p> \u6a21\u578b \u51bb\u7ed3\u6240\u6709\u7279\u5f81\u7f51\u7edc \u51bb\u7ed3\u6d45\u5c42\u7279\u5f81\u7f51\u7edc \u5168\u91cf\u5fae\u8c03+\u56fa\u5b9a\u5b66\u4e60\u7387 \u5168\u91cf\u5fae\u8c03+\u8c03\u5927\u5206\u7c7b\u5668\u5b66\u4e60\u7387 \u5168\u91cf\u5fae\u8c03+\u8c03\u5927\u6df1\u5c42\u7f51\u7edc\u5b66\u4e60\u7387 mobilenet v3-small 48.66% 76.83% 88.35% 88.89% 88.68%"},{"location":"zh/how_to_guides/finetune_with_a_custom_dataset/#_16","title":"\u6a21\u578b\u9884\u6d4b","text":"<p>\u53c2\u8003MindCV\u5fae\u8c03\u6559\u7a0b\u4e2d\u53ef\u89c6\u5316\u6a21\u578b\u63a8\u7406\u7ed3\u679c\u5c0f\u8282\uff0c\u6216\u662f\u5728validate.py\u4e2d\u52a0\u5165\u5982\u4e0b\u4ee3\u7801\u751f\u6210\u50a8\u5b58\u6d4b\u8bd5\u96c6\u771f\u5b9e\u503c\u4e0e\u9884\u6d4b\u503c\u7684\u6587\u672c\u6587\u4ef6./ckpt/pred.txt\uff1a</p> <pre><code># ... after model.eval()\n\n# predited label\npred = np.argmax(model.predict(images).asnumpy(), axis=1)\n\n# real label\nimages, labels = next(loader_eval.create_tuple_iterator())\n\n# write pred.txt\nprediction = np.array([pred, labels]).transpose()\nnp.savetxt(\"./ckpt/pred.txt\", prediction, fmt=\"%s\", header=\"pred \\t real\")\n</code></pre>"},{"location":"zh/how_to_guides/finetune_with_a_custom_dataset/#_17","title":"\u9644\u5f55","text":"<p>\u4ee5\u4e0b\u8868\u683c\u5c55\u793a\u4e86\u4f7f\u7528MindCV\u5728\u591a\u4e2aCNN\u6a21\u578b\u4e0a\u5bf9Aircraft\u6570\u636e\u96c6\u8fdb\u884c\u5168\u91cf\u5fae\u8c03\u7684\u7cbe\u5ea6\uff08Top 1%\uff09\u5bf9\u6bd4\u4fe1\u606f\uff0c\u8be5\u6570\u636e\u96c6\u4e0a\u53ef\u5b9e\u73b0\u7684\u5206\u7c7b\u7cbe\u5ea6\u53c2\u89c1Aircraft leaderboard\u548cpaperwithcode\u7f51\u7ad9\u3002</p> \u6a21\u578b MindCV\u5168\u91cf\u5fae\u8c03\u7cbe\u5ea6 \u53c2\u8003\u7cbe\u5ea6 mobilenet v3-small 88.35% - mobilenet v3-large 92.22% 83.8% convnext-tiny 93.69% 84.23% resnest50 86.82% -"},{"location":"zh/how_to_guides/write_a_new_model/","title":"\u6a21\u578b\u7f16\u5199\u6307\u5357","text":"<p>\u672c\u6587\u6863\u63d0\u4f9b\u4e86\u7f16\u5199MindSpore\u5957\u4ef6\u4e2d\u7684\u6a21\u578b\u5b9a\u4e49\u6587\u4ef6<code>model.py</code>\u7684\u53c2\u8003\u6a21\u677f\uff0c\u65e8\u5728\u63d0\u4f9b\u4e00\u79cd\u7edf\u4e00\u7684\u4ee3\u7801\u98ce\u683c\u3002</p> <p>\u63a5\u4e0b\u6765\u6211\u4eec\u4ee5\u76f8\u5bf9\u7b80\u5355\u7684\u65b0\u6a21\u578b<code>MLP-Mixer</code>\u4f5c\u4e3a\u793a\u4f8b\u3002</p>"},{"location":"zh/how_to_guides/write_a_new_model/#_2","title":"\u6587\u4ef6\u5934","text":"<p>\u8be5\u6587\u4ef6\u7684**\u7b80\u8981\u63cf\u8ff0**\u3002\u5305\u542b\u6a21\u578b\u540d\u79f0\u548c\u8bba\u6587\u9898\u76ee\u3002\u5982\u4e0b\u6240\u793a\uff1a</p> <pre><code>\"\"\"\nMindSpore implementation of `${MODEL_NAME}`.\nRefer to ${PAPER_NAME}.\n\"\"\"\n</code></pre>"},{"location":"zh/how_to_guides/write_a_new_model/#_3","title":"\u6a21\u5757\u5bfc\u5165","text":"<p>\u6a21\u5757\u5bfc\u5165\u5206\u4e3a\u4e09\u79cd\u7c7b\u578b\u3002\u5206\u522b\u4e3a</p> <ul> <li>Python\u539f\u751f\u6216\u7b2c\u4e09\u65b9\u5e93\u3002\u5982<code>import math</code>\u3001<code>import numpy as np</code>\u7b49\u7b49\u3002\u5e94\u5f53\u653e\u5728\u7b2c\u4e00\u68af\u961f\u3002</li> <li>MindSpore\u76f8\u5173\u6a21\u5757\u3002\u5982<code>import mindspore.nn as nn</code>\u3001<code>import mindspore.ops as ops</code>\u7b49\u7b49\u3002\u5e94\u5f53\u653e\u5728\u7b2c\u4e8c\u68af\u961f\u3002</li> <li>\u5957\u4ef6\u5305\u5185\u6a21\u5757\u3002\u5982<code>from .layers.classifier import ClassifierHead</code>\u7b49\u7b49\u3002\u5e94\u5f53\u653e\u5728\u7b2c\u4e09\u68af\u961f\uff0c\u5e76\u4f7f\u7528\u76f8\u5bf9\u5bfc\u5165\u3002</li> </ul> <p>\u793a\u4f8b\u5982\u4e0b\uff1a</p> <pre><code>import math\nfrom collections import OrderedDict\n\nimport mindspore.nn as nn\nimport mindspore.ops as ops\nimport mindspore.common.initializer as init\n\nfrom .utils import load_pretrained\nfrom .layers.classifier import ClassifierHead\n</code></pre> <p>\u4ec5\u5bfc\u5165\u5fc5\u987b\u7684\u6a21\u5757\u6216\u5305\uff0c\u907f\u514d\u5bfc\u5165\u65e0\u7528\u5305\u3002</p>"},{"location":"zh/how_to_guides/write_a_new_model/#__all__","title":"<code>__all__</code>","text":"<p>Python \u6ca1\u6709\u539f\u751f\u7684\u53ef\u89c1\u6027\u63a7\u5236\uff0c\u5176\u53ef\u89c1\u6027\u7684\u7ef4\u62a4\u662f\u9760\u4e00\u5957\u9700\u8981\u5927\u5bb6\u81ea\u89c9\u9075\u5b88\u7684\u201c\u7ea6\u5b9a\u201d\u3002<code>__all__</code> \u662f\u9488\u5bf9\u6a21\u5757\u516c\u5f00\u63a5\u53e3\u7684\u4e00\u79cd\u7ea6\u5b9a\uff0c\u4ee5\u63d0\u4f9b\u4e86\u201d\u767d\u540d\u5355\u201c\u7684\u5f62\u5f0f\u66b4\u9732\u63a5\u53e3\u3002\u5982\u679c\u5b9a\u4e49\u4e86<code>__all__</code>\uff0c\u5176\u4ed6\u6587\u4ef6\u4e2d\u4f7f\u7528<code>from xxx import *</code>\u5bfc\u5165\u8be5\u6587\u4ef6\u65f6\uff0c\u53ea\u4f1a\u5bfc\u5165<code>__all__</code>\u5217\u51fa\u7684\u6210\u5458\uff0c\u53ef\u4ee5\u5176\u4ed6\u6210\u5458\u90fd\u88ab\u6392\u9664\u5728\u5916\u3002</p> <p>\u6211\u4eec\u7ea6\u5b9a\u6a21\u578b\u4e2d\u5bf9\u5916\u66b4\u9732\u7684\u63a5\u53e3\u5305\u62ec\u4e3b\u6a21\u578b\u7c7b\u4ee5\u53ca\u8fd4\u56de\u4e0d\u540c\u89c4\u683c\u6a21\u578b\u7684\u51fd\u6570\uff0c\u4f8b\u5982\uff1a</p> <pre><code>__all__ = [\n    \"MLPMixer\",\n    \"mlp_mixer_s_p32\",\n    \"mlp_mixer_s_p16\",\n    ...\n]\n</code></pre> <p>\u5176\u4e2d<code>\"MLPMixer\"</code>\u662f\u4e3b\u6a21\u578b\u7c7b\uff0c<code>\"mlp_mixer_s_p32\"</code>\u548c<code>\"mlp_mixer_s_p16\"</code>\u7b49\u662f\u8fd4\u56de\u4e0d\u540c\u89c4\u683c\u6a21\u578b\u7684\u51fd\u6570\u3002\u4e00\u822c\u6765\u8bf4\u5b50\u6a21\u578b\uff0c\u5373\u67d0<code>Layer</code>\u6216\u67d0<code>Block</code>\u662f\u4e0d\u5e94\u8be5\u88ab\u5176\u4ed6\u6587\u4ef6\u6240\u5171\u7528\u7684\u3002\u5982\u82e5\u6b64\uff0c\u5e94\u5f53\u8003\u8651\u5c06\u8be5\u5b50\u6a21\u578b\u63d0\u53d6\u5230<code>${MINDCLS}/models/layers</code>\u4e0b\u9762\u4f5c\u4e3a\u516c\u7528\u6a21\u5757\uff0c\u5982<code>SEBlock</code>\u7b49\u3002</p>"},{"location":"zh/how_to_guides/write_a_new_model/#_4","title":"\u5b50\u6a21\u578b","text":"<p>\u6211\u4eec\u90fd\u77e5\u9053\u4e00\u4e2a\u6df1\u5ea6\u6a21\u578b\u662f\u7531\u591a\u5c42\u7ec4\u6210\u7684\u7f51\u7edc\u3002\u5176\u4e2d\u67d0\u4e9b\u5c42\u53ef\u4ee5\u7ec4\u6210\u76f8\u540c\u62d3\u6251\u7ed3\u6784\u7684\u5b50\u6a21\u578b\uff0c\u6211\u4eec\u4e00\u822c\u79f0\u5176\u4e3a<code>Layer</code>\u6216\u8005<code>Block</code>\uff0c\u4f8b\u5982<code>ResidualBlock</code>\u7b49\u3002\u8fd9\u79cd\u62bd\u8c61\u6709\u5229\u4e8e\u6211\u4eec\u7406\u89e3\u6574\u4e2a\u6a21\u578b\u7ed3\u6784\uff0c\u4e5f\u6709\u5229\u4e8e\u4ee3\u7801\u7684\u7f16\u5199\u3002</p> <p>\u6211\u4eec\u5e94\u5f53\u901a\u8fc7\u7c7b\u6ce8\u91ca\u5bf9\u5b50\u6a21\u578b\u8fdb\u884c\u529f\u80fd\u7684\u7b80\u8981\u63cf\u8ff0\u3002\u5728<code>MindSpore</code>\u4e2d\uff0c\u6a21\u578b\u7684\u7c7b\u7ee7\u627f\u4e8e<code>nn.Cell</code>\uff0c\u4e00\u822c\u6765\u8bf4\u6211\u4eec\u9700\u8981\u91cd\u8f7d\u4ee5\u4e0b\u4e24\u4e2a\u51fd\u6570\uff1a</p> <ul> <li>\u5728<code>__init__</code>\u51fd\u6570\u4e2d\uff0c\u6211\u4eec\u5e94\u5f53\u5b9a\u4e49\u6a21\u578b\u4e2d\u9700\u8981\u7528\u5230\u7684\u795e\u7ecf\u7f51\u7edc\u5c42\uff08<code>__init__</code>\u4e2d\u7684\u53c2\u6570\u8981\u8fdb\u884c\u53c2\u6570\u7c7b\u578b\u58f0\u660e\uff0c\u5373type hint\uff09\u3002</li> <li>\u5728<code>construct</code>\u51fd\u6570\u4e2d\u6211\u4eec\u5b9a\u4e49\u6a21\u578b\u524d\u5411\u903b\u8f91\u3002</li> </ul> <p>\u793a\u4f8b\u5982\u4e0b\uff1a</p> <pre><code>class MixerBlock(nn.Cell):\n    \"\"\"Mixer Layer with token-mixing MLP and channel-mixing MLP\"\"\"\n\n    def __init__(self,\n                 n_patches: int,\n                 n_channels: int,\n                 token_dim: int,\n                 channel_dim: int,\n                 dropout: float = 0.\n                 ) -&gt; None:\n        super().__init__()\n        self.token_mix = nn.SequentialCell(\n            nn.LayerNorm((n_channels,)),\n            TransPose((0, 2, 1)),\n            FeedForward(n_patches, token_dim, dropout),\n            TransPose((0, 2, 1))\n        )\n        self.channel_mix = nn.SequentialCell(\n            nn.LayerNorm((n_channels,)),\n            FeedForward(n_channels, channel_dim, dropout),\n        )\n\n    def construct(self, x):\n        x = x + self.token_mix(x)\n        x = x + self.channel_mix(x)\n        return x\n</code></pre> <p>\u5728<code>nn.Cell</code>\u7c7b\u7684\u7f16\u5199\u8fc7\u7a0b\u4e2d\uff0c\u6709\u4e24\u4e2a\u503c\u5f97\u6ce8\u610f\u7684\u65b9\u9762</p> <ul> <li> <p>CellList &amp; SequentialCell</p> </li> <li> <p>CellList is just a container that contains a list of neural network layers(Cell). The Cells contained by it can be properly registered, and will be visible by all Cell methods. We must overwrite the forward calculation, that is, the construct function.</p> </li> <li> <p>SequentialCell is a container that holds a sequential list of layers(Cell). The Cells may have a name(OrderedDict) or not(List). We don't need to implement forward computation, which is done according to the order of the sequential list.</p> </li> <li> <p>construct</p> </li> <li> <p>Assert is not supported. [RuntimeError: ParseStatement] Unsupported statement 'Assert'.</p> </li> <li> <p>Usage of single operator\u3002\u8c03\u7528\u7b97\u5b50\u65f6\uff08\u5982concat, reshape, mean\uff09\uff0c\u4f7f\u7528\u51fd\u6570\u5f0f\u63a5\u53e3 mindspore.ops.functional (\u5982 output=ops.concat((x1, x2)))\uff0c\u907f\u514d\u5148\u5728__init__\u4e2d\u5b9e\u4f8b\u5316\u539f\u59cb\u7b97\u5b50 ops.Primitive  (\u5982self.concat=ops.Concat()) \u518d\u5728construct\u4e2d\u8c03\u7528\uff08output=self.concat((x1, x2))\uff09\u3002</p> </li> </ul>"},{"location":"zh/how_to_guides/write_a_new_model/#_5","title":"\u4e3b\u6a21\u578b","text":"<p>\u4e3b\u6a21\u578b\u662f\u8bba\u6587\u4e2d\u6240\u63d0\u51fa\u7684\u7f51\u7edc\u6a21\u578b\u5b9a\u4e49\uff0c\u7531\u591a\u4e2a\u5b50\u6a21\u578b\u5806\u53e0\u800c\u6210\u3002\u5b83\u662f\u9002\u7528\u4e8e\u5206\u7c7b\u3001\u68c0\u6d4b\u7b49\u4efb\u52a1\u7684\u6700\u9876\u5c42\u7f51\u7edc\u3002\u5b83\u5728\u4ee3\u7801\u4e66\u5199\u4e0a\u4e0e\u5b50\u6a21\u578b\u4e0a\u57fa\u672c\u7c7b\u4f3c\uff0c\u4f46\u6709\u51e0\u5904\u4e0d\u540c\u3002</p> <ul> <li>\u7c7b\u6ce8\u91ca\u3002\u6211\u4eec\u5e94\u5f53\u5728\u6b64\u7ed9\u51fa\u8bba\u6587\u7684\u9898\u76ee\u548c\u94fe\u63a5\u3002\u53e6\u5916\u7531\u4e8e\u8be5\u7c7b\u5bf9\u5916\u66b4\u9732\uff0c\u6211\u4eec\u6700\u597d\u4e5f\u52a0\u4e0a\u7c7b\u521d\u59cb\u5316\u53c2\u6570\u7684\u8bf4\u660e\u3002\u8be6\u89c1\u4e0b\u65b9\u4ee3\u7801\u3002</li> <li><code>forward_features</code>\u51fd\u6570\u3002\u5728\u51fd\u6570\u5185\u5bf9\u6a21\u578b\u7684\u7279\u5f81\u7f51\u7edc\u7684\u8fd0\u7b97\u5b9a\u4e49\u3002</li> <li><code>forward_head</code>\u51fd\u6570\u3002\u5728\u51fd\u6570\u5185\u5bf9\u6a21\u578b\u7684\u5206\u7c7b\u5668\u7684\u8fd0\u7b97\u8fdb\u884c\u5b9a\u4e49\u3002</li> <li><code>construct</code>\u51fd\u6570\u3002\u5728\u51fd\u6570\u8c03\u7528\u7279\u5f81\u7f51\u7edc\u548c\u5206\u7c7b\u5668\u7684\u8fd0\u7b97\u3002</li> <li><code>_initialize_weights</code>\u51fd\u6570\u3002\u6211\u4eec\u7ea6\u5b9a\u6a21\u578b\u53c2\u6570\u7684\u968f\u673a\u521d\u59cb\u5316\u7531\u8be5\u6210\u5458\u51fd\u6570\u5b8c\u6210\u3002\u8be6\u89c1\u4e0b\u65b9\u4ee3\u7801\u3002</li> </ul> <p>\u793a\u4f8b\u5982\u4e0b\uff1a</p> <pre><code>class MLPMixer(nn.Cell):\n    r\"\"\"MLP-Mixer model class, based on\n    `\"MLP-Mixer: An all-MLP Architecture for Vision\" &lt;https://arxiv.org/abs/2105.01601&gt;`_\n\n    Args:\n        depth (int) : number of MixerBlocks.\n        patch_size (Union[int, tuple]) : size of a single image patch.\n        n_patches (int) : number of patches.\n        n_channels (int) : channels(dimension) of a single embedded patch.\n        token_dim (int) : hidden dim of token-mixing MLP.\n        channel_dim (int) : hidden dim of channel-mixing MLP.\n        in_channels(int): number the channels of the input. Default: 3.\n        n_classes (int) : number of classification classes. Default: 1000.\n    \"\"\"\n\n    def __init__(self,\n                 depth: int,\n                 patch_size: Union[int, tuple],\n                 n_patches: int,\n                 n_channels: int,\n                 token_dim: int,\n                 channel_dim: int,\n                 in_channels: int = 3,\n                 n_classes: int = 1000,\n                 ) -&gt; None:\n        super().__init__()\n        self.n_patches = n_patches\n        self.n_channels = n_channels\n        # patch with shape of (3, patch_size, patch_size) is embedded to n_channels dim feature.\n        self.to_patch_embedding = nn.SequentialCell(\n            nn.Conv2d(in_chans, n_channels, patch_size, patch_size, pad_mode=\"pad\", padding=0),\n            TransPose(permutation=(0, 2, 1), embedding=True),\n        )\n        self.mixer_blocks = nn.SequentialCell()\n        for _ in range(depth):\n            self.mixer_blocks.append(MixerBlock(n_patches, n_channels, token_dim, channel_dim))\n        self.layer_norm = nn.LayerNorm((n_channels,))\n        self.mlp_head = nn.Dense(n_channels, n_classes)\n        self._initialize_weights()\n\n    def forward_features(self, x: Tensor) -&gt; Tensor:\n        x = self.to_patch_embedding(x)\n        x = self.mixer_blocks(x)\n        x = self.layer_norm(x)\n        return ops.mean(x, 1)\n\n    def forward_head(self, x: Tensor)-&gt; Tensor:\n        return self.mlp_head(x)\n\n    def construct(self, x: Tensor) -&gt; Tensor:\n        x = self.forward_features(x)\n        return self.forward_head(x)\n\n    def _initialize_weights(self) -&gt; None:\n        for name, m in self.cells_and_names():\n            if isinstance(m, nn.Conv2d):\n                m.weight.set_data(init.initializer(init.Normal(0.01, 0), m.weight.shape))\n                if m.bias is not None:\n                    m.bias.set_data(init.initializer(init.Constant(0), m.bias.shape))\n            elif isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm1d):\n                m.gamma.set_data(init.initializer(init.Constant(1), m.gamma.shape))\n                if m.beta is not None:\n                    m.beta.set_data(init.initializer(init.Constant(0.0001), m.beta.shape))\n            elif isinstance(m, nn.Dense):\n                m.weight.set_data(init.initializer(init.Normal(0.01, 0), m.weight.shape))\n                if m.bias is not None:\n                    m.bias.set_data(init.initializer(init.Constant(0), m.bias.shape))\n</code></pre>"},{"location":"zh/how_to_guides/write_a_new_model/#_6","title":"\u89c4\u683c\u51fd\u6570","text":"<p>\u8bba\u6587\u4e2d\u6240\u63d0\u51fa\u7684\u6a21\u578b\u53ef\u80fd\u6709\u4e0d\u540c\u89c4\u683c\u7684\u53d8\u79cd\uff0c\u5982<code>channel</code>\u7684\u5927\u5c0f\u3001<code>depth</code>\u7684\u5927\u5c0f\u7b49\u7b49\u3002\u8fd9\u4e9b\u53d8\u79cd\u7684\u5177\u4f53\u914d\u7f6e\u5e94\u8be5\u901a\u8fc7\u89c4\u683c\u51fd\u6570\u4f53\u73b0\uff0c\u89c4\u683c\u7684\u63a5\u53e3\u53c2\u6570\uff1a pretrained, num_classes, in_channels \u547d\u540d\u8981\u7edf\u4e00\uff0c\u540c\u65f6\u5728\u89c4\u683c\u51fd\u6570\u5185\u8fd8\u8981\u8fdb\u884cpretrain loading\u64cd\u4f5c\u3002\u6bcf\u4e00\u4e2a\u89c4\u683c\u51fd\u6570\u5bf9\u5e94\u4e00\u79cd\u786e\u5b9a\u914d\u7f6e\u7684\u89c4\u683c\u53d8\u79cd\u3002\u914d\u7f6e\u901a\u8fc7\u5165\u53c2\u4f20\u5165\u4e3b\u6a21\u578b\u7c7b\u7684\u5b9a\u4e49\uff0c\u5e76\u8fd4\u56de\u5b9e\u4f8b\u5316\u7684\u4e3b\u6a21\u578b\u7c7b\u3002\u53e6\u5916\uff0c\u8fd8\u9700\u901a\u8fc7\u6dfb\u52a0\u88c5\u9970\u5668<code>@register_model</code>\u5c06\u8be5\u6a21\u578b\u7684\u6b64\u89c4\u683c\u6ce8\u518c\u5230\u5305\u5185\u3002</p> <p>\u793a\u4f8b\u5982\u4e0b\uff1a</p> <pre><code>@register_model\ndef mlp_mixer_s_p16(pretrained: bool = False, num_classes: int = 1000, in_channels: int = 3, **kwargs):\n    nl, pr, ls, hs, ds, dc = 8, 16, 196, 512, 256, 2048\n    _check_resolution_and_length_of_patch(pr, ls)\n    model = MLPMixer(depth=nl, patch_size=pr, n_patches=ls, n_channels=hs, token_dim=ds,\n                    channel_dim=dc, in_chans=in_chans, n_classes=num_classes, **kwargs)\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n    return model\n\n@register_model\ndef mlp_mixer_b_p32(pretrained: bool = False, num_classes: int = 1000, in_channels: int = 3, **kwargs):\n    nl, pr, ls, hs, ds, dc = 12, 32, 49, 768, 384, 3072\n    _check_resolution_and_length_of_patch(pr, ls)\n    model = MLPMixer(depth=nl, patch_size=pr, n_patches=ls, n_channels=hs, token_dim=ds,\n                    channel_dim=dc, in_chans=in_chans, n_classes=num_classes, **kwargs)\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes=num_classes, in_channels=in_channels)\n    return model\n</code></pre>"},{"location":"zh/how_to_guides/write_a_new_model/#main","title":"\u9a8c\u8bc1main\uff08\u53ef\u9009\uff09","text":"<p>\u521d\u59cb\u7f16\u5199\u9636\u6bb5\u5e94\u5f53\u4fdd\u8bc1\u6a21\u578b\u662f\u53ef\u8fd0\u884c\u7684\u3002\u53ef\u901a\u8fc7\u4e0b\u8ff0\u4ee3\u7801\u5757\u8fdb\u884c\u57fa\u7840\u9a8c\u8bc1\uff1a</p> <pre><code>if __name__ == '__main__':\n    import numpy as np\n    import mindspore\n    from mindspore import Tensor\n\n    model = mlp_mixer_s_p16()\n    print(model)\n    dummy_input = Tensor(np.random.rand(8, 3, 224, 224), dtype=mindspore.float32)\n    y = model(dummy_input)\n    print(y.shape)\n</code></pre>"},{"location":"zh/how_to_guides/write_a_new_model/#_7","title":"\u53c2\u8003\u793a\u4f8b","text":"<ul> <li>densenet.py</li> <li>shufflenetv1.py</li> <li>shufflenetv2.py</li> <li>mixnet.py</li> <li>mlp_mixer.py</li> </ul>"},{"location":"zh/notes/changelog/","title":"\u66f4\u65b0\u65e5\u5fd7","text":"<p>\u5373\u5c06\u5230\u6765</p>"},{"location":"zh/notes/code_of_conduct/","title":"\u884c\u4e3a\u51c6\u5219","text":"<p>\u5373\u5c06\u5230\u6765</p>"},{"location":"zh/notes/contributing/","title":"\u8d21\u732e","text":""},{"location":"zh/notes/contributing/#mindcv","title":"MindCV \u8d21\u732e\u6307\u5357","text":"<p>\u6b22\u8fce\u8d21\u732e\uff0c\u6211\u4eec\u5c06\u4e0d\u80dc\u611f\u6fc0\uff01\u6bcf\u4e00\u4efd\u8d21\u732e\u90fd\u662f\u6709\u76ca\u7684\uff0c\u8bf7\u63a5\u53d7\u6211\u4eec\u7684\u8d5e\u626c\u3002</p>"},{"location":"zh/notes/contributing/#_1","title":"\u8d21\u732e\u8005\u8bb8\u53ef\u534f\u8bae","text":"<p>\u9996\u6b21\u5411 MindCV \u793e\u533a\u63d0\u4ea4\u4ee3\u7801\u524d\u9700\u7b7e\u7f72 CLA\u3002</p> <p>\u4e2a\u4eba\u8d21\u732e\u8005\u8bf7\u53c2\u8003 ICLA \u5728\u7ebf\u6587\u6863 \u4e86\u89e3\u8be6\u7ec6\u4fe1\u606f\u3002</p>"},{"location":"zh/notes/contributing/#_2","title":"\u8d21\u732e\u7c7b\u578b","text":""},{"location":"zh/notes/contributing/#_3","title":"\u62a5\u544a\u9519\u8bef","text":"<p>\u62a5\u544a\u9519\u8bef\u81f3 https://github.com/mindspore-lab/mindcv/issues.</p> <p>\u5982\u679c\u60a8\u8981\u62a5\u544a\u9519\u8bef\uff0c\u8bf7\u5305\u62ec\uff1a</p> <ul> <li>\u60a8\u7684\u64cd\u4f5c\u7cfb\u7edf\u540d\u79f0\u548c\u7248\u672c\u3002</li> <li>\u4efb\u4f55\u53ef\u80fd\u6709\u52a9\u4e8e\u6545\u969c\u6392\u9664\u7684\u672c\u5730\u8bbe\u7f6e\u8be6\u7ec6\u4fe1\u606f\u3002</li> <li>\u91cd\u73b0\u9519\u8bef\u7684\u8be6\u7ec6\u6b65\u9aa4\u3002</li> </ul>"},{"location":"zh/notes/contributing/#bugs","title":"\u4fee\u590dBugs","text":"<p>\u67e5\u9605GitHub issues\u4ee5\u4e86\u89e3Bugs\u3002\u4efb\u4f55\u5e26\u6709\u201cbug\u201d\u548c\u201chelp wanted\u201d\u6807\u7b7e\u7684issue\u90fd\u5bf9\u60f3\u8981\u89e3\u51b3\u5b83\u7684\u4eba\u5f00\u653e\u3002</p>"},{"location":"zh/notes/contributing/#features","title":"\u5b9e\u73b0features","text":"<p>\u67e5\u9605GitHub issues\u4ee5\u4e86\u89e3features\u3002\u4efb\u4f55\u6807\u6709\u201cenhancement\u201d\u548c\u201chelp wanted\u201d\u7684issue\u90fd\u5bf9\u60f3\u8981\u5b9e\u73b0\u5b83\u7684\u4eba\u5f00\u653e\u3002</p>"},{"location":"zh/notes/contributing/#_4","title":"\u7f16\u5199\u6587\u6863","text":"<p>MindCV\u901a\u5e38\u53ef\u4ee5\u4f7f\u7528\u591a\u79cd\u65b9\u5f0f\u7f16\u5199\u6587\u6863\uff0c\u53ef\u4ee5\u7f16\u5199\u5728\u5b98\u65b9MindCV\u6587\u6863\u4e2d\uff0c\u6216\u8005\u7f16\u5199\u5728docstrings\u4e2d\uff0c\u751a\u81f3\u53ef\u4ee5\u7f16\u5199\u5728\u7f51\u7edc\u4e0a\u7684\u535a\u5ba2\u3001\u6587\u7ae0\u4e0a\u3002</p>"},{"location":"zh/notes/contributing/#_5","title":"\u63d0\u4ea4\u53cd\u9988","text":"<p>\u53d1\u9001\u53cd\u9988\u7684\u6700\u4f73\u65b9\u5f0f\u662f\u5728 https://github.com/mindspore-lab/mindcv/issues \u4e0a\u63d0\u4ea4\u95ee\u9898\u3002</p> <p>\u5982\u679c\u60a8\u8981\u63d0\u51fa\u4e00\u9879\u529f\u80fd\uff1a</p> <ul> <li>\u8be6\u7ec6\u8bf4\u660e\u5b83\u5c06\u5982\u4f55\u5de5\u4f5c\u3002</li> <li>\u5c3d\u53ef\u80fd\u7f29\u5c0f\u8303\u56f4\uff0c\u4f7f\u5176\u66f4\u6613\u4e8e\u5b9e\u65bd\u3002</li> <li>\u8bf7\u8bb0\u4f4f\uff0c\u8fd9\u662f\u4e00\u4e2a\u5fd7\u613f\u8005\u9a71\u52a8\u7684\u9879\u76ee\uff0c\u6b22\u8fce\u8d21\u732e :)</li> </ul>"},{"location":"zh/notes/contributing/#_6","title":"\u5165\u95e8","text":"<p>\u51c6\u5907\u597d\u8d21\u732e\u4e86\u5417\uff1f\u4ee5\u4e0b\u662f\u5982\u4f55\u8bbe\u7f6e <code>mindcv</code> \u8fdb\u884c\u672c\u5730\u5f00\u53d1\u3002</p> <ol> <li>\u5728 GitHub \u4e0a fork <code>mindcv</code> \u4ee3\u7801\u4ed3\u3002</li> <li>\u5728\u672c\u5730\u514b\u9686\u60a8\u7684 fork\uff1a</li> </ol> <pre><code>git clone git@github.com:your_name_here/mindcv.git\n</code></pre> <p>\u4e4b\u540e\uff0c\u60a8\u5e94\u8be5\u5c06\u5b98\u65b9\u4ee3\u7801\u4ed3\u6dfb\u52a0\u4e3aupstream\u4ee3\u7801\u4ed3\uff1a</p> <pre><code>git remote add upper git@github.com:mindspore-lab/mindcv\n</code></pre> <ol> <li>\u5c06\u672c\u5730\u526f\u672c\u914d\u7f6e\u5230 conda \u73af\u5883\u4e2d\u3002\u5047\u8bbe\u60a8\u5df2\u5b89\u88c5 conda\uff0c\u60a8\u53ef\u4ee5\u6309\u7167\u4ee5\u4e0b\u65b9\u5f0f\u8bbe\u7f6e fork \u4ee5\u8fdb\u884c\u672c\u5730\u5f00\u53d1\uff1a</li> </ol> <pre><code>conda create -n mindcv python=3.8\nconda activate mindcv\ncd mindcv\npip install -e \u3002\n</code></pre> <ol> <li>\u4e3a\u672c\u5730\u5f00\u53d1\u521b\u5efa\u4e00\u4e2a\u5206\u652f\uff1a</li> </ol> <pre><code>git checkout -b name-of-your-bugfix-or-feature\n</code></pre> <p>\u73b0\u5728\u60a8\u53ef\u4ee5\u5728\u672c\u5730\u8fdb\u884c\u66f4\u6539\u3002</p> <ol> <li>\u5b8c\u6210\u66f4\u6539\u540e\uff0c\u68c0\u67e5\u60a8\u7684\u66f4\u6539\u662f\u5426\u901a\u8fc7\u4e86linters\u548ctests\u68c0\u67e5\uff1a</li> </ol> <pre><code>pre-commit run --show-diff-on-failure --color=always --all-files\npytest\n</code></pre> <p>\u5982\u679c\u6240\u6709\u9759\u6001 linting \u90fd\u901a\u8fc7\uff0c\u60a8\u5c06\u83b7\u5f97\u5982\u4e0b\u8f93\u51fa\uff1a</p> <p></p> <p>\u5426\u5219\uff0c\u60a8\u9700\u8981\u6839\u636e\u8f93\u51fa\u4fee\u590d\u8b66\u544a\uff1a</p> <p></p> <p>\u8981\u83b7\u53d6 pre-commit \u548c pytest\uff0c\u53ea\u9700\u4f7f\u7528 pip \u5b89\u88c5\u5b83\u4eec\u5230\u60a8\u7684 conda \u73af\u5883\u4e2d\u5373\u53ef\u3002</p> <ol> <li>\u63d0\u4ea4\u60a8\u7684\u66f4\u6539\u5e76\u5c06\u60a8\u7684\u5206\u652f\u63a8\u9001\u5230 GitHub\uff1a</li> </ol> <pre><code>git add .\ngit commit -m \u201c\u60a8\u5bf9\u66f4\u6539\u7684\u8be6\u7ec6\u63cf\u8ff0\u3002\u201d\ngit push origin name-of-your-bugfix-or-feature\n</code></pre> <ol> <li>\u901a\u8fc7 GitHub \u7f51\u7ad9\u63d0\u4ea4pull request\u3002</li> </ol>"},{"location":"zh/notes/contributing/#pull-request","title":"pull request\u6307\u5357","text":"<p>\u5728\u63d0\u4ea4pull request\u4e4b\u524d\uff0c\u8bf7\u68c0\u67e5\u5b83\u662f\u5426\u7b26\u5408\u4ee5\u4e0b\u6307\u5357\uff1a</p> <ol> <li>pull request\u5e94\u5305\u62ec\u6d4b\u8bd5\u3002</li> <li>\u5982\u679cpull request\u6dfb\u52a0\u4e86\u529f\u80fd\uff0c\u5219\u5e94\u66f4\u65b0\u6587\u6863\u3002\u5c06\u65b0\u529f\u80fd\u653e\u5165\u5e26\u6709docstring\u7684\u51fd\u6570\u4e2d\uff0c\u5e76\u5c06\u7279\u6027\u6dfb\u52a0\u5230 README.md \u4e2d\u7684\u5217\u8868\u4e2d\u3002</li> <li>pull request\u5e94\u9002\u7528\u4e8e Python 3.7\u30013.8 \u548c 3.9 \u4ee5\u53ca PyPy\u3002\u68c0\u67e5    https://github.com/mindspore-lab/mindcv/actions    \u5e76\u786e\u4fdd\u6240\u6709\u53d7\u652f\u6301\u7684 Python \u7248\u672c\u7684\u6d4b\u8bd5\u90fd\u901a\u8fc7\u3002</li> </ol>"},{"location":"zh/notes/contributing/#_7","title":"\u63d0\u793a","text":"<p>\u60a8\u53ef\u4ee5\u5b89\u88c5 git hook\u811a\u672c\uff0c\u800c\u4e0d\u662f\u624b\u52a8\u4f7f\u7528 <code>pre-commit run -a</code> \u8fdb\u884c linting\u3002</p> <p>\u8fd0\u884cflowing command\u6765\u8bbe\u7f6e git hook\u811a\u672c</p> <pre><code>pre-commit install\n</code></pre> <p>\u73b0\u5728 <code>pre-commit</code> \u5c06\u5728 <code>git commit</code> \u4e0a\u81ea\u52a8\u8fd0\u884c\uff01</p>"},{"location":"zh/notes/contributing/#_8","title":"\u53d1\u5e03","text":"<p>\u63d0\u9192\u7ef4\u62a4\u8005\u5982\u4f55\u90e8\u7f72\u3002\u786e\u4fdd\u63d0\u4ea4\u6240\u6709\u66f4\u6539\uff08\u5305\u62ec HISTORY.md \u4e2d\u7684\u6761\u76ee\uff09\uff0c\u7136\u540e\u8fd0\u884c\uff1a</p> <pre><code>bump2version patch # possible: major / minor / patch\ngit push\ngit push --tags\n</code></pre> <p>\u5982\u679c\u6d4b\u8bd5\u901a\u8fc7\uff0cGitHub Action \u5c06\u90e8\u7f72\u5230 PyPI\u3002</p>"},{"location":"zh/notes/faq/","title":"\u5e38\u89c1\u95ee\u9898","text":"<p>\u5373\u5c06\u5230\u6765</p>"},{"location":"zh/tutorials/configuration/","title":"\u914d\u7f6e","text":"<p>MindCV\u5957\u4ef6\u53ef\u4ee5\u901a\u8fc7python\u7684argparse\u5e93\u548cPyYAML\u5e93\u89e3\u6790\u6a21\u578b\u7684yaml\u6587\u4ef6\u6765\u8fdb\u884c\u53c2\u6570\u7684\u914d\u7f6e\u3002 \u4e0b\u9762\u6211\u4eec\u4ee5squeezenet_1.0\u6a21\u578b\u4e3a\u4f8b\uff0c\u89e3\u91ca\u5982\u4f55\u914d\u7f6e\u76f8\u5e94\u7684\u53c2\u6570\u3002</p>"},{"location":"zh/tutorials/configuration/#_2","title":"\u57fa\u7840\u73af\u5883","text":"<ol> <li>\u53c2\u6570\u8bf4\u660e</li> </ol> <ul> <li> <p>mode\uff1a\u4f7f\u7528\u9759\u6001\u56fe\u6a21\u5f0f\uff080\uff09\u6216\u52a8\u6001\u56fe\u6a21\u5f0f\uff081\uff09\u3002</p> </li> <li> <p>distribute\uff1a\u662f\u5426\u4f7f\u7528\u5206\u5e03\u5f0f\u3002</p> </li> </ul> <ol> <li> <p>yaml\u6587\u4ef6\u6837\u4f8b</p> <pre><code>mode: 0\ndistribute: True\n...\n</code></pre> </li> <li> <p>parse\u53c2\u6570\u8bbe\u7f6e</p> <pre><code>python train.py --mode 0 --distribute False ...\n</code></pre> </li> <li> <p>\u5bf9\u5e94\u7684\u4ee3\u7801\u793a\u4f8b</p> <p><code>args.mode</code>\u4ee3\u8868\u53c2\u6570<code>mode</code>, <code>args.distribute</code>\u4ee3\u8868\u53c2\u6570<code>distribute</code>\u3002</p> <pre><code>def train(args):\n    ms.set_context(mode=args.mode)\n\n    if args.distribute:\n        init()\n        device_num = get_group_size()\n        rank_id = get_rank()\n        ms.set_auto_parallel_context(device_num=device_num,\n                                     parallel_mode='data_parallel',\n                                     gradients_mean=True)\n    else:\n        device_num = None\n        rank_id = None\n    ...\n</code></pre> </li> </ol>"},{"location":"zh/tutorials/configuration/#_3","title":"\u6570\u636e\u96c6","text":"<ol> <li>\u53c2\u6570\u8bf4\u660e</li> </ol> <ul> <li> <p>dataset\uff1a\u6570\u636e\u96c6\u540d\u79f0\u3002</p> </li> <li> <p>data_dir\uff1a\u6570\u636e\u96c6\u6587\u4ef6\u6240\u5728\u8def\u5f84\u3002</p> </li> <li> <p>shuffle\uff1a\u662f\u5426\u8fdb\u884c\u6570\u636e\u6df7\u6d17\u3002</p> </li> <li> <p>dataset_download\uff1a\u662f\u5426\u4e0b\u8f7d\u6570\u636e\u96c6\u3002</p> </li> <li> <p>batch_size\uff1a\u6bcf\u4e2a\u6279\u5904\u7406\u6570\u636e\u5305\u542b\u7684\u6570\u636e\u6761\u76ee\u3002</p> </li> <li> <p>drop_remainder\uff1a\u5f53\u6700\u540e\u4e00\u4e2a\u6279\u5904\u7406\u6570\u636e\u5305\u542b\u7684\u6570\u636e\u6761\u76ee\u5c0f\u4e8e batch_size \u65f6\uff0c\u662f\u5426\u5c06\u8be5\u6279\u5904\u7406\u4e22\u5f03\u3002</p> </li> <li> <p>num_parallel_workers\uff1a\u8bfb\u53d6\u6570\u636e\u7684\u5de5\u4f5c\u7ebf\u7a0b\u6570\u3002</p> </li> </ul> <ol> <li> <p>yaml\u6587\u4ef6\u6837\u4f8b</p> <pre><code>dataset: 'imagenet'\ndata_dir: './imagenet2012'\nshuffle: True\ndataset_download: False\nbatch_size: 32\ndrop_remainder: True\nnum_parallel_workers: 8\n...\n</code></pre> </li> <li> <p>parse\u53c2\u6570\u8bbe\u7f6e</p> <pre><code>python train.py ... --dataset imagenet --data_dir ./imagenet2012 --shuffle True \\\n    --dataset_download False --batch_size 32 --drop_remainder True \\\n    --num_parallel_workers 8 ...\n</code></pre> </li> <li> <p>\u5bf9\u5e94\u7684\u4ee3\u7801\u793a\u4f8b</p> <pre><code>def train(args):\n    ...\n    dataset_train = create_dataset(\n        name=args.dataset,\n        root=args.data_dir,\n        split='train',\n        shuffle=args.shuffle,\n        num_samples=args.num_samples,\n        num_shards=device_num,\n        shard_id=rank_id,\n        num_parallel_workers=args.num_parallel_workers,\n        download=args.dataset_download,\n        num_aug_repeats=args.aug_repeats)\n\n    ...\n    target_transform = transforms.OneHot(num_classes) if args.loss == 'BCE' else None\n\n    loader_train = create_loader(\n        dataset=dataset_train,\n        batch_size=args.batch_size,\n        drop_remainder=args.drop_remainder,\n        is_training=True,\n        mixup=args.mixup,\n        cutmix=args.cutmix,\n        cutmix_prob=args.cutmix_prob,\n        num_classes=args.num_classes,\n        transform=transform_list,\n        target_transform=target_transform,\n        num_parallel_workers=args.num_parallel_workers,\n    )\n    ...\n</code></pre> </li> </ol>"},{"location":"zh/tutorials/configuration/#_4","title":"\u6570\u636e\u589e\u5f3a","text":"<ol> <li>\u53c2\u6570\u8bf4\u660e</li> </ol> <ul> <li> <p>image_resize\uff1a\u56fe\u50cf\u7684\u8f93\u51fa\u5c3a\u5bf8\u5927\u5c0f\u3002</p> </li> <li> <p>scale\uff1a\u8981\u88c1\u526a\u7684\u539f\u59cb\u5c3a\u5bf8\u5927\u5c0f\u7684\u5404\u4e2a\u5c3a\u5bf8\u7684\u8303\u56f4\u3002</p> </li> <li> <p>ratio\uff1a\u88c1\u526a\u5bbd\u9ad8\u6bd4\u7684\u8303\u56f4\u3002</p> </li> <li> <p>hfilp\uff1a\u56fe\u50cf\u88ab\u7ffb\u8f6c\u7684\u6982\u7387\u3002</p> </li> <li> <p>interpolation\uff1a\u56fe\u50cf\u63d2\u503c\u65b9\u5f0f\u3002</p> </li> <li> <p>crop_pct\uff1a\u8f93\u5165\u56fe\u50cf\u4e2d\u5fc3\u88c1\u526a\u767e\u5206\u6bd4\u3002</p> </li> <li> <p>color_jitter\uff1a\u989c\u8272\u6296\u52a8\u56e0\u5b50\uff08\u4eae\u5ea6\u8c03\u6574\u56e0\u5b50\uff0c\u5bf9\u6bd4\u5ea6\u8c03\u6574\u56e0\u5b50\uff0c\u9971\u548c\u5ea6\u8c03\u6574\u56e0\u5b50\uff09\u3002</p> </li> <li> <p>re_prob\uff1a\u6267\u884c\u968f\u673a\u64e6\u9664\u7684\u6982\u7387\u3002</p> </li> </ul> <ol> <li> <p>yaml\u6587\u4ef6\u6837\u4f8b</p> <pre><code>image_resize: 224\nscale: [0.08, 1.0]\nratio: [0.75, 1.333]\nhflip: 0.5\ninterpolation: 'bilinear'\ncrop_pct: 0.875\ncolor_jitter: [0.4, 0.4, 0.4]\nre_prob: 0.5\n...\n</code></pre> </li> <li> <p>parse\u53c2\u6570\u8bbe\u7f6e</p> <pre><code>python train.py ... --image_resize 224 --scale [0.08, 1.0] --ratio [0.75, 1.333] \\\n    --hflip 0.5 --interpolation \"bilinear\" --crop_pct 0.875 \\\n    --color_jitter [0.4, 0.4, 0.4] --re_prob 0.5 ...\n</code></pre> </li> <li> <p>\u5bf9\u5e94\u7684\u4ee3\u7801\u793a\u4f8b</p> <pre><code>def train(args):\n    ...\n    transform_list = create_transforms(\n        dataset_name=args.dataset,\n        is_training=True,\n        image_resize=args.image_resize,\n        scale=args.scale,\n        ratio=args.ratio,\n        hflip=args.hflip,\n        vflip=args.vflip,\n        color_jitter=args.color_jitter,\n        interpolation=args.interpolation,\n        auto_augment=args.auto_augment,\n        mean=args.mean,\n        std=args.std,\n        re_prob=args.re_prob,\n        re_scale=args.re_scale,\n        re_ratio=args.re_ratio,\n        re_value=args.re_value,\n        re_max_attempts=args.re_max_attempts\n    )\n    ...\n</code></pre> </li> </ol>"},{"location":"zh/tutorials/configuration/#_5","title":"\u6a21\u578b","text":"<ol> <li>\u53c2\u6570\u8bf4\u660e</li> </ol> <ul> <li> <p>model\uff1a\u6a21\u578b\u540d\u79f0\u3002</p> </li> <li> <p>num_classes\uff1a\u5206\u7c7b\u7684\u7c7b\u522b\u6570\u3002</p> </li> <li> <p>pretrained\uff1a\u662f\u5426\u52a0\u8f7d\u9884\u8bad\u7ec3\u6a21\u578b\u3002</p> </li> <li> <p>ckpt_path\uff1a\u53c2\u6570\u6587\u4ef6\u6240\u5728\u7684\u8def\u5f84\u3002</p> </li> <li> <p>keep_checkpoint_max\uff1a\u6700\u591a\u4fdd\u5b58\u591a\u5c11\u4e2acheckpoint\u6587\u4ef6\u3002</p> </li> <li> <p>ckpt_save_dir\uff1a\u4fdd\u5b58\u53c2\u6570\u6587\u4ef6\u7684\u8def\u5f84\u3002</p> </li> <li> <p>epoch_size\uff1a\u8bad\u7ec3\u6267\u884c\u8f6e\u6b21\u3002</p> </li> <li> <p>dataset_sink_mode\uff1a\u6570\u636e\u662f\u5426\u76f4\u63a5\u4e0b\u6c89\u81f3\u5904\u7406\u5668\u8fdb\u884c\u5904\u7406\u3002</p> </li> <li> <p>amp_level\uff1a\u6df7\u5408\u7cbe\u5ea6\u7b49\u7ea7\u3002</p> </li> </ul> <ol> <li> <p>yaml\u6587\u4ef6\u6837\u4f8b</p> <pre><code>model: 'squeezenet1_0'\nnum_classes: 1000\npretrained: False\nckpt_path: './squeezenet1_0_gpu.ckpt'\nkeep_checkpoint_max: 10\nckpt_save_dir: './ckpt/'\nepoch_size: 200\ndataset_sink_mode: True\namp_level: 'O0'\n...\n</code></pre> </li> <li> <p>parse\u53c2\u6570\u8bbe\u7f6e</p> <pre><code>python train.py ... --model squeezenet1_0 --num_classes 1000 --pretrained False \\\n    --ckpt_path ./squeezenet1_0_gpu.ckpt --keep_checkpoint_max 10 \\\n    --ckpt_save_path ./ckpt/ --epoch_size 200 --dataset_sink_mode True \\\n    --amp_level O0 ...\n</code></pre> </li> <li> <p>\u5bf9\u5e94\u7684\u4ee3\u7801\u793a\u4f8b</p> <pre><code>def train(args):\n    ...\n    network = create_model(model_name=args.model,\n        num_classes=args.num_classes,\n        in_channels=args.in_channels,\n        drop_rate=args.drop_rate,\n        drop_path_rate=args.drop_path_rate,\n        pretrained=args.pretrained,\n        checkpoint_path=args.ckpt_path,\n        ema=args.ema\n    )\n    ...\n</code></pre> </li> </ol>"},{"location":"zh/tutorials/configuration/#_6","title":"\u635f\u5931\u51fd\u6570","text":"<ol> <li>\u53c2\u6570\u8bf4\u660e</li> </ol> <ul> <li> <p>loss\uff1a\u635f\u5931\u51fd\u6570\u7684\u7b80\u79f0\u3002</p> </li> <li> <p>label_smoothing\uff1a\u6807\u7b7e\u5e73\u6ed1\u503c\uff0c\u7528\u4e8e\u8ba1\u7b97Loss\u65f6\u9632\u6b62\u6a21\u578b\u8fc7\u62df\u5408\u7684\u6b63\u5219\u5316\u624b\u6bb5\u3002</p> </li> </ul> <ol> <li> <p>yaml\u6587\u4ef6\u6837\u4f8b</p> <pre><code>loss: 'CE'\nlabel_smoothing: 0.1\n...\n</code></pre> </li> <li> <p>parse\u53c2\u6570\u8bbe\u7f6e</p> <pre><code>python train.py ... --loss CE --label_smoothing 0.1 ...\n</code></pre> </li> <li> <p>\u5bf9\u5e94\u7684\u4ee3\u7801\u793a\u4f8b</p> <pre><code>def train(args):\n    ...\n    loss = create_loss(name=args.loss,\n        reduction=args.reduction,\n        label_smoothing=args.label_smoothing,\n        aux_factor=args.aux_factor\n    )\n    ...\n</code></pre> </li> </ol>"},{"location":"zh/tutorials/configuration/#_7","title":"\u5b66\u4e60\u7387\u7b56\u7565","text":"<ol> <li>\u53c2\u6570\u8bf4\u660e</li> </ol> <ul> <li> <p>scheduler\uff1a\u5b66\u4e60\u7387\u7b56\u7565\u7684\u540d\u79f0\u3002</p> </li> <li> <p>min_lr\uff1a\u5b66\u4e60\u7387\u7684\u6700\u5c0f\u503c\u3002</p> </li> <li> <p>lr\uff1a\u5b66\u4e60\u7387\u7684\u6700\u5927\u503c\u3002</p> </li> <li> <p>warmup_epochs\uff1a\u5b66\u4e60\u7387warmup\u7684\u8f6e\u6b21\u3002</p> </li> <li> <p>decay_epochs\uff1a\u8fdb\u884c\u8870\u51cf\u7684step\u6570\u3002</p> </li> </ul> <ol> <li> <p>yaml\u6587\u4ef6\u6837\u4f8b</p> <pre><code>scheduler: 'cosine_decay'\nmin_lr: 0.0\nlr: 0.01\nwarmup_epochs: 0\ndecay_epochs: 200\n...\n</code></pre> </li> <li> <p>parse\u53c2\u6570\u8bbe\u7f6e</p> <pre><code>python train.py ... --scheduler cosine_decay --min_lr 0.0 --lr 0.01 \\\n    --warmup_epochs 0 --decay_epochs 200 ...\n</code></pre> </li> <li> <p>\u5bf9\u5e94\u7684\u4ee3\u7801\u793a\u4f8b</p> <pre><code>def train(args):\n    ...\n    lr_scheduler = create_scheduler(num_batches,\n        scheduler=args.scheduler,\n        lr=args.lr,\n        min_lr=args.min_lr,\n        warmup_epochs=args.warmup_epochs,\n        warmup_factor=args.warmup_factor,\n        decay_epochs=args.decay_epochs,\n        decay_rate=args.decay_rate,\n        milestones=args.multi_step_decay_milestones,\n        num_epochs=args.epoch_size,\n        lr_epoch_stair=args.lr_epoch_stair\n    )\n    ...\n</code></pre> </li> </ol>"},{"location":"zh/tutorials/configuration/#_8","title":"\u4f18\u5316\u5668","text":"<ol> <li>\u53c2\u6570\u8bf4\u660e</li> </ol> <ul> <li> <p>opt\uff1a\u4f18\u5316\u5668\u540d\u79f0\u3002</p> </li> <li> <p>weight_decay_filter\uff1a\u6743\u91cd\u8870\u51cf\u8fc7\u6ee4\u5668 \uff08\u8fc7\u6ee4\u4e00\u4e9b\u53c2\u6570\uff0c \u4f7f\u5176\u5728\u8ddf\u65b0\u65f6\u4e0d\u505a\u6743\u91cd\u8870\u51cf\uff09\u3002</p> </li> <li> <p>momentum\uff1a\u79fb\u52a8\u5e73\u5747\u7684\u52a8\u91cf\u3002</p> </li> <li> <p>weight_decay\uff1a\u6743\u91cd\u8870\u51cf\uff08L2 penalty\uff09\u3002</p> </li> <li> <p>loss_scale\uff1a\u68af\u5ea6\u7f29\u653e\u7cfb\u6570</p> </li> <li> <p>use_nesterov\uff1a\u662f\u5426\u4f7f\u7528Nesterov Accelerated Gradient (NAG)\u7b97\u6cd5\u66f4\u65b0\u68af\u5ea6\u3002</p> </li> </ul> <ol> <li> <p>yaml\u6587\u4ef6\u6837\u4f8b</p> <pre><code>opt: 'momentum'\nweight_decay_filter: 'norm_and_bias'\nmomentum: 0.9\nweight_decay: 0.00007\nloss_scale: 1024\nuse_nesterov: False\n...\n</code></pre> </li> <li> <p>parse\u53c2\u6570\u8bbe\u7f6e</p> <pre><code>python train.py ... --opt momentum --weight_decay_filter 'norm_and_bias\" --weight_decay 0.00007 \\\n    --loss_scale 1024 --use_nesterov False ...\n</code></pre> </li> <li> <p>\u5bf9\u5e94\u7684\u4ee3\u7801\u793a\u4f8b</p> <pre><code>def train(args):\n    ...\n    if args.ema:\n        optimizer = create_optimizer(network.trainable_params(),\n            opt=args.opt,\n            lr=lr_scheduler,\n            weight_decay=args.weight_decay,\n            momentum=args.momentum,\n            nesterov=args.use_nesterov,\n            weight_decay_filter=args.weight_decay_filter,\n            loss_scale=args.loss_scale,\n            checkpoint_path=opt_ckpt_path,\n            eps=args.eps\n        )\n    else:\n        optimizer = create_optimizer(network.trainable_params(),\n            opt=args.opt,\n            lr=lr_scheduler,\n            weight_decay=args.weight_decay,\n            momentum=args.momentum,\n            nesterov=args.use_nesterov,\n            weight_decay_filter=args.weight_decay_filter,\n            checkpoint_path=opt_ckpt_path,\n            eps=args.eps\n        )\n    ...\n</code></pre> </li> </ol>"},{"location":"zh/tutorials/configuration/#yamlparse","title":"Yaml\u548cParse\u7ec4\u5408\u4f7f\u7528","text":"<p>\u4f7f\u7528parse\u8bbe\u7f6e\u53c2\u6570\u53ef\u4ee5\u8986\u76d6yaml\u6587\u4ef6\u4e2d\u7684\u53c2\u6570\u8bbe\u7f6e\u3002\u4ee5\u4e0b\u9762\u7684shell\u547d\u4ee4\u4e3a\u4f8b\uff0c</p> <pre><code>python train.py -c ./configs/squeezenet/squeezenet_1.0_gpu.yaml --data_dir ./data\n</code></pre> <p>\u4e0a\u9762\u7684\u547d\u4ee4\u5c06<code>args.data_dir</code>\u53c2\u6570\u7684\u503c\u7531yaml\u6587\u4ef6\u4e2d\u7684 <code>./imagenet2012</code> \u8986\u76d6\u4e3a <code>./data</code>\u3002</p>"},{"location":"zh/tutorials/deployment/","title":"\u90e8\u7f72\u63a8\u7406\u670d\u52a1","text":"<p>MindSpore Serving\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u3001\u9ad8\u6027\u80fd\u7684\u63a8\u7406\u670d\u52a1\u6a21\u5757\uff0c\u65e8\u5728\u5e2e\u52a9MindSpore\u5f00\u53d1\u8005\u5728\u751f\u4ea7\u73af\u5883\u4e2d\u9ad8\u6548\u90e8\u7f72\u5728\u7ebf\u63a8\u7406\u670d\u52a1\u3002\u5f53\u7528\u6237\u4f7f\u7528MindSpore\u5b8c\u6210\u6a21\u578b\u8bad\u7ec3\u540e\uff0c\u5bfc\u51faMindSpore\u6a21\u578b\uff0c\u5373\u53ef\u4f7f\u7528MindSpore Serving\u521b\u5efa\u8be5\u6a21\u578b\u7684\u63a8\u7406\u670d\u52a1\u3002</p> <p>\u672c\u6587\u4ee5mobilenet_v2_100\u7f51\u7edc\u4e3a\u4f8b\uff0c\u6f14\u793a\u57fa\u4e8eMindSpore Serving\u8fdb\u884c\u90e8\u7f72\u63a8\u7406\u670d\u52a1\u7684\u65b9\u6cd5\u3002</p>"},{"location":"zh/tutorials/deployment/#_2","title":"\u73af\u5883\u51c6\u5907","text":"<p>\u8fdb\u884c\u90e8\u7f72\u524d\uff0c\u9700\u786e\u4fdd\u5df2\u7ecf\u6b63\u786e\u5b89\u88c5\u4e86MindSpore Serving\uff0c\u5e76\u914d\u7f6e\u4e86\u73af\u5883\u53d8\u91cf\u3002MindSpore Serving\u5b89\u88c5\u548c\u914d\u7f6e\u53ef\u4ee5\u53c2\u8003MindSpore Serving\u5b89\u88c5\u9875\u9762 \u3002</p>"},{"location":"zh/tutorials/deployment/#_3","title":"\u6a21\u578b\u5bfc\u51fa","text":"<p>\u5b9e\u73b0\u8de8\u5e73\u53f0\u6216\u786c\u4ef6\u6267\u884c\u63a8\u7406\uff08\u5982\u6607\u817eAI\u5904\u7406\u5668\u3001MindSpore\u7aef\u4fa7\u3001GPU\u7b49\uff09\uff0c\u9700\u8981\u901a\u8fc7\u7f51\u7edc\u5b9a\u4e49\u548cCheckPoint\u751f\u6210MindIR\u683c\u5f0f\u6a21\u578b\u6587\u4ef6\u3002\u5728MindSpore\u4e2d\uff0c\u7f51\u7edc\u6a21\u578b\u5bfc\u51fa\u7684\u51fd\u6570\u4e3a<code>export</code>\uff0c\u4e3b\u8981\u53c2\u6570\u5982\u4e0b\u6240\u793a\uff1a</p> <ul> <li><code>net</code>\uff1aMindSpore\u7f51\u7edc\u7ed3\u6784\u3002</li> <li><code>inputs</code>\uff1a\u7f51\u7edc\u7684\u8f93\u5165\uff0c\u652f\u6301\u8f93\u5165\u7c7b\u578b\u4e3aTensor\u3002\u5f53\u8f93\u5165\u6709\u591a\u4e2a\u65f6\uff0c\u9700\u8981\u4e00\u8d77\u4f20\u5165\uff0c\u5982<code>ms.export(network, ms.Tensor(input1), ms.Tensor(input2), file_name='network', file_format='MINDIR')</code>\u3002</li> <li><code>file_name</code>\uff1a\u5bfc\u51fa\u6a21\u578b\u7684\u6587\u4ef6\u540d\u79f0\uff0c\u5982\u679c<code>file_name</code>\u6ca1\u6709\u5305\u542b\u5bf9\u5e94\u7684\u540e\u7f00\u540d(\u5982.mindir)\uff0c\u8bbe\u7f6e<code>file_format</code>\u540e\u7cfb\u7edf\u4f1a\u4e3a\u6587\u4ef6\u540d\u81ea\u52a8\u6dfb\u52a0\u540e\u7f00\u3002</li> <li><code>file_format</code>\uff1aMindSpore\u76ee\u524d\u652f\u6301\u5bfc\u51fa\u201dAIR\u201d\uff0c\u201dONNX\u201d\u548c\u201dMINDIR\u201d\u683c\u5f0f\u7684\u6a21\u578b\u3002</li> </ul> <p>\u4e0b\u9762\u4ee3\u7801\u4ee5mobilenet_v2_100\u4e3a\u4f8b\uff0c\u5bfc\u51faMindCV\u7684\u9884\u8bad\u7ec3\u7f51\u7edc\u6a21\u578b\uff0c\u83b7\u5f97MindIR\u683c\u5f0f\u6a21\u578b\u6587\u4ef6\u3002</p> <pre><code>from mindcv.models import create_model\nimport numpy as np\nimport mindspore as ms\n\nmodel = create_model(model_name='mobilenet_v2_100', num_classes=1000, pretrained=True)\n\ninput_np = np.random.uniform(0.0, 1.0, size=[1, 3, 224, 224]).astype(np.float32)\n\n# \u5bfc\u51fa\u6587\u4ef6mobilenet_v2_100.mindir\u5230\u5f53\u524d\u6587\u4ef6\u5939\nms.export(model, ms.Tensor(input_np), file_name='mobilenet_v2_100', file_format='MINDIR')\n</code></pre>"},{"location":"zh/tutorials/deployment/#serving","title":"\u90e8\u7f72Serving\u63a8\u7406\u670d\u52a1","text":""},{"location":"zh/tutorials/deployment/#_4","title":"\u914d\u7f6e\u670d\u52a1","text":"<p>\u542f\u52a8Serving\u670d\u52a1\uff0c\u6267\u884c\u672c\u6559\u7a0b\u9700\u8981\u5982\u4e0b\u6587\u4ef6\u5217\u8868:</p> <pre><code>demo\n\u251c\u2500\u2500 mobilenet_v2_100\n\u2502   \u251c\u2500\u2500 1\n\u2502   \u2502   \u2514\u2500\u2500 mobilenet_v2_100.mindir\n\u2502   \u2514\u2500\u2500 servable_config.py\n\u2502\u2500\u2500 serving_server.py\n\u251c\u2500\u2500 serving_client.py\n\u251c\u2500\u2500 imagenet1000_clsidx_to_labels.txt\n\u2514\u2500\u2500 test_image\n    \u251c\u2500 dog\n    \u2502   \u251c\u2500 dog.jpg\n    \u2502   \u2514\u2500 \u2026\u2026\n    \u2514\u2500 \u2026\u2026\n</code></pre> <ul> <li><code>mobilenet_v2_100</code>\u4e3a\u6a21\u578b\u6587\u4ef6\u5939\uff0c\u6587\u4ef6\u5939\u540d\u5373\u4e3a\u6a21\u578b\u540d\u3002</li> <li><code>mobilenet_v2_100.mindir</code>\u4e3a\u4e0a\u4e00\u6b65\u7f51\u7edc\u751f\u6210\u7684\u6a21\u578b\u6587\u4ef6\uff0c\u653e\u7f6e\u5728\u6587\u4ef6\u59391\u4e0b\uff0c1\u4e3a\u7248\u672c\u53f7\uff0c\u4e0d\u540c\u7684\u7248\u672c\u653e\u7f6e\u5728\u4e0d\u540c\u7684\u6587\u4ef6\u5939\u4e0b\uff0c\u7248\u672c\u53f7\u9700\u4ee5\u7eaf\u6570\u5b57\u4e32\u547d\u540d\uff0c\u9ed8\u8ba4\u914d\u7f6e\u4e0b\u542f\u52a8\u6700\u5927\u6570\u503c\u7684\u7248\u672c\u53f7\u7684\u6a21\u578b\u6587\u4ef6\u3002</li> <li><code>servable_config.py</code>\u4e3a\u6a21\u578b\u914d\u7f6e\u811a\u672c\uff0c\u5bf9\u6a21\u578b\u8fdb\u884c\u58f0\u660e\u3001\u5165\u53c2\u548c\u51fa\u53c2\u5b9a\u4e49\u3002</li> <li><code>serving_server.py</code>\u4e3a\u542f\u52a8\u670d\u52a1\u811a\u672c\u6587\u4ef6\u3002</li> <li><code>serving_client.py</code>\u4e3a\u542f\u52a8\u5ba2\u6237\u7aef\u811a\u672c\u6587\u4ef6\u3002</li> <li><code>imagenet1000_clsidx_to_labels.txt</code>\u4e3aImageNet\u6570\u636e\u96c61000\u4e2a\u7c7b\u522b\u7684\u7d22\u5f15\uff0c\u53ef\u4ee5\u5728examples/data/\u4e2d\u5f97\u5230\u3002</li> <li><code>test_image</code>\u4e2d\u4e3a\u6d4b\u8bd5\u56fe\u7247\uff0c\u53ef\u4ee5\u5728README\u4e2d\u5f97\u5230\u3002</li> </ul> <p>\u5176\u4e2d\uff0c\u6a21\u578b\u914d\u7f6e\u6587\u4ef6<code>servable_config.py</code>\u5185\u5bb9\u5982\u4e0b\uff1a</p> <pre><code>from mindspore_serving.server import register\n\n# \u8fdb\u884c\u6a21\u578b\u58f0\u660e\uff0c\u5176\u4e2ddeclare_model\u5165\u53c2model_file\u6307\u793a\u6a21\u578b\u7684\u6587\u4ef6\u540d\u79f0\uff0cmodel_format\u6307\u793a\u6a21\u578b\u7684\u6a21\u578b\u7c7b\u522b\nmodel = register.declare_model(model_file=\"mobilenet_v2_100.mindir\", model_format=\"MindIR\")\n\n# Servable\u65b9\u6cd5\u7684\u5165\u53c2\u7531Python\u65b9\u6cd5\u7684\u5165\u53c2\u6307\u5b9a\uff0cServable\u65b9\u6cd5\u7684\u51fa\u53c2\u7531register_method\u7684output_names\u6307\u5b9a\n@register.register_method(output_names=[\"score\"])\ndef predict(image):\n    x = register.add_stage(model, image, outputs_count=1)\n    return x\n</code></pre>"},{"location":"zh/tutorials/deployment/#_5","title":"\u542f\u52a8\u670d\u52a1","text":"<p>MindSpore\u7684<code>server</code>\u51fd\u6570\u63d0\u4f9b\u4e24\u79cd\u670d\u52a1\u90e8\u7f72\uff0c\u4e00\u79cd\u662fgRPC\u65b9\u5f0f\uff0c\u4e00\u79cd\u662f\u901a\u8fc7RESTful\u65b9\u5f0f\uff0c\u672c\u6559\u7a0b\u4ee5gRPC\u65b9\u5f0f\u4e3a\u4f8b\u3002\u670d\u52a1\u542f\u52a8\u811a\u672c<code>serving_server.py</code>\u628a\u672c\u5730\u76ee\u5f55\u4e0b\u7684<code>mobilenet_v2_100</code>\u90e8\u7f72\u5230\u8bbe\u59070\uff0c\u5e76\u542f\u52a8\u5730\u5740\u4e3a127.0.0.1:5500\u7684gRPC\u670d\u52a1\u5668\u3002\u811a\u672c\u6587\u4ef6\u5185\u5bb9\u5982\u4e0b\uff1a</p> <pre><code>import os\nimport sys\nfrom mindspore_serving import server\n\ndef start():\n    servable_dir = os.path.dirname(os.path.realpath(sys.argv[0]))\n\n    servable_config = server.ServableStartConfig(servable_directory=servable_dir, servable_name=\"mobilenet_v2_100\",\n                                                 device_ids=0)\n    server.start_servables(servable_configs=servable_config)\n    server.start_grpc_server(address=\"127.0.0.1:5500\")\n\nif __name__ == \"__main__\":\n    start()\n</code></pre> <p>\u5f53\u670d\u52a1\u7aef\u6253\u5370\u5982\u4e0b\u65e5\u5fd7\u65f6\uff0c\u8868\u793aServing gRPC\u670d\u52a1\u542f\u52a8\u6210\u529f\u3002</p> <pre><code>Serving gRPC server start success, listening on 127.0.0.1:5500\n</code></pre>"},{"location":"zh/tutorials/deployment/#_6","title":"\u6267\u884c\u63a8\u7406","text":"<p>\u4f7f\u7528<code>serving_client.py</code>\uff0c\u542f\u52a8Python\u5ba2\u6237\u7aef\u3002\u5ba2\u6237\u7aef\u811a\u672c\u4f7f\u7528<code>mindcv.data</code>\u7684<code>create_transforms</code>, <code>create_dataset</code>\u548c<code>create_loader</code>\u51fd\u6570\uff0c\u8fdb\u884c\u56fe\u7247\u9884\u5904\u7406\uff0c\u518d\u4f20\u9001\u7ed9Serving\u670d\u52a1\u5668\u3002\u5bf9\u670d\u52a1\u5668\u8fd4\u56de\u7684\u7ed3\u679c\u8fdb\u884c\u540e\u5904\u7406\uff0c\u6253\u5370\u56fe\u7247\u7684\u9884\u6d4b\u6807\u7b7e\u3002</p> <pre><code>import os\nfrom mindspore_serving.client import Client\nimport numpy as np\nfrom mindcv.data import create_transforms, create_dataset, create_loader\n\nnum_workers = 1\n\n# \u6570\u636e\u96c6\u76ee\u5f55\u8def\u5f84\ndata_dir = \"./test_image/\"\n\ndataset = create_dataset(root=data_dir, split='', num_parallel_workers=num_workers)\ntransforms_list = create_transforms(dataset_name='ImageNet', is_training=False)\ndata_loader = create_loader(\n    dataset=dataset,\n    batch_size=1,\n    is_training=False,\n    num_classes=1000,\n    transform=transforms_list,\n    num_parallel_workers=num_workers\n)\nwith open(\"imagenet1000_clsidx_to_labels.txt\") as f:\n    idx2label = eval(f.read())\n\ndef postprocess(score):\n    max_idx = np.argmax(score)\n    return idx2label[max_idx]\n\ndef predict():\n    client = Client(\"127.0.0.1:5500\", \"mobilenet_v2_100\", \"predict\")\n    instances = []\n    images, _ = next(data_loader.create_tuple_iterator())\n    image_np = images.asnumpy().squeeze()\n    instances.append({\"image\": image_np})\n    result = client.infer(instances)\n\n    for instance in result:\n        label = postprocess(instance[\"score\"])\n        print(label)\n\nif __name__ == '__main__':\n    predict()\n</code></pre> <p>\u6267\u884c\u540e\u663e\u793a\u5982\u4e0b\u8fd4\u56de\u503c\uff0c\u8bf4\u660eServing\u670d\u52a1\u5df2\u6b63\u786e\u6267\u884cmobilenet_v2_100\u7f51\u7edc\u6a21\u578b\u7684\u63a8\u7406\u3002 <pre><code>Labrador retriever\n</code></pre></p>"},{"location":"zh/tutorials/finetune/","title":"\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\u4e0a\u7684\u6a21\u578b\u5fae\u8c03\u8bad\u7ec3","text":"<p>\u5728\u6b64\u6559\u7a0b\u4e2d\uff0c\u60a8\u5c06\u5b66\u4f1a\u5982\u4f55\u4f7f\u7528MindCV\u5957\u4ef6\u8fdb\u884c\u8fc1\u79fb\u5b66\u4e60\uff0c\u4ee5\u89e3\u51b3\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\u4e0a\u7684\u56fe\u50cf\u5206\u7c7b\u95ee\u9898\u3002 \u5728\u6df1\u5ea6\u5b66\u4e60\u4efb\u52a1\u4e2d\uff0c\u5e38\u89c1\u9047\u5230\u8bad\u7ec3\u6570\u636e\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u6b64\u65f6\u76f4\u63a5\u8bad\u7ec3\u6574\u4e2a\u7f51\u7edc\u5f80\u5f80\u96be\u4ee5\u8fbe\u5230\u7406\u60f3\u7684\u7cbe\u5ea6\u3002 \u4e00\u4e2a\u6bd4\u8f83\u597d\u7684\u505a\u6cd5\u662f\uff0c\u4f7f\u7528\u4e00\u4e2a\u5728\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a(\u4e0e\u4efb\u52a1\u6570\u636e\u8f83\u4e3a\u63a5\u8fd1)\u9884\u8bad\u7ec3\u597d\u7684\u6a21\u578b\uff0c\u7136\u540e\u4f7f\u7528\u8be5\u6a21\u578b\u6765\u521d\u59cb\u5316\u7f51\u7edc\u7684\u6743\u91cd\u53c2\u6570\u6216\u4f5c\u4e3a\u56fa\u5b9a\u7279\u5f81\u63d0\u53d6\u5668\u5e94\u7528\u4e8e\u7279\u5b9a\u7684\u4efb\u52a1\u4e2d\u3002</p> <p>\u6b64\u6559\u7a0b\u5c06\u4ee5\u4f7f\u7528ImageNet\u4e0a\u9884\u8bad\u7ec3\u7684DenseNet\u6a21\u578b\u4e3a\u4f8b\uff0c\u4ecb\u7ecd\u4e24\u79cd\u4e0d\u540c\u7684\u5fae\u8c03\u7b56\u7565\uff0c\u89e3\u51b3\u5c0f\u6837\u672c\u60c5\u51b5\u4e0b\u72fc\u548c\u72d7\u7684\u56fe\u50cf\u5206\u7c7b\u95ee\u9898:</p> <ol> <li>\u6574\u4f53\u6a21\u578b\u5fae\u8c03\u3002</li> <li>\u51bb\u7ed3\u7279\u5f81\u7f51\u7edc(freeze backbone)\uff0c\u53ea\u5fae\u8c03\u5206\u7c7b\u5668\u3002</li> </ol> <p>\u8fc1\u79fb\u5b66\u4e60\u8be6\u7ec6\u5185\u5bb9\u89c1Stanford University CS231n</p>"},{"location":"zh/tutorials/finetune/#_2","title":"\u6570\u636e\u51c6\u5907","text":""},{"location":"zh/tutorials/finetune/#_3","title":"\u4e0b\u8f7d\u6570\u636e\u96c6","text":"<p>\u4e0b\u8f7d\u6848\u4f8b\u6240\u7528\u5230\u7684\u72d7\u4e0e\u72fc\u5206\u7c7b\u6570\u636e\u96c6\uff0c \u6bcf\u4e2a\u7c7b\u522b\u5404\u6709120\u5f20\u8bad\u7ec3\u56fe\u50cf\u4e0e30\u5f20\u9a8c\u8bc1\u56fe\u50cf\u3002\u4f7f\u7528<code>mindcv.utils.download</code>\u63a5\u53e3\u4e0b\u8f7d\u6570\u636e\u96c6\uff0c\u5e76\u5c06\u4e0b\u8f7d\u540e\u7684\u6570\u636e\u96c6\u81ea\u52a8\u89e3\u538b\u5230\u5f53\u524d\u76ee\u5f55\u4e0b\u3002</p> <pre><code>import os\nfrom mindcv.utils.download import DownLoad\n\ndataset_url = \"https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/notebook/datasets/intermediate/Canidae_data.zip\"\nroot_dir = \"./\"\n\nif not os.path.exists(os.path.join(root_dir, 'data/Canidae')):\n    DownLoad().download_and_extract_archive(dataset_url, root_dir)\n</code></pre> <p>\u6570\u636e\u96c6\u7684\u76ee\u5f55\u7ed3\u6784\u5982\u4e0b\uff1a</p> <pre><code>data/\n\u2514\u2500\u2500 Canidae\n    \u251c\u2500\u2500 train\n    \u2502   \u251c\u2500\u2500 dogs\n    \u2502   \u2514\u2500\u2500 wolves\n    \u2514\u2500\u2500 val\n        \u251c\u2500\u2500 dogs\n        \u2514\u2500\u2500 wolves\n</code></pre>"},{"location":"zh/tutorials/finetune/#_4","title":"\u6570\u636e\u96c6\u52a0\u8f7d\u53ca\u5904\u7406","text":""},{"location":"zh/tutorials/finetune/#_5","title":"\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\u7684\u52a0\u8f7d","text":"<p>\u901a\u8fc7\u8c03\u7528<code>mindcv.data</code>\u4e2d\u7684<code>create_dataset</code>\u51fd\u6570\uff0c\u6211\u4eec\u53ef\u8f7b\u677e\u5730\u52a0\u8f7d\u9884\u8bbe\u7684\u6570\u636e\u96c6\u4ee5\u53ca\u81ea\u5b9a\u4e49\u7684\u6570\u636e\u96c6\u3002</p> <ul> <li>\u5f53\u53c2\u6570<code>name</code>\u8bbe\u4e3a\u7a7a\u65f6\uff0c\u6307\u5b9a\u4e3a\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\u3002(\u9ed8\u8ba4\u503c)</li> <li>\u5f53\u53c2\u6570<code>name</code>\u8bbe\u4e3a<code>MNIST</code>, <code>CIFAR10</code>\u7b49\u6807\u51c6\u6570\u636e\u96c6\u540d\u79f0\u65f6\uff0c\u6307\u5b9a\u4e3a\u9884\u8bbe\u6570\u636e\u96c6\u3002</li> </ul> <p>\u540c\u65f6\uff0c\u6211\u4eec\u9700\u8981\u8bbe\u5b9a\u6570\u636e\u96c6\u7684\u8def\u5f84<code>data_dir</code>\u548c\u6570\u636e\u5207\u5206\u7684\u540d\u79f0<code>split</code> (\u5982train, val)\uff0c\u4ee5\u52a0\u8f7d\u5bf9\u5e94\u7684\u8bad\u7ec3\u96c6\u6216\u8005\u9a8c\u8bc1\u96c6\u3002</p> <pre><code>from mindcv.data import create_dataset, create_transforms, create_loader\n\nnum_workers = 8\n\n# \u6570\u636e\u96c6\u76ee\u5f55\u8def\u5f84\ndata_dir = \"./data/Canidae/\"\n\n# \u52a0\u8f7d\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\ndataset_train = create_dataset(root=data_dir, split='train', num_parallel_workers=num_workers)\ndataset_val = create_dataset(root=data_dir, split='val', num_parallel_workers=num_workers)\n</code></pre> <p>\u6ce8\u610f: \u81ea\u5b9a\u4e49\u6570\u636e\u96c6\u7684\u76ee\u5f55\u7ed3\u6784\u5e94\u4e0eImageNet\u4e00\u6837\uff0c\u5373root -&gt; split -&gt; class -&gt; image \u7684\u5c42\u6b21\u7ed3\u6784</p> <pre><code>DATASET_NAME\n    \u251c\u2500\u2500 split1(e.g. train)/\n    \u2502  \u251c\u2500\u2500 class1/\n    \u2502  \u2502   \u251c\u2500\u2500 000001.jpg\n    \u2502  \u2502   \u251c\u2500\u2500 000002.jpg\n    \u2502  \u2502   \u2514\u2500\u2500 ....\n    \u2502  \u2514\u2500\u2500 class2/\n    \u2502      \u251c\u2500\u2500 000001.jpg\n    \u2502      \u251c\u2500\u2500 000002.jpg\n    \u2502      \u2514\u2500\u2500 ....\n    \u2514\u2500\u2500 split2/\n       \u251c\u2500\u2500 class1/\n       \u2502   \u251c\u2500\u2500 000001.jpg\n       \u2502   \u251c\u2500\u2500 000002.jpg\n       \u2502   \u2514\u2500\u2500 ....\n       \u2514\u2500\u2500 class2/\n           \u251c\u2500\u2500 000001.jpg\n           \u251c\u2500\u2500 000002.jpg\n           \u2514\u2500\u2500 ....\n</code></pre>"},{"location":"zh/tutorials/finetune/#_6","title":"\u6570\u636e\u5904\u7406\u53ca\u589e\u5f3a","text":"<p>\u9996\u5148\u6211\u4eec\u901a\u8fc7\u8c03\u7528<code>create_transforms</code>\u51fd\u6570, \u83b7\u5f97\u9884\u8bbe\u7684\u6570\u636e\u5904\u7406\u548c\u589e\u5f3a\u7b56\u7565(transform list)\uff0c\u6b64\u4efb\u52a1\u4e2d\uff0c\u56e0\u72fc\u72d7\u56fe\u50cf\u548cImageNet\u6570\u636e\u4e00\u81f4\uff08\u5373domain\u4e00\u81f4\uff09\uff0c\u6211\u4eec\u6307\u5b9a\u53c2\u6570<code>dataset_name</code>\u4e3aImageNet\uff0c\u76f4\u63a5\u7528\u9884\u8bbe\u597d\u7684ImageNet\u7684\u6570\u636e\u5904\u7406\u548c\u56fe\u50cf\u589e\u5f3a\u7b56\u7565\u3002<code>create_transforms</code> \u540c\u6837\u652f\u6301\u591a\u79cd\u81ea\u5b9a\u4e49\u7684\u5904\u7406\u548c\u589e\u5f3a\u64cd\u4f5c\uff0c\u4ee5\u53ca\u81ea\u52a8\u589e\u5f3a\u7b56\u7565(AutoAug)\u3002\u8be6\u89c1API\u8bf4\u660e\u3002</p> <p>\u6211\u4eec\u5c06\u5f97\u5230\u7684transform list\u4f20\u5165<code>create_loader()</code>\uff0c\u5e76\u6307\u5b9a<code>batch_size</code>\u548c\u5176\u4ed6\u53c2\u6570\uff0c\u5373\u53ef\u5b8c\u6210\u8bad\u7ec3\u548c\u9a8c\u8bc1\u6570\u636e\u7684\u51c6\u5907\uff0c\u8fd4\u56de<code>Dataset</code> Object\uff0c\u4f5c\u4e3a\u6a21\u578b\u7684\u8f93\u5165\u3002</p> <pre><code># \u5b9a\u4e49\u548c\u83b7\u53d6\u6570\u636e\u5904\u7406\u53ca\u589e\u5f3a\u64cd\u4f5c\ntrans_train = create_transforms(dataset_name='ImageNet', is_training=True)\ntrans_val = create_transforms(dataset_name='ImageNet',is_training=False)\n\nloader_train = create_loader(\n    dataset=dataset_train,\n    batch_size=16,\n    is_training=True,\n    num_classes=2,\n    transform=trans_train,\n    num_parallel_workers=num_workers,\n)\nloader_val = create_loader(\n    dataset=dataset_val,\n    batch_size=5,\n    is_training=True,\n    num_classes=2,\n    transform=trans_val,\n    num_parallel_workers=num_workers,\n)\n</code></pre>"},{"location":"zh/tutorials/finetune/#_7","title":"\u6570\u636e\u96c6\u53ef\u89c6\u5316","text":"<p>\u5bf9\u4e8e<code>create_loader</code>\u63a5\u53e3\u8fd4\u56de\u7684\u5b8c\u6210\u6570\u636e\u52a0\u8f7d\u7684Dataset object\uff0c\u6211\u4eec\u53ef\u4ee5\u901a\u8fc7 <code>create_tuple_iterator</code> \u63a5\u53e3\u521b\u5efa\u6570\u636e\u8fed\u4ee3\u5668\uff0c\u4f7f\u7528 <code>next</code> \u8fed\u4ee3\u8bbf\u95ee\u6570\u636e\u96c6\uff0c\u8bfb\u53d6\u5230\u4e00\u4e2abatch\u7684\u6570\u636e\u3002</p> <pre><code>images, labels = next(loader_train.create_tuple_iterator())\nprint(\"Tensor of image\", images.shape)\nprint(\"Labels:\", labels)\n</code></pre> <pre><code>Tensor of image (16, 3, 224, 224)\nLabels: [0 1 1 1 1 0 0 0 0 0 1 0 1 0 1 1]\n</code></pre> <p>\u5bf9\u83b7\u53d6\u5230\u7684\u56fe\u50cf\u53ca\u6807\u7b7e\u6570\u636e\u8fdb\u884c\u53ef\u89c6\u5316\uff0c\u6807\u9898\u4e3a\u56fe\u50cf\u5bf9\u5e94\u7684label\u540d\u79f0\u3002</p> <pre><code>import matplotlib.pyplot as plt\nimport numpy as np\n\n# class_name\u5bf9\u5e94label\uff0c\u6309\u6587\u4ef6\u5939\u5b57\u7b26\u4e32\u4ece\u5c0f\u5230\u5927\u7684\u987a\u5e8f\u6807\u8bb0label\nclass_name = {0: \"dogs\", 1: \"wolves\"}\n\nplt.figure(figsize=(15, 7))\nfor i in range(len(labels)):\n    # \u83b7\u53d6\u56fe\u50cf\u53ca\u5176\u5bf9\u5e94\u7684label\n    data_image = images[i].asnumpy()\n    data_label = labels[i]\n    # \u5904\u7406\u56fe\u50cf\u4f9b\u5c55\u793a\u4f7f\u7528\n    data_image = np.transpose(data_image, (1, 2, 0))\n    mean = np.array([0.485, 0.456, 0.406])\n    std = np.array([0.229, 0.224, 0.225])\n    data_image = std * data_image + mean\n    data_image = np.clip(data_image, 0, 1)\n    # \u663e\u793a\u56fe\u50cf\n    plt.subplot(3, 6, i + 1)\n    plt.imshow(data_image)\n    plt.title(class_name[int(labels[i].asnumpy())])\n    plt.axis(\"off\")\n\nplt.show()\n</code></pre> <p></p>"},{"location":"zh/tutorials/finetune/#_8","title":"\u6a21\u578b\u5fae\u8c03","text":""},{"location":"zh/tutorials/finetune/#1","title":"1. \u6574\u4f53\u6a21\u578b\u5fae\u8c03","text":""},{"location":"zh/tutorials/finetune/#_9","title":"\u9884\u8bad\u7ec3\u6a21\u578b\u52a0\u8f7d","text":"<p>\u6211\u4eec\u4f7f\u7528<code>mindcv.models.densenet</code>\u4e2d\u5b9a\u4e49DenseNet121\u7f51\u7edc\uff0c\u5f53\u63a5\u53e3\u4e2d\u7684<code>pretrained</code>\u53c2\u6570\u8bbe\u7f6e\u4e3aTrue\u65f6\uff0c\u53ef\u4ee5\u81ea\u52a8\u4e0b\u8f7d\u7f51\u7edc\u6743\u91cd\u3002 \u7531\u4e8e\u8be5\u9884\u8bad\u7ec3\u6a21\u578b\u662f\u9488\u5bf9ImageNet\u6570\u636e\u96c6\u4e2d\u76841000\u4e2a\u7c7b\u522b\u8fdb\u884c\u5206\u7c7b\u7684\uff0c\u8fd9\u91cc\u6211\u4eec\u8bbe\u5b9a<code>num_classes=2</code>, DenseNet\u7684classifier(\u5373\u6700\u540e\u7684FC\u5c42)\u8f93\u51fa\u8c03\u6574\u4e3a\u4e24\u7ef4\uff0c\u6b64\u65f6\u53ea\u52a0\u8f7dbackbone\u7684\u9884\u8bad\u7ec3\u6743\u91cd\uff0c\u800cclassifier\u5219\u4f7f\u7528\u521d\u59cb\u503c\u3002</p> <pre><code>from mindcv.models import create_model\n\nnetwork = create_model(model_name='densenet121', num_classes=2, pretrained=True)\n</code></pre> <p>DenseNet\u7684\u5177\u4f53\u7ed3\u6784\u53ef\u53c2\u89c1DenseNet\u8bba\u6587\u3002</p>"},{"location":"zh/tutorials/finetune/#_10","title":"\u6a21\u578b\u8bad\u7ec3","text":"<p>\u4f7f\u7528\u5df2\u52a0\u8f7d\u5904\u7406\u597d\u7684\u5e26\u6807\u7b7e\u7684\u72fc\u548c\u72d7\u56fe\u50cf\uff0c\u5bf9DenseNet\u8fdb\u884c\u5fae\u8c03\u7f51\u7edc\u3002\u6ce8\u610f\uff0c\u5bf9\u6574\u4f53\u6a21\u578b\u505a\u5fae\u8c03\u65f6\uff0c\u5e94\u4f7f\u7528\u8f83\u5c0f\u7684learning rate\u3002</p> <pre><code>from mindcv.loss import create_loss\nfrom mindcv.optim import create_optimizer\nfrom mindcv.scheduler import create_scheduler\nfrom mindspore import Model, LossMonitor, TimeMonitor\n\n# \u5b9a\u4e49\u4f18\u5316\u5668\u548c\u635f\u5931\u51fd\u6570\nopt = create_optimizer(network.trainable_params(), opt='adam', lr=1e-4)\nloss = create_loss(name='CE')\n\n# \u5b9e\u4f8b\u5316\u6a21\u578b\nmodel = Model(network, loss_fn=loss, optimizer=opt, metrics={'accuracy'})\nmodel.train(10, loader_train, callbacks=[LossMonitor(5), TimeMonitor(5)], dataset_sink_mode=False)\n</code></pre> <pre><code>epoch: 1 step: 5, loss is 0.5195528864860535\nepoch: 1 step: 10, loss is 0.2654373049736023\nepoch: 1 step: 15, loss is 0.28758567571640015\nTrain epoch time: 17270.144 ms, per step time: 1151.343 ms\nepoch: 2 step: 5, loss is 0.1807008981704712\nepoch: 2 step: 10, loss is 0.1700802594423294\nepoch: 2 step: 15, loss is 0.09752683341503143\nTrain epoch time: 1372.549 ms, per step time: 91.503 ms\nepoch: 3 step: 5, loss is 0.13594701886177063\nepoch: 3 step: 10, loss is 0.03628234937787056\nepoch: 3 step: 15, loss is 0.039737217128276825\nTrain epoch time: 1453.237 ms, per step time: 96.882 ms\nepoch: 4 step: 5, loss is 0.014213413000106812\nepoch: 4 step: 10, loss is 0.030747078359127045\nepoch: 4 step: 15, loss is 0.0798817127943039\nTrain epoch time: 1331.237 ms, per step time: 88.749 ms\nepoch: 5 step: 5, loss is 0.009510636329650879\nepoch: 5 step: 10, loss is 0.02603740245103836\nepoch: 5 step: 15, loss is 0.051846928894519806\nTrain epoch time: 1312.737 ms, per step time: 87.516 ms\nepoch: 6 step: 5, loss is 0.1163717582821846\nepoch: 6 step: 10, loss is 0.02439398318529129\nepoch: 6 step: 15, loss is 0.02564268559217453\nTrain epoch time: 1434.704 ms, per step time: 95.647 ms\nepoch: 7 step: 5, loss is 0.013310655951499939\nepoch: 7 step: 10, loss is 0.02289542555809021\nepoch: 7 step: 15, loss is 0.1992517113685608\nTrain epoch time: 1275.935 ms, per step time: 85.062 ms\nepoch: 8 step: 5, loss is 0.015928998589515686\nepoch: 8 step: 10, loss is 0.011409260332584381\nepoch: 8 step: 15, loss is 0.008141174912452698\nTrain epoch time: 1323.102 ms, per step time: 88.207 ms\nepoch: 9 step: 5, loss is 0.10395607352256775\nepoch: 9 step: 10, loss is 0.23055407404899597\nepoch: 9 step: 15, loss is 0.04896317049860954\nTrain epoch time: 1261.067 ms, per step time: 84.071 ms\nepoch: 10 step: 5, loss is 0.03162381425499916\nepoch: 10 step: 10, loss is 0.13094250857830048\nepoch: 10 step: 15, loss is 0.020028553903102875\nTrain epoch time: 1217.958 ms, per step time: 81.197 ms\n</code></pre>"},{"location":"zh/tutorials/finetune/#_11","title":"\u6a21\u578b\u8bc4\u4f30","text":"<p>\u5728\u8bad\u7ec3\u5b8c\u6210\u540e\uff0c\u6211\u4eec\u5728\u9a8c\u8bc1\u96c6\u4e0a\u8bc4\u4f30\u6a21\u578b\u7684\u7cbe\u5ea6\u3002</p> <pre><code>res = model.eval(loader_val)\nprint(res)\n</code></pre> <pre><code>{'accuracy': 1.0}\n</code></pre>"},{"location":"zh/tutorials/finetune/#_12","title":"\u53ef\u89c6\u5316\u6a21\u578b\u63a8\u7406\u7ed3\u679c","text":"<p>\u5b9a\u4e49 <code>visualize_mode</code> \u51fd\u6570\uff0c\u53ef\u89c6\u5316\u6a21\u578b\u9884\u6d4b\u3002</p> <pre><code>import matplotlib.pyplot as plt\nimport mindspore as ms\n\ndef visualize_model(model, val_dl, num_classes=2):\n    # \u52a0\u8f7d\u9a8c\u8bc1\u96c6\u7684\u6570\u636e\u8fdb\u884c\u9a8c\u8bc1\n    images, labels= next(val_dl.create_tuple_iterator())\n    # \u9884\u6d4b\u56fe\u50cf\u7c7b\u522b\n    output = model.predict(images)\n    pred = np.argmax(output.asnumpy(), axis=1)\n    # \u663e\u793a\u56fe\u50cf\u53ca\u56fe\u50cf\u7684\u9884\u6d4b\u503c\n    images = images.asnumpy()\n    labels = labels.asnumpy()\n    class_name = {0: \"dogs\", 1: \"wolves\"}\n    plt.figure(figsize=(15, 7))\n    for i in range(len(labels)):\n        plt.subplot(3, 6, i + 1)\n        # \u82e5\u9884\u6d4b\u6b63\u786e\uff0c\u663e\u793a\u4e3a\u84dd\u8272\uff1b\u82e5\u9884\u6d4b\u9519\u8bef\uff0c\u663e\u793a\u4e3a\u7ea2\u8272\n        color = 'blue' if pred[i] == labels[i] else 'red'\n        plt.title('predict:{}'.format(class_name[pred[i]]), color=color)\n        picture_show = np.transpose(images[i], (1, 2, 0))\n        mean = np.array([0.485, 0.456, 0.406])\n        std = np.array([0.229, 0.224, 0.225])\n        picture_show = std * picture_show + mean\n        picture_show = np.clip(picture_show, 0, 1)\n        plt.imshow(picture_show)\n        plt.axis('off')\n\n    plt.show()\n</code></pre> <p>\u4f7f\u7528\u5fae\u8c03\u8fc7\u540e\u7684\u6a21\u578b\u5bf9\u9a8c\u8bc1\u96c6\u7684\u72fc\u548c\u72d7\u56fe\u50cf\u6570\u636e\u8fdb\u884c\u9884\u6d4b\u3002\u82e5\u9884\u6d4b\u5b57\u4f53\u4e3a\u84dd\u8272\u8868\u793a\u9884\u6d4b\u6b63\u786e\uff0c\u82e5\u9884\u6d4b\u5b57\u4f53\u4e3a\u7ea2\u8272\u8868\u793a\u9884\u6d4b\u9519\u8bef\u3002</p> <pre><code>visualize_model(model, loader_val)\n</code></pre> <p></p>"},{"location":"zh/tutorials/finetune/#2","title":"2. \u51bb\u7ed3\u7279\u5f81\u7f51\u7edc, \u5fae\u8c03\u5206\u7c7b\u5668","text":""},{"location":"zh/tutorials/finetune/#_13","title":"\u51bb\u7ed3\u7279\u5f81\u7f51\u7edc\u7684\u53c2\u6570","text":"<p>\u9996\u5148\uff0c\u6211\u4eec\u8981\u51bb\u7ed3\u9664\u6700\u540e\u4e00\u5c42\u5206\u7c7b\u5668\u4e4b\u5916\u7684\u6240\u6709\u7f51\u7edc\u5c42\uff0c\u5373\u5c06\u76f8\u5e94\u7684\u5c42\u53c2\u6570\u7684<code>requires_grad</code>\u5c5e\u6027\u8bbe\u7f6e\u4e3a<code>False</code>\uff0c\u4f7f\u5176\u4e0d\u5728\u53cd\u5411\u4f20\u64ad\u4e2d\u8ba1\u7b97\u68af\u5ea6\u53ca\u66f4\u65b0\u53c2\u6570\u3002</p> <p>\u56e0\u4e3a<code>mindcv.models</code> \u4e2d\u6240\u6709\u7684\u6a21\u578b\u5747\u4ee5<code>classifier</code> \u6765\u6807\u8bc6\u548c\u547d\u540d\u6a21\u578b\u7684\u5206\u7c7b\u5668(\u5373Dense\u5c42)\uff0c\u6240\u4ee5\u901a\u8fc7 <code>classifier.weight</code> \u548c <code>classifier.bias</code> \u5373\u53ef\u7b5b\u9009\u51fa\u5206\u7c7b\u5668\u5916\u7684\u5404\u5c42\u53c2\u6570\uff0c\u5c06\u5176<code>requires_grad</code>\u5c5e\u6027\u8bbe\u7f6e\u4e3a<code>False</code>.</p> <pre><code># freeze backbone\nfor param in network.get_parameters():\n    if param.name not in [\"classifier.weight\", \"classifier.bias\"]:\n        param.requires_grad = False\n</code></pre>"},{"location":"zh/tutorials/finetune/#_14","title":"\u5fae\u8c03\u5206\u7c7b\u5668","text":"<p>\u56e0\u4e3a\u7279\u5f81\u7f51\u7edc\u5df2\u7ecf\u56fa\u5b9a\uff0c\u6211\u4eec\u4e0d\u5fc5\u62c5\u5fc3\u8bad\u7ec3\u8fc7\u7a0b\u4f1adistort pratrained features\uff0c\u56e0\u6b64\uff0c\u76f8\u6bd4\u4e8e\u7b2c\u4e00\u79cd\u65b9\u6cd5\uff0c\u6211\u4eec\u53ef\u4ee5\u5c06learning rate\u8c03\u5927\u4e00\u4e9b\u3002</p> <p>\u4e0e\u6ca1\u6709\u9884\u8bad\u7ec3\u6a21\u578b\u76f8\u6bd4\uff0c\u5c06\u8282\u7ea6\u4e00\u5927\u534a\u65f6\u95f4\uff0c\u56e0\u4e3a\u6b64\u65f6\u53ef\u4ee5\u4e0d\u7528\u8ba1\u7b97\u90e8\u5206\u68af\u5ea6\u3002</p> <pre><code># \u52a0\u8f7d\u6570\u636e\u96c6\ndataset_train = create_dataset(root=data_dir, split='train', num_parallel_workers=num_workers)\nloader_train = create_loader(\n    dataset=dataset_train,\n    batch_size=16,\n    is_training=True,\n    num_classes=2,\n    transform=trans_train,\n    num_parallel_workers=num_workers,\n)\n\n# \u5b9a\u4e49\u4f18\u5316\u5668\u548c\u635f\u5931\u51fd\u6570\nopt = create_optimizer(network.trainable_params(), opt='adam', lr=1e-3)\nloss = create_loss(name='CE')\n\n# \u5b9e\u4f8b\u5316\u6a21\u578b\nmodel = Model(network, loss_fn=loss, optimizer=opt, metrics={'accuracy'})\n\nmodel.train(10, loader_train, callbacks=[LossMonitor(5), TimeMonitor(5)], dataset_sink_mode=False)\n</code></pre> <pre><code>epoch: 1 step: 5, loss is 0.051333948969841\nepoch: 1 step: 10, loss is 0.02043312042951584\nepoch: 1 step: 15, loss is 0.16161368787288666\nTrain epoch time: 10228.601 ms, per step time: 681.907 ms\nepoch: 2 step: 5, loss is 0.002121545374393463\nepoch: 2 step: 10, loss is 0.0009798109531402588\nepoch: 2 step: 15, loss is 0.015776708722114563\nTrain epoch time: 562.543 ms, per step time: 37.503 ms\nepoch: 3 step: 5, loss is 0.008056879043579102\nepoch: 3 step: 10, loss is 0.0009347647428512573\nepoch: 3 step: 15, loss is 0.028648357838392258\nTrain epoch time: 523.249 ms, per step time: 34.883 ms\nepoch: 4 step: 5, loss is 0.001014217734336853\nepoch: 4 step: 10, loss is 0.0003159046173095703\nepoch: 4 step: 15, loss is 0.0007699579000473022\nTrain epoch time: 508.886 ms, per step time: 33.926 ms\nepoch: 5 step: 5, loss is 0.0015687644481658936\nepoch: 5 step: 10, loss is 0.012090332806110382\nepoch: 5 step: 15, loss is 0.004598274827003479\nTrain epoch time: 507.243 ms, per step time: 33.816 ms\nepoch: 6 step: 5, loss is 0.010022152215242386\nepoch: 6 step: 10, loss is 0.0066385045647621155\nepoch: 6 step: 15, loss is 0.0036080628633499146\nTrain epoch time: 517.646 ms, per step time: 34.510 ms\nepoch: 7 step: 5, loss is 0.01344013586640358\nepoch: 7 step: 10, loss is 0.0008538365364074707\nepoch: 7 step: 15, loss is 0.14135593175888062\nTrain epoch time: 511.513 ms, per step time: 34.101 ms\nepoch: 8 step: 5, loss is 0.01626245677471161\nepoch: 8 step: 10, loss is 0.02871556021273136\nepoch: 8 step: 15, loss is 0.010110966861248016\nTrain epoch time: 545.678 ms, per step time: 36.379 ms\nepoch: 9 step: 5, loss is 0.008498094975948334\nepoch: 9 step: 10, loss is 0.2588501274585724\nepoch: 9 step: 15, loss is 0.0014278888702392578\nTrain epoch time: 499.243 ms, per step time: 33.283 ms\nepoch: 10 step: 5, loss is 0.021337147802114487\nepoch: 10 step: 10, loss is 0.00829876959323883\nepoch: 10 step: 15, loss is 0.008352771401405334\nTrain epoch time: 465.600 ms, per step time: 31.040 ms\n</code></pre>"},{"location":"zh/tutorials/finetune/#_15","title":"\u6a21\u578b\u8bc4\u4f30","text":"<p>\u8bad\u7ec3\u5b8c\u6210\u4e4b\u540e\uff0c\u6211\u4eec\u5728\u9a8c\u8bc1\u96c6\u4e0a\u8bc4\u4f30\u6a21\u578b\u7684\u51c6\u786e\u7387\u3002</p> <pre><code>dataset_val = create_dataset(root=data_dir, split='val', num_parallel_workers=num_workers)\nloader_val = create_loader(\n    dataset=dataset_val,\n    batch_size=5,\n    is_training=True,\n    num_classes=2,\n    transform=trans_val,\n    num_parallel_workers=num_workers,\n)\n\nres = model.eval(loader_val)\nprint(res)\n</code></pre> <pre><code>{'accuracy': 1.0}\n</code></pre>"},{"location":"zh/tutorials/finetune/#_16","title":"\u53ef\u89c6\u5316\u6a21\u578b\u9884\u6d4b","text":"<p>\u4f7f\u7528\u5fae\u8c03\u8fc7\u540e\u7684\u6a21\u578b\u4ef6\u5bf9\u9a8c\u8bc1\u96c6\u7684\u72fc\u548c\u72d7\u56fe\u50cf\u6570\u636e\u8fdb\u884c\u9884\u6d4b\u3002\u82e5\u9884\u6d4b\u5b57\u4f53\u4e3a\u84dd\u8272\u8868\u793a\u9884\u6d4b\u6b63\u786e\uff0c\u82e5\u9884\u6d4b\u5b57\u4f53\u4e3a\u7ea2\u8272\u8868\u793a\u9884\u6d4b\u9519\u8bef\u3002</p> <pre><code>visualize_model(model, loader_val)\n</code></pre> <p></p> <p>\u5fae\u8c03\u540e\u7684\u72fc\u72d7\u9884\u6d4b\u7ed3\u679c\u5747\u6b63\u786e</p>"},{"location":"zh/tutorials/inference/","title":"\u56fe\u50cf\u5206\u7c7b\u9884\u6d4b","text":"<p>\u672c\u6559\u7a0b\u4ecb\u7ecd\u5982\u4f55\u5728MindCV\u4e2d\u8c03\u7528\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u5728\u6d4b\u8bd5\u56fe\u50cf\u4e0a\u8fdb\u884c\u5206\u7c7b\u9884\u6d4b\u3002</p>"},{"location":"zh/tutorials/inference/#_2","title":"\u6a21\u578b\u52a0\u8f7d","text":""},{"location":"zh/tutorials/inference/#_3","title":"\u67e5\u770b\u5168\u90e8\u53ef\u7528\u7684\u7f51\u7edc\u6a21\u578b","text":"<p>\u901a\u8fc7\u8c03\u7528<code>mindcv.models</code>\u4e2d\u7684<code>registry.list_models</code>\u51fd\u6570\uff0c\u53ef\u4ee5\u6253\u5370\u51fa\u5168\u90e8\u7f51\u7edc\u6a21\u578b\u7684\u540d\u5b57\uff0c\u4e00\u4e2a\u7f51\u7edc\u5728\u4e0d\u540c\u53c2\u6570\u914d\u7f6e\u4e0b\u7684\u6a21\u578b\u4e5f\u4f1a\u5206\u522b\u6253\u5370\u51fa\u6765\uff0c\u4f8b\u5982resnet18 / resnet34 / resnet50 / resnet101 / resnet152\u3002</p> <pre><code>import sys\nsys.path.append(\"..\")\nfrom mindcv.models import registry\nregistry.list_models()\n</code></pre> <pre><code>['BiT_resnet50',\n 'repmlp_b224',\n 'repmlp_b256',\n 'repmlp_d256',\n 'repmlp_l256',\n 'repmlp_t224',\n 'repmlp_t256',\n 'convit_base',\n 'convit_base_plus',\n 'convit_small',\n ...\n 'visformer_small',\n 'visformer_small_v2',\n 'visformer_tiny',\n 'visformer_tiny_v2',\n 'vit_b_16_224',\n 'vit_b_16_384',\n 'vit_b_32_224',\n 'vit_b_32_384',\n 'vit_l_16_224',\n 'vit_l_16_384',\n 'vit_l_32_224',\n 'xception']\n</code></pre>"},{"location":"zh/tutorials/inference/#_4","title":"\u52a0\u8f7d\u9884\u8bad\u7ec3\u6a21\u578b","text":"<p>\u6211\u4eec\u4ee5resnet50\u6a21\u578b\u4e3a\u4f8b\uff0c\u4ecb\u7ecd\u4e24\u79cd\u4f7f\u7528<code>mindcv.models</code>\u4e2d<code>create_model</code>\u51fd\u6570\u8fdb\u884c\u6a21\u578bcheckpoint\u52a0\u8f7d\u7684\u65b9\u6cd5\u3002</p> <p>1). \u5f53\u63a5\u53e3\u4e2d\u7684<code>pretrained</code>\u53c2\u6570\u8bbe\u7f6e\u4e3aTrue\u65f6\uff0c\u53ef\u4ee5\u81ea\u52a8\u4e0b\u8f7d\u7f51\u7edc\u6743\u91cd\u3002</p> <pre><code>from mindcv.models import create_model\nmodel = create_model(model_name='resnet50', num_classes=1000, pretrained=True)\n# \u5207\u6362\u7f51\u7edc\u7684\u6267\u884c\u903b\u8f91\u4e3a\u63a8\u7406\u573a\u666f\nmodel.set_train(False)\n</code></pre> <pre><code>102453248B [00:16, 6092186.31B/s]\n\nResNet&lt;\n  (conv1): Conv2d&lt;input_channels=3, output_channels=64, kernel_size=(7, 7), stride=(2, 2), pad_mode=pad, padding=3, dilation=(1, 1), group=1, has_bias=False, weight_init=normal, bias_init=zeros, format=NCHW&gt;\n  (bn1): BatchNorm2d&lt;num_features=64, eps=1e-05, momentum=0.9, gamma=Parameter (name=bn1.gamma, shape=(64,), dtype=Float32, requires_grad=True), beta=Parameter (name=bn1.beta, shape=(64,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=bn1.moving_mean, shape=(64,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=bn1.moving_variance, shape=(64,), dtype=Float32, requires_grad=False)&gt;\n  (relu): ReLU&lt;&gt;\n  (max_pool): MaxPool2d&lt;kernel_size=3, stride=2, pad_mode=SAME&gt;\n  ...\n  (pool): GlobalAvgPooling&lt;&gt;\n  (classifier): Dense&lt;input_channels=2048, output_channels=1000, has_bias=True&gt;\n  &gt;\n</code></pre> <p>2). \u5f53\u63a5\u53e3\u4e2d\u7684<code>checkpoint_path</code>\u53c2\u6570\u8bbe\u7f6e\u4e3a\u6587\u4ef6\u8def\u5f84\u65f6\uff0c\u53ef\u4ee5\u4ece\u672c\u5730\u52a0\u8f7d\u540e\u7f00\u4e3a<code>.ckpt</code>\u7684\u6a21\u578b\u53c2\u6570\u6587\u4ef6\u3002</p> <pre><code>from mindcv.models import create_model\nmodel = create_model(model_name='resnet50', num_classes=1000, checkpoint_path='./resnet50_224.ckpt')\n# \u5207\u6362\u7f51\u7edc\u7684\u6267\u884c\u903b\u8f91\u4e3a\u63a8\u7406\u573a\u666f\nmodel.set_train(False)\n</code></pre>"},{"location":"zh/tutorials/inference/#_5","title":"\u6570\u636e\u51c6\u5907","text":""},{"location":"zh/tutorials/inference/#_6","title":"\u6784\u9020\u6570\u636e\u96c6","text":"<p>\u8fd9\u91cc\uff0c\u6211\u4eec\u4e0b\u8f7d\u4e00\u5f20Wikipedia\u7684\u56fe\u7247\u4f5c\u4e3a\u6d4b\u8bd5\u56fe\u7247\uff0c\u4f7f\u7528<code>mindcv.data</code>\u4e2d\u7684<code>create_dataset</code>\u51fd\u6570\uff0c\u4e3a\u5355\u5f20\u56fe\u7247\u6784\u9020\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\u3002</p> <pre><code>from mindcv.data import create_dataset\nnum_workers = 1\n# \u6570\u636e\u96c6\u76ee\u5f55\u8def\u5f84\ndata_dir = \"./data/\"\ndataset = create_dataset(root=data_dir, split='test', num_parallel_workers=num_workers)\n# \u56fe\u50cf\u53ef\u89c6\nfrom PIL import Image\nImage.open(\"./data/test/dog/dog.jpg\")\n</code></pre> <p></p>"},{"location":"zh/tutorials/inference/#_7","title":"\u6570\u636e\u9884\u5904\u7406","text":"<p>\u901a\u8fc7\u8c03\u7528<code>create_transforms</code>\u51fd\u6570\uff0c\u83b7\u5f97\u9884\u8bad\u7ec3\u6a21\u578b\u4f7f\u7528\u7684ImageNet\u6570\u636e\u96c6\u7684\u6570\u636e\u5904\u7406\u7b56\u7565(transform list)\u3002</p> <p>\u6211\u4eec\u5c06\u5f97\u5230\u7684transform list\u4f20\u5165<code>create_loader</code>\u51fd\u6570\uff0c\u6307\u5b9a<code>batch_size=1</code>\u548c\u5176\u4ed6\u53c2\u6570\uff0c\u5373\u53ef\u5b8c\u6210\u6d4b\u8bd5\u6570\u636e\u7684\u51c6\u5907\uff0c\u8fd4\u56de<code>Dataset</code> Object\uff0c\u4f5c\u4e3a\u6a21\u578b\u7684\u8f93\u5165\u3002</p> <pre><code>from mindcv.data import create_transforms, create_loader\ntransforms_list = create_transforms(dataset_name='imagenet', is_training=False)\ndata_loader = create_loader(\n    dataset=dataset,\n    batch_size=1,\n    is_training=False,\n    num_classes=1000,\n    transform=transforms_list,\n    num_parallel_workers=num_workers\n)\n</code></pre>"},{"location":"zh/tutorials/inference/#_8","title":"\u6a21\u578b\u63a8\u7406","text":"<p>\u5c06\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\u7684\u56fe\u7247\u4f20\u5165\u6a21\u578b\uff0c\u83b7\u5f97\u63a8\u7406\u7684\u7ed3\u679c\u3002\u8fd9\u91cc\u4f7f\u7528<code>mindspore.ops</code>\u7684<code>Squeeze</code>\u51fd\u6570\u53bb\u9664batch\u7ef4\u5ea6\u3002</p> <pre><code>import mindspore.ops as P\nimport numpy as np\nimages, _ = next(data_loader.create_tuple_iterator())\noutput = P.Squeeze()(model(images))\npred = np.argmax(output.asnumpy())\n</code></pre> <pre><code>with open(\"imagenet1000_clsidx_to_labels.txt\") as f:\n    idx2label = eval(f.read())\nprint('predict: {}'.format(idx2label[pred]))\n</code></pre> <pre><code>predict: Labrador retriever\n</code></pre>"},{"location":"zh/tutorials/quick_start/","title":"\u5feb\u901f\u5165\u95e8","text":"<p>MindCV\u662f\u4e00\u4e2a\u57fa\u4e8eMindSpore\u5f00\u53d1\u7684\uff0c\u81f4\u529b\u4e8e\u8ba1\u7b97\u673a\u89c6\u89c9\u76f8\u5173\u6280\u672f\u7814\u53d1\u7684\u5f00\u6e90\u5de5\u5177\u7bb1\u3002 \u5b83\u63d0\u4f9b\u5927\u91cf\u7684\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u7684\u7ecf\u5178\u6a21\u578b\u548cSoTA\u6a21\u578b\u4ee5\u53ca\u5b83\u4eec\u7684\u9884\u8bad\u7ec3\u6743\u91cd\u3002\u540c\u65f6\uff0c\u8fd8\u63d0\u4f9b\u4e86AutoAugment\u7b49SoTA\u7b97\u6cd5\u6765\u63d0\u9ad8\u6027\u80fd\u3002 \u901a\u8fc7\u89e3\u8026\u7684\u6a21\u5757\u8bbe\u8ba1\uff0c\u60a8\u53ef\u4ee5\u8f7b\u677e\u5730\u5c06MindCV\u5e94\u7528\u5230\u60a8\u81ea\u5df1\u7684CV\u4efb\u52a1\u4e2d\u3002\u672c\u6559\u7a0b\u4e2d\u6211\u4eec\u5c06\u63d0\u4f9b\u4e00\u4e2a\u5feb\u901f\u4e0a\u624bMindCV\u7684\u6307\u5357\u3002</p> <p>\u672c\u6559\u7a0b\u5c06\u4ee5DenseNet\u5206\u7c7b\u6a21\u578b\u4e3a\u4f8b\uff0c\u5b9e\u73b0\u5bf9CIFAR-10\u6570\u636e\u96c6\u7684\u8fc1\u79fb\u5b66\u4e60\uff0c\u5e76\u5728\u6b64\u6d41\u7a0b\u4e2d\u5bf9MindCV\u5404\u6a21\u5757\u7684\u7528\u6cd5\u4f5c\u8bb2\u89e3\u3002</p>"},{"location":"zh/tutorials/quick_start/#_2","title":"\u73af\u5883\u51c6\u5907","text":"<p>\u8be6\u89c1\u5b89\u88c5\u3002</p>"},{"location":"zh/tutorials/quick_start/#_3","title":"\u6570\u636e","text":""},{"location":"zh/tutorials/quick_start/#_4","title":"\u6570\u636e\u96c6","text":"<p>\u901a\u8fc7mindcv.data\u4e2d\u7684create_dataset\u6a21\u5757\uff0c\u6211\u4eec\u53ef\u4ee5\u5feb\u901f\u5730\u8bfb\u53d6\u6807\u51c6\u6570\u636e\u96c6\u6216\u81ea\u5b9a\u4e49\u7684\u6570\u636e\u96c6\u3002</p> <pre><code>import os\nfrom mindcv.data import create_dataset, create_transforms, create_loader\n\ncifar10_dir = './datasets/cifar/cifar-10-batches-bin'  # \u4f60\u7684\u6570\u636e\u5b58\u653e\u8def\u5f84\nnum_classes = 10  # \u7c7b\u522b\u6570\nnum_workers = 8  # \u6570\u636e\u8bfb\u53d6\u53ca\u52a0\u8f7d\u7684\u5de5\u4f5c\u7ebf\u7a0b\u6570\n\n# \u521b\u5efa\u6570\u636e\u96c6\ndataset_train = create_dataset(\n    name='cifar10', root=cifar10_dir, split='train', shuffle=True, num_parallel_workers=num_workers\n)\n</code></pre>"},{"location":"zh/tutorials/quick_start/#_5","title":"\u6570\u636e\u53d8\u6362","text":"<p>create_transforms\u51fd\u6570\u53ef\u76f4\u63a5\u751f\u6210\u9002\u914d\u6807\u51c6\u6570\u636e\u96c6\u7684\u6570\u636e\u5904\u7406\u589e\u5f3a\u7b56\u7565(transform list)\uff0c\u5305\u62ecCifar10, ImageNet\u4e0a\u5e38\u7528\u7684\u6570\u636e\u5904\u7406\u7b56\u7565\u3002</p> <pre><code># \u521b\u5efa\u6240\u9700\u7684\u6570\u636e\u589e\u5f3a\u64cd\u4f5c\u7684\u5217\u8868\ntrans = create_transforms(dataset_name='cifar10', image_resize=224)\n</code></pre>"},{"location":"zh/tutorials/quick_start/#_6","title":"\u6570\u636e\u52a0\u8f7d","text":"<p>\u901a\u8fc7mindcv.data.create_loader\u51fd\u6570\uff0c\u8fdb\u884c\u6570\u636e\u8f6c\u6362\u548cbatch\u5207\u5206\u52a0\u8f7d\uff0c\u6211\u4eec\u9700\u8981\u5c06create_transforms\u8fd4\u56de\u7684transform_list\u4f20\u5165\u3002</p> <pre><code># \u6267\u884c\u6570\u636e\u589e\u5f3a\u64cd\u4f5c\uff0c\u751f\u6210\u6240\u9700\u6570\u636e\u96c6\u3002\nloader_train = create_loader(dataset=dataset_train,\n                             batch_size=64,\n                             is_training=True,\n                             num_classes=num_classes,\n                             transform=trans,\n                             num_parallel_workers=num_workers)\n\nnum_batches = loader_train.get_dataset_size()\n</code></pre> <p>\u5728notebook\u4e2d\u907f\u514d\u91cd\u590d\u6267\u884ccreate_loader\u5355\u4e2aCell\uff0c\u6216\u5728\u6267\u884ccreate_dataset\u4e4b\u540e\u518d\u6b21\u6267\u884c\u3002</p>"},{"location":"zh/tutorials/quick_start/#_7","title":"\u6a21\u578b\u521b\u5efa\u548c\u52a0\u8f7d","text":"<p>\u4f7f\u7528create_model\u63a5\u53e3\u83b7\u5f97\u5b9e\u4f8b\u5316\u7684DenseNet\uff0c\u5e76\u52a0\u8f7d\u9884\u8bad\u7ec3\u6743\u91cddensenet_121_224.ckpt\uff08ImageNet\u6570\u636e\u96c6\u8bad\u7ec3\u5f97\u5230\uff09\u3002</p> <pre><code>from mindcv.models import create_model\n\n# \u5b9e\u4f8b\u5316 DenseNet-121 \u6a21\u578b\u5e76\u52a0\u8f7d\u9884\u8bad\u7ec3\u6743\u91cd\u3002\nnetwork = create_model(model_name='densenet121', num_classes=num_classes, pretrained=True)\n</code></pre> <p>\u7531\u4e8eCIFAR-10\u548cImageNet\u6570\u636e\u96c6\u6240\u9700\u7c7b\u522b\u6570\u91cf\u4e0d\u540c\uff0c\u5206\u7c7b\u5668\u53c2\u6570\u65e0\u6cd5\u5171\u4eab\uff0c\u51fa\u73b0\u5206\u7c7b\u5668\u53c2\u6570\u65e0\u6cd5\u52a0\u8f7d\u7684\u544a\u8b66\u4e0d\u5f71\u54cd\u5fae\u8c03\u3002</p>"},{"location":"zh/tutorials/quick_start/#_8","title":"\u635f\u5931\u51fd\u6570","text":"<p>\u901a\u8fc7create_loss\u63a5\u53e3\u83b7\u5f97\u635f\u5931\u51fd\u6570</p> <pre><code>from mindcv.loss import create_loss\n\nloss = create_loss(name='CE')\n</code></pre>"},{"location":"zh/tutorials/quick_start/#_9","title":"\u5b66\u4e60\u7387\u8c03\u5ea6\u5668","text":"<p>\u4f7f\u7528create_scheduler\u63a5\u53e3\u8bbe\u7f6e\u5b66\u4e60\u7387\u7b56\u7565\u3002</p> <pre><code>from mindcv.scheduler import create_scheduler\n\n# \u8bbe\u7f6e\u5b66\u4e60\u7387\u7b56\u7565\nlr_scheduler = create_scheduler(steps_per_epoch=num_batches,\n                                scheduler='constant',\n                                lr=0.0001)\n</code></pre>"},{"location":"zh/tutorials/quick_start/#_10","title":"\u4f18\u5316\u5668","text":"<p>\u4f7f\u7528create_optimizer\u63a5\u53e3\u521b\u5efa\u4f18\u5316\u5668\u3002</p> <pre><code>from mindcv.optim import create_optimizer\n\n# \u8bbe\u7f6e\u4f18\u5316\u5668\nopt = create_optimizer(network.trainable_params(), opt='adam', lr=lr_scheduler)\n</code></pre>"},{"location":"zh/tutorials/quick_start/#_11","title":"\u8bad\u7ec3","text":"<p>\u4f7f\u7528mindspore.Model\u63a5\u53e3\u6839\u636e\u7528\u6237\u4f20\u5165\u7684\u53c2\u6570\u5c01\u88c5\u53ef\u8bad\u7ec3\u7684\u5b9e\u4f8b\u3002</p> <pre><code>from mindspore import Model\n\n# \u5c01\u88c5\u53ef\u8bad\u7ec3\u6216\u63a8\u7406\u7684\u5b9e\u4f8b\nmodel = Model(network, loss_fn=loss, optimizer=opt, metrics={'accuracy'})\n</code></pre> <p>\u4f7f\u7528<code>mindspore.Model.train</code>\u63a5\u53e3\u8fdb\u884c\u6a21\u578b\u8bad\u7ec3\u3002</p> <pre><code>from mindspore import LossMonitor, TimeMonitor, CheckpointConfig, ModelCheckpoint\n\n# \u8bbe\u7f6e\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4fdd\u5b58\u7f51\u7edc\u53c2\u6570\u7684\u56de\u8c03\u51fd\u6570\nckpt_save_dir = './ckpt'\nckpt_config = CheckpointConfig(save_checkpoint_steps=num_batches)\nckpt_cb = ModelCheckpoint(prefix='densenet121-cifar10',\n                          directory=ckpt_save_dir,\n                          config=ckpt_config)\n\nmodel.train(5, loader_train, callbacks=[LossMonitor(num_batches//5), TimeMonitor(num_batches//5), ckpt_cb], dataset_sink_mode=False)\n</code></pre> <pre><code>[WARNING] DEVICE(1769,ffff87c70ac0,python):2022-12-21-16:04:30.001.890 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Default/network-WithLossCell/_loss_fn-CrossEntropySmooth/GatherD-op5273] don't support int64, reduce precision from int64 to int32.\n\n\nepoch: 1 step: 156, loss is 2.0816354751586914\nepoch: 1 step: 312, loss is 1.4474115371704102\nepoch: 1 step: 468, loss is 0.8935483694076538\nepoch: 1 step: 624, loss is 0.5588696002960205\nepoch: 1 step: 780, loss is 0.3161369860172272\n\n\n[WARNING] DEVICE(1769,ffff87c70ac0,python):2022-12-21-16:09:20.261.851 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Default/network-WithLossCell/_loss_fn-CrossEntropySmooth/GatherD-op16720] don't support int64, reduce precision from int64 to int32.\n\n\nTrain epoch time: 416429.509 ms, per step time: 532.519 ms\nepoch: 2 step: 154, loss is 0.19752007722854614\nepoch: 2 step: 310, loss is 0.14635677635669708\nepoch: 2 step: 466, loss is 0.3511860966682434\nepoch: 2 step: 622, loss is 0.12542471289634705\nepoch: 2 step: 778, loss is 0.22351759672164917\nTrain epoch time: 156746.872 ms, per step time: 200.444 ms\nepoch: 3 step: 152, loss is 0.08965137600898743\nepoch: 3 step: 308, loss is 0.22765043377876282\nepoch: 3 step: 464, loss is 0.19035443663597107\nepoch: 3 step: 620, loss is 0.06591956317424774\nepoch: 3 step: 776, loss is 0.0934530645608902\nTrain epoch time: 156574.210 ms, per step time: 200.223 ms\nepoch: 4 step: 150, loss is 0.03782692924141884\nepoch: 4 step: 306, loss is 0.023876197636127472\nepoch: 4 step: 462, loss is 0.038690414279699326\nepoch: 4 step: 618, loss is 0.15388774871826172\nepoch: 4 step: 774, loss is 0.1581358164548874\nTrain epoch time: 158398.108 ms, per step time: 202.555 ms\nepoch: 5 step: 148, loss is 0.06556802988052368\nepoch: 5 step: 304, loss is 0.006707251071929932\nepoch: 5 step: 460, loss is 0.02353120595216751\nepoch: 5 step: 616, loss is 0.014183484017848969\nepoch: 5 step: 772, loss is 0.09367241710424423\nTrain epoch time: 154978.618 ms, per step time: 198.182 ms\n</code></pre>"},{"location":"zh/tutorials/quick_start/#_12","title":"\u8bc4\u4f30","text":"<p>\u73b0\u5728\u8ba9\u6211\u4eec\u5728CIFAR-10\u4e0a\u5bf9\u521a\u521a\u8bad\u7ec3\u7684\u6a21\u578b\u8fdb\u884c\u8bc4\u4f30\u3002</p> <pre><code># \u52a0\u8f7d\u9a8c\u8bc1\u6570\u636e\u96c6\ndataset_val = create_dataset(name='cifar10', root=cifar10_dir, split='test', shuffle=True, num_parallel_workers=num_workers, download=download)\n\n# \u6267\u884c\u6570\u636e\u589e\u5f3a\u64cd\u4f5c\uff0c\u751f\u6210\u6240\u9700\u6570\u636e\u96c6\u3002\nloader_val = create_loader(dataset=dataset_val,\n                           batch_size=64,\n                           is_training=False,\n                           num_classes=num_classes,\n                           transform=trans,\n                           num_parallel_workers=num_workers)\n</code></pre> <p>\u52a0\u8f7d\u5fae\u8c03\u540e\u7684\u53c2\u6570\u6587\u4ef6\uff08densenet121-cifar10-5_782.ckpt\uff09\u5230\u6a21\u578b\u3002</p> <p>\u6839\u636e\u7528\u6237\u4f20\u5165\u7684\u53c2\u6570\u5c01\u88c5\u53ef\u63a8\u7406\u7684\u5b9e\u4f8b\uff0c\u52a0\u8f7d\u9a8c\u8bc1\u6570\u636e\u96c6\uff0c\u9a8c\u8bc1\u5fae\u8c03\u7684 DenseNet121\u6a21\u578b\u7cbe\u5ea6\u3002</p> <pre><code># \u9a8c\u8bc1\u5fae\u8c03\u540e\u7684DenseNet121\u7684\u7cbe\u5ea6\nacc = model.eval(loader_val, dataset_sink_mode=False)\nprint(acc)\n</code></pre> <pre><code>[WARNING] DEVICE(1769,ffff87c70ac0,python):2022-12-21-16:24:11.927.472 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Default/network-WithLossCell/_loss_fn-CrossEntropySmooth/GatherD-op24314] don't support int64, reduce precision from int64 to int32.\n\n\n{'accuracy': 0.951}\n\n\n[WARNING] DEVICE(1769,ffff87c70ac0,python):2022-12-21-16:25:01.871.273 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[Default/network-WithLossCell/_loss_fn-CrossEntropySmooth/GatherD-op27139] don't support int64, reduce precision from int64 to int32.\n</code></pre>"},{"location":"zh/tutorials/quick_start/#yaml","title":"\u4f7f\u7528YAML\u6587\u4ef6\u8fdb\u884c\u6a21\u578b\u8bad\u7ec3\u548c\u9a8c\u8bc1","text":"<p>\u6211\u4eec\u8fd8\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528\u8bbe\u7f6e\u597d\u6a21\u578b\u53c2\u6570\u7684yaml\u6587\u4ef6\uff0c\u901a\u8fc7<code>train.py</code>\u548c<code>validate.py</code>\u811a\u672c\u6765\u5feb\u901f\u6765\u5bf9\u6a21\u578b\u8fdb\u884c\u8bad\u7ec3\u548c\u9a8c\u8bc1\u3002\u4ee5\u4e0b\u662f\u5728ImageNet\u4e0a\u8bad\u7ec3SqueezenetV1\u7684\u793a\u4f8b \uff08\u9700\u8981\u5c06ImageNet\u63d0\u524d\u4e0b\u8f7d\u5230\u76ee\u5f55\u4e0b\uff09</p> <p>\u8be6\u7ec6\u6559\u7a0b\u8bf7\u53c2\u8003 \u4f7f\u7528yaml\u6587\u4ef6\u7684\u6559\u7a0b</p> <pre><code>#  \u5355\u5361\u8bad\u7ec3\npython train.py -c configs/squeezenet/squeezenet_1.0_gpu.yaml --data_dir /path/to/dataset --distribute False\n</code></pre> <pre><code>python validate.py -c configs/squeezenet/squeezenet_1.0_gpu.yaml --data_dir /path/to/dataset --ckpt_path /path/to/ckpt\n</code></pre>"}]}